{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mass 2: Microcalorimeter Analysis Software Suite Overview of Mass 2 Mass is a Python software suite designed to analyze pulse records from high-resolution, cryogenic microcalorimeters. We use Mass with pulse records from x-ray and gamma-ray spectrometers, performing a sequence of analysis, or \"calibration\" steps to extract a high-precision estimate of the energy from each record. With raw pulse records and Mass, you can: Analyze data from one or multiple detectors at one time. Analyze data from one or more raw pulse data files per detector. Analyze a fixed dataset taken in the past, or perform \"online\" analysis of a dataset still being acquired. Analyze data from time-division multiplexed (TDM) and microwave-multiplexed (\u00b5MUX) systems. Compute and apply \"optimal filters\" of various types. With or without raw pulse records, further analysis tasks that Mass helps with include: Choose and apply data cuts. Fix complex line shapes in an energy spectrum. Estimate and apply accurate functions for absolute-energy calibration. Win friends and influence people. Major concepts Mass2 is built around a few core technologies: Pola.rs , a high-performance modern dataframe library. Organizes the data structures and provides data I/O. Numpy . For numerical arrays and scientific computation. Scipy . Additional algorithms for scientific computation. LMFIT . Convenient non-linear curve fitting. A particularly important change is that Mass 2 leverages Pola.rs for data organization. Therefore, for a critical portion of the code, the larger open-source software community provides the documentation, bug fixes, and testing.","title":"Home"},{"location":"#mass-2-microcalorimeter-analysis-software-suite","text":"","title":"Mass 2: Microcalorimeter Analysis Software Suite"},{"location":"#overview-of-mass-2","text":"Mass is a Python software suite designed to analyze pulse records from high-resolution, cryogenic microcalorimeters. We use Mass with pulse records from x-ray and gamma-ray spectrometers, performing a sequence of analysis, or \"calibration\" steps to extract a high-precision estimate of the energy from each record. With raw pulse records and Mass, you can: Analyze data from one or multiple detectors at one time. Analyze data from one or more raw pulse data files per detector. Analyze a fixed dataset taken in the past, or perform \"online\" analysis of a dataset still being acquired. Analyze data from time-division multiplexed (TDM) and microwave-multiplexed (\u00b5MUX) systems. Compute and apply \"optimal filters\" of various types. With or without raw pulse records, further analysis tasks that Mass helps with include: Choose and apply data cuts. Fix complex line shapes in an energy spectrum. Estimate and apply accurate functions for absolute-energy calibration. Win friends and influence people.","title":"Overview of Mass 2"},{"location":"#major-concepts","text":"Mass2 is built around a few core technologies: Pola.rs , a high-performance modern dataframe library. Organizes the data structures and provides data I/O. Numpy . For numerical arrays and scientific computation. Scipy . Additional algorithms for scientific computation. LMFIT . Convenient non-linear curve fitting. A particularly important change is that Mass 2 leverages Pola.rs for data organization. Therefore, for a critical portion of the code, the larger open-source software community provides the documentation, bug fixes, and testing.","title":"Major concepts"},{"location":"LJHfiles/","text":"LJH Memorial File Format LJH Version 2.2.0 is the current LJH file format (since 2015). An LJH file contains segmented, time-series data that represent separate triggered pulses observed in a single cryogenic microcalorimeter. The LJH file format consists of a human-readable ASCII header followed by an arbitrary number of binary data records . Information in the header specifies the exact length in bytes of each data record. Header Information The human-readable ASCII header is the start of the LJH file. That means you can say less myfile_chan5.ljh at a unix terminal and get meaningful information about the file before the gibberish starts. Handy, right? The header is somewhat fragile (it might have been better written in YAML or TOML or even JSON, but we decided just to live with it). It consists of key-value pairs, with a format of Key: value , one pair per line. Header Notes: Lines begining with # are usually ignored. #End of Header marks the end of the header and the transition to the binary data. #End of Description has special meaning if System description of this File: has been read. Newlines are the newlines of the digitizing computer. The interpreting program must accept LF, CR, or CRLF. Capitalization must be matched. One space follows a colon. Any additional spaces are treated as part of the value. Programs that read LJH files ignore header keys that are unexpected or unneeded. #LJH Memorial File Format This line indicates that the file is based on format described here. Save File Format Version: 2.2.0 Software Version: DASTARD version 0.2.15 Software Git Hash: 85ab821 Data source: Abaco These lines uniquely identify the exact format, so the interpreting program can adapt. While the first line should be sufficient for this purpose, the second and third lines take in the possibility that a particular program may have a bug. The interpreting program may be aware of this bug and compensate. The Data source is meant for later human reference. Number of rows: 74 Number of columns: 1 Row number (from 0-73 inclusive): 12 Column number (from 0-0 inclusive): 0 Number of channels: 74 Channel name: chan12 Channel: 12 ChannelIndex (in dastard): 12 Dastard inserts this information to help downstream analysis tools understand the array being used when this file was acquired. Digitized Word Size in Bytes: 2 Each sample is stored in this many bytes. Location: LLNL Cryostat: C3PO Thermometer: GRT1 Temperature (Ohm or K): 0.1 Bridge range: 20.0E3 Magnetic field (A or Gauss): 0.75 Detector: SnTES#8 Sample: Orange peel Excitation/Source: none Operator: Leisure Larry Like the several lines above, most lines are comments for later human use and are not interpreted by general-purpose LJH readers. System description of this File: blah blah blah User description of this File: blah blah blah #End of Description This is a multiline comment. Once the Description of this File: line is read, all following lines are concantenated until #End of Description is read. Again, this is ignored by programs that read LJH files. Number of Digitizers: 1 Number of Active Channels: 2 The number of digitizers and channels present in the file are given so that space may be allocated for them by the interpreting program, if necessary. Timestamp offset (s): 3016738980.049000 The meaning of this and the means of interpreting it are dependent upon the particular programs creating and reading this file. It was a necessary offset in earlier versions of LJH, where we did not reserve enough bytes per record to record a full timestamp. In LJH 2.2, it serves as a simple zero-time (all records should be no earlier than this \"offset\"). Server Start Time: 18 Nov 2022, 15:47:34 MST First Record Time: 18 Nov 2022, 16:54:15 MST These times show when the server (Dastard, in this case) started running, and when the first record was written to this file. Timebase: 5.000000E-8 Number of samples per point: 1 Timebase gives the sampling period (in seconds). Number of samples per point is generally 1, but can be more in special cases where samples are averaged and downsampled before recording. Presamples: 256 Total Samples: 1024 Total samples is the actual record length in samples. The trigger point will be located at sample number Presamples. Binary Information If you read an LJH file until the characters #End of Header plus the following CR and/or LF, then the remainder of the file is the binary section. It consists of a sequence of data records. Each record starts with a 16-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 16+L*M . All values in the data record are little endian. The first 8-byte word is the subframe counter. It counts the number of subframe times read out since the server started. If the server has to resynchronize on the raw data, then the subframe counter will be incremented by an estimate to account for the time missed. For TDM data, the subframe rate is equal to the row rate, also known as the line rate . For \u00b5MUX data, subframes run at a multiple of the frame rate given by the Subframe divisions: value in the LJH header (typically 64). The second 8-byte word is the POSIX microsecond time, i.e., the time in microseconds since 1 January 1970 00:00 UT. (Warning: this will overflow in only 292,226 years if you interpret it as a signed number.) The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for TDM feedback or for \u00b5MUX data.) Earlier versions of the LJH standard Binary Information (for LJH version 2.1.0) Version 2.1.0 follows. This version was made obsolete in 2015 by version 2.2.0. Each record starts with a 6-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 6+L*M . All values in the data record are little endian. The first byte is a \"4 microsecond tick\". That is, it counts microseconds past the millisecond counter and records the count divided by 4. Beware that previous versions of LJH used the first byte to signify something about the type of data. Igor seems to ignore this byte, though, so I think we're okay to stuff timing information into it. The second byte used to signify a channel number N, which corresponds to the Nth channel described in the header. Channel number 255 is reserved for temperature readout with the DMM. Since 2010, this has always been meaningless. The next 4 bytes are an unsigned 32-bit number that is the value of a millisecond counter on the digitizing computer. The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for the TDM feedback.) Changelog Version 2.2.0 (6 Aug 2015) Changed the binary definition to include 8 bytes each record for pulse timing (microsecond precision) and frame number. Version 2.1.0 (23 Sep 2011) Used the first byte of each record to get 4 microsec timing resolution instead of 1 ms. Version 2.0.0 (27 Mar 2011) Defined inversion and offset more clearly Version 2.0.0 (5 Jan 2001) Changed definition of discrimination level Version 2.0.0 (24 May 2000) since most PCs have least significant byte first, the binary information has been changed to default Version 1.1.0 (8 May 2000) added a few more user and channel parameters as well as provisions for temperature monitoring Initial 1.0.0 (5 Aug 1999) definition by Larry J. Hiller","title":"LJH file format"},{"location":"LJHfiles/#ljh-memorial-file-format","text":"LJH Version 2.2.0 is the current LJH file format (since 2015). An LJH file contains segmented, time-series data that represent separate triggered pulses observed in a single cryogenic microcalorimeter. The LJH file format consists of a human-readable ASCII header followed by an arbitrary number of binary data records . Information in the header specifies the exact length in bytes of each data record.","title":"LJH Memorial File Format"},{"location":"LJHfiles/#header-information","text":"The human-readable ASCII header is the start of the LJH file. That means you can say less myfile_chan5.ljh at a unix terminal and get meaningful information about the file before the gibberish starts. Handy, right? The header is somewhat fragile (it might have been better written in YAML or TOML or even JSON, but we decided just to live with it). It consists of key-value pairs, with a format of Key: value , one pair per line.","title":"Header Information"},{"location":"LJHfiles/#header-notes","text":"Lines begining with # are usually ignored. #End of Header marks the end of the header and the transition to the binary data. #End of Description has special meaning if System description of this File: has been read. Newlines are the newlines of the digitizing computer. The interpreting program must accept LF, CR, or CRLF. Capitalization must be matched. One space follows a colon. Any additional spaces are treated as part of the value. Programs that read LJH files ignore header keys that are unexpected or unneeded. #LJH Memorial File Format This line indicates that the file is based on format described here. Save File Format Version: 2.2.0 Software Version: DASTARD version 0.2.15 Software Git Hash: 85ab821 Data source: Abaco These lines uniquely identify the exact format, so the interpreting program can adapt. While the first line should be sufficient for this purpose, the second and third lines take in the possibility that a particular program may have a bug. The interpreting program may be aware of this bug and compensate. The Data source is meant for later human reference. Number of rows: 74 Number of columns: 1 Row number (from 0-73 inclusive): 12 Column number (from 0-0 inclusive): 0 Number of channels: 74 Channel name: chan12 Channel: 12 ChannelIndex (in dastard): 12 Dastard inserts this information to help downstream analysis tools understand the array being used when this file was acquired. Digitized Word Size in Bytes: 2 Each sample is stored in this many bytes. Location: LLNL Cryostat: C3PO Thermometer: GRT1 Temperature (Ohm or K): 0.1 Bridge range: 20.0E3 Magnetic field (A or Gauss): 0.75 Detector: SnTES#8 Sample: Orange peel Excitation/Source: none Operator: Leisure Larry Like the several lines above, most lines are comments for later human use and are not interpreted by general-purpose LJH readers. System description of this File: blah blah blah User description of this File: blah blah blah #End of Description This is a multiline comment. Once the Description of this File: line is read, all following lines are concantenated until #End of Description is read. Again, this is ignored by programs that read LJH files. Number of Digitizers: 1 Number of Active Channels: 2 The number of digitizers and channels present in the file are given so that space may be allocated for them by the interpreting program, if necessary. Timestamp offset (s): 3016738980.049000 The meaning of this and the means of interpreting it are dependent upon the particular programs creating and reading this file. It was a necessary offset in earlier versions of LJH, where we did not reserve enough bytes per record to record a full timestamp. In LJH 2.2, it serves as a simple zero-time (all records should be no earlier than this \"offset\"). Server Start Time: 18 Nov 2022, 15:47:34 MST First Record Time: 18 Nov 2022, 16:54:15 MST These times show when the server (Dastard, in this case) started running, and when the first record was written to this file. Timebase: 5.000000E-8 Number of samples per point: 1 Timebase gives the sampling period (in seconds). Number of samples per point is generally 1, but can be more in special cases where samples are averaged and downsampled before recording. Presamples: 256 Total Samples: 1024 Total samples is the actual record length in samples. The trigger point will be located at sample number Presamples.","title":"Header Notes:"},{"location":"LJHfiles/#binary-information","text":"If you read an LJH file until the characters #End of Header plus the following CR and/or LF, then the remainder of the file is the binary section. It consists of a sequence of data records. Each record starts with a 16-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 16+L*M . All values in the data record are little endian. The first 8-byte word is the subframe counter. It counts the number of subframe times read out since the server started. If the server has to resynchronize on the raw data, then the subframe counter will be incremented by an estimate to account for the time missed. For TDM data, the subframe rate is equal to the row rate, also known as the line rate . For \u00b5MUX data, subframes run at a multiple of the frame rate given by the Subframe divisions: value in the LJH header (typically 64). The second 8-byte word is the POSIX microsecond time, i.e., the time in microseconds since 1 January 1970 00:00 UT. (Warning: this will overflow in only 292,226 years if you interpret it as a signed number.) The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for TDM feedback or for \u00b5MUX data.)","title":"Binary Information"},{"location":"LJHfiles/#earlier-versions-of-the-ljh-standard","text":"","title":"Earlier versions of the LJH standard"},{"location":"LJHfiles/#binary-information-for-ljh-version-210","text":"Version 2.1.0 follows. This version was made obsolete in 2015 by version 2.2.0. Each record starts with a 6-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 6+L*M . All values in the data record are little endian. The first byte is a \"4 microsecond tick\". That is, it counts microseconds past the millisecond counter and records the count divided by 4. Beware that previous versions of LJH used the first byte to signify something about the type of data. Igor seems to ignore this byte, though, so I think we're okay to stuff timing information into it. The second byte used to signify a channel number N, which corresponds to the Nth channel described in the header. Channel number 255 is reserved for temperature readout with the DMM. Since 2010, this has always been meaningless. The next 4 bytes are an unsigned 32-bit number that is the value of a millisecond counter on the digitizing computer. The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for the TDM feedback.)","title":"Binary Information (for LJH version 2.1.0)"},{"location":"LJHfiles/#changelog","text":"Version 2.2.0 (6 Aug 2015) Changed the binary definition to include 8 bytes each record for pulse timing (microsecond precision) and frame number. Version 2.1.0 (23 Sep 2011) Used the first byte of each record to get 4 microsec timing resolution instead of 1 ms. Version 2.0.0 (27 Mar 2011) Defined inversion and offset more clearly Version 2.0.0 (5 Jan 2001) Changed definition of discrimination level Version 2.0.0 (24 May 2000) since most PCs have least significant byte first, the binary information has been changed to default Version 1.1.0 (8 May 2000) added a few more user and channel parameters as well as provisions for temperature monitoring Initial 1.0.0 (5 Aug 1999) definition by Larry J. Hiller","title":"Changelog"},{"location":"about/","text":"Authors MASS is the work of physicists from Quantum Sensors Division of the NIST Boulder Labs and the University of Colorado Physics Department , with substantial contributions from: Joe Fowler , project director Galen O'Neil , co-director Dan Becker Young-Il Joe Jamie Titus Joshua Ho Many collaborators, who have made many bug reports, bug fixes, and feature requests. Major Versions MASS version 2 was begin in August 2025. It is still unstable, in alpha or pre-alpha status. Find it at https://github.com/usnistgov/mass2 MASS version 1 was begun in November 2010. Bug-fix development continues. Find it at https://github.com/usnistgov/mass","title":"About"},{"location":"about/#authors","text":"MASS is the work of physicists from Quantum Sensors Division of the NIST Boulder Labs and the University of Colorado Physics Department , with substantial contributions from: Joe Fowler , project director Galen O'Neil , co-director Dan Becker Young-Il Joe Jamie Titus Joshua Ho Many collaborators, who have made many bug reports, bug fixes, and feature requests.","title":"Authors"},{"location":"about/#major-versions","text":"MASS version 2 was begin in August 2025. It is still unstable, in alpha or pre-alpha status. Find it at https://github.com/usnistgov/mass2 MASS version 1 was begun in November 2010. Bug-fix development continues. Find it at https://github.com/usnistgov/mass","title":"Major Versions"},{"location":"docstrings/","text":"Automatic documentation generated from docstrings Channel dataclass Source code in mass2/core/channel.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 @dataclass ( frozen = True ) # noqa: PLR0904 class Channel : df : pl . DataFrame = field ( repr = False ) header : ChannelHeader = field ( repr = True ) npulses : int noise : NoiseChannel | None = field ( default = None , repr = False ) good_expr : bool | pl . Expr = True df_history : list [ pl . DataFrame ] = field ( default_factory = list , repr = False ) steps : CalSteps = field ( default_factory = CalSteps . new_empty ) steps_elapsed_s : list [ float ] = field ( default_factory = list ) transform_raw : Callable | None = None def mo_stepplots ( self ): desc_ind = { step . description : i for i , step in enumerate ( self . steps )} first_non_summarize_step = self . steps [ 0 ] for step in self . steps : if isinstance ( step , SummarizeStep ): continue first_non_summarize_step = step break mo_ui = mo . ui . dropdown ( desc_ind , value = first_non_summarize_step . description , label = f \"choose step for ch { self . header . ch_num } \" , ) def show (): return self . _mo_stepplots_explicit ( mo_ui ) def step_ind (): return mo_ui . value mo_ui . show = show mo_ui . step_ind = step_ind return mo_ui def _mo_stepplots_explicit ( self , mo_ui ): step_ind = mo_ui . step_ind () self . step_plot ( step_ind ) fig = plt . gcf () return mo . vstack ([ mo_ui , misc . show ( fig )]) def get_step ( self , index ): if index < 0 : # normalize the index to a positive index index = len ( self . steps ) + index step = self . steps [ index ] return step , index def step_plot ( self , step_ind , ** kwargs ): step , step_ind = self . get_step ( step_ind ) if step_ind + 1 == len ( self . df_history ): df_after = self . df else : df_after = self . df_history [ step_ind + 1 ] return step . dbg_plot ( df_after , ** kwargs ) def plot_hist ( self , col , bin_edges , axis = None ): axis = misc . plot_hist_of_series ( self . good_series ( col ), bin_edges , axis ) axis . set_title ( f \"ch { self . header . ch_num } plot_hist\" ) return axis def plot_hists ( self , col , bin_edges , group_by_col , axis = None , use_good_expr = True , use_expr = True , skip_none = True , ): \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () def plot_scatter ( self , x_col , y_col , color_col = None , use_expr = True , use_good_expr = True , skip_none = True , ax = None , ): if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # set current axis so I can use plt api if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr df_small = self . df . lazy () . filter ( filter_expr ) . select ( x_col , y_col , color_col ) . collect () for ( name ,), data in df_small . group_by ( color_col , maintain_order = True ): if name is None and skip_none and color_col is not None : continue plt . plot ( data . select ( x_col ) . to_series (), data . select ( y_col ) . to_series (), \".\" , label = name , ) plt . xlabel ( str ( x_col )) plt . ylabel ( str ( y_col )) title_str = f \"\"\" { self . header . description } use_expr= { str ( use_expr ) } good_expr= { str ( self . good_expr ) } \"\"\" plt . title ( title_str ) if color_col is not None : plt . legend ( title = color_col ) plt . tight_layout () def good_series ( self , col , use_expr = True ): return mass2 . good_series ( self . df , col , self . good_expr , use_expr ) def rough_cal_combinatoric ( self , line_names , uncalibrated_col , calibrated_col , ph_smoothing_fwhm , n_extra = 3 , use_expr = True , ) -> \"Channel\" : step = mass2 . RoughCalibrationStep . learn_combinatoric ( self , line_names , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal_combinatoric_height_info ( self , line_names , line_heights_allowed , uncalibrated_col , calibrated_col , ph_smoothing_fwhm , n_extra = 3 , use_expr = True , ) -> \"Channel\" : step = mass2 . RoughCalibrationStep . learn_combinatoric_height_info ( self , line_names , line_heights_allowed , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal ( # noqa: PLR0917 self , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : bool | pl . Expr = True , max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"Channel\" : step = mass2 . RoughCalibrationStep . learn_3peak ( self , line_names , uncalibrated_col , calibrated_col , use_expr , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , fwhm_pulse_height_units , n_extra_peaks , acceptable_rms_residual_e , ) return self . with_step ( step ) def with_step ( self , step ) -> \"Channel\" : t_start = time . time () df2 = step . calc_from_df ( self . df ) elapsed_s = time . time () - t_start ch2 = Channel ( df = df2 , header = self . header , npulses = self . npulses , noise = self . noise , good_expr = step . good_expr , df_history = self . df_history + [ self . df ], steps = self . steps . with_step ( step ), steps_elapsed_s = self . steps_elapsed_s + [ elapsed_s ], ) return ch2 def with_steps ( self , steps ) -> \"Channel\" : ch2 = self for step in steps : ch2 = ch2 . with_step ( step ) return ch2 def with_good_expr ( self , good_expr , replace = False ) -> \"Channel\" : # the default value of self.good_expr is True # and_(True) will just add visual noise when looking at good_expr and not affect behavior if not replace and good_expr is not True : good_expr = good_expr . and_ ( self . good_expr ) return Channel ( df = self . df , header = self . header , npulses = self . npulses , noise = self . noise , good_expr = good_expr , df_history = self . df_history , steps = self . steps , steps_elapsed_s = self . steps_elapsed_s , ) def with_good_expr_pretrig_rms_and_postpeak_deriv ( self , n_sigma_pretrig_rms = 20 , n_sigma_postpeak_deriv = 20 , replace = False ) -> \"Channel\" : max_postpeak_deriv = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"postpeak_deriv\" ] . to_numpy (), nsigma = n_sigma_postpeak_deriv ) max_pretrig_rms = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"pretrig_rms\" ] . to_numpy (), nsigma = n_sigma_pretrig_rms ) good_expr = ( pl . col ( \"postpeak_deriv\" ) < max_postpeak_deriv ) . and_ ( pl . col ( \"pretrig_rms\" ) < max_pretrig_rms ) return self . with_good_expr ( good_expr , replace ) def with_range_around_median ( self , col , range_up , range_down ): med = np . median ( self . df [ col ] . to_numpy ()) return self . with_good_expr ( pl . col ( col ) . is_between ( med - range_down , med + range_up )) def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs , replace = False , use_prev_good_expr = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs , replace = False , use_prev_good_expr = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) @functools . cache def typical_peak_ind ( self , col = \"pulse\" ): raw = self . df . limit ( 100 )[ col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) return int ( np . median ( raw . argmax ( axis = 1 ))) def summarize_pulses ( self , col = \"pulse\" , pretrigger_ignore_samples = 0 , peak_index = None ) -> \"Channel\" : if peak_index is None : peak_index = self . typical_peak_ind ( col ) step = SummarizeStep ( inputs = [ col ], output = [ \"many\" ], good_expr = self . good_expr , use_expr = True , frametime_s = self . header . frametime_s , peak_index = peak_index , pulse_col = col , pretrigger_ignore_samples = pretrigger_ignore_samples , n_presamples = self . header . n_presamples , transform_raw = self . transform_raw , ) return self . with_step ( step ) def correct_pretrig_mean_jumps ( self , uncorrected = \"pretrig_mean\" , corrected = \"ptm_jf\" , period = 4096 ): step = mass2 . PretrigMeanJumpFixStep ( inputs = [ uncorrected ], output = [ corrected ], good_expr = self . good_expr , use_expr = True , period = period , ) return self . with_step ( step ) def filter5lag ( self , pulse_col = \"pulse\" , peak_y_col = \"5lagy\" , peak_x_col = \"5lagx\" , f_3db = 25e3 , use_expr = True , ) -> \"Channel\" : avg_pulse = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( pulse_col ) . limit ( 2000 ) . collect () . to_series () . to_numpy () . mean ( axis = 0 ) ) assert self . noise spectrum5lag = self . noise . spectrum ( trunc_front = 2 , trunc_back = 2 ) filter5lag = FilterMaker . create_5lag ( avg_signal = avg_pulse , n_pretrigger = self . header . n_presamples , noise_psd = spectrum5lag . psd , noise_autocorr_vec = spectrum5lag . autocorr_vec , dt = self . header . frametime_s , f_3db = f_3db , ) step = Filter5LagStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter5lag , spectrum = spectrum5lag , transform_raw = self . transform_raw , ) return self . with_step ( step ) def good_df ( self , cols = pl . all (), use_expr = True ): return self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( cols ) . collect () def bad_df ( self , cols = pl . all (), use_expr = True ): return self . df . lazy () . filter ( self . good_expr . not_ ()) . filter ( use_expr ) . select ( cols ) . collect () def good_serieses ( self , cols , use_expr ): df2 = self . good_df ( cols , use_expr ) return [ df2 [ col ] for col in cols ] def driftcorrect ( self , indicator_col = \"pretrig_mean\" , uncorrected_col = \"5lagy\" , corrected_col = None , use_expr = True , ) -> \"Channel\" : # by defining a seperate learn method that takes ch as an argument, # we can move all the code for the step outside of Channel step = DriftCorrectStep . learn ( ch = self , indicator_col = indicator_col , uncorrected_col = uncorrected_col , corrected_col = corrected_col , use_expr = use_expr , ) return self . with_step ( step ) def linefit ( # noqa: PLR0917 self , line , col , use_expr = True , has_linear_background = False , has_tails = False , dlo = 50 , dhi = 50 , binsize = 0.5 , params_update = lmfit . Parameters (), ): model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = self . header . description , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def step_summary ( self ): return [( type ( a ) . __name__ , b ) for ( a , b ) in zip ( self . steps , self . steps_elapsed_s )] def __hash__ ( self ) -> int : # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other ): # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results # only checks if the ids match, does not try to be equal if all contents are equal return id ( self ) == id ( other ) @classmethod def from_ljh ( cls , path , noise_path = None , keep_posix_usec = False , transform_raw : Callable | None = None ) -> \"Channel\" : if not noise_path : noise_channel = None else : noise_channel = NoiseChannel . from_ljh ( noise_path ) ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars ( keep_posix_usec ) header = ChannelHeader . from_ljh_header_df ( header_df ) channel = Channel ( df , header = header , npulses = ljh . npulses , noise = noise_channel , transform_raw = transform_raw ) return channel @classmethod def from_off ( cls , off ) -> \"Channel\" : df = pl . from_numpy ( off . _mmap ) df = ( df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) . select ( pl . exclude ( \"unixnano\" )) ) df_header = pl . DataFrame ( off . header ) df_header = df_header . with_columns ( pl . Series ( \"Filename\" , [ off . filename ])) header = ChannelHeader ( f \" { os . path . split ( off . filename )[ 1 ] } \" , off . header [ \"ChannelNumberMatchingName\" ], off . framePeriodSeconds , off . _mmap [ \"recordPreSamples\" ][ 0 ], off . _mmap [ \"recordSamples\" ][ 0 ], df_header , ) channel = cls ( df , header , off . nPulses ) return channel def with_experiment_state_df ( self , df_es , force_timestamp_monotonic = False ) -> \"Channel\" : if not self . df [ \"timestamp\" ] . is_sorted (): df = self . df . select ( pl . col ( \"timestamp\" ) . cum_max () . alias ( \"timestamp\" )) . with_columns ( self . df . select ( pl . exclude ( \"timestamp\" ))) # print(\"WARNING: in with_experiment_state_df, timestamp is not monotonic, forcing it to be\") # print(\"This is likely a BUG in DASTARD.\") else : df = self . df df2 = df . join_asof ( df_es , on = \"timestamp\" , strategy = \"backward\" ) return self . with_replacement_df ( df2 ) def with_external_trigger_df ( self , df_ext : pl . DataFrame ): # df2 = self.df.join_asof(df_ext, ) raise NotImplementedError ( \"not implemented\" ) def with_replacement_df ( self , df2 ) -> \"Channel\" : return Channel ( df = df2 , header = self . header , npulses = self . npulses , noise = self . noise , good_expr = self . good_expr , df_history = self . df_history , steps = self . steps , transform_raw = self . transform_raw , ) def with_columns ( self , df2 ) -> \"Channel\" : df3 = self . df . with_columns ( df2 ) return self . with_replacement_df ( df3 ) def multifit_quadratic_gain_cal ( self , multifit : MultiFit , previous_cal_step_index , calibrated_col , use_expr = True , ) -> \"Channel\" : step = MultiFitQuadraticGainCalStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def multifit_mass_cal ( self , multifit : MultiFit , previous_cal_step_index , calibrated_col , use_expr = True , ) -> \"Channel\" : step = MultiFitMassCalibrationStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def concat_df ( self , df ) -> \"Channel\" : ch2 = Channel ( pl . concat ([ self . df , df ]), self . header , self . npulses , self . noise , self . good_expr ) # we won't copy over df_history and steps. I don't think you should use this when those are filled in? return ch2 def concat_ch ( self , ch ) -> \"Channel\" : ch2 = self . concat_df ( ch . df ) return ch2 def phase_correct_mass_specific_lines ( self , indicator_col , uncorrected_col , line_names , previous_cal_step_index , corrected_col = None , use_expr = True , ) -> \"Channel\" : if corrected_col is None : corrected_col = uncorrected_col + \"_pc\" step = mass2 . core . phase_correct_steps . phase_correct_mass_specific_lines ( self , indicator_col , uncorrected_col , corrected_col , previous_cal_step_index , line_names , use_expr , ) return self . with_step ( step ) def as_bad ( self , error_type , error_msg , backtrace ): return BadChannel ( self , error_type , error_msg , backtrace ) def save_steps ( self , filename ): steps = { self . header . ch_num : self . steps [:]} misc . pickle_object ( steps , filename ) return steps def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr : pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample : int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log : bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) if limits is None : in_limit = np . ones ( len ( y ), dtype = bool ) else : in_limit = np . logical_and ( y [:] > limits [ 0 ], y [:] < limits [ 1 ]) contents , _ , _ = plt . hist ( y [ in_limit ], 200 , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" ) plot_hists ( col , bin_edges , group_by_col , axis = None , use_good_expr = True , use_expr = True , skip_none = True ) Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channel.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def plot_hists ( self , col , bin_edges , group_by_col , axis = None , use_good_expr = True , use_expr = True , skip_none = True , ): \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () plot_summaries ( use_expr_in = None , downsample = None , log = False ) Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters use_expr : pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use self.good_expr downsample : int | None, optional Plot only every one of downsample pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log : bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. Source code in mass2/core/channel.py 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr : pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample : int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log : bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) if limits is None : in_limit = np . ones ( len ( y ), dtype = bool ) else : in_limit = np . logical_and ( y [:] > limits [ 0 ], y [:] < limits [ 1 ]) contents , _ , _ = plt . hist ( y [ in_limit ], 200 , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" ) with_good_expr_below_nsigma_outlier_resistant ( col_nsigma_pairs , replace = False , use_prev_good_expr = True ) always sets lower limit at 0, don't use for values that can be negative Source code in mass2/core/channel.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs , replace = False , use_prev_good_expr = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) with_good_expr_nsigma_range_outlier_resistant ( col_nsigma_pairs , replace = False , use_prev_good_expr = True ) always sets lower limit at 0, don't use for values that can be negative Source code in mass2/core/channel.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs , replace = False , use_prev_good_expr = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) Channels dataclass Source code in mass2/core/channels.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 @dataclass ( frozen = True ) # noqa: PLR0904 class Channels : channels : dict [ int , Channel ] description : str bad_channels : dict [ int , BadChannel ] = field ( default_factory = dict ) @property def ch0 ( self ): assert len ( self . channels ) > 0 , \"channels must be non-empty\" return next ( iter ( self . channels . values ())) @functools . cache def dfg ( self , exclude = \"pulse\" ): # return a dataframe containing good pulses from each channel, # exluding \"pulse\" by default # and including columns \"key\" (to be removed?) and \"ch_num\" # the more common call should be to wrap this in a convenient plotter dfs = [] for ch_num , channel in self . channels . items (): df = channel . df . select ( pl . exclude ( exclude )) . filter ( channel . good_expr ) # key_series = pl.Series(\"key\", dtype=pl.Int64).extend_constant(key, len(df)) assert ch_num == channel . header . ch_num ch_series = pl . Series ( \"ch_num\" , dtype = pl . Int64 ) . extend_constant ( channel . header . ch_num , len ( df )) dfs . append ( df . with_columns ( ch_series )) return pl . concat ( dfs ) def linefit ( # noqa: PLR0917 self , line , col , use_expr = True , has_linear_background = False , has_tails = False , dlo = 50 , dhi = 50 , binsize = 0.5 , params_update = lmfit . Parameters (), ): model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = mass2 . misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = f \" { len ( self . channels ) } channels, { self . description } \" , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def plot_hist ( self , col , bin_edges , use_expr = True , axis = None ): df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () ax = mass2 . misc . plot_hist_of_series ( df_small [ col ], bin_edges , axis ) ax . set_title ( f \" { len ( self . channels ) } channels, { self . description } \" ) def plot_hists ( self , col , bin_edges , group_by_col , axis = None , use_expr = None , skip_none = True ): \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () def map ( self , f : Callable , allow_throw : bool = False ) -> \"Channels\" : new_channels = {} new_bad_channels = {} for key , channel in self . channels . items (): try : new_channels [ key ] = f ( channel ) except KeyboardInterrupt : raise except Exception as ex : error_type : type = type ( ex ) error_message : str = str ( ex ) backtrace : str = traceback . format_exc () if allow_throw : raise print ( f \" { key =} { channel =} failed this step\" ) print ( f \" { error_type =} \" ) print ( f \" { error_message =} \" ) new_bad_channels [ key ] = channel . as_bad ( error_type , error_message , backtrace ) new_bad_channels = mass2 . misc . merge_dicts_ordered_by_keys ( self . bad_channels , new_bad_channels ) return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def set_bad ( self , ch_num , msg , require_ch_num_exists = True ): new_channels = {} new_bad_channels = {} if require_ch_num_exists : assert ch_num in self . channels . keys (), f \" { ch_num } can't be set bad because it does not exist\" for key , channel in self . channels . items (): if key == ch_num : new_bad_channels [ key ] = channel . as_bad ( None , msg , None ) else : new_channels [ key ] = channel return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def linefit_joblib ( self , line , col , prefer = \"threads\" , n_jobs = 4 ): def work ( key ): channel = self . channels [ key ] return channel . linefit ( line , col ) parallel = joblib . Parallel ( n_jobs = n_jobs , prefer = prefer ) # its not clear if threads are better.... what blocks the gil? results = parallel ( joblib . delayed ( work )( key ) for key in self . channels . keys ()) return results def __hash__ ( self ): # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other ): return id ( self ) == id ( other ) @classmethod def from_ljh_path_pairs ( cls , pulse_noise_pairs , description ): channels = {} for pulse_path , noise_path in pulse_noise_pairs : channel = Channel . from_ljh ( pulse_path , noise_path ) assert channel . header . ch_num not in channels . keys () channels [ channel . header . ch_num ] = channel return cls ( channels , description ) @classmethod def from_off_paths ( cls , off_paths , description ): channels = {} for path in off_paths : ch = Channel . from_off ( mass2 . off . OffFile ( path )) channels [ ch . header . ch_num ] = ch return cls ( channels , description ) @classmethod def from_ljh_folder ( cls , pulse_folder : str , noise_folder : str | None = None , limit : int | None = None , exclude_ch_nums : list [ int ] | None = None , ): assert os . path . isdir ( pulse_folder ), f \" { pulse_folder =} { noise_folder =} \" if exclude_ch_nums is None : exclude_ch_nums = [] if noise_folder is None : paths = mass2 . ljhutil . find_ljh_files ( pulse_folder , exclude_ch_nums = exclude_ch_nums ) if limit is not None : paths = paths [: limit ] pairs = [( path , \"\" ) for path in paths ] else : assert os . path . isdir ( noise_folder ), f \" { pulse_folder =} { noise_folder =} \" pairs = mass2 . ljhutil . match_files_by_channel ( pulse_folder , noise_folder , limit = limit , exclude_ch_nums = exclude_ch_nums ) description = f \"from_ljh_folder { pulse_folder =} { noise_folder =} \" print ( f \" { description } \" ) print ( f \" from_ljh_folder has { len ( pairs ) } pairs\" ) data = cls . from_ljh_path_pairs ( pairs , description ) print ( f \" and the Channels obj has { len ( data . channels ) } pairs\" ) return data def get_an_ljh_path ( self ): return pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) def get_path_in_output_folder ( self , filename ): ljh_path = self . get_an_ljh_path () base_name , post_chan = ljh_path . name . split ( \"_chan\" ) date , run_num = base_name . split ( \"_run\" ) # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") output_dir = ljh_path . parent . parent / f \" { run_num } moss_output\" output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir / filename def get_experiment_state_df ( self , experiment_state_path = None ): if experiment_state_path is None : ljh_path = self . get_an_ljh_path () experiment_state_path = mass2 . ljhutil . experiment_state_path_from_ljh_path ( ljh_path ) df = pl . read_csv ( experiment_state_path , new_columns = [ \"unixnano\" , \"state_label\" ]) # _col0, _col1 = df.columns df_es = df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) # strip whitespace from state_label column sl_series = df . select ( pl . col ( \"state_label\" ) . str . strip_chars ()) . to_series () df_es = df_es . with_columns ( state_label = pl . Series ( values = sl_series , dtype = pl . Categorical )) return df_es def with_experiment_state_by_path ( self , experiment_state_path = None ): df_es = self . get_experiment_state_df ( experiment_state_path ) return self . with_experiment_state_df ( df_es ) def with_external_trigger_by_path ( self , path = None ): raise NotImplementedError ( \"not implemented\" ) def with_external_trigger_df ( self , df_ext ): raise NotImplementedError ( \"not implemented\" ) def with_experiment_state_df ( self , df_es ): # this is not as performant as making use_exprs for states # and using .set_sorted on the timestamp column ch2s = {} for ch_num , ch in self . channels . items (): ch2s [ ch_num ] = ch . with_experiment_state_df ( df_es ) return Channels ( ch2s , self . description ) def with_steps_dict ( self , steps_dict ): def load_steps ( channel ): try : steps = steps_dict [ channel . header . ch_num ] except KeyError : raise Exception ( \"steps dict did not contain steps for this ch_num\" ) return channel . with_steps ( steps ) return self . map ( load_steps ) def save_steps ( self , filename ): steps = {} for channum , ch in self . channels . items (): steps [ channum ] = ch . steps [:] mass2 . misc . pickle_object ( steps , filename ) return steps def load_steps ( self , filename ): steps = mass2 . misc . unpickle_object ( filename ) return self . with_steps_dict ( steps ) def parent_folder_path ( self ): parent_folder_path = pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) . parent . parent print ( f \" { parent_folder_path =} \" ) return parent_folder_path def concat_data ( self , other_data ): # sorting here to show intention, but I think set is sorted by insertion order as # an implementation detail so this may not do anything ch_nums = sorted ( list ( set ( self . channels . keys ()) . intersection ( other_data . channels . keys ()))) channels2 = {} for ch_num in ch_nums : ch = self . channels [ ch_num ] other_ch = other_data . channels [ ch_num ] ch2 = ch . concat_ch ( other_ch ) channels2 [ ch_num ] = ch2 return Channels ( channels2 , self . description + other_data . description ) @classmethod def from_df ( cls , df_in , frametime_s = np . nan , n_presamples = None , n_samples = None , description = \"from Channels.channels_from_df\" , ): # requres a column named \"ch_num\" containing the channel number keys_df = df_in . partition_by ( by = [ \"ch_num\" ], as_dict = True ) dfs = { keys [ 0 ]: df for ( keys , df ) in keys_df . items ()} channels = {} for ch_num , df in dfs . items (): channels [ ch_num ] = Channel ( df , header = ChannelHeader ( description = \"from df\" , ch_num = ch_num , frametime_s = frametime_s , n_presamples = n_presamples , n_samples = n_samples , df = df , ), npulses = len ( df ), ) return Channels ( channels , description ) plot_hists ( col , bin_edges , group_by_col , axis = None , use_expr = None , skip_none = True ) Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channels.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def plot_hists ( self , col , bin_edges , group_by_col , axis = None , use_expr = None , skip_none = True ): \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () Classes to create time-domain and Fourier-domain optimal filters. Filter dataclass Bases: ABC A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level. Returns Filter A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted `variance` due to noise and the resulting `predicted_v_over_dv`, the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: `nominal_peak`. Source code in mass2/core/optimal_filtering.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 @dataclass ( frozen = True ) class Filter ( ABC ): \"\"\"A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level. Returns ------- Filter A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted `variance` due to noise and the resulting `predicted_v_over_dv`, the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: `nominal_peak`. \"\"\" values : np . ndarray nominal_peak : float variance : float predicted_v_over_dv : float dt_values : np . ndarray | None const_values : np . ndarray | None signal_model : np . ndarray | None dt_model : np . ndarray | None convolution_lags : int = 1 fmax : float | None = None f_3db : float | None = None cut_pre : int = 0 cut_post : int = 0 @property @abstractmethod def is_arrival_time_safe ( self ): \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property @abstractmethod def _filter_type ( self ): return \"illegal: this is supposed to be an abstract base class\" def plot ( self , axis : plt . Axes | None = None , ** kwargs ): \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout () def report ( self , std_energy : float = 5898.8 ): \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" ) @abstractmethod def filter_records ( self , x : npt . ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: pass is_arrival_time_safe abstractmethod property Is this an arrival-time-safe filter? plot ( axis = None , ** kwargs ) Make a plot of the filter Parameters axis : plt.Axes, optional A pre-existing axis to plot on, by default None Source code in mass2/core/optimal_filtering.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def plot ( self , axis : plt . Axes | None = None , ** kwargs ): \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout () report ( std_energy = 5898.8 ) Report on estimated V/dV for the filter. Parameters std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 Source code in mass2/core/optimal_filtering.py 298 299 300 301 302 303 304 305 306 307 308 309 310 def report ( self , std_energy : float = 5898.8 ): \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" ) Filter5Lag dataclass Bases: Filter Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015. Returns Filter5Lag An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 @dataclass ( frozen = True ) class Filter5Lag ( Filter ): \"\"\"Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015. Returns ------- Filter5Lag An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ): assert self . convolution_lags == 5 @property def is_arrival_time_safe ( self ): \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property def _filter_type ( self ): return \"5lag\" # These parameters fit a parabola to any 5 evenly-spaced points FIVELAG_FITTER = ( np . array ( ( ( - 6 , 24 , 34 , 24 , - 6 ), ( - 14 , - 7 , 0 , 7 , 14 ), ( 10 , - 5 , - 10 , - 5 , 10 ), ), dtype = float , ) / 70.0 ) def filter_records ( self , x : npt . ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x is_arrival_time_safe property Is this an arrival-time-safe filter? filter_records ( x ) Filter one microcalorimeter record or an array of records. Parameters x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, where x[i, :] is pulse record number i . Returns tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises AssertionError If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 def filter_records ( self , x : npt . ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x FilterATS dataclass Bases: Filter Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015. Returns FilterATS An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 @dataclass ( frozen = True ) class FilterATS ( Filter ): \"\"\"Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015. Returns ------- FilterATS An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ): assert self . convolution_lags == 1 assert self . dt_values is not None @property def is_arrival_time_safe ( self ): \"\"\"Is this an arrival-time-safe filter?\"\"\" return True @property def _filter_type ( self ): return \"ats\" def filter_records ( self , x : npt . ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values ) is_arrival_time_safe property Is this an arrival-time-safe filter? filter_records ( x ) Filter one microcalorimeter record or an array of records. Parameters x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises AssertionError If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 def filter_records ( self , x : npt . ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values ) FilterMaker dataclass An object capable of creating optimal filter based on a single signal and noise set. Arguments: npt.ArrayLike The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the peak value of this filter (that is, peak value relative to the baseline level). n_pretrigger : int The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only n_pretrigger samples at the start of a record. noise_autocorr : Optional[npt.ArrayLike] The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of avg_signal . noise_psd : Optional[npt.ArrayLike] The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of avg_signal , and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method compute_fourier() will not work. whitener : Optional[ToeplitzWhitener] An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes noise_autocorr if both are given. sample_time_sec : float The time step between samples in avg_signal and noise_autocorr (in seconds). This must be given if fmax or f_3db are ever to be used. peak : float The peak amplitude of the standard signal Notes If both noise_autocorr and whitener are None, then methods compute_5lag and compute_ats will both fail, as they require a time-domain characterization of the noise. The units of noise_autocorr are the square of the units used in signal_model and/or peak . The units of whitener are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. The units of noise_psd are square signal units, per Hertz. Returns FilterMaker An object that can make a variety of optimal filters, assuming a single signal and noise analysis. Source code in mass2/core/optimal_filtering.py 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 @dataclass ( frozen = True ) class FilterMaker : \"\"\"An object capable of creating optimal filter based on a single signal and noise set. Arguments: signal_model : npt.ArrayLike The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the *peak value* of this filter (that is, peak value relative to the baseline level). n_pretrigger : int The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only `n_pretrigger` samples at the start of a record. noise_autocorr : Optional[npt.ArrayLike] The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of `avg_signal`. noise_psd : Optional[npt.ArrayLike] The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of `avg_signal`, and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method `compute_fourier()` will not work. whitener : Optional[ToeplitzWhitener] An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes `noise_autocorr` if both are given. sample_time_sec : float The time step between samples in `avg_signal` and `noise_autocorr` (in seconds). This must be given if `fmax` or `f_3db` are ever to be used. peak : float The peak amplitude of the standard signal Notes ----- * If both `noise_autocorr` and `whitener` are None, then methods `compute_5lag` and `compute_ats` will both fail, as they require a time-domain characterization of the noise. * The units of `noise_autocorr` are the square of the units used in `signal_model` and/or `peak`. The units of `whitener` are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. * The units of `noise_psd` are square signal units, per Hertz. Returns ------- FilterMaker An object that can make a variety of optimal filters, assuming a single signal and noise analysis. \"\"\" signal_model : npt . ArrayLike n_pretrigger : int noise_autocorr : npt . ArrayLike | None = None noise_psd : npt . ArrayLike | None = None dt_model : npt . ArrayLike | None = None whitener : ToeplitzWhitener | None = None sample_time_sec : float = 0.0 peak : float = 0.0 @classmethod def create_5lag ( cls , avg_signal : npt . ArrayLike , n_pretrigger : int , noise_psd : npt . ArrayLike , noise_autocorr_vec : npt . ArrayLike , dt : float , fmax : float | None = None , f_3db : float | None = None , ): \"\"\"create_5lag creates a 5-lag filter directly, first creating, then using and deleting a `FilterMaker`. Parameters ---------- avg_signal : npt.ArrayLike The average signal to be used for filter construction n_pretrigger : int Number of samples before the trigger noise_psd : npt.ArrayLike Noise power-spectral density (starting at 0 and ending at the critical/Nyquist frequency) noise_autocorr_vec : npt.ArrayLike Noise autocorrelation dt : float Sampling period fmax : Optional[float], optional A hard low-pass limit, Fourier frequencies above this (in Hz) will be given zero amplitude, by default None f_3db : Optional[float], optional A soft low-pass limit, Fourier frequencies above this (in Hz) will be rolled off by a 1-pole filter with this 3 dB point (in Hz), by default None Returns ------- Filter A 5-lag optimal filter. \"\"\" avg_signal = np . asarray ( avg_signal ) peak_signal = np . amax ( avg_signal ) - avg_signal [ 0 ] maker = cls ( avg_signal , n_pretrigger , noise_psd = noise_psd , noise_autocorr = noise_autocorr_vec , sample_time_sec = dt , peak = peak_signal , ) return maker . compute_5lag ( fmax = fmax , f_3db = f_3db ) def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) # Time domain filters shorten = 2 truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_sig = TS ( truncated_signal ) Rinv_1 = TS ( np . ones ( n )) filt_noconst = Rinv_1 . sum () * Rinv_sig - Rinv_sig . sum () * Rinv_1 band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post ) def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , ) def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , self . noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post ) def _compute_autocorr ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> np . ndarray : \"\"\"Return the noise autocorrelation, if any, cut down by the requested number of values at the start and end. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- np.ndarray The noise autocorrelation of the appropriate length. Or a length-0 array if not known. \"\"\" # If there's an autocorrelation, cut it down to length. if self . noise_autocorr is None : return np . array ([], dtype = float ) N = len ( np . asarray ( self . signal_model )) return np . asarray ( self . noise_autocorr )[: N - ( cut_pre + cut_post )] def _normalize_signal ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> tuple [ np . ndarray , float , np . ndarray ]: \"\"\"Compute the normalized signal, peak value, and first-order arrival-time model. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- tuple[np.ndarray, float, np.ndarray] (sig, pk, dsig), where `sig` is the nominal signal model (normalized to have unit amplitude), `pk` is the peak values of the nominal signal, and `dsig` is the delta between `sig` that differ by one sample in arrival time. The `dsig` will be an empty array if no arrival-time model is known. Raises ------ ValueError If negative numbers of samples are to be cut, or the entire record is to be cut. \"\"\" avg_signal = np . array ( self . signal_model ) ns = len ( avg_signal ) pre_avg = avg_signal [ cut_pre : self . n_pretrigger - 1 ] . mean () if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) # Unless passed in, find the signal's peak value. This is normally peak=(max-pretrigger). # If signal is negative-going, however, then peak=(pretrigger-min). if self . peak > 0.0 : peak_signal = self . peak else : a = avg_signal [ cut_pre : ns - cut_post ] . min () b = avg_signal [ cut_pre : ns - cut_post ] . max () is_negative = pre_avg - a > b - pre_avg if is_negative : peak_signal = a - pre_avg else : peak_signal = b - pre_avg # avg_signal: normalize to have unit peak avg_signal -= pre_avg rescale = 1 / np . max ( avg_signal ) avg_signal *= rescale avg_signal [: self . n_pretrigger ] = 0.0 avg_signal = avg_signal [ cut_pre : ns - cut_post ] if self . dt_model is None : dt_model = np . array ([], dtype = float ) else : dt_model = self . dt_model * rescale dt_model = dt_model [ cut_pre : ns - cut_post ] return avg_signal , peak_signal , dt_model @staticmethod def _normalize_5lag_filter ( f : np . ndarray , avg_signal : np . ndarray ): \"\"\"Rescale 5-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) <= len ( avg_signal ) - 4 conv = np . zeros ( 5 , dtype = float ) for i in range ( 5 ): conv [ i ] = np . dot ( f , avg_signal [ i : i + len ( f )]) x = np . linspace ( - 2 , 2 , 5 ) fit = np . polyfit ( x , conv , 2 ) fit_ctr = - 0.5 * fit [ 1 ] / fit [ 0 ] fit_peak = np . polyval ( fit , fit_ctr ) f *= 1.0 / fit_peak @staticmethod def _normalize_filter ( f : np . ndarray , avg_signal : np . ndarray ): \"\"\"Rescale single-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) == len ( avg_signal ) f *= 1 / np . dot ( f , avg_signal ) compute_5lag ( fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns Filter A 5-lag optimal filter. Raises ValueError Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) # Time domain filters shorten = 2 truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_sig = TS ( truncated_signal ) Rinv_1 = TS ( np . ones ( n )) filt_noconst = Rinv_1 . sum () * Rinv_sig - Rinv_sig . sum () * Rinv_1 band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post ) compute_ats ( fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns Filter An arrival-time-safe optimal filter. Raises ValueError Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , self . noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post ) compute_fourier ( fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of fmax and f_3db are allowed. Parameters fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ValueError Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , ) create_5lag ( avg_signal , n_pretrigger , noise_psd , noise_autocorr_vec , dt , fmax = None , f_3db = None ) classmethod create_5lag creates a 5-lag filter directly, first creating, then using and deleting a FilterMaker . Parameters avg_signal : npt.ArrayLike The average signal to be used for filter construction n_pretrigger : int Number of samples before the trigger noise_psd : npt.ArrayLike Noise power-spectral density (starting at 0 and ending at the critical/Nyquist frequency) noise_autocorr_vec : npt.ArrayLike Noise autocorrelation dt : float Sampling period fmax : Optional[float], optional A hard low-pass limit, Fourier frequencies above this (in Hz) will be given zero amplitude, by default None f_3db : Optional[float], optional A soft low-pass limit, Fourier frequencies above this (in Hz) will be rolled off by a 1-pole filter with this 3 dB point (in Hz), by default None Returns Filter A 5-lag optimal filter. Source code in mass2/core/optimal_filtering.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 @classmethod def create_5lag ( cls , avg_signal : npt . ArrayLike , n_pretrigger : int , noise_psd : npt . ArrayLike , noise_autocorr_vec : npt . ArrayLike , dt : float , fmax : float | None = None , f_3db : float | None = None , ): \"\"\"create_5lag creates a 5-lag filter directly, first creating, then using and deleting a `FilterMaker`. Parameters ---------- avg_signal : npt.ArrayLike The average signal to be used for filter construction n_pretrigger : int Number of samples before the trigger noise_psd : npt.ArrayLike Noise power-spectral density (starting at 0 and ending at the critical/Nyquist frequency) noise_autocorr_vec : npt.ArrayLike Noise autocorrelation dt : float Sampling period fmax : Optional[float], optional A hard low-pass limit, Fourier frequencies above this (in Hz) will be given zero amplitude, by default None f_3db : Optional[float], optional A soft low-pass limit, Fourier frequencies above this (in Hz) will be rolled off by a 1-pole filter with this 3 dB point (in Hz), by default None Returns ------- Filter A 5-lag optimal filter. \"\"\" avg_signal = np . asarray ( avg_signal ) peak_signal = np . amax ( avg_signal ) - avg_signal [ 0 ] maker = cls ( avg_signal , n_pretrigger , noise_psd = noise_psd , noise_autocorr = noise_autocorr_vec , sample_time_sec = dt , peak = peak_signal , ) return maker . compute_5lag ( fmax = fmax , f_3db = f_3db ) ToeplitzWhitener dataclass An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: tw.whiten(v) returns Wv; it is equivalent to tw(v) tw.solveWT(v) returns inv(W')*v tw.applyWT(v) returns W'v tw.solveW(v) returns inv(W)*v Arguments theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients Returns ToeplitzWhitener Object that can perform approximate, time-invariant noise whitening. Raises ValueError If the operative methods are passed an array of dimension higher than 2. Source code in mass2/core/optimal_filtering.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 @dataclass ( frozen = True ) class ToeplitzWhitener : \"\"\"An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: * `tw.whiten(v)` returns Wv; it is equivalent to `tw(v)` * `tw.solveWT(v)` returns inv(W')*v * `tw.applyWT(v)` returns W'v * `tw.solveW(v)` returns inv(W)*v Arguments --------- theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients Returns ------- ToeplitzWhitener Object that can perform approximate, time-invariant noise whitening. Raises ------ ValueError If the operative methods are passed an array of dimension higher than 2. \"\"\" theta : np . ndarray phi : np . ndarray @property def p ( self ): return len ( self . phi ) - 1 @property def q ( self ): return len ( self . theta ) - 1 def whiten ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v ) def __call__ ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y def solveW ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y def solveWT ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :] def applyWT ( self , v : npt . ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :] def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR ) W ( N ) Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. Source code in mass2/core/optimal_filtering.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR ) __call__ ( v ) Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def __call__ ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y applyWT ( v ) Return vector (or matrix of column vectors) W'v Source code in mass2/core/optimal_filtering.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def applyWT ( self , v : npt . ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :] solveW ( v ) Return unwhitened vector (or matrix of column vectors) inv(W)*v Source code in mass2/core/optimal_filtering.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def solveW ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y solveWT ( v ) Return vector (or matrix of column vectors) inv(W')*v Source code in mass2/core/optimal_filtering.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def solveWT ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :] whiten ( v ) Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 65 66 67 def whiten ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v ) band_limit ( modelmatrix , sample_time_sec , fmax , f_3db ) Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input modelmatrix in-place. No effect if both fmax and f_3db are None . Parameters modelmatrix : np.ndarray The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec : float The sampling period, normally in seconds. fmax : Optional[float] The hard maximum frequency (units are inverse of sample_time_sec units, or Hz) f_3db : Optional[float] The 1-pole low-pass filter's 3 dB point (same units as fmax ) Source code in mass2/core/optimal_filtering.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def band_limit ( modelmatrix : np . ndarray , sample_time_sec : float , fmax : float | None , f_3db : float | None ): \"\"\"Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input `modelmatrix` in-place. No effect if both `fmax` and `f_3db` are `None`. Parameters ---------- modelmatrix : np.ndarray The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec : float The sampling period, normally in seconds. fmax : Optional[float] The hard maximum frequency (units are inverse of `sample_time_sec` units, or Hz) f_3db : Optional[float] The 1-pole low-pass filter's 3 dB point (same units as `fmax`) \"\"\" if fmax is None and f_3db is None : return # Handle the 2D case by calling this function once per column. assert len ( modelmatrix . shape ) <= 2 if len ( modelmatrix . shape ) == 2 : for i in range ( modelmatrix . shape [ 1 ]): band_limit ( modelmatrix [:, i ], sample_time_sec , fmax , f_3db ) return vector = modelmatrix filt_length = len ( vector ) sig_ft = np . fft . rfft ( vector ) freq = np . fft . fftfreq ( filt_length , d = sample_time_sec ) freq = np . abs ( freq [: len ( sig_ft )]) if fmax is not None : sig_ft [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft /= 1.0 + ( 1.0 * freq / f_3db ) ** 2 # n=filt_length is needed when filt_length is ODD vector [:] = np . fft . irfft ( sig_ft , n = filt_length ) bracketR ( q , noise ) Return the dot product (q^T R q) for vector and matrix R constructed from the vector by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). Source code in mass2/core/optimal_filtering.py 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 def bracketR ( q , noise ): \"\"\"Return the dot product (q^T R q) for vector <q> and matrix R constructed from the vector <noise> by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). \"\"\" if len ( noise ) < len ( q ): raise ValueError ( f \"Vector q (length { len ( q ) } ) cannot be longer than the noise (length { len ( noise ) } )\" ) n = len ( q ) r = np . zeros ( 2 * n - 1 , dtype = float ) r [ n - 1 :] = noise [: n ] r [ n - 1 :: - 1 ] = noise [: n ] dot = 0.0 for i in range ( n ): dot += q [ i ] * r [ n - i - 1 : 2 * n - i - 1 ] . dot ( q ) return dot","title":"Docstrings"},{"location":"docstrings/#automatic-documentation-generated-from-docstrings","text":"","title":"Automatic documentation generated from docstrings"},{"location":"docstrings/#mass2.core.channel.Channel","text":"Source code in mass2/core/channel.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 @dataclass ( frozen = True ) # noqa: PLR0904 class Channel : df : pl . DataFrame = field ( repr = False ) header : ChannelHeader = field ( repr = True ) npulses : int noise : NoiseChannel | None = field ( default = None , repr = False ) good_expr : bool | pl . Expr = True df_history : list [ pl . DataFrame ] = field ( default_factory = list , repr = False ) steps : CalSteps = field ( default_factory = CalSteps . new_empty ) steps_elapsed_s : list [ float ] = field ( default_factory = list ) transform_raw : Callable | None = None def mo_stepplots ( self ): desc_ind = { step . description : i for i , step in enumerate ( self . steps )} first_non_summarize_step = self . steps [ 0 ] for step in self . steps : if isinstance ( step , SummarizeStep ): continue first_non_summarize_step = step break mo_ui = mo . ui . dropdown ( desc_ind , value = first_non_summarize_step . description , label = f \"choose step for ch { self . header . ch_num } \" , ) def show (): return self . _mo_stepplots_explicit ( mo_ui ) def step_ind (): return mo_ui . value mo_ui . show = show mo_ui . step_ind = step_ind return mo_ui def _mo_stepplots_explicit ( self , mo_ui ): step_ind = mo_ui . step_ind () self . step_plot ( step_ind ) fig = plt . gcf () return mo . vstack ([ mo_ui , misc . show ( fig )]) def get_step ( self , index ): if index < 0 : # normalize the index to a positive index index = len ( self . steps ) + index step = self . steps [ index ] return step , index def step_plot ( self , step_ind , ** kwargs ): step , step_ind = self . get_step ( step_ind ) if step_ind + 1 == len ( self . df_history ): df_after = self . df else : df_after = self . df_history [ step_ind + 1 ] return step . dbg_plot ( df_after , ** kwargs ) def plot_hist ( self , col , bin_edges , axis = None ): axis = misc . plot_hist_of_series ( self . good_series ( col ), bin_edges , axis ) axis . set_title ( f \"ch { self . header . ch_num } plot_hist\" ) return axis def plot_hists ( self , col , bin_edges , group_by_col , axis = None , use_good_expr = True , use_expr = True , skip_none = True , ): \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () def plot_scatter ( self , x_col , y_col , color_col = None , use_expr = True , use_good_expr = True , skip_none = True , ax = None , ): if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # set current axis so I can use plt api if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr df_small = self . df . lazy () . filter ( filter_expr ) . select ( x_col , y_col , color_col ) . collect () for ( name ,), data in df_small . group_by ( color_col , maintain_order = True ): if name is None and skip_none and color_col is not None : continue plt . plot ( data . select ( x_col ) . to_series (), data . select ( y_col ) . to_series (), \".\" , label = name , ) plt . xlabel ( str ( x_col )) plt . ylabel ( str ( y_col )) title_str = f \"\"\" { self . header . description } use_expr= { str ( use_expr ) } good_expr= { str ( self . good_expr ) } \"\"\" plt . title ( title_str ) if color_col is not None : plt . legend ( title = color_col ) plt . tight_layout () def good_series ( self , col , use_expr = True ): return mass2 . good_series ( self . df , col , self . good_expr , use_expr ) def rough_cal_combinatoric ( self , line_names , uncalibrated_col , calibrated_col , ph_smoothing_fwhm , n_extra = 3 , use_expr = True , ) -> \"Channel\" : step = mass2 . RoughCalibrationStep . learn_combinatoric ( self , line_names , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal_combinatoric_height_info ( self , line_names , line_heights_allowed , uncalibrated_col , calibrated_col , ph_smoothing_fwhm , n_extra = 3 , use_expr = True , ) -> \"Channel\" : step = mass2 . RoughCalibrationStep . learn_combinatoric_height_info ( self , line_names , line_heights_allowed , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal ( # noqa: PLR0917 self , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : bool | pl . Expr = True , max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"Channel\" : step = mass2 . RoughCalibrationStep . learn_3peak ( self , line_names , uncalibrated_col , calibrated_col , use_expr , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , fwhm_pulse_height_units , n_extra_peaks , acceptable_rms_residual_e , ) return self . with_step ( step ) def with_step ( self , step ) -> \"Channel\" : t_start = time . time () df2 = step . calc_from_df ( self . df ) elapsed_s = time . time () - t_start ch2 = Channel ( df = df2 , header = self . header , npulses = self . npulses , noise = self . noise , good_expr = step . good_expr , df_history = self . df_history + [ self . df ], steps = self . steps . with_step ( step ), steps_elapsed_s = self . steps_elapsed_s + [ elapsed_s ], ) return ch2 def with_steps ( self , steps ) -> \"Channel\" : ch2 = self for step in steps : ch2 = ch2 . with_step ( step ) return ch2 def with_good_expr ( self , good_expr , replace = False ) -> \"Channel\" : # the default value of self.good_expr is True # and_(True) will just add visual noise when looking at good_expr and not affect behavior if not replace and good_expr is not True : good_expr = good_expr . and_ ( self . good_expr ) return Channel ( df = self . df , header = self . header , npulses = self . npulses , noise = self . noise , good_expr = good_expr , df_history = self . df_history , steps = self . steps , steps_elapsed_s = self . steps_elapsed_s , ) def with_good_expr_pretrig_rms_and_postpeak_deriv ( self , n_sigma_pretrig_rms = 20 , n_sigma_postpeak_deriv = 20 , replace = False ) -> \"Channel\" : max_postpeak_deriv = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"postpeak_deriv\" ] . to_numpy (), nsigma = n_sigma_postpeak_deriv ) max_pretrig_rms = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"pretrig_rms\" ] . to_numpy (), nsigma = n_sigma_pretrig_rms ) good_expr = ( pl . col ( \"postpeak_deriv\" ) < max_postpeak_deriv ) . and_ ( pl . col ( \"pretrig_rms\" ) < max_pretrig_rms ) return self . with_good_expr ( good_expr , replace ) def with_range_around_median ( self , col , range_up , range_down ): med = np . median ( self . df [ col ] . to_numpy ()) return self . with_good_expr ( pl . col ( col ) . is_between ( med - range_down , med + range_up )) def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs , replace = False , use_prev_good_expr = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs , replace = False , use_prev_good_expr = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) @functools . cache def typical_peak_ind ( self , col = \"pulse\" ): raw = self . df . limit ( 100 )[ col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) return int ( np . median ( raw . argmax ( axis = 1 ))) def summarize_pulses ( self , col = \"pulse\" , pretrigger_ignore_samples = 0 , peak_index = None ) -> \"Channel\" : if peak_index is None : peak_index = self . typical_peak_ind ( col ) step = SummarizeStep ( inputs = [ col ], output = [ \"many\" ], good_expr = self . good_expr , use_expr = True , frametime_s = self . header . frametime_s , peak_index = peak_index , pulse_col = col , pretrigger_ignore_samples = pretrigger_ignore_samples , n_presamples = self . header . n_presamples , transform_raw = self . transform_raw , ) return self . with_step ( step ) def correct_pretrig_mean_jumps ( self , uncorrected = \"pretrig_mean\" , corrected = \"ptm_jf\" , period = 4096 ): step = mass2 . PretrigMeanJumpFixStep ( inputs = [ uncorrected ], output = [ corrected ], good_expr = self . good_expr , use_expr = True , period = period , ) return self . with_step ( step ) def filter5lag ( self , pulse_col = \"pulse\" , peak_y_col = \"5lagy\" , peak_x_col = \"5lagx\" , f_3db = 25e3 , use_expr = True , ) -> \"Channel\" : avg_pulse = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( pulse_col ) . limit ( 2000 ) . collect () . to_series () . to_numpy () . mean ( axis = 0 ) ) assert self . noise spectrum5lag = self . noise . spectrum ( trunc_front = 2 , trunc_back = 2 ) filter5lag = FilterMaker . create_5lag ( avg_signal = avg_pulse , n_pretrigger = self . header . n_presamples , noise_psd = spectrum5lag . psd , noise_autocorr_vec = spectrum5lag . autocorr_vec , dt = self . header . frametime_s , f_3db = f_3db , ) step = Filter5LagStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter5lag , spectrum = spectrum5lag , transform_raw = self . transform_raw , ) return self . with_step ( step ) def good_df ( self , cols = pl . all (), use_expr = True ): return self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( cols ) . collect () def bad_df ( self , cols = pl . all (), use_expr = True ): return self . df . lazy () . filter ( self . good_expr . not_ ()) . filter ( use_expr ) . select ( cols ) . collect () def good_serieses ( self , cols , use_expr ): df2 = self . good_df ( cols , use_expr ) return [ df2 [ col ] for col in cols ] def driftcorrect ( self , indicator_col = \"pretrig_mean\" , uncorrected_col = \"5lagy\" , corrected_col = None , use_expr = True , ) -> \"Channel\" : # by defining a seperate learn method that takes ch as an argument, # we can move all the code for the step outside of Channel step = DriftCorrectStep . learn ( ch = self , indicator_col = indicator_col , uncorrected_col = uncorrected_col , corrected_col = corrected_col , use_expr = use_expr , ) return self . with_step ( step ) def linefit ( # noqa: PLR0917 self , line , col , use_expr = True , has_linear_background = False , has_tails = False , dlo = 50 , dhi = 50 , binsize = 0.5 , params_update = lmfit . Parameters (), ): model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = self . header . description , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def step_summary ( self ): return [( type ( a ) . __name__ , b ) for ( a , b ) in zip ( self . steps , self . steps_elapsed_s )] def __hash__ ( self ) -> int : # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other ): # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results # only checks if the ids match, does not try to be equal if all contents are equal return id ( self ) == id ( other ) @classmethod def from_ljh ( cls , path , noise_path = None , keep_posix_usec = False , transform_raw : Callable | None = None ) -> \"Channel\" : if not noise_path : noise_channel = None else : noise_channel = NoiseChannel . from_ljh ( noise_path ) ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars ( keep_posix_usec ) header = ChannelHeader . from_ljh_header_df ( header_df ) channel = Channel ( df , header = header , npulses = ljh . npulses , noise = noise_channel , transform_raw = transform_raw ) return channel @classmethod def from_off ( cls , off ) -> \"Channel\" : df = pl . from_numpy ( off . _mmap ) df = ( df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) . select ( pl . exclude ( \"unixnano\" )) ) df_header = pl . DataFrame ( off . header ) df_header = df_header . with_columns ( pl . Series ( \"Filename\" , [ off . filename ])) header = ChannelHeader ( f \" { os . path . split ( off . filename )[ 1 ] } \" , off . header [ \"ChannelNumberMatchingName\" ], off . framePeriodSeconds , off . _mmap [ \"recordPreSamples\" ][ 0 ], off . _mmap [ \"recordSamples\" ][ 0 ], df_header , ) channel = cls ( df , header , off . nPulses ) return channel def with_experiment_state_df ( self , df_es , force_timestamp_monotonic = False ) -> \"Channel\" : if not self . df [ \"timestamp\" ] . is_sorted (): df = self . df . select ( pl . col ( \"timestamp\" ) . cum_max () . alias ( \"timestamp\" )) . with_columns ( self . df . select ( pl . exclude ( \"timestamp\" ))) # print(\"WARNING: in with_experiment_state_df, timestamp is not monotonic, forcing it to be\") # print(\"This is likely a BUG in DASTARD.\") else : df = self . df df2 = df . join_asof ( df_es , on = \"timestamp\" , strategy = \"backward\" ) return self . with_replacement_df ( df2 ) def with_external_trigger_df ( self , df_ext : pl . DataFrame ): # df2 = self.df.join_asof(df_ext, ) raise NotImplementedError ( \"not implemented\" ) def with_replacement_df ( self , df2 ) -> \"Channel\" : return Channel ( df = df2 , header = self . header , npulses = self . npulses , noise = self . noise , good_expr = self . good_expr , df_history = self . df_history , steps = self . steps , transform_raw = self . transform_raw , ) def with_columns ( self , df2 ) -> \"Channel\" : df3 = self . df . with_columns ( df2 ) return self . with_replacement_df ( df3 ) def multifit_quadratic_gain_cal ( self , multifit : MultiFit , previous_cal_step_index , calibrated_col , use_expr = True , ) -> \"Channel\" : step = MultiFitQuadraticGainCalStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def multifit_mass_cal ( self , multifit : MultiFit , previous_cal_step_index , calibrated_col , use_expr = True , ) -> \"Channel\" : step = MultiFitMassCalibrationStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def concat_df ( self , df ) -> \"Channel\" : ch2 = Channel ( pl . concat ([ self . df , df ]), self . header , self . npulses , self . noise , self . good_expr ) # we won't copy over df_history and steps. I don't think you should use this when those are filled in? return ch2 def concat_ch ( self , ch ) -> \"Channel\" : ch2 = self . concat_df ( ch . df ) return ch2 def phase_correct_mass_specific_lines ( self , indicator_col , uncorrected_col , line_names , previous_cal_step_index , corrected_col = None , use_expr = True , ) -> \"Channel\" : if corrected_col is None : corrected_col = uncorrected_col + \"_pc\" step = mass2 . core . phase_correct_steps . phase_correct_mass_specific_lines ( self , indicator_col , uncorrected_col , corrected_col , previous_cal_step_index , line_names , use_expr , ) return self . with_step ( step ) def as_bad ( self , error_type , error_msg , backtrace ): return BadChannel ( self , error_type , error_msg , backtrace ) def save_steps ( self , filename ): steps = { self . header . ch_num : self . steps [:]} misc . pickle_object ( steps , filename ) return steps def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr : pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample : int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log : bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) if limits is None : in_limit = np . ones ( len ( y ), dtype = bool ) else : in_limit = np . logical_and ( y [:] > limits [ 0 ], y [:] < limits [ 1 ]) contents , _ , _ = plt . hist ( y [ in_limit ], 200 , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" )","title":"Channel"},{"location":"docstrings/#mass2.core.channel.Channel.plot_hists","text":"Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channel.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def plot_hists ( self , col , bin_edges , group_by_col , axis = None , use_good_expr = True , use_expr = True , skip_none = True , ): \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout ()","title":"plot_hists"},{"location":"docstrings/#mass2.core.channel.Channel.plot_summaries","text":"Plot a summary of the data set, including time series and histograms of key pulse properties.","title":"plot_summaries"},{"location":"docstrings/#mass2.core.channel.Channel.plot_summaries--parameters","text":"use_expr : pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use self.good_expr downsample : int | None, optional Plot only every one of downsample pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log : bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. Source code in mass2/core/channel.py 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr : pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample : int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log : bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) if limits is None : in_limit = np . ones ( len ( y ), dtype = bool ) else : in_limit = np . logical_and ( y [:] > limits [ 0 ], y [:] < limits [ 1 ]) contents , _ , _ = plt . hist ( y [ in_limit ], 200 , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" )","title":"Parameters"},{"location":"docstrings/#mass2.core.channel.Channel.with_good_expr_below_nsigma_outlier_resistant","text":"always sets lower limit at 0, don't use for values that can be negative Source code in mass2/core/channel.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs , replace = False , use_prev_good_expr = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace )","title":"with_good_expr_below_nsigma_outlier_resistant"},{"location":"docstrings/#mass2.core.channel.Channel.with_good_expr_nsigma_range_outlier_resistant","text":"always sets lower limit at 0, don't use for values that can be negative Source code in mass2/core/channel.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs , replace = False , use_prev_good_expr = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace )","title":"with_good_expr_nsigma_range_outlier_resistant"},{"location":"docstrings/#mass2.core.channels.Channels","text":"Source code in mass2/core/channels.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 @dataclass ( frozen = True ) # noqa: PLR0904 class Channels : channels : dict [ int , Channel ] description : str bad_channels : dict [ int , BadChannel ] = field ( default_factory = dict ) @property def ch0 ( self ): assert len ( self . channels ) > 0 , \"channels must be non-empty\" return next ( iter ( self . channels . values ())) @functools . cache def dfg ( self , exclude = \"pulse\" ): # return a dataframe containing good pulses from each channel, # exluding \"pulse\" by default # and including columns \"key\" (to be removed?) and \"ch_num\" # the more common call should be to wrap this in a convenient plotter dfs = [] for ch_num , channel in self . channels . items (): df = channel . df . select ( pl . exclude ( exclude )) . filter ( channel . good_expr ) # key_series = pl.Series(\"key\", dtype=pl.Int64).extend_constant(key, len(df)) assert ch_num == channel . header . ch_num ch_series = pl . Series ( \"ch_num\" , dtype = pl . Int64 ) . extend_constant ( channel . header . ch_num , len ( df )) dfs . append ( df . with_columns ( ch_series )) return pl . concat ( dfs ) def linefit ( # noqa: PLR0917 self , line , col , use_expr = True , has_linear_background = False , has_tails = False , dlo = 50 , dhi = 50 , binsize = 0.5 , params_update = lmfit . Parameters (), ): model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = mass2 . misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = f \" { len ( self . channels ) } channels, { self . description } \" , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def plot_hist ( self , col , bin_edges , use_expr = True , axis = None ): df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () ax = mass2 . misc . plot_hist_of_series ( df_small [ col ], bin_edges , axis ) ax . set_title ( f \" { len ( self . channels ) } channels, { self . description } \" ) def plot_hists ( self , col , bin_edges , group_by_col , axis = None , use_expr = None , skip_none = True ): \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () def map ( self , f : Callable , allow_throw : bool = False ) -> \"Channels\" : new_channels = {} new_bad_channels = {} for key , channel in self . channels . items (): try : new_channels [ key ] = f ( channel ) except KeyboardInterrupt : raise except Exception as ex : error_type : type = type ( ex ) error_message : str = str ( ex ) backtrace : str = traceback . format_exc () if allow_throw : raise print ( f \" { key =} { channel =} failed this step\" ) print ( f \" { error_type =} \" ) print ( f \" { error_message =} \" ) new_bad_channels [ key ] = channel . as_bad ( error_type , error_message , backtrace ) new_bad_channels = mass2 . misc . merge_dicts_ordered_by_keys ( self . bad_channels , new_bad_channels ) return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def set_bad ( self , ch_num , msg , require_ch_num_exists = True ): new_channels = {} new_bad_channels = {} if require_ch_num_exists : assert ch_num in self . channels . keys (), f \" { ch_num } can't be set bad because it does not exist\" for key , channel in self . channels . items (): if key == ch_num : new_bad_channels [ key ] = channel . as_bad ( None , msg , None ) else : new_channels [ key ] = channel return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def linefit_joblib ( self , line , col , prefer = \"threads\" , n_jobs = 4 ): def work ( key ): channel = self . channels [ key ] return channel . linefit ( line , col ) parallel = joblib . Parallel ( n_jobs = n_jobs , prefer = prefer ) # its not clear if threads are better.... what blocks the gil? results = parallel ( joblib . delayed ( work )( key ) for key in self . channels . keys ()) return results def __hash__ ( self ): # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other ): return id ( self ) == id ( other ) @classmethod def from_ljh_path_pairs ( cls , pulse_noise_pairs , description ): channels = {} for pulse_path , noise_path in pulse_noise_pairs : channel = Channel . from_ljh ( pulse_path , noise_path ) assert channel . header . ch_num not in channels . keys () channels [ channel . header . ch_num ] = channel return cls ( channels , description ) @classmethod def from_off_paths ( cls , off_paths , description ): channels = {} for path in off_paths : ch = Channel . from_off ( mass2 . off . OffFile ( path )) channels [ ch . header . ch_num ] = ch return cls ( channels , description ) @classmethod def from_ljh_folder ( cls , pulse_folder : str , noise_folder : str | None = None , limit : int | None = None , exclude_ch_nums : list [ int ] | None = None , ): assert os . path . isdir ( pulse_folder ), f \" { pulse_folder =} { noise_folder =} \" if exclude_ch_nums is None : exclude_ch_nums = [] if noise_folder is None : paths = mass2 . ljhutil . find_ljh_files ( pulse_folder , exclude_ch_nums = exclude_ch_nums ) if limit is not None : paths = paths [: limit ] pairs = [( path , \"\" ) for path in paths ] else : assert os . path . isdir ( noise_folder ), f \" { pulse_folder =} { noise_folder =} \" pairs = mass2 . ljhutil . match_files_by_channel ( pulse_folder , noise_folder , limit = limit , exclude_ch_nums = exclude_ch_nums ) description = f \"from_ljh_folder { pulse_folder =} { noise_folder =} \" print ( f \" { description } \" ) print ( f \" from_ljh_folder has { len ( pairs ) } pairs\" ) data = cls . from_ljh_path_pairs ( pairs , description ) print ( f \" and the Channels obj has { len ( data . channels ) } pairs\" ) return data def get_an_ljh_path ( self ): return pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) def get_path_in_output_folder ( self , filename ): ljh_path = self . get_an_ljh_path () base_name , post_chan = ljh_path . name . split ( \"_chan\" ) date , run_num = base_name . split ( \"_run\" ) # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") output_dir = ljh_path . parent . parent / f \" { run_num } moss_output\" output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir / filename def get_experiment_state_df ( self , experiment_state_path = None ): if experiment_state_path is None : ljh_path = self . get_an_ljh_path () experiment_state_path = mass2 . ljhutil . experiment_state_path_from_ljh_path ( ljh_path ) df = pl . read_csv ( experiment_state_path , new_columns = [ \"unixnano\" , \"state_label\" ]) # _col0, _col1 = df.columns df_es = df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) # strip whitespace from state_label column sl_series = df . select ( pl . col ( \"state_label\" ) . str . strip_chars ()) . to_series () df_es = df_es . with_columns ( state_label = pl . Series ( values = sl_series , dtype = pl . Categorical )) return df_es def with_experiment_state_by_path ( self , experiment_state_path = None ): df_es = self . get_experiment_state_df ( experiment_state_path ) return self . with_experiment_state_df ( df_es ) def with_external_trigger_by_path ( self , path = None ): raise NotImplementedError ( \"not implemented\" ) def with_external_trigger_df ( self , df_ext ): raise NotImplementedError ( \"not implemented\" ) def with_experiment_state_df ( self , df_es ): # this is not as performant as making use_exprs for states # and using .set_sorted on the timestamp column ch2s = {} for ch_num , ch in self . channels . items (): ch2s [ ch_num ] = ch . with_experiment_state_df ( df_es ) return Channels ( ch2s , self . description ) def with_steps_dict ( self , steps_dict ): def load_steps ( channel ): try : steps = steps_dict [ channel . header . ch_num ] except KeyError : raise Exception ( \"steps dict did not contain steps for this ch_num\" ) return channel . with_steps ( steps ) return self . map ( load_steps ) def save_steps ( self , filename ): steps = {} for channum , ch in self . channels . items (): steps [ channum ] = ch . steps [:] mass2 . misc . pickle_object ( steps , filename ) return steps def load_steps ( self , filename ): steps = mass2 . misc . unpickle_object ( filename ) return self . with_steps_dict ( steps ) def parent_folder_path ( self ): parent_folder_path = pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) . parent . parent print ( f \" { parent_folder_path =} \" ) return parent_folder_path def concat_data ( self , other_data ): # sorting here to show intention, but I think set is sorted by insertion order as # an implementation detail so this may not do anything ch_nums = sorted ( list ( set ( self . channels . keys ()) . intersection ( other_data . channels . keys ()))) channels2 = {} for ch_num in ch_nums : ch = self . channels [ ch_num ] other_ch = other_data . channels [ ch_num ] ch2 = ch . concat_ch ( other_ch ) channels2 [ ch_num ] = ch2 return Channels ( channels2 , self . description + other_data . description ) @classmethod def from_df ( cls , df_in , frametime_s = np . nan , n_presamples = None , n_samples = None , description = \"from Channels.channels_from_df\" , ): # requres a column named \"ch_num\" containing the channel number keys_df = df_in . partition_by ( by = [ \"ch_num\" ], as_dict = True ) dfs = { keys [ 0 ]: df for ( keys , df ) in keys_df . items ()} channels = {} for ch_num , df in dfs . items (): channels [ ch_num ] = Channel ( df , header = ChannelHeader ( description = \"from df\" , ch_num = ch_num , frametime_s = frametime_s , n_presamples = n_presamples , n_samples = n_samples , df = df , ), npulses = len ( df ), ) return Channels ( channels , description )","title":"Channels"},{"location":"docstrings/#mass2.core.channels.Channels.plot_hists","text":"Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channels.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def plot_hists ( self , col , bin_edges , group_by_col , axis = None , use_expr = None , skip_none = True ): \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () Classes to create time-domain and Fourier-domain optimal filters.","title":"plot_hists"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter","text":"Bases: ABC A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level.","title":"Filter"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter--returns","text":"Filter A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted `variance` due to noise and the resulting `predicted_v_over_dv`, the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: `nominal_peak`. Source code in mass2/core/optimal_filtering.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 @dataclass ( frozen = True ) class Filter ( ABC ): \"\"\"A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level. Returns ------- Filter A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted `variance` due to noise and the resulting `predicted_v_over_dv`, the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: `nominal_peak`. \"\"\" values : np . ndarray nominal_peak : float variance : float predicted_v_over_dv : float dt_values : np . ndarray | None const_values : np . ndarray | None signal_model : np . ndarray | None dt_model : np . ndarray | None convolution_lags : int = 1 fmax : float | None = None f_3db : float | None = None cut_pre : int = 0 cut_post : int = 0 @property @abstractmethod def is_arrival_time_safe ( self ): \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property @abstractmethod def _filter_type ( self ): return \"illegal: this is supposed to be an abstract base class\" def plot ( self , axis : plt . Axes | None = None , ** kwargs ): \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout () def report ( self , std_energy : float = 5898.8 ): \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" ) @abstractmethod def filter_records ( self , x : npt . ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: pass","title":"Returns"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.is_arrival_time_safe","text":"Is this an arrival-time-safe filter?","title":"is_arrival_time_safe"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.plot","text":"Make a plot of the filter","title":"plot"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.plot--parameters","text":"axis : plt.Axes, optional A pre-existing axis to plot on, by default None Source code in mass2/core/optimal_filtering.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def plot ( self , axis : plt . Axes | None = None , ** kwargs ): \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout ()","title":"Parameters"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.report","text":"Report on estimated V/dV for the filter.","title":"report"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.report--parameters","text":"std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 Source code in mass2/core/optimal_filtering.py 298 299 300 301 302 303 304 305 306 307 308 309 310 def report ( self , std_energy : float = 5898.8 ): \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" )","title":"Parameters"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag","text":"Bases: Filter Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015.","title":"Filter5Lag"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag--returns","text":"Filter5Lag An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 @dataclass ( frozen = True ) class Filter5Lag ( Filter ): \"\"\"Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015. Returns ------- Filter5Lag An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ): assert self . convolution_lags == 5 @property def is_arrival_time_safe ( self ): \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property def _filter_type ( self ): return \"5lag\" # These parameters fit a parabola to any 5 evenly-spaced points FIVELAG_FITTER = ( np . array ( ( ( - 6 , 24 , 34 , 24 , - 6 ), ( - 14 , - 7 , 0 , 7 , 14 ), ( 10 , - 5 , - 10 , - 5 , 10 ), ), dtype = float , ) / 70.0 ) def filter_records ( self , x : npt . ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x","title":"Returns"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag.is_arrival_time_safe","text":"Is this an arrival-time-safe filter?","title":"is_arrival_time_safe"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag.filter_records","text":"Filter one microcalorimeter record or an array of records.","title":"filter_records"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag.filter_records--parameters","text":"x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, where x[i, :] is pulse record number i .","title":"Parameters"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag.filter_records--returns","text":"tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value.","title":"Returns"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag.filter_records--raises","text":"AssertionError If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 def filter_records ( self , x : npt . ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x","title":"Raises"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS","text":"Bases: Filter Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015.","title":"FilterATS"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS--returns","text":"FilterATS An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 @dataclass ( frozen = True ) class FilterATS ( Filter ): \"\"\"Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015. Returns ------- FilterATS An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ): assert self . convolution_lags == 1 assert self . dt_values is not None @property def is_arrival_time_safe ( self ): \"\"\"Is this an arrival-time-safe filter?\"\"\" return True @property def _filter_type ( self ): return \"ats\" def filter_records ( self , x : npt . ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values )","title":"Returns"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS.is_arrival_time_safe","text":"Is this an arrival-time-safe filter?","title":"is_arrival_time_safe"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS.filter_records","text":"Filter one microcalorimeter record or an array of records.","title":"filter_records"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS.filter_records--parameters","text":"x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records.","title":"Parameters"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS.filter_records--returns","text":"tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value.","title":"Returns"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS.filter_records--raises","text":"AssertionError If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 def filter_records ( self , x : npt . ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : npt.ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values )","title":"Raises"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker","text":"An object capable of creating optimal filter based on a single signal and noise set. Arguments: npt.ArrayLike The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the peak value of this filter (that is, peak value relative to the baseline level). n_pretrigger : int The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only n_pretrigger samples at the start of a record. noise_autocorr : Optional[npt.ArrayLike] The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of avg_signal . noise_psd : Optional[npt.ArrayLike] The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of avg_signal , and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method compute_fourier() will not work. whitener : Optional[ToeplitzWhitener] An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes noise_autocorr if both are given. sample_time_sec : float The time step between samples in avg_signal and noise_autocorr (in seconds). This must be given if fmax or f_3db are ever to be used. peak : float The peak amplitude of the standard signal","title":"FilterMaker"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker--notes","text":"If both noise_autocorr and whitener are None, then methods compute_5lag and compute_ats will both fail, as they require a time-domain characterization of the noise. The units of noise_autocorr are the square of the units used in signal_model and/or peak . The units of whitener are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. The units of noise_psd are square signal units, per Hertz.","title":"Notes"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker--returns","text":"FilterMaker An object that can make a variety of optimal filters, assuming a single signal and noise analysis. Source code in mass2/core/optimal_filtering.py 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 @dataclass ( frozen = True ) class FilterMaker : \"\"\"An object capable of creating optimal filter based on a single signal and noise set. Arguments: signal_model : npt.ArrayLike The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the *peak value* of this filter (that is, peak value relative to the baseline level). n_pretrigger : int The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only `n_pretrigger` samples at the start of a record. noise_autocorr : Optional[npt.ArrayLike] The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of `avg_signal`. noise_psd : Optional[npt.ArrayLike] The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of `avg_signal`, and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method `compute_fourier()` will not work. whitener : Optional[ToeplitzWhitener] An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes `noise_autocorr` if both are given. sample_time_sec : float The time step between samples in `avg_signal` and `noise_autocorr` (in seconds). This must be given if `fmax` or `f_3db` are ever to be used. peak : float The peak amplitude of the standard signal Notes ----- * If both `noise_autocorr` and `whitener` are None, then methods `compute_5lag` and `compute_ats` will both fail, as they require a time-domain characterization of the noise. * The units of `noise_autocorr` are the square of the units used in `signal_model` and/or `peak`. The units of `whitener` are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. * The units of `noise_psd` are square signal units, per Hertz. Returns ------- FilterMaker An object that can make a variety of optimal filters, assuming a single signal and noise analysis. \"\"\" signal_model : npt . ArrayLike n_pretrigger : int noise_autocorr : npt . ArrayLike | None = None noise_psd : npt . ArrayLike | None = None dt_model : npt . ArrayLike | None = None whitener : ToeplitzWhitener | None = None sample_time_sec : float = 0.0 peak : float = 0.0 @classmethod def create_5lag ( cls , avg_signal : npt . ArrayLike , n_pretrigger : int , noise_psd : npt . ArrayLike , noise_autocorr_vec : npt . ArrayLike , dt : float , fmax : float | None = None , f_3db : float | None = None , ): \"\"\"create_5lag creates a 5-lag filter directly, first creating, then using and deleting a `FilterMaker`. Parameters ---------- avg_signal : npt.ArrayLike The average signal to be used for filter construction n_pretrigger : int Number of samples before the trigger noise_psd : npt.ArrayLike Noise power-spectral density (starting at 0 and ending at the critical/Nyquist frequency) noise_autocorr_vec : npt.ArrayLike Noise autocorrelation dt : float Sampling period fmax : Optional[float], optional A hard low-pass limit, Fourier frequencies above this (in Hz) will be given zero amplitude, by default None f_3db : Optional[float], optional A soft low-pass limit, Fourier frequencies above this (in Hz) will be rolled off by a 1-pole filter with this 3 dB point (in Hz), by default None Returns ------- Filter A 5-lag optimal filter. \"\"\" avg_signal = np . asarray ( avg_signal ) peak_signal = np . amax ( avg_signal ) - avg_signal [ 0 ] maker = cls ( avg_signal , n_pretrigger , noise_psd = noise_psd , noise_autocorr = noise_autocorr_vec , sample_time_sec = dt , peak = peak_signal , ) return maker . compute_5lag ( fmax = fmax , f_3db = f_3db ) def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) # Time domain filters shorten = 2 truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_sig = TS ( truncated_signal ) Rinv_1 = TS ( np . ones ( n )) filt_noconst = Rinv_1 . sum () * Rinv_sig - Rinv_sig . sum () * Rinv_1 band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post ) def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , ) def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , self . noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post ) def _compute_autocorr ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> np . ndarray : \"\"\"Return the noise autocorrelation, if any, cut down by the requested number of values at the start and end. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- np.ndarray The noise autocorrelation of the appropriate length. Or a length-0 array if not known. \"\"\" # If there's an autocorrelation, cut it down to length. if self . noise_autocorr is None : return np . array ([], dtype = float ) N = len ( np . asarray ( self . signal_model )) return np . asarray ( self . noise_autocorr )[: N - ( cut_pre + cut_post )] def _normalize_signal ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> tuple [ np . ndarray , float , np . ndarray ]: \"\"\"Compute the normalized signal, peak value, and first-order arrival-time model. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- tuple[np.ndarray, float, np.ndarray] (sig, pk, dsig), where `sig` is the nominal signal model (normalized to have unit amplitude), `pk` is the peak values of the nominal signal, and `dsig` is the delta between `sig` that differ by one sample in arrival time. The `dsig` will be an empty array if no arrival-time model is known. Raises ------ ValueError If negative numbers of samples are to be cut, or the entire record is to be cut. \"\"\" avg_signal = np . array ( self . signal_model ) ns = len ( avg_signal ) pre_avg = avg_signal [ cut_pre : self . n_pretrigger - 1 ] . mean () if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) # Unless passed in, find the signal's peak value. This is normally peak=(max-pretrigger). # If signal is negative-going, however, then peak=(pretrigger-min). if self . peak > 0.0 : peak_signal = self . peak else : a = avg_signal [ cut_pre : ns - cut_post ] . min () b = avg_signal [ cut_pre : ns - cut_post ] . max () is_negative = pre_avg - a > b - pre_avg if is_negative : peak_signal = a - pre_avg else : peak_signal = b - pre_avg # avg_signal: normalize to have unit peak avg_signal -= pre_avg rescale = 1 / np . max ( avg_signal ) avg_signal *= rescale avg_signal [: self . n_pretrigger ] = 0.0 avg_signal = avg_signal [ cut_pre : ns - cut_post ] if self . dt_model is None : dt_model = np . array ([], dtype = float ) else : dt_model = self . dt_model * rescale dt_model = dt_model [ cut_pre : ns - cut_post ] return avg_signal , peak_signal , dt_model @staticmethod def _normalize_5lag_filter ( f : np . ndarray , avg_signal : np . ndarray ): \"\"\"Rescale 5-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) <= len ( avg_signal ) - 4 conv = np . zeros ( 5 , dtype = float ) for i in range ( 5 ): conv [ i ] = np . dot ( f , avg_signal [ i : i + len ( f )]) x = np . linspace ( - 2 , 2 , 5 ) fit = np . polyfit ( x , conv , 2 ) fit_ctr = - 0.5 * fit [ 1 ] / fit [ 0 ] fit_peak = np . polyval ( fit , fit_ctr ) f *= 1.0 / fit_peak @staticmethod def _normalize_filter ( f : np . ndarray , avg_signal : np . ndarray ): \"\"\"Rescale single-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) == len ( avg_signal ) f *= 1 / np . dot ( f , avg_signal )","title":"Returns"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_5lag","text":"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed.","title":"compute_5lag"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_5lag--parameters","text":"fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0","title":"Parameters"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_5lag--returns","text":"Filter A 5-lag optimal filter.","title":"Returns"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_5lag--raises","text":"ValueError Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) # Time domain filters shorten = 2 truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_sig = TS ( truncated_signal ) Rinv_1 = TS ( np . ones ( n )) filt_noconst = Rinv_1 . sum () * Rinv_sig - Rinv_sig . sum () * Rinv_1 band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post )","title":"Raises"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_ats","text":"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed.","title":"compute_ats"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_ats--parameters","text":"fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0","title":"Parameters"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_ats--returns","text":"Filter An arrival-time-safe optimal filter.","title":"Returns"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_ats--raises","text":"ValueError Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , self . noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post )","title":"Raises"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_fourier","text":"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of fmax and f_3db are allowed.","title":"compute_fourier"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_fourier--parameters","text":"fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0","title":"Parameters"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_fourier--returns","text":"Filter A 5-lag optimal filter, computed in the Fourier domain.","title":"Returns"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_fourier--raises","text":"ValueError Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , )","title":"Raises"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.create_5lag","text":"create_5lag creates a 5-lag filter directly, first creating, then using and deleting a FilterMaker .","title":"create_5lag"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.create_5lag--parameters","text":"avg_signal : npt.ArrayLike The average signal to be used for filter construction n_pretrigger : int Number of samples before the trigger noise_psd : npt.ArrayLike Noise power-spectral density (starting at 0 and ending at the critical/Nyquist frequency) noise_autocorr_vec : npt.ArrayLike Noise autocorrelation dt : float Sampling period fmax : Optional[float], optional A hard low-pass limit, Fourier frequencies above this (in Hz) will be given zero amplitude, by default None f_3db : Optional[float], optional A soft low-pass limit, Fourier frequencies above this (in Hz) will be rolled off by a 1-pole filter with this 3 dB point (in Hz), by default None","title":"Parameters"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.create_5lag--returns","text":"Filter A 5-lag optimal filter. Source code in mass2/core/optimal_filtering.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 @classmethod def create_5lag ( cls , avg_signal : npt . ArrayLike , n_pretrigger : int , noise_psd : npt . ArrayLike , noise_autocorr_vec : npt . ArrayLike , dt : float , fmax : float | None = None , f_3db : float | None = None , ): \"\"\"create_5lag creates a 5-lag filter directly, first creating, then using and deleting a `FilterMaker`. Parameters ---------- avg_signal : npt.ArrayLike The average signal to be used for filter construction n_pretrigger : int Number of samples before the trigger noise_psd : npt.ArrayLike Noise power-spectral density (starting at 0 and ending at the critical/Nyquist frequency) noise_autocorr_vec : npt.ArrayLike Noise autocorrelation dt : float Sampling period fmax : Optional[float], optional A hard low-pass limit, Fourier frequencies above this (in Hz) will be given zero amplitude, by default None f_3db : Optional[float], optional A soft low-pass limit, Fourier frequencies above this (in Hz) will be rolled off by a 1-pole filter with this 3 dB point (in Hz), by default None Returns ------- Filter A 5-lag optimal filter. \"\"\" avg_signal = np . asarray ( avg_signal ) peak_signal = np . amax ( avg_signal ) - avg_signal [ 0 ] maker = cls ( avg_signal , n_pretrigger , noise_psd = noise_psd , noise_autocorr = noise_autocorr_vec , sample_time_sec = dt , peak = peak_signal , ) return maker . compute_5lag ( fmax = fmax , f_3db = f_3db )","title":"Returns"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener","text":"An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: tw.whiten(v) returns Wv; it is equivalent to tw(v) tw.solveWT(v) returns inv(W')*v tw.applyWT(v) returns W'v tw.solveW(v) returns inv(W)*v","title":"ToeplitzWhitener"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener--arguments","text":"theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients","title":"Arguments"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener--returns","text":"ToeplitzWhitener Object that can perform approximate, time-invariant noise whitening.","title":"Returns"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener--raises","text":"ValueError If the operative methods are passed an array of dimension higher than 2. Source code in mass2/core/optimal_filtering.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 @dataclass ( frozen = True ) class ToeplitzWhitener : \"\"\"An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: * `tw.whiten(v)` returns Wv; it is equivalent to `tw(v)` * `tw.solveWT(v)` returns inv(W')*v * `tw.applyWT(v)` returns W'v * `tw.solveW(v)` returns inv(W)*v Arguments --------- theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients Returns ------- ToeplitzWhitener Object that can perform approximate, time-invariant noise whitening. Raises ------ ValueError If the operative methods are passed an array of dimension higher than 2. \"\"\" theta : np . ndarray phi : np . ndarray @property def p ( self ): return len ( self . phi ) - 1 @property def q ( self ): return len ( self . theta ) - 1 def whiten ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v ) def __call__ ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y def solveW ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y def solveWT ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :] def applyWT ( self , v : npt . ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :] def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR )","title":"Raises"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.W","text":"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. Source code in mass2/core/optimal_filtering.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR )","title":"W"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.__call__","text":"Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def __call__ ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y","title":"__call__"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.applyWT","text":"Return vector (or matrix of column vectors) W'v Source code in mass2/core/optimal_filtering.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def applyWT ( self , v : npt . ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :]","title":"applyWT"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.solveW","text":"Return unwhitened vector (or matrix of column vectors) inv(W)*v Source code in mass2/core/optimal_filtering.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def solveW ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y","title":"solveW"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.solveWT","text":"Return vector (or matrix of column vectors) inv(W')*v Source code in mass2/core/optimal_filtering.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def solveWT ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :]","title":"solveWT"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.whiten","text":"Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 65 66 67 def whiten ( self , v : npt . ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v )","title":"whiten"},{"location":"docstrings/#mass2.core.optimal_filtering.band_limit","text":"Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input modelmatrix in-place. No effect if both fmax and f_3db are None .","title":"band_limit"},{"location":"docstrings/#mass2.core.optimal_filtering.band_limit--parameters","text":"modelmatrix : np.ndarray The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec : float The sampling period, normally in seconds. fmax : Optional[float] The hard maximum frequency (units are inverse of sample_time_sec units, or Hz) f_3db : Optional[float] The 1-pole low-pass filter's 3 dB point (same units as fmax ) Source code in mass2/core/optimal_filtering.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def band_limit ( modelmatrix : np . ndarray , sample_time_sec : float , fmax : float | None , f_3db : float | None ): \"\"\"Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input `modelmatrix` in-place. No effect if both `fmax` and `f_3db` are `None`. Parameters ---------- modelmatrix : np.ndarray The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec : float The sampling period, normally in seconds. fmax : Optional[float] The hard maximum frequency (units are inverse of `sample_time_sec` units, or Hz) f_3db : Optional[float] The 1-pole low-pass filter's 3 dB point (same units as `fmax`) \"\"\" if fmax is None and f_3db is None : return # Handle the 2D case by calling this function once per column. assert len ( modelmatrix . shape ) <= 2 if len ( modelmatrix . shape ) == 2 : for i in range ( modelmatrix . shape [ 1 ]): band_limit ( modelmatrix [:, i ], sample_time_sec , fmax , f_3db ) return vector = modelmatrix filt_length = len ( vector ) sig_ft = np . fft . rfft ( vector ) freq = np . fft . fftfreq ( filt_length , d = sample_time_sec ) freq = np . abs ( freq [: len ( sig_ft )]) if fmax is not None : sig_ft [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft /= 1.0 + ( 1.0 * freq / f_3db ) ** 2 # n=filt_length is needed when filt_length is ODD vector [:] = np . fft . irfft ( sig_ft , n = filt_length )","title":"Parameters"},{"location":"docstrings/#mass2.core.optimal_filtering.bracketR","text":"Return the dot product (q^T R q) for vector and matrix R constructed from the vector by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). Source code in mass2/core/optimal_filtering.py 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 def bracketR ( q , noise ): \"\"\"Return the dot product (q^T R q) for vector <q> and matrix R constructed from the vector <noise> by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). \"\"\" if len ( noise ) < len ( q ): raise ValueError ( f \"Vector q (length { len ( q ) } ) cannot be longer than the noise (length { len ( noise ) } )\" ) n = len ( q ) r = np . zeros ( 2 * n - 1 , dtype = float ) r [ n - 1 :] = noise [: n ] r [ n - 1 :: - 1 ] = noise [: n ] dot = 0.0 for i in range ( n ): dot += q [ i ] * r [ n - i - 1 : 2 * n - i - 1 ] . dot ( q ) return dot","title":"bracketR"},{"location":"filtering/","text":"Optimal filtering The FilterMaker interface The optimal filtering interface FilterMaker is based on Python dataclasses , a modern approach to Python objects based on having set of attributes fixed at creation time. When the dataclass is \"frozen\" (as in this case), it also does not allow changing the values of these attributes. Our intention with the new API is offer a range of objects that can perform optimal filtering, or create objects that do, but rejecting the proliferation of incompatible features that used to appear in slightly different flavors of filters. This API consists of two key objects: The Filter is a specific implementation of an optimal filter, designed to be used in one-lag or five-lag mode, and with fixed choices about low-pass filtering of the filter's values, or about giving zero weight to a number of initial or final samples in a record. Offers a filter_records(r) method to apply its optimal filter to one or more pulse records r . When r is a 2d array, each row corresponds to a pulse record. The FilterMaker contains a model of one channel's signal and noise. It is able to create various objects of the type Filter (or subtypes thereof). The user first creates a FilterMaker from the analyzed noise and signal, then uses it to generate an optimal filter (from a subclass of Filter ) with the desired properties. That object has a method filter_records(...) . Usage looks like the following: import numpy as np import mass2 n = 500 Maxsignal = 1000.0 sigma_noise = 1.0 tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) F5 = maker.compute_5lag() print(f\"Filter peak value: {F5.nominal_peak:.1f}\") print(f\"Filter rms value: {F5.variance**0.5:.4f}\") print(f\"Filter predicted V/dV (FWHM): {F5.predicted_v_over_dv:.4f}\") This code should produce an optimal filter F5 and a filter maker maker and generate the following output: Filter peak value: 1000.0 Filter rms value: 0.1549 Filter predicted V/dV (FWHM): 2741.6517 A test of normalization and filter variance import numpy as np import mass2 def verify_close(x, y, rtol=1e-5, topic=None): if topic is not None: print(f\"Checking {topic:20s}: \", end=\"\") isclose = np.isclose(x, y, rtol=rtol) print(f\"x={x:.4e}, y={y:.4e} are close to each other? {isclose}\") assert isclose def test_mass_5lag_filters(Maxsignal=100.0, sigma_noise=1.0, n=500): tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) F5 = maker.compute_5lag() # Check filter's normalization f = F5.values verify_close(Maxsignal, f.dot(truncated_signal), rtol=1e-5, topic = \"Filter normalization\") # Check filter's variance expected_dV = sigma_noise / n**0.5 * signal.max()/truncated_signal.std() verify_close(expected_dV, F5.variance**0.5, rtol=1e-5, topic=\"Expected variance\") # Check filter's V/dV calculation fwhm_sigma_ratio = np.sqrt(8*np.log(2)) expected_V_dV = Maxsignal / (expected_dV * fwhm_sigma_ratio) verify_close(expected_V_dV, F5.predicted_v_over_dv, rtol=1e-5, topic=\"Expected V/\\u03b4v\") print() test_mass_5lag_filters(100, 1.0, 500) test_mass_5lag_filters(400, 1.0, 500) test_mass_5lag_filters(100, 1.0, 1000) test_mass_5lag_filters(100, 2.0, 500) These four tests should yield the following output: Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=2.7417e+02, y=2.7417e+02 are close to each other? True Checking Filter normalization: x=4.0000e+02, y=4.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.0967e+03, y=1.0967e+03 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.0963e-01, y=1.0963e-01 are close to each other? True Checking Expected V/\u03b4v : x=3.8734e+02, y=3.8734e+02 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=3.0978e-01, y=3.0978e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.3708e+02, y=1.3708e+02 are close to each other? True","title":"Optimal filtering"},{"location":"filtering/#optimal-filtering","text":"","title":"Optimal filtering"},{"location":"filtering/#the-filtermaker-interface","text":"The optimal filtering interface FilterMaker is based on Python dataclasses , a modern approach to Python objects based on having set of attributes fixed at creation time. When the dataclass is \"frozen\" (as in this case), it also does not allow changing the values of these attributes. Our intention with the new API is offer a range of objects that can perform optimal filtering, or create objects that do, but rejecting the proliferation of incompatible features that used to appear in slightly different flavors of filters. This API consists of two key objects: The Filter is a specific implementation of an optimal filter, designed to be used in one-lag or five-lag mode, and with fixed choices about low-pass filtering of the filter's values, or about giving zero weight to a number of initial or final samples in a record. Offers a filter_records(r) method to apply its optimal filter to one or more pulse records r . When r is a 2d array, each row corresponds to a pulse record. The FilterMaker contains a model of one channel's signal and noise. It is able to create various objects of the type Filter (or subtypes thereof). The user first creates a FilterMaker from the analyzed noise and signal, then uses it to generate an optimal filter (from a subclass of Filter ) with the desired properties. That object has a method filter_records(...) . Usage looks like the following: import numpy as np import mass2 n = 500 Maxsignal = 1000.0 sigma_noise = 1.0 tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) F5 = maker.compute_5lag() print(f\"Filter peak value: {F5.nominal_peak:.1f}\") print(f\"Filter rms value: {F5.variance**0.5:.4f}\") print(f\"Filter predicted V/dV (FWHM): {F5.predicted_v_over_dv:.4f}\") This code should produce an optimal filter F5 and a filter maker maker and generate the following output: Filter peak value: 1000.0 Filter rms value: 0.1549 Filter predicted V/dV (FWHM): 2741.6517","title":"The FilterMaker interface"},{"location":"filtering/#a-test-of-normalization-and-filter-variance","text":"import numpy as np import mass2 def verify_close(x, y, rtol=1e-5, topic=None): if topic is not None: print(f\"Checking {topic:20s}: \", end=\"\") isclose = np.isclose(x, y, rtol=rtol) print(f\"x={x:.4e}, y={y:.4e} are close to each other? {isclose}\") assert isclose def test_mass_5lag_filters(Maxsignal=100.0, sigma_noise=1.0, n=500): tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) F5 = maker.compute_5lag() # Check filter's normalization f = F5.values verify_close(Maxsignal, f.dot(truncated_signal), rtol=1e-5, topic = \"Filter normalization\") # Check filter's variance expected_dV = sigma_noise / n**0.5 * signal.max()/truncated_signal.std() verify_close(expected_dV, F5.variance**0.5, rtol=1e-5, topic=\"Expected variance\") # Check filter's V/dV calculation fwhm_sigma_ratio = np.sqrt(8*np.log(2)) expected_V_dV = Maxsignal / (expected_dV * fwhm_sigma_ratio) verify_close(expected_V_dV, F5.predicted_v_over_dv, rtol=1e-5, topic=\"Expected V/\\u03b4v\") print() test_mass_5lag_filters(100, 1.0, 500) test_mass_5lag_filters(400, 1.0, 500) test_mass_5lag_filters(100, 1.0, 1000) test_mass_5lag_filters(100, 2.0, 500) These four tests should yield the following output: Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=2.7417e+02, y=2.7417e+02 are close to each other? True Checking Filter normalization: x=4.0000e+02, y=4.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.0967e+03, y=1.0967e+03 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.0963e-01, y=1.0963e-01 are close to each other? True Checking Expected V/\u03b4v : x=3.8734e+02, y=3.8734e+02 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=3.0978e-01, y=3.0978e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.3708e+02, y=1.3708e+02 are close to each other? True","title":"A test of normalization and filter variance"},{"location":"fluorescence/","text":"Fluorescence Lines Mass includes numerous features to help you analyze and model the fluorescence emission of various elements. Mass can Approximate the shape of the fluorescence line emission for certain lines (particularly the K-alpha and K-beta lines of elements from Mg to Zn, or Z=12 to 30). Generate random deviates, drawn from these same energy distributions. Fit a measured spectrum to on of these energy distributions. Examples 1. Plot the distribution Objects of the SpectralLine class are callable, and return their PDF given the energy as an array or scalar argument. import mass2 import numpy as np import pylab as plt spectrum = mass2.spectra[\"MnKAlpha\"] plt.clf() axis=plt.gca() for fwhm in (3,4,5,6,8,10): spectrum.plot(axis=axis,components=False,label=\"{}\".format(fwhm),setylim=False,instrument_gaussian_fwhm=fwhm); plt.legend(loc=\"upper left\") plt.title(\"Mn K$\\\\alpha$ distribution at various resolutions\") plt.xlabel(\"Energy (eV)\") 2. Generate random deviates from a fluorescence line shape Objects of the SpectralLine class roughly copy the API of the scipy type scipy.stats.rv_continuous and offer some of the methods, such as pdf , rvs .: energies0 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=0) energies3 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=3) plt.clf() contents0, bin_edges0, _ = plt.hist(energies0, 200, range=[5820,5960], histtype=\"step\") contents3, bin_edges3, _ = plt.hist(energies3, 200, range=[5820,5960], histtype=\"step\") bin_ctr = bin_edges3[:-1] + 0.5 * (bin_edges3[1] - bin_edges3[0]) plt.xlabel(\"Energy (eV)\") plt.ylabel(\"Counts per bin\") 3. Fit data to a fluorescence line model model = mass2.spectra[\"MnKAlpha\"].model() guess_params = model.guess(contents3, bin_ctr, dph_de=1.0) result = model.fit(contents3, guess_params, bin_centers=bin_ctr) result.plotm() print(result.best_values) fwhm = result.params[\"fwhm\"] print(f\"Estimated resolution (FWHM) = {fwhm.value}\u00b1{fwhm.stderr}\")","title":"Fluorescence lines"},{"location":"fluorescence/#fluorescence-lines","text":"Mass includes numerous features to help you analyze and model the fluorescence emission of various elements. Mass can Approximate the shape of the fluorescence line emission for certain lines (particularly the K-alpha and K-beta lines of elements from Mg to Zn, or Z=12 to 30). Generate random deviates, drawn from these same energy distributions. Fit a measured spectrum to on of these energy distributions.","title":"Fluorescence Lines"},{"location":"fluorescence/#examples","text":"","title":"Examples"},{"location":"fluorescence/#1-plot-the-distribution","text":"Objects of the SpectralLine class are callable, and return their PDF given the energy as an array or scalar argument. import mass2 import numpy as np import pylab as plt spectrum = mass2.spectra[\"MnKAlpha\"] plt.clf() axis=plt.gca() for fwhm in (3,4,5,6,8,10): spectrum.plot(axis=axis,components=False,label=\"{}\".format(fwhm),setylim=False,instrument_gaussian_fwhm=fwhm); plt.legend(loc=\"upper left\") plt.title(\"Mn K$\\\\alpha$ distribution at various resolutions\") plt.xlabel(\"Energy (eV)\")","title":"1. Plot the distribution"},{"location":"fluorescence/#2-generate-random-deviates-from-a-fluorescence-line-shape","text":"Objects of the SpectralLine class roughly copy the API of the scipy type scipy.stats.rv_continuous and offer some of the methods, such as pdf , rvs .: energies0 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=0) energies3 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=3) plt.clf() contents0, bin_edges0, _ = plt.hist(energies0, 200, range=[5820,5960], histtype=\"step\") contents3, bin_edges3, _ = plt.hist(energies3, 200, range=[5820,5960], histtype=\"step\") bin_ctr = bin_edges3[:-1] + 0.5 * (bin_edges3[1] - bin_edges3[0]) plt.xlabel(\"Energy (eV)\") plt.ylabel(\"Counts per bin\")","title":"2. Generate random deviates from a fluorescence line shape"},{"location":"fluorescence/#3-fit-data-to-a-fluorescence-line-model","text":"model = mass2.spectra[\"MnKAlpha\"].model() guess_params = model.guess(contents3, bin_ctr, dph_de=1.0) result = model.fit(contents3, guess_params, bin_centers=bin_ctr) result.plotm() print(result.best_values) fwhm = result.params[\"fwhm\"] print(f\"Estimated resolution (FWHM) = {fwhm.value}\u00b1{fwhm.stderr}\")","title":"3. Fit data to a fluorescence line model"},{"location":"hci_lines_from_asd/","text":"Highly Charged Ion (HCI) Lines from NIST ASD Motivation We often find ourselves hard coding line center positions into mass, which is prone to errors and can be tedious when there are many lines of interest to insert. In addition, the line positions would need to be manually updated for any changes in established results. In the case of highly charged ions, such as those produced in an electron beam ion trap (EBIT), there is a vast number of potential lines coming from almost any charge state of almost any element. Luckily, these lines are well documented through the NIST Atomic Spectral Database (ASD). Here, we have parsed a NIST ASD SQL dump and converted it into an easily Python readable pickle file. The hci_lines.py module implements the NIST_ASD class, which loads that pickle file and contains useful functions for working with the ASD data. It also automatically adds in some of the more common HCI lines that we commonly use in our EBIT data analyses. Exploring the methods of class NIST_ASD The class NIST_ASD can be initialized without arguments if the user wants to use the default ASD pickle file. This file is located at mass/calibration/nist_asd.pickle. A custom pickle file can be used by passing in the pickleFilename argument during initialization. The methods of the NIST_ASD class are described below: Usage examples Next, we will demonstrate usage of these methods with the example of Ne, a commonly injected gas at the NIST EBIT. import mass2 import mass2.calibration.hci_lines import numpy as numpy import pylab as plt test_asd = mass2.calibration.hci_lines.NIST_ASD() availableElements = test_asd.getAvailableElements() assert 'Ne' in availableElements availableNeCharges = test_asd.getAvailableSpectralCharges(element='Ne') assert 10 in availableNeCharges subsetNe10Levels = test_asd.getAvailableLevels(element='Ne', spectralCharge=10, maxLevels=6, getUncertainty=False) assert '2p 2P* J=1/2' in list(subsetNe10Levels.keys()) exampleNeLevel = test_asd.getSingleLevel(element='Ne', spectralCharge=10, conf='2p', term='2P*', JVal='1/2', getUncertainty=False) print(availableElements[:10]) print(availableNeCharges) for k, v in subsetNe10Levels.items(): subsetNe10Levels[k] = round(v, 1) print(subsetNe10Levels) print(f'{exampleNeLevel:.1f}') [np.str_('Sn'), np.str_('Cu'), np.str_('Na'), np.str_('As'), np.str_('Zn'), np.str_('Ne'), np.str_('Ge'), np.str_('Ga'), np.str_('Rb'), np.str_('Se')] [9, 1, 2, 3, 4, 5, 6, 7, 8, 10] {'1s 2S J=1/2': 0.0, '2p 2P* J=1/2': 1021.5, '2s 2S J=1/2': 1021.5, '2p 2P* J=3/2': 1022.0, '3p 2P* J=1/2': 1210.8, '3s 2S J=1/2': 1210.8} 1021.5 Functions for generating SpectralLine objects from ASD data The module also contains some functions outside of the NIST_ASD class that are useful for integration with MASS. First, the add_hci_line function which, takes arguments that are relevant in HCI work, including as element , spectr_ch , energies , widths , and ratios . The function calls mass2.calibration.fluorescence_lines.addline , generates a line name with the given parameters, and populates the various fields. As an example, let us create a H-like Be line. Here, we assume a lorentzian width of 0.1 eV. test_element = 'Be' test_charge = 4 test_conf = '2p' test_term = '2P*' test_JVal = '3/2' test_level = f'{test_conf} {test_term} J={test_JVal}' test_energy = test_asd.getSingleLevel( element=test_element, spectralCharge=test_charge, conf=test_conf, term=test_term, JVal=test_JVal, getUncertainty=False) test_line = mass2.calibration.hci_lines.add_hci_line(element=test_element, spectr_ch=test_charge, line_identifier=test_level, energies=[test_energy], widths=[0.1], ratios=[1.0]) assert test_line.peak_energy == test_energy print(mass2.spectra[f'{test_element}{test_charge} {test_conf} {test_term} J={test_JVal}']) print(f'{test_line.peak_energy:.1f}') SpectralLine: Be4 2p 2P* J=3/2 163.3 The name format for grabbing the line from mass2.spectra is shown above. The transition is uniquely specified by the element, charge, configuration, term, and J value. Below, we show what this line looks like assuming a zero-width Gaussian component. .. testcode:: test_line.plot() .. testcode:: :hide: plt.savefig(\"img/Be4_line_example.png\");plt.close() .. image:: img/Be4_line_example.png :width: 40% The module contains two other functions which are used to easily generate some lines from levels that are commonly observed at the NIST EBIT. These functions are add_H_like_lines_from_asd and add_He_like_lines_from_asd . As the names imply, these functions add H- and He-like lines to mass using the data in the ASD pickle. These functions require the asd and element arguments and also contain the optional maxLevels argument, which works similarly as the argument in the class methods. The module also automatically adds H- and He-like lines for the most commonly used elements, which includes 'N', 'O', 'Ne', and 'Ar'. Below, we check that common elements are being added as spectralLine objects and then add some of the lower order H- and He-like Ga lines. print([mass2.spectra['Ne10 2p 2P* J=3/2'], round(mass2.spectra['Ne10 2p 2P* J=3/2'].peak_energy,1)]) print([mass2.spectra['O7 1s.2p 1P* J=1'], round(mass2.spectra['O7 1s.2p 1P* J=1'].peak_energy,1)]) test_element = 'Ga' HLikeGaLines = mass2.calibration.hci_lines.add_H_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=6) HeLikeGaLines = mass2.calibration.hci_lines.add_He_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=7) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HLikeGaLines]) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HeLikeGaLines]) [SpectralLine: Ne10 2p 2P* J=3/2, np.float64(1022.0)] [SpectralLine: O7 1s.2p 1P* J=1, np.float64(574.0)] [[SpectralLine: Ga31 2p 2P* J=1/2, np.float64(9917.0)], [SpectralLine: Ga31 2s 2S J=1/2, np.float64(9918.0)], [SpectralLine: Ga31 2p 2P* J=3/2, np.float64(9960.3)], [SpectralLine: Ga31 3p 2P* J=1/2, np.float64(11767.7)], [SpectralLine: Ga31 3s 2S J=1/2, np.float64(11768.0)], [SpectralLine: Ga31 3d 2D J=3/2, np.float64(11780.5)]] [[SpectralLine: Ga30 1s.2s 3S J=1, np.float64(9535.6)], [SpectralLine: Ga30 1s.2p 3P* J=0, np.float64(9571.8)], [SpectralLine: Ga30 1s.2p 3P* J=1, np.float64(9574.4)], [SpectralLine: Ga30 1s.2s 1S J=0, np.float64(9574.6)], [SpectralLine: Ga30 1s.2p 3P* J=2, np.float64(9607.4)], [SpectralLine: Ga30 1s.2p 1P* J=1, np.float64(9628.2)], [SpectralLine: Ga30 1s.3s 3S J=1, np.float64(11304.6)]] hci_lines.py Uses pickle file containing NIST ASD levels data to generate some commonly used HCI lines in mass. Meant to be a replacement for _highly_charged_ion_lines.py, which hard codes in line parameters. The pickle file can be gzip-compressed, provided the compressed filename ends with \".gz\". February 2020 Paul Szypryt NIST_ASD Class for working with a pickled atomic spectra database Source code in mass2/calibration/hci_lines.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class NIST_ASD : \"\"\"Class for working with a pickled atomic spectra database\"\"\" def __init__ ( self , pickleFilename = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Args: pickleFilename: (default None) ASD pickle file name, as str, if not using default \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], DEFAULT_PICKLE_PATH ) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) def getAvailableElements ( self ): \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) def getAvailableSpectralCharges ( self , element ): \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Args: element: str representing atomic symbol of element, e.g. 'Ne' \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) def getAvailableLevels ( self , element , spectralCharge , requiredConf = None , requiredTerm = None , requiredJVal = None , maxLevels = None , units = \"eV\" , getUncertainty = True , ): \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Args: element: str representing atomic symbol of element, e.g. 'Ne' spectralCharge: int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf: (default None) filters results to those with ``conf == requiredConf`` requiredTerm: (default None) filters results to those with ``term == requiredTerm`` requiredJVal: (default None) filters results to those with ``JVal == requiredJVal`` maxLevels: (default None) the maximum number of levels (sorted by energy) to return units: (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values \"\"\" spectralCharge = int ( spectralCharge ) levelsDict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeConf = False includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] if requiredConf is None : includeConf = True elif conf == requiredConf : includeConf = True if requiredTerm is None : includeTerm = True elif term == requiredTerm : includeTerm = True if requiredJVal is None : includeJVal = True elif JVal == requiredJVal : includeJVal = True # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] else : levelsDict = None print ( \"Unit type not supported, please use eV or cm-1\" ) except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict def getSingleLevel ( self , element , spectralCharge , conf , term , JVal , units = \"eV\" , getUncertainty = True ): \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Args: element: str representing atomic symbol of element, e.g. 'Ne' spectralCharge: int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf: str representing nuclear configuration, e.g. '2p' term: str representing nuclear term, e.g. '2P*' JVal: str representing total angular momentum J, e.g. '3/2' units: (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values getUncertainty: (default True) if True, includes uncertainties in list of levels \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : levelEnergy = None print ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy __init__ ( pickleFilename = None ) Loads ASD pickle file (optionally gzipped) Parameters: pickleFilename \u2013 (default None) ASD pickle file name, as str, if not using default Source code in mass2/calibration/hci_lines.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , pickleFilename = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Args: pickleFilename: (default None) ASD pickle file name, as str, if not using default \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], DEFAULT_PICKLE_PATH ) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) getAvailableElements () Returns a list of all available elements from the ASD pickle file Source code in mass2/calibration/hci_lines.py 48 49 50 51 def getAvailableElements ( self ): \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) getAvailableLevels ( element , spectralCharge , requiredConf = None , requiredTerm = None , requiredJVal = None , maxLevels = None , units = 'eV' , getUncertainty = True ) For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters: element \u2013 str representing atomic symbol of element, e.g. 'Ne' spectralCharge \u2013 int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf \u2013 (default None) filters results to those with conf == requiredConf requiredTerm \u2013 (default None) filters results to those with term == requiredTerm requiredJVal \u2013 (default None) filters results to those with JVal == requiredJVal maxLevels \u2013 (default None) the maximum number of levels (sorted by energy) to return units \u2013 (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values Source code in mass2/calibration/hci_lines.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def getAvailableLevels ( self , element , spectralCharge , requiredConf = None , requiredTerm = None , requiredJVal = None , maxLevels = None , units = \"eV\" , getUncertainty = True , ): \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Args: element: str representing atomic symbol of element, e.g. 'Ne' spectralCharge: int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf: (default None) filters results to those with ``conf == requiredConf`` requiredTerm: (default None) filters results to those with ``term == requiredTerm`` requiredJVal: (default None) filters results to those with ``JVal == requiredJVal`` maxLevels: (default None) the maximum number of levels (sorted by energy) to return units: (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values \"\"\" spectralCharge = int ( spectralCharge ) levelsDict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeConf = False includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] if requiredConf is None : includeConf = True elif conf == requiredConf : includeConf = True if requiredTerm is None : includeTerm = True elif term == requiredTerm : includeTerm = True if requiredJVal is None : includeJVal = True elif JVal == requiredJVal : includeJVal = True # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] else : levelsDict = None print ( \"Unit type not supported, please use eV or cm-1\" ) except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict getAvailableSpectralCharges ( element ) For a given element, returns a list of all available charge states from the ASD pickle file Parameters: element \u2013 str representing atomic symbol of element, e.g. 'Ne' Source code in mass2/calibration/hci_lines.py 53 54 55 56 57 58 59 60 def getAvailableSpectralCharges ( self , element ): \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Args: element: str representing atomic symbol of element, e.g. 'Ne' \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) getSingleLevel ( element , spectralCharge , conf , term , JVal , units = 'eV' , getUncertainty = True ) Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters: element \u2013 str representing atomic symbol of element, e.g. 'Ne' spectralCharge \u2013 int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf \u2013 str representing nuclear configuration, e.g. '2p' term \u2013 str representing nuclear term, e.g. '2P*' JVal \u2013 str representing total angular momentum J, e.g. '3/2' units \u2013 (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values getUncertainty \u2013 (default True) if True, includes uncertainties in list of levels Source code in mass2/calibration/hci_lines.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def getSingleLevel ( self , element , spectralCharge , conf , term , JVal , units = \"eV\" , getUncertainty = True ): \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Args: element: str representing atomic symbol of element, e.g. 'Ne' spectralCharge: int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf: str representing nuclear configuration, e.g. '2p' term: str representing nuclear term, e.g. '2P*' JVal: str representing total angular momentum J, e.g. '3/2' units: (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values getUncertainty: (default True) if True, includes uncertainties in list of levels \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : levelEnergy = None print ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy hci_models.py Some useful methods for initializing GenericLineModel and CompositeMLEModel objects applied to HCI lines. June 2020 Paul Szypryt add_bg_model ( generic_model , vary_slope = False ) Adds a LinearBackgroundModel to a generic lmfit model Parameters: generic_model \u2013 Generic lmfit model object to which to add a linear background model vary_slope \u2013 (default False) allows a varying linear slope rather than just constant value Source code in mass2/calibration/hci_models.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def add_bg_model ( generic_model , vary_slope = False ): \"\"\"Adds a LinearBackgroundModel to a generic lmfit model Args: generic_model: Generic lmfit model object to which to add a linear background model vary_slope: (default False) allows a varying linear slope rather than just constant value \"\"\" composite_name = generic_model . _name bg_prefix = f \" { composite_name } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) background_model = mass2 . calibration . line_models . LinearBackgroundModel ( name = f \" { composite_name } Background\" , prefix = bg_prefix ) background_model . set_param_hint ( \"bg_slope\" , vary = vary_slope ) composite_model = generic_model + background_model composite_model . name = composite_name return composite_model initialize_HLike_2P_model ( element , conf , has_linear_background = False , has_tails = False , vary_amp_ratio = False ) Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters: element \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' conf \u2013 nuclear configuration as str, e.g. '2p' or '3p' has_linear_background \u2013 (default False) include a single linear background on top of the 2 Lorentzians has_tails \u2013 (default False) include low energy tail in the model vary_amp_ratio \u2013 (default False) allow the ratio of the J=3/2 to J=1/2 states to vary away from 2 Source code in mass2/calibration/hci_models.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def initialize_HLike_2P_model ( element , conf , has_linear_background = False , has_tails = False , vary_amp_ratio = False ): \"\"\"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Args: element: atomic symbol as str, e.g. 'Ne' or 'Ar' conf: nuclear configuration as str, e.g. '2p' or '3p' has_linear_background: (default False) include a single linear background on top of the 2 Lorentzians has_tails: (default False) include low energy tail in the model vary_amp_ratio: (default False) allow the ratio of the J=3/2 to J=1/2 states to vary away from 2 \"\"\" # Set up line names and lmfit prefixes charge = int ( xraydb . atomic_number ( element )) line_name_1_2 = f \" { element }{ charge } { conf } 2P* J=1/2\" line_name_3_2 = f \" { element }{ charge } { conf } 2P* J=3/2\" prefix_1_2 = f \" { line_name_1_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) prefix_3_2 = f \" { line_name_3_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) # Initialize individual lines and models line_1_2 = spectra [ line_name_1_2 ]() line_3_2 = spectra [ line_name_3_2 ]() model_1_2 = line_1_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_1_2 ) model_3_2 = line_3_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_3_2 ) # Initialize composite model and set addition H-like constraints composite_name = f \" { element }{ charge } { conf } \" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = [ model_1_2 , model_3_2 ], has_linear_background = has_linear_background , peak_component_name = line_name_3_2 , ) amp_ratio_param_name = f \" { element }{ charge } _ { conf } _amp_ratio\" composite_model . set_param_hint ( name = amp_ratio_param_name , value = 0.5 , min = 0.0 , vary = vary_amp_ratio ) composite_model . set_param_hint ( f \" { prefix_1_2 } integral\" , expr = f \" { prefix_3_2 } integral * { amp_ratio_param_name } \" ) return composite_model initialize_HeLike_complex_model ( element , has_linear_background = False , has_tails = False , additional_line_names = []) Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P J=1, and 1s.2p 1P J=1 lines. Parameters: element \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background \u2013 (default False) include a single linear background on top of the Lorentzian models has_tails \u2013 (default False) include low energy tail in the model additional_line_names \u2013 (default []) additional line names to include in model, e.g. low level Li/Be-like features Source code in mass2/calibration/hci_models.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def initialize_HeLike_complex_model ( element , has_linear_background = False , has_tails = False , additional_line_names = []): \"\"\"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P* J=1, and 1s.2p 1P* J=1 lines. Args: element: atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background: (default False) include a single linear background on top of the Lorentzian models has_tails: (default False) include low energy tail in the model additional_line_names: (default []) additional line names to include in model, e.g. low level Li/Be-like features \"\"\" # Set up line names charge = int ( xraydb . atomic_number ( element ) - 1 ) line_name_1s2s_3S = f \" { element }{ charge } 1s.2s 3S J=1\" line_name_1s2p_3P = f \" { element }{ charge } 1s.2p 3P* J=1\" line_name_1s2p_1P = f \" { element }{ charge } 1s.2p 1P* J=1\" line_names = np . hstack ([[ line_name_1s2s_3S , line_name_1s2p_3P , line_name_1s2p_1P ], additional_line_names ]) # Set up lines and models based on line_names # individual_lines = [spectra[i_line_name]() for i_line_name in line_names] individual_models = [ initialize_hci_line_model ( i_line_name , has_linear_background = False , has_tails = has_tails ) for i_line_name in line_names ] # Set up composite model composite_name = f \" { element }{ charge } 1s2s_2p Complex\" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = individual_models , has_linear_background = has_linear_background , peak_component_name = line_name_1s2p_1P , ) return composite_model initialize_hci_composite_model ( composite_name , individual_models , has_linear_background = False , peak_component_name = None ) Initializes composite lmfit model from the sum of input models Parameters: composite_name \u2013 str name given to composite line model line_models \u2013 array of lmfit models to sum into a composite model has_linear_background \u2013 (default False) include a single linear background on top of group of lorentzians peak_component_name \u2013 designate a component to be a peak for energy, all expressions are referenced to this component Source code in mass2/calibration/hci_models.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def initialize_hci_composite_model ( composite_name , individual_models , has_linear_background = False , peak_component_name = None ): \"\"\"Initializes composite lmfit model from the sum of input models Args: composite_name: str name given to composite line model line_models: array of lmfit models to sum into a composite model has_linear_background: (default False) include a single linear background on top of group of lorentzians peak_component_name: designate a component to be a peak for energy, all expressions are referenced to this component \"\"\" composite_model = np . sum ( individual_models ) composite_model . name = composite_name if has_linear_background : composite_model = add_bg_model ( composite_model ) # Workaround for energy calibration using composite models, pick 1st GenericLineModel component line_model_components = [ i_comp for i_comp in composite_model . components if isinstance ( i_comp , mass2 . calibration . line_models . GenericLineModel ) ] if peak_component_name is None : peak_component_name = line_model_components [ 0 ] . _name peak_component_index = [ i_comp . _name for i_comp in line_model_components ] . index ( peak_component_name ) peak_component = line_model_components [ peak_component_index ] composite_model . peak_prefix = peak_component . prefix composite_model . peak_energy = peak_component . spect . peak_energy # Set up some constraints relative to peak_component num_line_components = len ( line_model_components ) line_component_prefixes = [ iComp . prefix for iComp in line_model_components ] line_component_energies = [ iComp . spect . peak_energy for iComp in line_model_components ] for i in np . arange ( num_line_components ): if i != peak_component_index : # Single fwhm across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } fwhm\" , expr = f \" { composite_model . peak_prefix } fwhm\" ) # Single dph_de across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } dph_de\" , expr = f \" { composite_model . peak_prefix } dph_de\" ) # Fixed energy separation based on database values separation = line_component_energies [ i ] - composite_model . peak_energy hint = f \"( { separation } * { composite_model . peak_prefix } dph_de) + { composite_model . peak_prefix } peak_ph\" composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } peak_ph\" , expr = hint ) composite_model . shortname = composite_name return composite_model initialize_hci_line_model ( line_name , has_linear_background = False , has_tails = False ) Initializes a single lorentzian hci lmfit model. Reformats line_name to create a lmfit valid prefix. Parameters: line_name \u2013 name of line within mass2.spectra has_linear_background \u2013 (default False) include linear background in the model has_tails \u2013 (default False) include low energy tail in the model Source code in mass2/calibration/hci_models.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def initialize_hci_line_model ( line_name , has_linear_background = False , has_tails = False ): \"\"\"Initializes a single lorentzian hci lmfit model. Reformats line_name to create a lmfit valid prefix. Args: line_name: name of line within mass2.spectra has_linear_background: (default False) include linear background in the model has_tails: (default False) include low energy tail in the model \"\"\" line = spectra [ line_name ]() prefix = f \" { line_name } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) line_model = line . model ( has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix ) line_model . shortname = line_name return line_model models ( has_linear_background = False , has_tails = False , vary_Hlike_amp_ratio = False , additional_Helike_complex_lines = []) Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters: has_linear_background \u2013 (default False) include a single linear background on top of the 2 Lorentzians has_tails \u2013 (default False) include low energy tail in the model vary_Hlike_amp_ratio \u2013 (default False) allow the ratio of the J=3/2 to J=1/2 H-like states to vary additional_Helike_complex_lines: (default []) additional line names to include inHe-like complex model, e.g. low level Li/Be-like features Source code in mass2/calibration/hci_models.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def models ( has_linear_background = False , has_tails = False , vary_Hlike_amp_ratio = False , additional_Helike_complex_lines = []): \"\"\" Generates some commonly used HCI line models that can be used for energy calibration, etc. Args: has_linear_background: (default False) include a single linear background on top of the 2 Lorentzians has_tails: (default False) include low energy tail in the model vary_Hlike_amp_ratio: (default False) allow the ratio of the J=3/2 to J=1/2 H-like states to vary additional_Helike_complex_lines: (default []) additional line names to include inHe-like complex model, e.g. low level Li/Be-like features \"\"\" models_dict = {} # Make some common H-like 2P* models conf_Hlike_2P_dict = {} conf_Hlike_2P_dict [ \"N\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"O\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ne\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ar\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] for i_element in list ( conf_Hlike_2P_dict . keys ()): for i_conf in conf_Hlike_2P_dict [ i_element ]: Hlike_model = initialize_HLike_2P_model ( i_element , i_conf , has_linear_background = has_linear_background , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio , ) models_dict [ Hlike_model . _name ] = Hlike_model # Make some common He-like 1s2s,2p complex and higher order 1p* models # He-like lines Helike_complex_elements = [ \"N\" , \"O\" , \"Ne\" , \"Ar\" ] for i_element in Helike_complex_elements : Helike_model = initialize_HeLike_complex_model ( i_element , has_linear_background = has_linear_background , has_tails = has_tails , additional_line_names = additional_Helike_complex_lines , ) models_dict [ Helike_model . _name ] = Helike_model # 1s.np 1P* lines for n>=3 conf_Helike_1P_dict = {} conf_Helike_1P_dict [ \"N\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"O\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ne\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ar\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] for i_element in list ( conf_Helike_1P_dict . keys ()): i_charge = int ( xraydb . atomic_number ( i_element ) - 1 ) for i_conf in conf_Helike_1P_dict [ i_element ]: Helike_line_name = f \" { i_element }{ i_charge } { i_conf } 1P* J=1\" Helike_model = initialize_hci_line_model ( Helike_line_name , has_linear_background = has_linear_background , has_tails = has_tails ) models_dict [ Helike_model . _name ] = Helike_model # Some more complicated cases # 500 eV region of H-/He-like N N6_1s3p_model = initialize_hci_line_model ( \"N6 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) N7_2p_model = initialize_HLike_2P_model ( \"N\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) N_500eV_model = initialize_hci_composite_model ( \"N 500eV Region\" , [ N6_1s3p_model , N7_2p_model ], has_linear_background = has_linear_background , peak_component_name = \"N7 2p 2P* J=3/2\" , ) models_dict [ N_500eV_model . _name ] = N_500eV_model # 660 eV region of H-/He-like O O8_2p_model = initialize_HLike_2P_model ( \"O\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) O7_1s3p_model = initialize_hci_line_model ( \"O7 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) O_660eV_model = initialize_hci_composite_model ( \"O 660eV Region\" , [ O8_2p_model , O7_1s3p_model ], has_linear_background = has_linear_background , peak_component_name = \"O8 2p 2P* J=3/2\" , ) models_dict [ O_660eV_model . _name ] = O_660eV_model return models_dict","title":"HCI lines"},{"location":"hci_lines_from_asd/#highly-charged-ion-hci-lines-from-nist-asd","text":"","title":"Highly Charged Ion (HCI) Lines from NIST ASD"},{"location":"hci_lines_from_asd/#motivation","text":"We often find ourselves hard coding line center positions into mass, which is prone to errors and can be tedious when there are many lines of interest to insert. In addition, the line positions would need to be manually updated for any changes in established results. In the case of highly charged ions, such as those produced in an electron beam ion trap (EBIT), there is a vast number of potential lines coming from almost any charge state of almost any element. Luckily, these lines are well documented through the NIST Atomic Spectral Database (ASD). Here, we have parsed a NIST ASD SQL dump and converted it into an easily Python readable pickle file. The hci_lines.py module implements the NIST_ASD class, which loads that pickle file and contains useful functions for working with the ASD data. It also automatically adds in some of the more common HCI lines that we commonly use in our EBIT data analyses.","title":"Motivation"},{"location":"hci_lines_from_asd/#exploring-the-methods-of-class-nist_asd","text":"The class NIST_ASD can be initialized without arguments if the user wants to use the default ASD pickle file. This file is located at mass/calibration/nist_asd.pickle. A custom pickle file can be used by passing in the pickleFilename argument during initialization. The methods of the NIST_ASD class are described below:","title":"Exploring the methods of class NIST_ASD"},{"location":"hci_lines_from_asd/#usage-examples","text":"Next, we will demonstrate usage of these methods with the example of Ne, a commonly injected gas at the NIST EBIT. import mass2 import mass2.calibration.hci_lines import numpy as numpy import pylab as plt test_asd = mass2.calibration.hci_lines.NIST_ASD() availableElements = test_asd.getAvailableElements() assert 'Ne' in availableElements availableNeCharges = test_asd.getAvailableSpectralCharges(element='Ne') assert 10 in availableNeCharges subsetNe10Levels = test_asd.getAvailableLevels(element='Ne', spectralCharge=10, maxLevels=6, getUncertainty=False) assert '2p 2P* J=1/2' in list(subsetNe10Levels.keys()) exampleNeLevel = test_asd.getSingleLevel(element='Ne', spectralCharge=10, conf='2p', term='2P*', JVal='1/2', getUncertainty=False) print(availableElements[:10]) print(availableNeCharges) for k, v in subsetNe10Levels.items(): subsetNe10Levels[k] = round(v, 1) print(subsetNe10Levels) print(f'{exampleNeLevel:.1f}') [np.str_('Sn'), np.str_('Cu'), np.str_('Na'), np.str_('As'), np.str_('Zn'), np.str_('Ne'), np.str_('Ge'), np.str_('Ga'), np.str_('Rb'), np.str_('Se')] [9, 1, 2, 3, 4, 5, 6, 7, 8, 10] {'1s 2S J=1/2': 0.0, '2p 2P* J=1/2': 1021.5, '2s 2S J=1/2': 1021.5, '2p 2P* J=3/2': 1022.0, '3p 2P* J=1/2': 1210.8, '3s 2S J=1/2': 1210.8} 1021.5","title":"Usage examples"},{"location":"hci_lines_from_asd/#functions-for-generating-spectralline-objects-from-asd-data","text":"The module also contains some functions outside of the NIST_ASD class that are useful for integration with MASS. First, the add_hci_line function which, takes arguments that are relevant in HCI work, including as element , spectr_ch , energies , widths , and ratios . The function calls mass2.calibration.fluorescence_lines.addline , generates a line name with the given parameters, and populates the various fields. As an example, let us create a H-like Be line. Here, we assume a lorentzian width of 0.1 eV. test_element = 'Be' test_charge = 4 test_conf = '2p' test_term = '2P*' test_JVal = '3/2' test_level = f'{test_conf} {test_term} J={test_JVal}' test_energy = test_asd.getSingleLevel( element=test_element, spectralCharge=test_charge, conf=test_conf, term=test_term, JVal=test_JVal, getUncertainty=False) test_line = mass2.calibration.hci_lines.add_hci_line(element=test_element, spectr_ch=test_charge, line_identifier=test_level, energies=[test_energy], widths=[0.1], ratios=[1.0]) assert test_line.peak_energy == test_energy print(mass2.spectra[f'{test_element}{test_charge} {test_conf} {test_term} J={test_JVal}']) print(f'{test_line.peak_energy:.1f}') SpectralLine: Be4 2p 2P* J=3/2 163.3 The name format for grabbing the line from mass2.spectra is shown above. The transition is uniquely specified by the element, charge, configuration, term, and J value. Below, we show what this line looks like assuming a zero-width Gaussian component. .. testcode:: test_line.plot() .. testcode:: :hide: plt.savefig(\"img/Be4_line_example.png\");plt.close() .. image:: img/Be4_line_example.png :width: 40% The module contains two other functions which are used to easily generate some lines from levels that are commonly observed at the NIST EBIT. These functions are add_H_like_lines_from_asd and add_He_like_lines_from_asd . As the names imply, these functions add H- and He-like lines to mass using the data in the ASD pickle. These functions require the asd and element arguments and also contain the optional maxLevels argument, which works similarly as the argument in the class methods. The module also automatically adds H- and He-like lines for the most commonly used elements, which includes 'N', 'O', 'Ne', and 'Ar'. Below, we check that common elements are being added as spectralLine objects and then add some of the lower order H- and He-like Ga lines. print([mass2.spectra['Ne10 2p 2P* J=3/2'], round(mass2.spectra['Ne10 2p 2P* J=3/2'].peak_energy,1)]) print([mass2.spectra['O7 1s.2p 1P* J=1'], round(mass2.spectra['O7 1s.2p 1P* J=1'].peak_energy,1)]) test_element = 'Ga' HLikeGaLines = mass2.calibration.hci_lines.add_H_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=6) HeLikeGaLines = mass2.calibration.hci_lines.add_He_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=7) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HLikeGaLines]) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HeLikeGaLines]) [SpectralLine: Ne10 2p 2P* J=3/2, np.float64(1022.0)] [SpectralLine: O7 1s.2p 1P* J=1, np.float64(574.0)] [[SpectralLine: Ga31 2p 2P* J=1/2, np.float64(9917.0)], [SpectralLine: Ga31 2s 2S J=1/2, np.float64(9918.0)], [SpectralLine: Ga31 2p 2P* J=3/2, np.float64(9960.3)], [SpectralLine: Ga31 3p 2P* J=1/2, np.float64(11767.7)], [SpectralLine: Ga31 3s 2S J=1/2, np.float64(11768.0)], [SpectralLine: Ga31 3d 2D J=3/2, np.float64(11780.5)]] [[SpectralLine: Ga30 1s.2s 3S J=1, np.float64(9535.6)], [SpectralLine: Ga30 1s.2p 3P* J=0, np.float64(9571.8)], [SpectralLine: Ga30 1s.2p 3P* J=1, np.float64(9574.4)], [SpectralLine: Ga30 1s.2s 1S J=0, np.float64(9574.6)], [SpectralLine: Ga30 1s.2p 3P* J=2, np.float64(9607.4)], [SpectralLine: Ga30 1s.2p 1P* J=1, np.float64(9628.2)], [SpectralLine: Ga30 1s.3s 3S J=1, np.float64(11304.6)]] hci_lines.py Uses pickle file containing NIST ASD levels data to generate some commonly used HCI lines in mass. Meant to be a replacement for _highly_charged_ion_lines.py, which hard codes in line parameters. The pickle file can be gzip-compressed, provided the compressed filename ends with \".gz\". February 2020 Paul Szypryt","title":"Functions for generating SpectralLine objects from ASD data"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD","text":"Class for working with a pickled atomic spectra database Source code in mass2/calibration/hci_lines.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class NIST_ASD : \"\"\"Class for working with a pickled atomic spectra database\"\"\" def __init__ ( self , pickleFilename = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Args: pickleFilename: (default None) ASD pickle file name, as str, if not using default \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], DEFAULT_PICKLE_PATH ) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) def getAvailableElements ( self ): \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) def getAvailableSpectralCharges ( self , element ): \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Args: element: str representing atomic symbol of element, e.g. 'Ne' \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) def getAvailableLevels ( self , element , spectralCharge , requiredConf = None , requiredTerm = None , requiredJVal = None , maxLevels = None , units = \"eV\" , getUncertainty = True , ): \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Args: element: str representing atomic symbol of element, e.g. 'Ne' spectralCharge: int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf: (default None) filters results to those with ``conf == requiredConf`` requiredTerm: (default None) filters results to those with ``term == requiredTerm`` requiredJVal: (default None) filters results to those with ``JVal == requiredJVal`` maxLevels: (default None) the maximum number of levels (sorted by energy) to return units: (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values \"\"\" spectralCharge = int ( spectralCharge ) levelsDict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeConf = False includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] if requiredConf is None : includeConf = True elif conf == requiredConf : includeConf = True if requiredTerm is None : includeTerm = True elif term == requiredTerm : includeTerm = True if requiredJVal is None : includeJVal = True elif JVal == requiredJVal : includeJVal = True # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] else : levelsDict = None print ( \"Unit type not supported, please use eV or cm-1\" ) except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict def getSingleLevel ( self , element , spectralCharge , conf , term , JVal , units = \"eV\" , getUncertainty = True ): \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Args: element: str representing atomic symbol of element, e.g. 'Ne' spectralCharge: int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf: str representing nuclear configuration, e.g. '2p' term: str representing nuclear term, e.g. '2P*' JVal: str representing total angular momentum J, e.g. '3/2' units: (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values getUncertainty: (default True) if True, includes uncertainties in list of levels \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : levelEnergy = None print ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy","title":"NIST_ASD"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.__init__","text":"Loads ASD pickle file (optionally gzipped) Parameters: pickleFilename \u2013 (default None) ASD pickle file name, as str, if not using default Source code in mass2/calibration/hci_lines.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , pickleFilename = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Args: pickleFilename: (default None) ASD pickle file name, as str, if not using default \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], DEFAULT_PICKLE_PATH ) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle )","title":"__init__"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getAvailableElements","text":"Returns a list of all available elements from the ASD pickle file Source code in mass2/calibration/hci_lines.py 48 49 50 51 def getAvailableElements ( self ): \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ())","title":"getAvailableElements"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getAvailableLevels","text":"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters: element \u2013 str representing atomic symbol of element, e.g. 'Ne' spectralCharge \u2013 int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf \u2013 (default None) filters results to those with conf == requiredConf requiredTerm \u2013 (default None) filters results to those with term == requiredTerm requiredJVal \u2013 (default None) filters results to those with JVal == requiredJVal maxLevels \u2013 (default None) the maximum number of levels (sorted by energy) to return units \u2013 (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values Source code in mass2/calibration/hci_lines.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def getAvailableLevels ( self , element , spectralCharge , requiredConf = None , requiredTerm = None , requiredJVal = None , maxLevels = None , units = \"eV\" , getUncertainty = True , ): \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Args: element: str representing atomic symbol of element, e.g. 'Ne' spectralCharge: int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf: (default None) filters results to those with ``conf == requiredConf`` requiredTerm: (default None) filters results to those with ``term == requiredTerm`` requiredJVal: (default None) filters results to those with ``JVal == requiredJVal`` maxLevels: (default None) the maximum number of levels (sorted by energy) to return units: (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values \"\"\" spectralCharge = int ( spectralCharge ) levelsDict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeConf = False includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] if requiredConf is None : includeConf = True elif conf == requiredConf : includeConf = True if requiredTerm is None : includeTerm = True elif term == requiredTerm : includeTerm = True if requiredJVal is None : includeJVal = True elif JVal == requiredJVal : includeJVal = True # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] else : levelsDict = None print ( \"Unit type not supported, please use eV or cm-1\" ) except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict","title":"getAvailableLevels"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getAvailableSpectralCharges","text":"For a given element, returns a list of all available charge states from the ASD pickle file Parameters: element \u2013 str representing atomic symbol of element, e.g. 'Ne' Source code in mass2/calibration/hci_lines.py 53 54 55 56 57 58 59 60 def getAvailableSpectralCharges ( self , element ): \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Args: element: str representing atomic symbol of element, e.g. 'Ne' \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ())","title":"getAvailableSpectralCharges"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getSingleLevel","text":"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters: element \u2013 str representing atomic symbol of element, e.g. 'Ne' spectralCharge \u2013 int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf \u2013 str representing nuclear configuration, e.g. '2p' term \u2013 str representing nuclear term, e.g. '2P*' JVal \u2013 str representing total angular momentum J, e.g. '3/2' units \u2013 (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values getUncertainty \u2013 (default True) if True, includes uncertainties in list of levels Source code in mass2/calibration/hci_lines.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def getSingleLevel ( self , element , spectralCharge , conf , term , JVal , units = \"eV\" , getUncertainty = True ): \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Args: element: str representing atomic symbol of element, e.g. 'Ne' spectralCharge: int representing spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf: str representing nuclear configuration, e.g. '2p' term: str representing nuclear term, e.g. '2P*' JVal: str representing total angular momentum J, e.g. '3/2' units: (default 'eV') 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values getUncertainty: (default True) if True, includes uncertainties in list of levels \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : levelEnergy = None print ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy hci_models.py Some useful methods for initializing GenericLineModel and CompositeMLEModel objects applied to HCI lines. June 2020 Paul Szypryt","title":"getSingleLevel"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.add_bg_model","text":"Adds a LinearBackgroundModel to a generic lmfit model Parameters: generic_model \u2013 Generic lmfit model object to which to add a linear background model vary_slope \u2013 (default False) allows a varying linear slope rather than just constant value Source code in mass2/calibration/hci_models.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def add_bg_model ( generic_model , vary_slope = False ): \"\"\"Adds a LinearBackgroundModel to a generic lmfit model Args: generic_model: Generic lmfit model object to which to add a linear background model vary_slope: (default False) allows a varying linear slope rather than just constant value \"\"\" composite_name = generic_model . _name bg_prefix = f \" { composite_name } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) background_model = mass2 . calibration . line_models . LinearBackgroundModel ( name = f \" { composite_name } Background\" , prefix = bg_prefix ) background_model . set_param_hint ( \"bg_slope\" , vary = vary_slope ) composite_model = generic_model + background_model composite_model . name = composite_name return composite_model","title":"add_bg_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_HLike_2P_model","text":"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters: element \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' conf \u2013 nuclear configuration as str, e.g. '2p' or '3p' has_linear_background \u2013 (default False) include a single linear background on top of the 2 Lorentzians has_tails \u2013 (default False) include low energy tail in the model vary_amp_ratio \u2013 (default False) allow the ratio of the J=3/2 to J=1/2 states to vary away from 2 Source code in mass2/calibration/hci_models.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def initialize_HLike_2P_model ( element , conf , has_linear_background = False , has_tails = False , vary_amp_ratio = False ): \"\"\"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Args: element: atomic symbol as str, e.g. 'Ne' or 'Ar' conf: nuclear configuration as str, e.g. '2p' or '3p' has_linear_background: (default False) include a single linear background on top of the 2 Lorentzians has_tails: (default False) include low energy tail in the model vary_amp_ratio: (default False) allow the ratio of the J=3/2 to J=1/2 states to vary away from 2 \"\"\" # Set up line names and lmfit prefixes charge = int ( xraydb . atomic_number ( element )) line_name_1_2 = f \" { element }{ charge } { conf } 2P* J=1/2\" line_name_3_2 = f \" { element }{ charge } { conf } 2P* J=3/2\" prefix_1_2 = f \" { line_name_1_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) prefix_3_2 = f \" { line_name_3_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) # Initialize individual lines and models line_1_2 = spectra [ line_name_1_2 ]() line_3_2 = spectra [ line_name_3_2 ]() model_1_2 = line_1_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_1_2 ) model_3_2 = line_3_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_3_2 ) # Initialize composite model and set addition H-like constraints composite_name = f \" { element }{ charge } { conf } \" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = [ model_1_2 , model_3_2 ], has_linear_background = has_linear_background , peak_component_name = line_name_3_2 , ) amp_ratio_param_name = f \" { element }{ charge } _ { conf } _amp_ratio\" composite_model . set_param_hint ( name = amp_ratio_param_name , value = 0.5 , min = 0.0 , vary = vary_amp_ratio ) composite_model . set_param_hint ( f \" { prefix_1_2 } integral\" , expr = f \" { prefix_3_2 } integral * { amp_ratio_param_name } \" ) return composite_model","title":"initialize_HLike_2P_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_HeLike_complex_model","text":"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P J=1, and 1s.2p 1P J=1 lines. Parameters: element \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background \u2013 (default False) include a single linear background on top of the Lorentzian models has_tails \u2013 (default False) include low energy tail in the model additional_line_names \u2013 (default []) additional line names to include in model, e.g. low level Li/Be-like features Source code in mass2/calibration/hci_models.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def initialize_HeLike_complex_model ( element , has_linear_background = False , has_tails = False , additional_line_names = []): \"\"\"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P* J=1, and 1s.2p 1P* J=1 lines. Args: element: atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background: (default False) include a single linear background on top of the Lorentzian models has_tails: (default False) include low energy tail in the model additional_line_names: (default []) additional line names to include in model, e.g. low level Li/Be-like features \"\"\" # Set up line names charge = int ( xraydb . atomic_number ( element ) - 1 ) line_name_1s2s_3S = f \" { element }{ charge } 1s.2s 3S J=1\" line_name_1s2p_3P = f \" { element }{ charge } 1s.2p 3P* J=1\" line_name_1s2p_1P = f \" { element }{ charge } 1s.2p 1P* J=1\" line_names = np . hstack ([[ line_name_1s2s_3S , line_name_1s2p_3P , line_name_1s2p_1P ], additional_line_names ]) # Set up lines and models based on line_names # individual_lines = [spectra[i_line_name]() for i_line_name in line_names] individual_models = [ initialize_hci_line_model ( i_line_name , has_linear_background = False , has_tails = has_tails ) for i_line_name in line_names ] # Set up composite model composite_name = f \" { element }{ charge } 1s2s_2p Complex\" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = individual_models , has_linear_background = has_linear_background , peak_component_name = line_name_1s2p_1P , ) return composite_model","title":"initialize_HeLike_complex_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_hci_composite_model","text":"Initializes composite lmfit model from the sum of input models Parameters: composite_name \u2013 str name given to composite line model line_models \u2013 array of lmfit models to sum into a composite model has_linear_background \u2013 (default False) include a single linear background on top of group of lorentzians peak_component_name \u2013 designate a component to be a peak for energy, all expressions are referenced to this component Source code in mass2/calibration/hci_models.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def initialize_hci_composite_model ( composite_name , individual_models , has_linear_background = False , peak_component_name = None ): \"\"\"Initializes composite lmfit model from the sum of input models Args: composite_name: str name given to composite line model line_models: array of lmfit models to sum into a composite model has_linear_background: (default False) include a single linear background on top of group of lorentzians peak_component_name: designate a component to be a peak for energy, all expressions are referenced to this component \"\"\" composite_model = np . sum ( individual_models ) composite_model . name = composite_name if has_linear_background : composite_model = add_bg_model ( composite_model ) # Workaround for energy calibration using composite models, pick 1st GenericLineModel component line_model_components = [ i_comp for i_comp in composite_model . components if isinstance ( i_comp , mass2 . calibration . line_models . GenericLineModel ) ] if peak_component_name is None : peak_component_name = line_model_components [ 0 ] . _name peak_component_index = [ i_comp . _name for i_comp in line_model_components ] . index ( peak_component_name ) peak_component = line_model_components [ peak_component_index ] composite_model . peak_prefix = peak_component . prefix composite_model . peak_energy = peak_component . spect . peak_energy # Set up some constraints relative to peak_component num_line_components = len ( line_model_components ) line_component_prefixes = [ iComp . prefix for iComp in line_model_components ] line_component_energies = [ iComp . spect . peak_energy for iComp in line_model_components ] for i in np . arange ( num_line_components ): if i != peak_component_index : # Single fwhm across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } fwhm\" , expr = f \" { composite_model . peak_prefix } fwhm\" ) # Single dph_de across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } dph_de\" , expr = f \" { composite_model . peak_prefix } dph_de\" ) # Fixed energy separation based on database values separation = line_component_energies [ i ] - composite_model . peak_energy hint = f \"( { separation } * { composite_model . peak_prefix } dph_de) + { composite_model . peak_prefix } peak_ph\" composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } peak_ph\" , expr = hint ) composite_model . shortname = composite_name return composite_model","title":"initialize_hci_composite_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_hci_line_model","text":"Initializes a single lorentzian hci lmfit model. Reformats line_name to create a lmfit valid prefix. Parameters: line_name \u2013 name of line within mass2.spectra has_linear_background \u2013 (default False) include linear background in the model has_tails \u2013 (default False) include low energy tail in the model Source code in mass2/calibration/hci_models.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def initialize_hci_line_model ( line_name , has_linear_background = False , has_tails = False ): \"\"\"Initializes a single lorentzian hci lmfit model. Reformats line_name to create a lmfit valid prefix. Args: line_name: name of line within mass2.spectra has_linear_background: (default False) include linear background in the model has_tails: (default False) include low energy tail in the model \"\"\" line = spectra [ line_name ]() prefix = f \" { line_name } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) line_model = line . model ( has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix ) line_model . shortname = line_name return line_model","title":"initialize_hci_line_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.models","text":"Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters: has_linear_background \u2013 (default False) include a single linear background on top of the 2 Lorentzians has_tails \u2013 (default False) include low energy tail in the model vary_Hlike_amp_ratio \u2013 (default False) allow the ratio of the J=3/2 to J=1/2 H-like states to vary additional_Helike_complex_lines: (default []) additional line names to include inHe-like complex model, e.g. low level Li/Be-like features Source code in mass2/calibration/hci_models.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def models ( has_linear_background = False , has_tails = False , vary_Hlike_amp_ratio = False , additional_Helike_complex_lines = []): \"\"\" Generates some commonly used HCI line models that can be used for energy calibration, etc. Args: has_linear_background: (default False) include a single linear background on top of the 2 Lorentzians has_tails: (default False) include low energy tail in the model vary_Hlike_amp_ratio: (default False) allow the ratio of the J=3/2 to J=1/2 H-like states to vary additional_Helike_complex_lines: (default []) additional line names to include inHe-like complex model, e.g. low level Li/Be-like features \"\"\" models_dict = {} # Make some common H-like 2P* models conf_Hlike_2P_dict = {} conf_Hlike_2P_dict [ \"N\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"O\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ne\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ar\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] for i_element in list ( conf_Hlike_2P_dict . keys ()): for i_conf in conf_Hlike_2P_dict [ i_element ]: Hlike_model = initialize_HLike_2P_model ( i_element , i_conf , has_linear_background = has_linear_background , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio , ) models_dict [ Hlike_model . _name ] = Hlike_model # Make some common He-like 1s2s,2p complex and higher order 1p* models # He-like lines Helike_complex_elements = [ \"N\" , \"O\" , \"Ne\" , \"Ar\" ] for i_element in Helike_complex_elements : Helike_model = initialize_HeLike_complex_model ( i_element , has_linear_background = has_linear_background , has_tails = has_tails , additional_line_names = additional_Helike_complex_lines , ) models_dict [ Helike_model . _name ] = Helike_model # 1s.np 1P* lines for n>=3 conf_Helike_1P_dict = {} conf_Helike_1P_dict [ \"N\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"O\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ne\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ar\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] for i_element in list ( conf_Helike_1P_dict . keys ()): i_charge = int ( xraydb . atomic_number ( i_element ) - 1 ) for i_conf in conf_Helike_1P_dict [ i_element ]: Helike_line_name = f \" { i_element }{ i_charge } { i_conf } 1P* J=1\" Helike_model = initialize_hci_line_model ( Helike_line_name , has_linear_background = has_linear_background , has_tails = has_tails ) models_dict [ Helike_model . _name ] = Helike_model # Some more complicated cases # 500 eV region of H-/He-like N N6_1s3p_model = initialize_hci_line_model ( \"N6 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) N7_2p_model = initialize_HLike_2P_model ( \"N\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) N_500eV_model = initialize_hci_composite_model ( \"N 500eV Region\" , [ N6_1s3p_model , N7_2p_model ], has_linear_background = has_linear_background , peak_component_name = \"N7 2p 2P* J=3/2\" , ) models_dict [ N_500eV_model . _name ] = N_500eV_model # 660 eV region of H-/He-like O O8_2p_model = initialize_HLike_2P_model ( \"O\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) O7_1s3p_model = initialize_hci_line_model ( \"O7 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) O_660eV_model = initialize_hci_composite_model ( \"O 660eV Region\" , [ O8_2p_model , O7_1s3p_model ], has_linear_background = has_linear_background , peak_component_name = \"O8 2p 2P* J=3/2\" , ) models_dict [ O_660eV_model . _name ] = O_660eV_model return models_dict","title":"models"},{"location":"line_fitting/","text":"Fitting spectral line models to data in MASS Joe Fowler, January 2020. Mass uses use the LMFIT package by wrapping it in the Mass2 class GenericLineModel and its subclasses. LMFit vs Scipy LMFIT has numerous advantages over the basic scipy.optimize module. Quoting from the LMFIT documentation, the user can: forget about the order of variables and refer to Parameters by meaningful names. place bounds on Parameters as attributes, without worrying about preserving the order of arrays for variables and boundaries. fix Parameters, without having to rewrite the objective function. place algebraic constraints on Parameters. The one disadvantage of the core LMFIT package for our purposes is that it minimizes the sum of squares of a vector instead of maximizing the Poisson likelihood. This is easily remedied, however, by replacing the usual computation of residuals with one that computes the square root of the Poisson likelihood contribution from each bin. Voil\u00e1! A maximum likelihood fitter for histograms. Advantages of LMFIT over our earlier, homemade approach to fitting line shapes include: Users can forget about the order of variables and refer to Parameters by meaningful names. Users can place algebraic constraints on Parameters. The interface for setting upper/lower bounds on parameters and for varying or fixing them is much more elegant, memorable, and simple than our homemade version. It ultimately wraps the scipy.optimize package and therefore inherits all of its advantages: a choice of over a dozen optimizers with highly technical documentation, some optimizers that aim for true global (not just local) optimization, and the countless expert-years that have been invested in perfecting it. LMFIT automatically computes numerous statistics of each fit including estimated uncertainties, correlations, and multiple quality-of-fit statistics (information criteria as well as chi-square) and offers user-friendly fit reports. See the MinimizerResult object. It's the work of Matt Newville, an x-ray scientist responsible for the excellent ifeffit and its successor Larch . Above all, its documentation is complete, already written, and maintained by not-us. Usage guide This overview is hardly complete, but we hope it can be a quick-start guide and also hint at how you can convert your own analysis work from the old to the new, preferred fitting methods. The underlying spectral line shape models Objects of the type SpectralLine encode the line shape of a fluorescence line, as a sum of Voigt or Lorentzian distributions. Because they inherit from scipy.stats.rv_continuous , they allow computation of cumulative distribution functions and the simulation of data drawn from the distribution. An example of the creation and usage is: import numpy as np import pylab as plt import mass2 import mass2.materials # Known lines are accessed by: line = mass2.spectra[\"MnKAlpha\"] rng = np.random.default_rng(1066) N = 100000 energies = line.rvs(size=N, instrument_gaussian_fwhm=2.2, rng=rng) # draw from the distribution plt.clf() sim, bin_edges, _ = plt.hist(energies, 120, [5865, 5925], histtype=\"step\"); binsize = bin_edges[1] - bin_edges[0] e = bin_edges[:-1] + 0.5*binsize plt.plot(e, line(e, instrument_gaussian_fwhm=2.2)*N*binsize, \"k\") plt.xlabel(\"Energy (eV)\") plt.title(\"Mn K$\\\\alpha$ random deviates and theory curve\") The SpectralLine object is useful to you if you need to generate simulated data, or to plot a line shape, as shown above. The objects that perform line fitting use the SpectralLine object to hold line shape information. You don't need to create a SpectralLine object for fitting, though; it will be done automatically. How to use the LMFIT-based models for fitting The simplest case of line fitting requires only 3 steps: create a model instance from a SpectralLine , guess its parameters from the data, and perform a fit with this guess. Plotting is not done as part of the fit--you have to do that separately. model = line.model() params = model.guess(sim, bin_centers=e, dph_de=1) resultA = model.fit(sim, params, bin_centers=e) # Fit again but with dPH/dE held at 1. # dPH/dE will be a free parameter for the fit by default, largely due to the history of MnKAlpha fits being so critical during development. # This will not work for nearly monochromatic lines, however, as the resolution (fwhm) and scale (dph_de) are exactly degenerate. # In practice, most fits are done with dph_de fixed. params = resultA.params.copy() resultB = model.fit(sim, params, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) # There are two plotting methods. The first is an LMfit built-in; the other (\"mass-style\") puts the # fit parameters on the plot. resultB.plot() resultB.plotm() # The best-fit params are found in resultB.params # and a dictionary of their values is resultB.best_values. # The parameters given as an argument to fit are unchanged. You can print a nicely formatted fit report with the method fit_report() : print(resultB.fit_report()) [[Model]] GenericLineModel(MnKAlpha) [[Fit Statistics]] # fitting method = least_squares # function evals = 15 # data points = 120 # variables = 4 chi-square = 100.565947 reduced chi-square = 0.86694782 Akaike info crit = -13.2013653 Bayesian info crit = -2.05139830 R-squared = 0.99999953 [[Variables]] fwhm: 2.21558094 +/- 0.02687437 (1.21%) (init = 2.217155) peak_ph: 5898.79525 +/- 0.00789761 (0.00%) (init = 5898.794) dph_de: 1 (fixed) integral: 99986.5425 +/- 314.455266 (0.31%) (init = 99985.8) background: 5.0098e-16 +/- 0.80578112 (160842446370819488.00%) (init = 2.791565e-09) bg_slope: 0 (fixed) [[Correlations]] (unreported correlations are < 0.100) C(integral, background) = -0.3147 C(fwhm, peak_ph) = -0.1121 Fitting with exponential tails (to low or high energy) Notice when you report the fit (or check the contents of the params or resultB.params objects), there are no parameters referring to exponential tails of a Bortels response. That's because the default fitter assumes a Gaussian response. If you want tails, that's a constructor argument: model = line.model(has_tails=True) params = model.guess(sim, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) resultC = model.fit(sim, params, bin_centers=e) resultC.plot() # print(resultC.fit_report()) By default, the has_tails=True will set up a non-zero low-energy tail and allow it to vary, while the high-energy tail is set to zero amplitude and doesn't vary. Use these numbered examples if you want to fit for a high-energy tail (1), to fix the low-E tail at some non-zero level (2) or to turn off the low-E tail completely (3): # 1. To let the low-E and high-E tail both vary simultaneously params[\"tail_share_hi\"].set(.1, vary=True) params[\"tail_tau_hi\"].set(30, vary=True) # 2. To fix the sum of low-E and high-E tail at a 10% level, with low-E tau=30 eV, but # the share of the low vs high tail can vary params[\"tail_frac\"].set(.1, vary=False) params[\"tail_tau\"].set(30, vary=False) # 3. To turn off low-E tail params[\"tail_frac\"].set(.1, vary=True) params[\"tail_share_hi\"].set(1, vary=False) params[\"tail_tau\"].set(vary=False) Fitting with a quantum efficiency model If you want to multiply the line models by a model of the quantum efficiency, you can do that. You need a qemodel function or callable function object that takes an energy (scalar or vector) and returns the corresponding efficiency. For example, you can use the \"Raven1 2019\" QE model from mass2.materials . The filter-stack models are not terribly fast to run, so it's best to compute once, spline the results, and pass that spline as the qemodel to line.model(qemodel=qemodel) . raven_filters = mass2.materials.efficiency_models.filterstack_models[\"RAVEN1 2019\"] eknots = np.linspace(100, 20000, 1991) qevalues = raven_filters(eknots) qemodel = mass2.mathstat.interpolate.CubicSpline(eknots, qevalues) model = line.model(qemodel=qemodel) resultD = model.fit(sim, resultB.params, bin_centers=e) resultD.plotm() # print(resultD.fit_report()) fit_counts = resultD.params[\"integral\"].value localqe = qemodel(mass2.STANDARD_FEATURES[\"MnKAlpha\"]) fit_observed = fit_counts*localqe fit_err = resultD.params[\"integral\"].stderr count_err = fit_err*localqe print(\"Fit finds {:.0f}\u00b1{:.0f} counts before QE, or {:.0f}\u00b1{:.0f} observed. True value {:d}.\".format( round(fit_counts, -1), round(fit_err, -1), round(fit_observed, -1), round(count_err, -1), N)) Fit finds 168810\u00b1530 counts before QE, or 100020\u00b1320 observed. True value 100000. When you fit with a non-trivial QE model, all fit parameters that refer to intensities of signal or background refer to a sensor with an ideal QE=1. These parameters include: integral background bg_slope That is, the fit values must be multiplied by the local QE to give the number of observed signal counts, background counts per bin, or background slope. With or without a QE model, \"integral\" refers to the number of photons that would be seen across all energies (not just in the range being fit). Fitting a simple Gaussian, Lorentzian, or Voigt function e_ctr = 1000.0 Nsig = 10000 Nbg = 1000 sigma = 1.0 x_gauss = rng.standard_normal(Nsig)*sigma + e_ctr hwhm = 1.0 x_lorentz = rng.standard_cauchy(Nsig)*hwhm + e_ctr x_voigt = rng.standard_cauchy(Nsig)*hwhm + rng.standard_normal(Nsig)*sigma + e_ctr bg = rng.uniform(e_ctr-5, e_ctr+5, size=Nbg) # Gaussian fit c, b = np.histogram(np.hstack([x_gauss, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = mass2.fluorescence_lines.SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, 0, 0) line.linetype = \"Gaussian\" model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultG = model.fit(c, params, bin_centers=bin_ctr) resultG.plotm() # print(resultG.fit_report()) # Lorentzian fit c, b = np.histogram(np.hstack([x_lorentz, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = mass2.fluorescence_lines.SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, 0) line.linetype = \"Lorentzian\" model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultL = model.fit(c, params, bin_centers=bin_ctr) resultL.plotm() # print(resultL.fit_report()) # Voigt fit c, b = np.histogram(np.hstack([x_voigt, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = mass2.fluorescence_lines.SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, sigma) line.linetype = \"Voigt\" model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultV = model.fit(c, params, bin_centers=bin_ctr) resultV.plotm() # print(resultV.fit_report()) More details of fitting fluorescence-line models Use p=model.guess(data, bin_centers=e, dph_de=dph_de) to create a heuristic for the starting parameters. Change starting values and toggle the vary attribute on parameters, as needed. For example: p[\"dph_de\"].set(1.0, vary=False) Use result=model.fit(data, p, bin_centers=e) to perform the fit and store the result. The result holds many attributes and methods (see MinimizerResult for full documentation). These include: result.params = the model's best-fit parameters object result.best_values = a dictionary of the best-fit parameter values result.best_fit = the model's y-values at the best-fit parameter values result.chisqr = the chi-squared statistic of the fit (here, -2log(L)) result.covar = the computed covariance result.fit_report() = return a pretty-printed string reporting on the fit result.plot_fit() = make a plot of the data and fit result.plot_residuals() = make a plot of the residuals (fit-data) result.plot() = make a plot of the data, fit, and residuals, generally plotm is preferred result.plotm() = make a plot of the data, fit, and fit params with dataset filename in title The tau values (scale lengths of exponential tails) in eV units. The parameter \"integral\" refers to the integrated number of counts across all energies (whether inside or beyond the fitted energy range).","title":"Line fitting"},{"location":"line_fitting/#fitting-spectral-line-models-to-data-in-mass","text":"Joe Fowler, January 2020. Mass uses use the LMFIT package by wrapping it in the Mass2 class GenericLineModel and its subclasses.","title":"Fitting spectral line models to data in MASS"},{"location":"line_fitting/#lmfit-vs-scipy","text":"LMFIT has numerous advantages over the basic scipy.optimize module. Quoting from the LMFIT documentation, the user can: forget about the order of variables and refer to Parameters by meaningful names. place bounds on Parameters as attributes, without worrying about preserving the order of arrays for variables and boundaries. fix Parameters, without having to rewrite the objective function. place algebraic constraints on Parameters. The one disadvantage of the core LMFIT package for our purposes is that it minimizes the sum of squares of a vector instead of maximizing the Poisson likelihood. This is easily remedied, however, by replacing the usual computation of residuals with one that computes the square root of the Poisson likelihood contribution from each bin. Voil\u00e1! A maximum likelihood fitter for histograms. Advantages of LMFIT over our earlier, homemade approach to fitting line shapes include: Users can forget about the order of variables and refer to Parameters by meaningful names. Users can place algebraic constraints on Parameters. The interface for setting upper/lower bounds on parameters and for varying or fixing them is much more elegant, memorable, and simple than our homemade version. It ultimately wraps the scipy.optimize package and therefore inherits all of its advantages: a choice of over a dozen optimizers with highly technical documentation, some optimizers that aim for true global (not just local) optimization, and the countless expert-years that have been invested in perfecting it. LMFIT automatically computes numerous statistics of each fit including estimated uncertainties, correlations, and multiple quality-of-fit statistics (information criteria as well as chi-square) and offers user-friendly fit reports. See the MinimizerResult object. It's the work of Matt Newville, an x-ray scientist responsible for the excellent ifeffit and its successor Larch . Above all, its documentation is complete, already written, and maintained by not-us.","title":"LMFit vs Scipy"},{"location":"line_fitting/#usage-guide","text":"This overview is hardly complete, but we hope it can be a quick-start guide and also hint at how you can convert your own analysis work from the old to the new, preferred fitting methods.","title":"Usage guide"},{"location":"line_fitting/#the-underlying-spectral-line-shape-models","text":"Objects of the type SpectralLine encode the line shape of a fluorescence line, as a sum of Voigt or Lorentzian distributions. Because they inherit from scipy.stats.rv_continuous , they allow computation of cumulative distribution functions and the simulation of data drawn from the distribution. An example of the creation and usage is: import numpy as np import pylab as plt import mass2 import mass2.materials # Known lines are accessed by: line = mass2.spectra[\"MnKAlpha\"] rng = np.random.default_rng(1066) N = 100000 energies = line.rvs(size=N, instrument_gaussian_fwhm=2.2, rng=rng) # draw from the distribution plt.clf() sim, bin_edges, _ = plt.hist(energies, 120, [5865, 5925], histtype=\"step\"); binsize = bin_edges[1] - bin_edges[0] e = bin_edges[:-1] + 0.5*binsize plt.plot(e, line(e, instrument_gaussian_fwhm=2.2)*N*binsize, \"k\") plt.xlabel(\"Energy (eV)\") plt.title(\"Mn K$\\\\alpha$ random deviates and theory curve\") The SpectralLine object is useful to you if you need to generate simulated data, or to plot a line shape, as shown above. The objects that perform line fitting use the SpectralLine object to hold line shape information. You don't need to create a SpectralLine object for fitting, though; it will be done automatically.","title":"The underlying spectral line shape models"},{"location":"line_fitting/#how-to-use-the-lmfit-based-models-for-fitting","text":"The simplest case of line fitting requires only 3 steps: create a model instance from a SpectralLine , guess its parameters from the data, and perform a fit with this guess. Plotting is not done as part of the fit--you have to do that separately. model = line.model() params = model.guess(sim, bin_centers=e, dph_de=1) resultA = model.fit(sim, params, bin_centers=e) # Fit again but with dPH/dE held at 1. # dPH/dE will be a free parameter for the fit by default, largely due to the history of MnKAlpha fits being so critical during development. # This will not work for nearly monochromatic lines, however, as the resolution (fwhm) and scale (dph_de) are exactly degenerate. # In practice, most fits are done with dph_de fixed. params = resultA.params.copy() resultB = model.fit(sim, params, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) # There are two plotting methods. The first is an LMfit built-in; the other (\"mass-style\") puts the # fit parameters on the plot. resultB.plot() resultB.plotm() # The best-fit params are found in resultB.params # and a dictionary of their values is resultB.best_values. # The parameters given as an argument to fit are unchanged. You can print a nicely formatted fit report with the method fit_report() : print(resultB.fit_report()) [[Model]] GenericLineModel(MnKAlpha) [[Fit Statistics]] # fitting method = least_squares # function evals = 15 # data points = 120 # variables = 4 chi-square = 100.565947 reduced chi-square = 0.86694782 Akaike info crit = -13.2013653 Bayesian info crit = -2.05139830 R-squared = 0.99999953 [[Variables]] fwhm: 2.21558094 +/- 0.02687437 (1.21%) (init = 2.217155) peak_ph: 5898.79525 +/- 0.00789761 (0.00%) (init = 5898.794) dph_de: 1 (fixed) integral: 99986.5425 +/- 314.455266 (0.31%) (init = 99985.8) background: 5.0098e-16 +/- 0.80578112 (160842446370819488.00%) (init = 2.791565e-09) bg_slope: 0 (fixed) [[Correlations]] (unreported correlations are < 0.100) C(integral, background) = -0.3147 C(fwhm, peak_ph) = -0.1121","title":"How to use the LMFIT-based models for fitting"},{"location":"line_fitting/#fitting-with-exponential-tails-to-low-or-high-energy","text":"Notice when you report the fit (or check the contents of the params or resultB.params objects), there are no parameters referring to exponential tails of a Bortels response. That's because the default fitter assumes a Gaussian response. If you want tails, that's a constructor argument: model = line.model(has_tails=True) params = model.guess(sim, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) resultC = model.fit(sim, params, bin_centers=e) resultC.plot() # print(resultC.fit_report()) By default, the has_tails=True will set up a non-zero low-energy tail and allow it to vary, while the high-energy tail is set to zero amplitude and doesn't vary. Use these numbered examples if you want to fit for a high-energy tail (1), to fix the low-E tail at some non-zero level (2) or to turn off the low-E tail completely (3): # 1. To let the low-E and high-E tail both vary simultaneously params[\"tail_share_hi\"].set(.1, vary=True) params[\"tail_tau_hi\"].set(30, vary=True) # 2. To fix the sum of low-E and high-E tail at a 10% level, with low-E tau=30 eV, but # the share of the low vs high tail can vary params[\"tail_frac\"].set(.1, vary=False) params[\"tail_tau\"].set(30, vary=False) # 3. To turn off low-E tail params[\"tail_frac\"].set(.1, vary=True) params[\"tail_share_hi\"].set(1, vary=False) params[\"tail_tau\"].set(vary=False)","title":"Fitting with exponential tails (to low or high energy)"},{"location":"line_fitting/#fitting-with-a-quantum-efficiency-model","text":"If you want to multiply the line models by a model of the quantum efficiency, you can do that. You need a qemodel function or callable function object that takes an energy (scalar or vector) and returns the corresponding efficiency. For example, you can use the \"Raven1 2019\" QE model from mass2.materials . The filter-stack models are not terribly fast to run, so it's best to compute once, spline the results, and pass that spline as the qemodel to line.model(qemodel=qemodel) . raven_filters = mass2.materials.efficiency_models.filterstack_models[\"RAVEN1 2019\"] eknots = np.linspace(100, 20000, 1991) qevalues = raven_filters(eknots) qemodel = mass2.mathstat.interpolate.CubicSpline(eknots, qevalues) model = line.model(qemodel=qemodel) resultD = model.fit(sim, resultB.params, bin_centers=e) resultD.plotm() # print(resultD.fit_report()) fit_counts = resultD.params[\"integral\"].value localqe = qemodel(mass2.STANDARD_FEATURES[\"MnKAlpha\"]) fit_observed = fit_counts*localqe fit_err = resultD.params[\"integral\"].stderr count_err = fit_err*localqe print(\"Fit finds {:.0f}\u00b1{:.0f} counts before QE, or {:.0f}\u00b1{:.0f} observed. True value {:d}.\".format( round(fit_counts, -1), round(fit_err, -1), round(fit_observed, -1), round(count_err, -1), N)) Fit finds 168810\u00b1530 counts before QE, or 100020\u00b1320 observed. True value 100000. When you fit with a non-trivial QE model, all fit parameters that refer to intensities of signal or background refer to a sensor with an ideal QE=1. These parameters include: integral background bg_slope That is, the fit values must be multiplied by the local QE to give the number of observed signal counts, background counts per bin, or background slope. With or without a QE model, \"integral\" refers to the number of photons that would be seen across all energies (not just in the range being fit).","title":"Fitting with a quantum efficiency model"},{"location":"line_fitting/#fitting-a-simple-gaussian-lorentzian-or-voigt-function","text":"e_ctr = 1000.0 Nsig = 10000 Nbg = 1000 sigma = 1.0 x_gauss = rng.standard_normal(Nsig)*sigma + e_ctr hwhm = 1.0 x_lorentz = rng.standard_cauchy(Nsig)*hwhm + e_ctr x_voigt = rng.standard_cauchy(Nsig)*hwhm + rng.standard_normal(Nsig)*sigma + e_ctr bg = rng.uniform(e_ctr-5, e_ctr+5, size=Nbg) # Gaussian fit c, b = np.histogram(np.hstack([x_gauss, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = mass2.fluorescence_lines.SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, 0, 0) line.linetype = \"Gaussian\" model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultG = model.fit(c, params, bin_centers=bin_ctr) resultG.plotm() # print(resultG.fit_report()) # Lorentzian fit c, b = np.histogram(np.hstack([x_lorentz, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = mass2.fluorescence_lines.SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, 0) line.linetype = \"Lorentzian\" model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultL = model.fit(c, params, bin_centers=bin_ctr) resultL.plotm() # print(resultL.fit_report()) # Voigt fit c, b = np.histogram(np.hstack([x_voigt, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = mass2.fluorescence_lines.SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, sigma) line.linetype = \"Voigt\" model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultV = model.fit(c, params, bin_centers=bin_ctr) resultV.plotm() # print(resultV.fit_report())","title":"Fitting a simple Gaussian, Lorentzian, or Voigt function"},{"location":"line_fitting/#more-details-of-fitting-fluorescence-line-models","text":"Use p=model.guess(data, bin_centers=e, dph_de=dph_de) to create a heuristic for the starting parameters. Change starting values and toggle the vary attribute on parameters, as needed. For example: p[\"dph_de\"].set(1.0, vary=False) Use result=model.fit(data, p, bin_centers=e) to perform the fit and store the result. The result holds many attributes and methods (see MinimizerResult for full documentation). These include: result.params = the model's best-fit parameters object result.best_values = a dictionary of the best-fit parameter values result.best_fit = the model's y-values at the best-fit parameter values result.chisqr = the chi-squared statistic of the fit (here, -2log(L)) result.covar = the computed covariance result.fit_report() = return a pretty-printed string reporting on the fit result.plot_fit() = make a plot of the data and fit result.plot_residuals() = make a plot of the residuals (fit-data) result.plot() = make a plot of the data, fit, and residuals, generally plotm is preferred result.plotm() = make a plot of the data, fit, and fit params with dataset filename in title The tau values (scale lengths of exponential tails) in eV units. The parameter \"integral\" refers to the integrated number of counts across all energies (whether inside or beyond the fitted energy range).","title":"More details of fitting fluorescence-line models"},{"location":"tests/","text":"Unit testing If you want to run the unit tests for mass2 go to your mass2 directory and do pytest . If you want to add tests to mass2 , you do not need to use the old unittest framework. Simpler, more modern tests in the pytest style are allowed (and preferred). Put any new tests in a file somewhere inside tests with a name like test_myfeature.py , it must match the pattern test_*.py to be found by pytest. On each commit to develop, the tests will be run automatically by GitHub Actions. See results of recent tests .","title":"Testing"},{"location":"tests/#unit-testing","text":"If you want to run the unit tests for mass2 go to your mass2 directory and do pytest . If you want to add tests to mass2 , you do not need to use the old unittest framework. Simpler, more modern tests in the pytest style are allowed (and preferred). Put any new tests in a file somewhere inside tests with a name like test_myfeature.py , it must match the pattern test_*.py to be found by pytest. On each commit to develop, the tests will be run automatically by GitHub Actions. See results of recent tests .","title":"Unit testing"},{"location":"xray_efficiency_models/","text":"Detector X-ray Efficiency Models This module requires the xraydb python package. It should be included in the mass2 installation. Otherwise, you should be able to install with pip install xraydb . Motivation For many analyses, it is important to estimate a x-ray spectrum as it would be seen from the source rather than as it would be measured with a set of detectors. This can be important, for example, when trying to determine line intensity ratios of two lines separated in energy space. Here, we attempt to model the effects that would cause the measured spectrum to be different from the true spectrum, such as energy dependent losses in transmission due to IR blocking filters and vacuum windows. Energy-dependent absorber efficiency can also be modeled. FilterStack class and subclass functions with premade efficiency models Here, we import the mass2.efficiency_models module and demonstrate the functionality with some of the premade efficiency models. Generally, these premade models are put in place for TES instruments with well known absorber and filter stack compositions. To demonstrate, we work with the 'EBIT 2018' model, which models the TES spectrometer setup at the NIST EBIT, as it was commissioned in 2018. This model includes a ~1um thick absorber, 3 ~100nm thick Al IR blocking filters, and LEX HT vacuum windows for both the TES and EBIT vacuums. We begin by importing efficiency_models and examining the EBIT efficiency model components. We can see that the model is made of many submodels (aka components) and that all the parameters have uncertainties. The EBIT system was particularly well characterized, so the uncertainties are fairly low. The presence of uncertainties requires some special handling in a few places, these docs will show some examples. import mass2 import mass2.materials # you have to explicitly import mass2.materials import numpy as np import pylab as plt from uncertainties import unumpy as unp # useful for working with arrays with uncertainties aka uarray from uncertainties import ufloat EBIT_model = mass2.materials.filterstack_models['EBIT 2018'] print(EBIT_model) <class 'mass2.materials.efficiency_models.FilterStack'>( Electroplated Au Absorber: <class 'mass2.materials.efficiency_models.Filter'>(Au 0.00186+/-0.00006 g/cm^2, fill_fraction=1.000+/-0, absorber=True) 50mK Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (3.04+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 3K Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.93+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 50K Filter: <class 'mass2.materials.efficiency_models.FilterStack'>( Al Film: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.77+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) Ni Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Ni 0.0134+/-0.0018 g/cm^2, fill_fraction=0.170+/-0.010, absorber=False) ) Luxel Window TES: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) Luxel Window EBIT: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) ) Next, we examine the function get_efficiency(xray_energies_eV) , which is an method of FilterStack . This can be called for the entire filter stack or for individual components in the filter stack. As an example, we look at the efficiency of the EBIT 2018 filter stack and the 50K filter component between 2ekV and 10 keV, at 1 keV steps. sparse_xray_energies_eV = np.arange(2000, 10000, 1000) stack_efficiency = EBIT_model.get_efficiency(sparse_xray_energies_eV) stack_efficiency_uncertain = EBIT_model.get_efficiency(sparse_xray_energies_eV, uncertain=True) # you have to opt into getting uncertainties out filter50K_efficiency = EBIT_model.components[3].get_efficiency(sparse_xray_energies_eV) print(\"stack efficiencies\") print([f\"{x}\" for x in stack_efficiency_uncertain]) # this is a hack to get uarrays to print with auto chosen number of sig figs print(stack_efficiency) # this is a hack to get uarrays to print with auto chosen number of sig figs print(unp.nominal_values(stack_efficiency)) # you can easily strip uncertainties, see uncertains package docs for more info print(\"filter50K efficiencies\") print(filter50K_efficiency) # if you want to remove the uncertainties, eg for plotting stack efficiencies ['0.335+/-0.008', '0.472+/-0.010', '0.456+/-0.010', '0.383+/-0.010', '0.307+/-0.009', '0.242+/-0.007', '0.191+/-0.006', '0.136+/-0.005'] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] filter50K efficiencies [0.77672107 0.81107679 0.8233861 0.84072724 0.86670307 0.89357999 0.9163624 0.83360284] ``` Here, we use the function `plot_efficiency(xray_energies_eV, ax)` to plot the efficiencies. `ax` defaults to None, but it can be used to plot the efficiencies on a user provided axis. Just like `get_efficiency`, `plot_efficiency` works with `FilterStack` and its subclasses. Testing with energy range 100 to 20,000 eV, 1 eV steps. ```python xray_energies_eV = np.arange(100,20000,10) EBIT_model.plot_efficiency(xray_energies_eV) EBIT_model.components[3].plot_efficiency(xray_energies_eV) # plt.savefig(\"img/filter_50K_efficiency.png\");plt.close() # plt.savefig(\"img/EBIT_efficiency.png\");plt.close() Creating your own custom filter stack model using FilterStack objects Now we will explore creating custom FilterStack objects and building up your very own filter stack model. First, we will create a general FilterStack object, representing a stack of filters. We will then populate this object with filters, which take the form of the various FilterStack object subclasses, such as Film , or even other FilterStack objects to create more complicated filters with multiple components. The add argument can be used to add a premade FilterStack object as a component of a different FilterStack object. We will start by adding some simple Film objects to the filter stack. This class requires a the name and material arguments, and the optical depth can be specified by passing in either area_density_g_per_cm2 or thickness_nm (but not both). By default, most FilterStack objects use the bulk density of a material to calculate the optical depth when the thickness_nm is used, but a custom density can be specified with the density_g_per_cm3 argument. In addition, a meshed style filter can be modelled using the fill_fraction argument. Finally, most FilterStack subclasses can use the absorber argument (default False), which will cause the object to return absorption, instead of transmittance, as the efficiency. All numerical arguments can be passed with our without uncertainties. If you don't have at least one number with specified uncertainty in a particular Film, the code will add a \u00b1100% uncertainty on that component. This way, hopefully you will notice that your uncertainty is higher than you expect, and double check the inputs. Read up on the uncertainties package for more info about how it works. custom_model = mass2.materials.FilterStack(name='My Filter Stack') custom_model.add_filter(name='My Bi Absorber', material='Bi', thickness_nm=ufloat(4.0e3, .1e3), absorber=True) custom_model.add_filter(name='My Al 50mK Filter', material='Al', thickness_nm=ufloat(100.0, 10)) custom_model.add_filter(name='My Si 3K Filter', material='Si', thickness_nm=ufloat(500.0, 2)) custom_filter = mass2.materials.FilterStack(name='My meshed 50K Filter') custom_filter.add_filter(name='Al Film', material='Al', thickness_nm=ufloat(100.0, 10)) custom_filter.add_filter(name='Ni Mesh', material='Ni', thickness_nm=ufloat(10.0e3, .1e3), fill_fraction=ufloat(0.2, 0.01)) custom_model.add(custom_filter) custom_model.plot_efficiency(xray_energies_eV) There are also some premade filter classes for filters that commonly show up in our instrument filter stacks. At the moment, the FilterStack subclasses listed below are implemented: - AlFilmWithOxide - models a typical IR blocking filter with native oxide layers, which can be important for thin filters. - AlFilmWithPolymer - models a similar IR blocking filter, but with increased structural support from a polymer backing. - LEX_HT - models LEX_HT vacuum windows, which contain a polymer backed Al film and stainless steel mesh. Usage examples and efficiency curves of these classes are shown below. premade_filter_stack = mass2.materials.FilterStack(name='A Stack of Premade Filters') f1 = mass2.materials.AlFilmWithOxide(name='My Oxidized Al Filter', Al_thickness_nm=50.0) f2 = mass2.materials.AlFilmWithPolymer(name='My Polymer Backed Al Filter', Al_thickness_nm=100.0, polymer_thickness_nm=200.0) f3 = mass2.materials.LEX_HT(name=\"My LEX HT Filter\") premade_filter_stack.add(f1) premade_filter_stack.add(f2) premade_filter_stack.add(f3) low_xray_energies_eV = np.arange(100,3000,5) premade_filter_stack.plot_efficiency(low_xray_energies_eV)","title":"X-ray models"},{"location":"xray_efficiency_models/#detector-x-ray-efficiency-models","text":"This module requires the xraydb python package. It should be included in the mass2 installation. Otherwise, you should be able to install with pip install xraydb .","title":"Detector X-ray Efficiency Models"},{"location":"xray_efficiency_models/#motivation","text":"For many analyses, it is important to estimate a x-ray spectrum as it would be seen from the source rather than as it would be measured with a set of detectors. This can be important, for example, when trying to determine line intensity ratios of two lines separated in energy space. Here, we attempt to model the effects that would cause the measured spectrum to be different from the true spectrum, such as energy dependent losses in transmission due to IR blocking filters and vacuum windows. Energy-dependent absorber efficiency can also be modeled.","title":"Motivation"},{"location":"xray_efficiency_models/#filterstack-class-and-subclass-functions-with-premade-efficiency-models","text":"Here, we import the mass2.efficiency_models module and demonstrate the functionality with some of the premade efficiency models. Generally, these premade models are put in place for TES instruments with well known absorber and filter stack compositions. To demonstrate, we work with the 'EBIT 2018' model, which models the TES spectrometer setup at the NIST EBIT, as it was commissioned in 2018. This model includes a ~1um thick absorber, 3 ~100nm thick Al IR blocking filters, and LEX HT vacuum windows for both the TES and EBIT vacuums. We begin by importing efficiency_models and examining the EBIT efficiency model components. We can see that the model is made of many submodels (aka components) and that all the parameters have uncertainties. The EBIT system was particularly well characterized, so the uncertainties are fairly low. The presence of uncertainties requires some special handling in a few places, these docs will show some examples. import mass2 import mass2.materials # you have to explicitly import mass2.materials import numpy as np import pylab as plt from uncertainties import unumpy as unp # useful for working with arrays with uncertainties aka uarray from uncertainties import ufloat EBIT_model = mass2.materials.filterstack_models['EBIT 2018'] print(EBIT_model) <class 'mass2.materials.efficiency_models.FilterStack'>( Electroplated Au Absorber: <class 'mass2.materials.efficiency_models.Filter'>(Au 0.00186+/-0.00006 g/cm^2, fill_fraction=1.000+/-0, absorber=True) 50mK Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (3.04+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 3K Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.93+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 50K Filter: <class 'mass2.materials.efficiency_models.FilterStack'>( Al Film: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.77+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) Ni Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Ni 0.0134+/-0.0018 g/cm^2, fill_fraction=0.170+/-0.010, absorber=False) ) Luxel Window TES: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) Luxel Window EBIT: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) ) Next, we examine the function get_efficiency(xray_energies_eV) , which is an method of FilterStack . This can be called for the entire filter stack or for individual components in the filter stack. As an example, we look at the efficiency of the EBIT 2018 filter stack and the 50K filter component between 2ekV and 10 keV, at 1 keV steps. sparse_xray_energies_eV = np.arange(2000, 10000, 1000) stack_efficiency = EBIT_model.get_efficiency(sparse_xray_energies_eV) stack_efficiency_uncertain = EBIT_model.get_efficiency(sparse_xray_energies_eV, uncertain=True) # you have to opt into getting uncertainties out filter50K_efficiency = EBIT_model.components[3].get_efficiency(sparse_xray_energies_eV) print(\"stack efficiencies\") print([f\"{x}\" for x in stack_efficiency_uncertain]) # this is a hack to get uarrays to print with auto chosen number of sig figs print(stack_efficiency) # this is a hack to get uarrays to print with auto chosen number of sig figs print(unp.nominal_values(stack_efficiency)) # you can easily strip uncertainties, see uncertains package docs for more info print(\"filter50K efficiencies\") print(filter50K_efficiency) # if you want to remove the uncertainties, eg for plotting stack efficiencies ['0.335+/-0.008', '0.472+/-0.010', '0.456+/-0.010', '0.383+/-0.010', '0.307+/-0.009', '0.242+/-0.007', '0.191+/-0.006', '0.136+/-0.005'] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] filter50K efficiencies [0.77672107 0.81107679 0.8233861 0.84072724 0.86670307 0.89357999 0.9163624 0.83360284] ``` Here, we use the function `plot_efficiency(xray_energies_eV, ax)` to plot the efficiencies. `ax` defaults to None, but it can be used to plot the efficiencies on a user provided axis. Just like `get_efficiency`, `plot_efficiency` works with `FilterStack` and its subclasses. Testing with energy range 100 to 20,000 eV, 1 eV steps. ```python xray_energies_eV = np.arange(100,20000,10) EBIT_model.plot_efficiency(xray_energies_eV) EBIT_model.components[3].plot_efficiency(xray_energies_eV) # plt.savefig(\"img/filter_50K_efficiency.png\");plt.close() # plt.savefig(\"img/EBIT_efficiency.png\");plt.close()","title":"FilterStack class and subclass functions with premade efficiency models"},{"location":"xray_efficiency_models/#creating-your-own-custom-filter-stack-model-using-filterstack-objects","text":"Now we will explore creating custom FilterStack objects and building up your very own filter stack model. First, we will create a general FilterStack object, representing a stack of filters. We will then populate this object with filters, which take the form of the various FilterStack object subclasses, such as Film , or even other FilterStack objects to create more complicated filters with multiple components. The add argument can be used to add a premade FilterStack object as a component of a different FilterStack object. We will start by adding some simple Film objects to the filter stack. This class requires a the name and material arguments, and the optical depth can be specified by passing in either area_density_g_per_cm2 or thickness_nm (but not both). By default, most FilterStack objects use the bulk density of a material to calculate the optical depth when the thickness_nm is used, but a custom density can be specified with the density_g_per_cm3 argument. In addition, a meshed style filter can be modelled using the fill_fraction argument. Finally, most FilterStack subclasses can use the absorber argument (default False), which will cause the object to return absorption, instead of transmittance, as the efficiency. All numerical arguments can be passed with our without uncertainties. If you don't have at least one number with specified uncertainty in a particular Film, the code will add a \u00b1100% uncertainty on that component. This way, hopefully you will notice that your uncertainty is higher than you expect, and double check the inputs. Read up on the uncertainties package for more info about how it works. custom_model = mass2.materials.FilterStack(name='My Filter Stack') custom_model.add_filter(name='My Bi Absorber', material='Bi', thickness_nm=ufloat(4.0e3, .1e3), absorber=True) custom_model.add_filter(name='My Al 50mK Filter', material='Al', thickness_nm=ufloat(100.0, 10)) custom_model.add_filter(name='My Si 3K Filter', material='Si', thickness_nm=ufloat(500.0, 2)) custom_filter = mass2.materials.FilterStack(name='My meshed 50K Filter') custom_filter.add_filter(name='Al Film', material='Al', thickness_nm=ufloat(100.0, 10)) custom_filter.add_filter(name='Ni Mesh', material='Ni', thickness_nm=ufloat(10.0e3, .1e3), fill_fraction=ufloat(0.2, 0.01)) custom_model.add(custom_filter) custom_model.plot_efficiency(xray_energies_eV) There are also some premade filter classes for filters that commonly show up in our instrument filter stacks. At the moment, the FilterStack subclasses listed below are implemented: - AlFilmWithOxide - models a typical IR blocking filter with native oxide layers, which can be important for thin filters. - AlFilmWithPolymer - models a similar IR blocking filter, but with increased structural support from a polymer backing. - LEX_HT - models LEX_HT vacuum windows, which contain a polymer backed Al film and stainless steel mesh. Usage examples and efficiency curves of these classes are shown below. premade_filter_stack = mass2.materials.FilterStack(name='A Stack of Premade Filters') f1 = mass2.materials.AlFilmWithOxide(name='My Oxidized Al Filter', Al_thickness_nm=50.0) f2 = mass2.materials.AlFilmWithPolymer(name='My Polymer Backed Al Filter', Al_thickness_nm=100.0, polymer_thickness_nm=200.0) f3 = mass2.materials.LEX_HT(name=\"My LEX HT Filter\") premade_filter_stack.add(f1) premade_filter_stack.add(f2) premade_filter_stack.add(f3) low_xray_energies_eV = np.arange(100,3000,5) premade_filter_stack.plot_efficiency(low_xray_energies_eV)","title":"Creating your own custom filter stack model using FilterStack objects"}]}