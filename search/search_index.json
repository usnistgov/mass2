{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mass 2: Microcalorimeter Analysis Software Suite Overview of Mass 2 Mass is a Python software suite designed to analyze pulse records from high-resolution, cryogenic microcalorimeters. We use Mass with pulse records from x-ray and gamma-ray spectrometers, performing a sequence of analysis, or \"calibration\" steps to extract a high-precision estimate of the energy from each record. With raw pulse records and Mass, you can: Analyze data from one or multiple detectors at one time. Analyze data from one or more raw pulse data files per detector. Analyze a fixed dataset taken in the past, or perform \"online\" analysis of a dataset still being acquired. Analyze data from time-division multiplexed (TDM) and microwave-multiplexed (\u00b5MUX) systems. Compute and apply \"optimal filters\" of various types. With or without raw pulse records, further analysis tasks that Mass helps with include: Choose and apply data cuts. Fix complex line shapes in an energy spectrum. Estimate and apply accurate functions for absolute-energy calibration. Win friends and influence people. Major concepts Mass2 is built around a few core technologies: Pola.rs , a high-performance modern dataframe library. Organizes the data structures and provides data I/O. Numpy . For numerical arrays and scientific computation. Scipy . Additional algorithms for scientific computation. LMFIT . Convenient non-linear curve fitting. A particularly important change is that Mass 2 leverages Pola.rs for data organization. Therefore, for a critical portion of the code, the larger open-source software community provides the documentation, bug fixes, and testing.","title":"Home"},{"location":"#mass-2-microcalorimeter-analysis-software-suite","text":"","title":"Mass 2: Microcalorimeter Analysis Software Suite"},{"location":"#overview-of-mass-2","text":"Mass is a Python software suite designed to analyze pulse records from high-resolution, cryogenic microcalorimeters. We use Mass with pulse records from x-ray and gamma-ray spectrometers, performing a sequence of analysis, or \"calibration\" steps to extract a high-precision estimate of the energy from each record. With raw pulse records and Mass, you can: Analyze data from one or multiple detectors at one time. Analyze data from one or more raw pulse data files per detector. Analyze a fixed dataset taken in the past, or perform \"online\" analysis of a dataset still being acquired. Analyze data from time-division multiplexed (TDM) and microwave-multiplexed (\u00b5MUX) systems. Compute and apply \"optimal filters\" of various types. With or without raw pulse records, further analysis tasks that Mass helps with include: Choose and apply data cuts. Fix complex line shapes in an energy spectrum. Estimate and apply accurate functions for absolute-energy calibration. Win friends and influence people.","title":"Overview of Mass 2"},{"location":"#major-concepts","text":"Mass2 is built around a few core technologies: Pola.rs , a high-performance modern dataframe library. Organizes the data structures and provides data I/O. Numpy . For numerical arrays and scientific computation. Scipy . Additional algorithms for scientific computation. LMFIT . Convenient non-linear curve fitting. A particularly important change is that Mass 2 leverages Pola.rs for data organization. Therefore, for a critical portion of the code, the larger open-source software community provides the documentation, bug fixes, and testing.","title":"Major concepts"},{"location":"LJHfiles/","text":"LJH Memorial File Format LJH Version 2.2.0 is the current LJH file format (since 2015). An LJH file contains segmented, time-series data that represent separate triggered pulses observed in a single cryogenic microcalorimeter. The LJH file format consists of a human-readable ASCII header followed by an arbitrary number of binary data records . Information in the header specifies the exact length in bytes of each data record. Header Information The human-readable ASCII header is the start of the LJH file. That means you can say less myfile_chan5.ljh at a unix terminal and get meaningful information about the file before the gibberish starts. Handy, right? The header is somewhat fragile (it might have been better written in YAML or TOML or even JSON, but we decided just to live with it). It consists of key-value pairs, with a format of Key: value , one pair per line. Header Notes: Lines begining with # are usually ignored. #End of Header marks the end of the header and the transition to the binary data. #End of Description has special meaning if System description of this File: has been read. Newlines are the newlines of the digitizing computer. The interpreting program must accept LF, CR, or CRLF. Capitalization must be matched. One space follows a colon. Any additional spaces are treated as part of the value. Programs that read LJH files ignore header keys that are unexpected or unneeded. #LJH Memorial File Format This line indicates that the file is based on format described here. Save File Format Version: 2.2.0 Software Version: DASTARD version 0.2.15 Software Git Hash: 85ab821 Data source: Abaco These lines uniquely identify the exact format, so the interpreting program can adapt. While the first line should be sufficient for this purpose, the second and third lines take in the possibility that a particular program may have a bug. The interpreting program may be aware of this bug and compensate. The Data source is meant for later human reference. Number of rows: 74 Number of columns: 1 Row number (from 0-73 inclusive): 12 Column number (from 0-0 inclusive): 0 Number of channels: 74 Channel name: chan12 Channel: 12 ChannelIndex (in dastard): 12 Dastard inserts this information to help downstream analysis tools understand the array being used when this file was acquired. Digitized Word Size in Bytes: 2 Each sample is stored in this many bytes. Location: LLNL Cryostat: C3PO Thermometer: GRT1 Temperature (Ohm or K): 0.1 Bridge range: 20.0E3 Magnetic field (A or Gauss): 0.75 Detector: SnTES#8 Sample: Orange peel Excitation/Source: none Operator: Leisure Larry Like the several lines above, most lines are comments for later human use and are not interpreted by general-purpose LJH readers. System description of this File: blah blah blah User description of this File: blah blah blah #End of Description This is a multiline comment. Once the Description of this File: line is read, all following lines are concantenated until #End of Description is read. Again, this is ignored by programs that read LJH files. Number of Digitizers: 1 Number of Active Channels: 2 The number of digitizers and channels present in the file are given so that space may be allocated for them by the interpreting program, if necessary. Timestamp offset (s): 3016738980.049000 The meaning of this and the means of interpreting it are dependent upon the particular programs creating and reading this file. It was a necessary offset in earlier versions of LJH, where we did not reserve enough bytes per record to record a full timestamp. In LJH 2.2, it serves as a simple zero-time (all records should be no earlier than this \"offset\"). Server Start Time: 18 Nov 2022, 15:47:34 MST First Record Time: 18 Nov 2022, 16:54:15 MST These times show when the server (Dastard, in this case) started running, and when the first record was written to this file. Timebase: 5.000000E-8 Number of samples per point: 1 Timebase gives the sampling period (in seconds). Number of samples per point is generally 1, but can be more in special cases where samples are averaged and downsampled before recording. Presamples: 256 Total Samples: 1024 Total samples is the actual record length in samples. The trigger point will be located at sample number Presamples. Binary Information If you read an LJH file until the characters #End of Header plus the following CR and/or LF, then the remainder of the file is the binary section. It consists of a sequence of data records. Each record starts with a 16-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 16+L*M . All values in the data record are little endian. The first 8-byte word is the subframe counter. It counts the number of subframe times read out since the server started. If the server has to resynchronize on the raw data, then the subframe counter will be incremented by an estimate to account for the time missed. For TDM data, the subframe rate is equal to the row rate, also known as the line rate . For \u00b5MUX data, subframes run at a multiple of the frame rate given by the Subframe divisions: value in the LJH header (typically 64). The second 8-byte word is the POSIX microsecond time, i.e., the time in microseconds since 1 January 1970 00:00 UT. (Warning: this will overflow in only 292,226 years if you interpret it as a signed number.) The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for TDM feedback or for \u00b5MUX data.) Earlier versions of the LJH standard Binary Information (for LJH version 2.1.0) Version 2.1.0 follows. This version was made obsolete in 2015 by version 2.2.0. Each record starts with a 6-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 6+L*M . All values in the data record are little endian. The first byte is a \"4 microsecond tick\". That is, it counts microseconds past the millisecond counter and records the count divided by 4. Beware that previous versions of LJH used the first byte to signify something about the type of data. Igor seems to ignore this byte, though, so I think we're okay to stuff timing information into it. The second byte used to signify a channel number N, which corresponds to the Nth channel described in the header. Channel number 255 is reserved for temperature readout with the DMM. Since 2010, this has always been meaningless. The next 4 bytes are an unsigned 32-bit number that is the value of a millisecond counter on the digitizing computer. The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for the TDM feedback.) Changelog Version 2.2.0 (6 Aug 2015) Changed the binary definition to include 8 bytes each record for pulse timing (microsecond precision) and frame number. Version 2.1.0 (23 Sep 2011) Used the first byte of each record to get 4 microsec timing resolution instead of 1 ms. Version 2.0.0 (27 Mar 2011) Defined inversion and offset more clearly Version 2.0.0 (5 Jan 2001) Changed definition of discrimination level Version 2.0.0 (24 May 2000) since most PCs have least significant byte first, the binary information has been changed to default Version 1.1.0 (8 May 2000) added a few more user and channel parameters as well as provisions for temperature monitoring Initial 1.0.0 (5 Aug 1999) definition by Larry J. Hiller","title":"LJH file format"},{"location":"LJHfiles/#ljh-memorial-file-format","text":"LJH Version 2.2.0 is the current LJH file format (since 2015). An LJH file contains segmented, time-series data that represent separate triggered pulses observed in a single cryogenic microcalorimeter. The LJH file format consists of a human-readable ASCII header followed by an arbitrary number of binary data records . Information in the header specifies the exact length in bytes of each data record.","title":"LJH Memorial File Format"},{"location":"LJHfiles/#header-information","text":"The human-readable ASCII header is the start of the LJH file. That means you can say less myfile_chan5.ljh at a unix terminal and get meaningful information about the file before the gibberish starts. Handy, right? The header is somewhat fragile (it might have been better written in YAML or TOML or even JSON, but we decided just to live with it). It consists of key-value pairs, with a format of Key: value , one pair per line.","title":"Header Information"},{"location":"LJHfiles/#header-notes","text":"Lines begining with # are usually ignored. #End of Header marks the end of the header and the transition to the binary data. #End of Description has special meaning if System description of this File: has been read. Newlines are the newlines of the digitizing computer. The interpreting program must accept LF, CR, or CRLF. Capitalization must be matched. One space follows a colon. Any additional spaces are treated as part of the value. Programs that read LJH files ignore header keys that are unexpected or unneeded. #LJH Memorial File Format This line indicates that the file is based on format described here. Save File Format Version: 2.2.0 Software Version: DASTARD version 0.2.15 Software Git Hash: 85ab821 Data source: Abaco These lines uniquely identify the exact format, so the interpreting program can adapt. While the first line should be sufficient for this purpose, the second and third lines take in the possibility that a particular program may have a bug. The interpreting program may be aware of this bug and compensate. The Data source is meant for later human reference. Number of rows: 74 Number of columns: 1 Row number (from 0-73 inclusive): 12 Column number (from 0-0 inclusive): 0 Number of channels: 74 Channel name: chan12 Channel: 12 ChannelIndex (in dastard): 12 Dastard inserts this information to help downstream analysis tools understand the array being used when this file was acquired. Digitized Word Size in Bytes: 2 Each sample is stored in this many bytes. Location: LLNL Cryostat: C3PO Thermometer: GRT1 Temperature (Ohm or K): 0.1 Bridge range: 20.0E3 Magnetic field (A or Gauss): 0.75 Detector: SnTES#8 Sample: Orange peel Excitation/Source: none Operator: Leisure Larry Like the several lines above, most lines are comments for later human use and are not interpreted by general-purpose LJH readers. System description of this File: blah blah blah User description of this File: blah blah blah #End of Description This is a multiline comment. Once the Description of this File: line is read, all following lines are concantenated until #End of Description is read. Again, this is ignored by programs that read LJH files. Number of Digitizers: 1 Number of Active Channels: 2 The number of digitizers and channels present in the file are given so that space may be allocated for them by the interpreting program, if necessary. Timestamp offset (s): 3016738980.049000 The meaning of this and the means of interpreting it are dependent upon the particular programs creating and reading this file. It was a necessary offset in earlier versions of LJH, where we did not reserve enough bytes per record to record a full timestamp. In LJH 2.2, it serves as a simple zero-time (all records should be no earlier than this \"offset\"). Server Start Time: 18 Nov 2022, 15:47:34 MST First Record Time: 18 Nov 2022, 16:54:15 MST These times show when the server (Dastard, in this case) started running, and when the first record was written to this file. Timebase: 5.000000E-8 Number of samples per point: 1 Timebase gives the sampling period (in seconds). Number of samples per point is generally 1, but can be more in special cases where samples are averaged and downsampled before recording. Presamples: 256 Total Samples: 1024 Total samples is the actual record length in samples. The trigger point will be located at sample number Presamples.","title":"Header Notes:"},{"location":"LJHfiles/#binary-information","text":"If you read an LJH file until the characters #End of Header plus the following CR and/or LF, then the remainder of the file is the binary section. It consists of a sequence of data records. Each record starts with a 16-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 16+L*M . All values in the data record are little endian. The first 8-byte word is the subframe counter. It counts the number of subframe times read out since the server started. If the server has to resynchronize on the raw data, then the subframe counter will be incremented by an estimate to account for the time missed. For TDM data, the subframe rate is equal to the row rate, also known as the line rate . For \u00b5MUX data, subframes run at a multiple of the frame rate given by the Subframe divisions: value in the LJH header (typically 64). The second 8-byte word is the POSIX microsecond time, i.e., the time in microseconds since 1 January 1970 00:00 UT. (Warning: this will overflow in only 292,226 years if you interpret it as a signed number.) The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for TDM feedback or for \u00b5MUX data.)","title":"Binary Information"},{"location":"LJHfiles/#earlier-versions-of-the-ljh-standard","text":"","title":"Earlier versions of the LJH standard"},{"location":"LJHfiles/#binary-information-for-ljh-version-210","text":"Version 2.1.0 follows. This version was made obsolete in 2015 by version 2.2.0. Each record starts with a 6-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 6+L*M . All values in the data record are little endian. The first byte is a \"4 microsecond tick\". That is, it counts microseconds past the millisecond counter and records the count divided by 4. Beware that previous versions of LJH used the first byte to signify something about the type of data. Igor seems to ignore this byte, though, so I think we're okay to stuff timing information into it. The second byte used to signify a channel number N, which corresponds to the Nth channel described in the header. Channel number 255 is reserved for temperature readout with the DMM. Since 2010, this has always been meaningless. The next 4 bytes are an unsigned 32-bit number that is the value of a millisecond counter on the digitizing computer. The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for the TDM feedback.)","title":"Binary Information (for LJH version 2.1.0)"},{"location":"LJHfiles/#changelog","text":"Version 2.2.0 (6 Aug 2015) Changed the binary definition to include 8 bytes each record for pulse timing (microsecond precision) and frame number. Version 2.1.0 (23 Sep 2011) Used the first byte of each record to get 4 microsec timing resolution instead of 1 ms. Version 2.0.0 (27 Mar 2011) Defined inversion and offset more clearly Version 2.0.0 (5 Jan 2001) Changed definition of discrimination level Version 2.0.0 (24 May 2000) since most PCs have least significant byte first, the binary information has been changed to default Version 1.1.0 (8 May 2000) added a few more user and channel parameters as well as provisions for temperature monitoring Initial 1.0.0 (5 Aug 1999) definition by Larry J. Hiller","title":"Changelog"},{"location":"about/","text":"Authors MASS is the work of physicists from Quantum Sensors Division of the NIST Boulder Labs and the University of Colorado Physics Department , with substantial contributions from: Joe Fowler , project director Galen O'Neil , co-director Dan Becker Young-Il Joe Jamie Titus Joshua Ho Grant Mondeel Many collaborators, who have made many bug reports, bug fixes, and feature requests. Major Versions MASS version 2 was begin in August 2025. It is still in beta status. Find it at https://github.com/usnistgov/mass2 MASS version 1 was begun in November 2010. Bug-fix development continues. Find it at https://github.com/usnistgov/mass","title":"About"},{"location":"about/#authors","text":"MASS is the work of physicists from Quantum Sensors Division of the NIST Boulder Labs and the University of Colorado Physics Department , with substantial contributions from: Joe Fowler , project director Galen O'Neil , co-director Dan Becker Young-Il Joe Jamie Titus Joshua Ho Grant Mondeel Many collaborators, who have made many bug reports, bug fixes, and feature requests.","title":"Authors"},{"location":"about/#major-versions","text":"MASS version 2 was begin in August 2025. It is still in beta status. Find it at https://github.com/usnistgov/mass2 MASS version 1 was begun in November 2010. Bug-fix development continues. Find it at https://github.com/usnistgov/mass","title":"Major Versions"},{"location":"demo_plot/","text":"Demonstration of live plots Here's a cell with plots generated by the mkdocs site-generation process: # mkdocs: render import matplotlib.pyplot as plt import numpy as np xpoints = np.linspace(1.1, 10, 100) ypoints = np.log(xpoints) plt.plot(xpoints, ypoints, \".-\")","title":"Demo plot"},{"location":"demo_plot/#demonstration-of-live-plots","text":"Here's a cell with plots generated by the mkdocs site-generation process: # mkdocs: render import matplotlib.pyplot as plt import numpy as np xpoints = np.linspace(1.1, 10, 100) ypoints = np.log(xpoints) plt.plot(xpoints, ypoints, \".-\")","title":"Demonstration of live plots"},{"location":"docstrings/","text":"Automatic documentation generated from docstrings This page is auto-generated from the docstrings of functions, methods, classes, and modules. Each lowest-level module in Mass2 (i.e., each python file) in mass2.core that you want documented and indexed for searching should be listed in this docstrings.md file. The non-core docstrings contain the docstrings for all modules other than mass2.core . Core mass2.core: Core Mass2 functionality, including file I/O, microcalorimeter channel bookkeeping, recipes, fitting, filtering, and more. Data structures and methods for handling a single microcalorimeter channel's pulse data and metadata. BadChannel dataclass A wrapper around Channel that includes error information. Source code in mass2/core/channel.py 1107 1108 1109 1110 1111 1112 1113 1114 @dataclass ( frozen = True ) class BadChannel : \"\"\"A wrapper around Channel that includes error information.\"\"\" ch : Channel error_type : type | None error_msg : str backtrace : str | None Channel dataclass A single microcalorimeter channel's pulse data and associated metadata. Source code in mass2/core/channel.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 @dataclass ( frozen = True ) # noqa: PLR0904 class Channel : \"\"\"A single microcalorimeter channel's pulse data and associated metadata.\"\"\" df : pl . DataFrame = field ( repr = False ) header : ChannelHeader = field ( repr = True ) npulses : int subframediv : int | None = None noise : NoiseChannel | None = field ( default = None , repr = False ) good_expr : pl . Expr = field ( default_factory = alwaysTrue ) df_history : list [ pl . DataFrame ] = field ( default_factory = list , repr = False ) steps : Recipe = field ( default_factory = Recipe . new_empty , repr = False ) steps_elapsed_s : list [ float ] = field ( default_factory = list ) transform_raw : Callable | None = None @property def shortname ( self ) -> str : \"\"\"A short name for this channel, suitable for plot titles.\"\"\" return self . header . description def mo_stepplots ( self ) -> mo . ui . dropdown : \"\"\"Marimo UI element to choose and display step plots, with a dropdown to choose channel number.\"\"\" desc_ind = { step . description : i for i , step in enumerate ( self . steps )} first_non_summarize_step = self . steps [ 0 ] for step in self . steps : if isinstance ( step , SummarizeStep ): continue first_non_summarize_step = step break mo_ui = mo . ui . dropdown ( desc_ind , value = first_non_summarize_step . description , label = f \"choose step for ch { self . header . ch_num } \" , ) def show () -> mo . Html : \"\"\"Show the selected step plot.\"\"\" return self . _mo_stepplots_explicit ( mo_ui ) def step_ind () -> Any : \"\"\"Get the selected step index from the dropdown item, if any.\"\"\" return mo_ui . value mo_ui . show = show mo_ui . step_ind = step_ind return mo_ui def _mo_stepplots_explicit ( self , mo_ui : mo . ui . dropdown ) -> mo . Html : \"\"\"Marimo UI element to choose and display step plots.\"\"\" step_ind = mo_ui . value self . step_plot ( step_ind ) fig = plt . gcf () return mo . vstack ([ mo_ui , misc . show ( fig )]) def get_step ( self , index : int ) -> tuple [ RecipeStep , int ]: \"\"\"Get the step at the given index, supporting negative indices.\"\"\" # normalize the index to a positive index if index < 0 : index = len ( self . steps ) + index step = self . steps [ index ] return step , index def step_plot ( self , step_ind : int , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a debug plot for the given step index, supporting negative indices.\"\"\" step , step_ind = self . get_step ( step_ind ) if step_ind + 1 == len ( self . df_history ): df_after = self . df else : df_after = self . df_history [ step_ind + 1 ] return step . dbg_plot ( df_after , ** kwargs ) def hist ( self , col : str , bin_edges : ArrayLike , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute a histogram of the given column, optionally filtering by good_expr and use_expr.\"\"\" if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col )) . collect () values = df_small [ col ] bin_centers , counts = misc . hist_of_series ( values , bin_edges ) return bin_centers , counts def plot_hist ( self , col : str , bin_edges : ArrayLike , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute and plot a histogram of the given column, optionally filtering by good_expr and use_expr.\"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis bin_centers , counts = self . hist ( col , bin_edges = bin_edges , use_good_expr = use_good_expr , use_expr = use_expr ) _ , step_size = misc . midpoints_and_step_size ( bin_edges ) plt . step ( bin_centers , counts , where = \"mid\" ) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } for { self . shortname } \" ) plt . tight_layout () return bin_centers , counts def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : str , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), skip_none : bool = True , ) -> tuple [ NDArray , dict [ str , NDArray ]]: \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group counts_dict : dict [ str , NDArray ] = {} for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] _ , step_size = misc . midpoints_and_step_size ( bin_edges ) bin_centers , counts = misc . hist_of_series ( values , bin_edges ) group_name_str = str ( group_name ) counts_dict [ group_name_str ] = counts plt . step ( bin_centers , counts , where = \"mid\" , label = group_name_str ) # Plot the histogram for the current group # if group_name == \"EBIT\": # ax.hist(values, bins=bin_edges, alpha=0.9, color=\"k\", label=group_name_str) # else: # ax.hist(values, bins=bin_edges, alpha=0.5, label=group_name_str) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () return bin_centers , counts_dict def plot_scatter ( self , x_col : str , y_col : str , color_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), use_good_expr : bool = True , skip_none : bool = True , ax : plt . Axes | None = None , ) -> None : \"\"\"Generate a scatter plot of `y_col` vs `x_col`, optionally colored by `color_col`.\"\"\" if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # set current axis so I can use plt api if use_good_expr : filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = self . good_expr df_small = self . df . lazy () . filter ( filter_expr ) . select ( x_col , y_col , color_col ) . collect () for ( name ,), data in df_small . group_by ( color_col , maintain_order = True ): if name is None and skip_none and color_col is not None : continue plt . plot ( data . select ( x_col ) . to_series (), data . select ( y_col ) . to_series (), \".\" , label = name , ) plt . xlabel ( str ( x_col )) plt . ylabel ( str ( y_col )) title_str = f \"\"\" { self . header . description } use_expr= { str ( use_expr ) } good_expr= { str ( self . good_expr ) } \"\"\" plt . title ( title_str ) if color_col is not None : plt . legend ( title = color_col ) plt . tight_layout () def good_series ( self , col : str , use_expr : pl . Expr = pl . lit ( True )) -> pl . Series : \"\"\"Return a Polars Series of the given column, filtered by good_expr and use_expr.\"\"\" return mass2 . misc . good_series ( self . df , col , self . good_expr , use_expr ) @property def last_avg_pulse ( self ) -> NDArray | None : \"\"\"Return the average pulse stored in the last recipe step that's an optimal filter step Returns ------- NDArray | None The last filtering step's signal model, or None if no such step \"\"\" for step in reversed ( self . steps ): if isinstance ( step , OptimalFilterStep ): return step . filter_maker . signal_model return None @property def last_filter ( self ) -> NDArray | None : \"\"\"Return the average pulse stored in the last recipe step that's an optimal filter step Returns ------- NDArray | None The last filtering step's signal model, or None if no such step \"\"\" for step in reversed ( self . steps ): if isinstance ( step , OptimalFilterStep ): return step . filter . values return None @property def last_noise_psd ( self ) -> tuple [ NDArray , NDArray ] | None : \"\"\"Return the noise PSD stored in the last recipe step that's an optimal filter step Returns ------- tuple[NDArray, NDArray] | None The last filtering step's (frequencies, noise spectrum), or None if no such step \"\"\" for step in reversed ( self . steps ): if isinstance ( step , OptimalFilterStep ) and step . spectrum is not None : return step . spectrum . frequencies , step . spectrum . psd return None @property def last_noise_autocorrelation ( self ) -> NDArray | None : \"\"\"Return the noise autocorrelation stored in the last recipe step that's an optimal filter step Returns ------- NDArray | None The last filtering step's noise autocorrelation, or None if no such step \"\"\" for step in reversed ( self . steps ): if isinstance ( step , OptimalFilterStep ) and step . spectrum is not None : return step . spectrum . autocorr_vec return None def rough_cal_combinatoric ( self , line_names : list [ str ], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying all combinatorically possible peak assignments.\"\"\" step = mass2 . core . RoughCalibrationStep . learn_combinatoric ( self , line_names , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal_combinatoric_height_info ( self , line_names : list [ str ], line_heights_allowed : list [ list [ int ]], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying all combinatorically possible peak assignments, using known relative peak heights to limit the possibilities.\"\"\" step = mass2 . core . RoughCalibrationStep . learn_combinatoric_height_info ( self , line_names , line_heights_allowed , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal ( # noqa: PLR0917 self , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying to assign the 3 brightest peaks, then fitting a line to those and looking for other peaks that fit that line. \"\"\" step = mass2 . core . RoughCalibrationStep . learn_3peak ( self , line_names , uncalibrated_col , calibrated_col , use_expr , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , fwhm_pulse_height_units , n_extra_peaks , acceptable_rms_residual_e , ) return self . with_step ( step ) def with_step ( self , step : RecipeStep ) -> \"Channel\" : \"\"\"Return a new Channel with the given step applied to generate new columns in the dataframe.\"\"\" t_start = time . time () df2 = step . calc_from_df ( self . df ) elapsed_s = time . time () - t_start ch2 = dataclasses . replace ( self , df = df2 , good_expr = step . good_expr , df_history = self . df_history + [ self . df ], steps = self . steps . with_step ( step ), steps_elapsed_s = self . steps_elapsed_s + [ elapsed_s ], ) return ch2 def with_steps ( self , steps : Recipe ) -> \"Channel\" : \"\"\"Return a new Channel with the given steps applied to generate new columns in the dataframe.\"\"\" ch2 = self for step in steps : ch2 = ch2 . with_step ( step ) return ch2 def with_good_expr ( self , good_expr : pl . Expr , replace : bool = False ) -> \"Channel\" : \"\"\"Return a new Channel with the given good_expr, combined with the existing good_expr by \"and\", of by replacing it entirely if `replace` is True.\"\"\" # the default value of self.good_expr is pl.lit(True) # and_(True) will just add visual noise when looking at good_expr and not affect behavior if not replace and good_expr is not True and not good_expr . meta . eq ( pl . lit ( True )): good_expr = good_expr . and_ ( self . good_expr ) return dataclasses . replace ( self , good_expr = good_expr ) def with_column_map_step ( self , input_col : str , output_col : str , f : Callable ) -> \"Channel\" : \"\"\"f should take a numpy array and return a numpy array with the same number of elements\"\"\" step = mass2 . core . recipe . ColumnAsNumpyMapStep ([ input_col ], [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), f = f ) return self . with_step ( step ) def with_good_expr_pretrig_rms_and_postpeak_deriv ( self , n_sigma_pretrig_rms : float = 20 , n_sigma_postpeak_deriv : float = 20 , replace : bool = False ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with pretrigger RMS or postpeak derivative above outlier-resistant thresholds.\"\"\" max_postpeak_deriv = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"postpeak_deriv\" ] . to_numpy (), nsigma = n_sigma_postpeak_deriv ) max_pretrig_rms = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"pretrig_rms\" ] . to_numpy (), nsigma = n_sigma_pretrig_rms ) good_expr = ( pl . col ( \"postpeak_deriv\" ) < max_postpeak_deriv ) . and_ ( pl . col ( \"pretrig_rms\" ) < max_pretrig_rms ) return self . with_good_expr ( good_expr , replace ) def with_range_around_median ( self , col : str , range_up : float , range_down : float ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with `col` outside the given range around its median.\"\"\" med = np . median ( self . df [ col ] . to_numpy ()) return self . with_good_expr ( pl . col ( col ) . is_between ( med - range_down , med + range_up )) def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) @functools . cache def typical_peak_ind ( self , col : str = \"pulse\" ) -> int : \"\"\"Return the typical peak index of the given column, using the median peak index for the first 100 pulses.\"\"\" raw = self . df . limit ( 100 )[ col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) return int ( np . median ( raw . argmax ( axis = 1 ))) def summarize_pulses ( self , col : str = \"pulse\" , pretrigger_ignore_samples : int = 0 , peak_index : int | None = None ) -> \"Channel\" : \"\"\"Summarize the pulses, adding columns for pulse height, pretrigger mean, etc.\"\"\" if peak_index is None : peak_index = self . typical_peak_ind ( col ) out_names = mass2 . core . pulse_algorithms . result_dtype . names # mypy (incorrectly) thinks `out_names` might be None, and `list(None)` is forbidden. Assertion makes it happy again. assert out_names is not None outputs = list ( out_names ) step = SummarizeStep ( inputs = [ col ], output = outputs , good_expr = self . good_expr , use_expr = pl . lit ( True ), frametime_s = self . header . frametime_s , peak_index = peak_index , pulse_col = col , pretrigger_ignore_samples = pretrigger_ignore_samples , n_presamples = self . header . n_presamples , transform_raw = self . transform_raw , ) return self . with_step ( step ) def correct_pretrig_mean_jumps ( self , uncorrected : str = \"pretrig_mean\" , corrected : str = \"ptm_jf\" , period : int = 4096 ) -> \"Channel\" : \"\"\"Correct pretrigger mean jumps in the raw pulse data, writing to a new column.\"\"\" step = mass2 . core . recipe . PretrigMeanJumpFixStep ( inputs = [ uncorrected ], output = [ corrected ], good_expr = self . good_expr , use_expr = pl . lit ( True ), period = period , ) return self . with_step ( step ) def with_select_step ( self , col_expr_dict : dict [ str , pl . Expr ]) -> \"Channel\" : \"\"\" This step is meant for interactive exploration; it's basically like the df.select() method, but it's saved as a step. \"\"\" extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in col_expr_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . SelectStep ( inputs = list ( inputs ), output = list ( col_expr_dict . keys ()), good_expr = self . good_expr , use_expr = pl . lit ( True ), col_expr_dict = col_expr_dict , ) return self . with_step ( step ) def with_categorize_step ( self , category_condition_dict : dict [ str , pl . Expr ], output_col : str = \"category\" ) -> \"Channel\" : \"\"\"Add a recipe step that categorizes pulses based on the given conditions.\"\"\" # ensure the first condition is True, to be used as a fallback first_expr = next ( iter ( category_condition_dict . values ())) if not first_expr . meta . eq ( pl . lit ( True )): category_condition_dict = { \"fallback\" : pl . lit ( True ), ** category_condition_dict } extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in category_condition_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . CategorizeStep ( inputs = list ( inputs ), output = [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), category_condition_dict = category_condition_dict , ) return self . with_step ( step ) def compute_average_pulse ( self , pulse_col : str = \"pulse\" , use_expr : pl . Expr = pl . lit ( True ), limit : int = 2000 ) -> NDArray : \"\"\"Compute an average pulse given a use expression. Parameters ---------- pulse_col : str, optional Name of the column in self.df containing raw pulses, by default \"pulse\" use_expr : pl.Expr, optional Selection (in addition to self.good_expr) to use, by default pl.lit(True) limit : int, optional Use no more than this many pulses, by default 2000 Returns ------- NDArray _description_ \"\"\" avg_pulse = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( pulse_col ) . limit ( limit ) . collect () . to_series () . to_numpy () . mean ( axis = 0 ) ) avg_pulse -= avg_pulse [: self . header . n_presamples ] . mean () return avg_pulse def filter5lag ( self , pulse_col : str = \"pulse\" , peak_y_col : str = \"5lagy\" , peak_x_col : str = \"5lagx\" , f_3db : float = 25e3 , use_expr : pl . Expr = pl . lit ( True ), time_constant_s_of_exp_to_be_orthogonal_to : float | None = None , ) -> \"Channel\" : \"\"\"Compute a 5-lag optimal filter and apply it. Parameters ---------- pulse_col : str, optional Which column contains raw data, by default \"pulse\" peak_y_col : str, optional Column to contain the optimal filter results, by default \"5lagy\" peak_x_col : str, optional Column to contain the 5-lag filter's estimate of arrival-time/phase, by default \"5lagx\" f_3db : float, optional A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr : pl.Expr, optional An expression to select pulses for averaging, by default pl.lit(True) time_constant_s_of_exp_to_be_orthogonal_to : float | None, optional Optionally an exponential decay time to make the filter insensitive to, by default None Returns ------- Channel This channel with a Filter5LagStep added to the recipe. \"\"\" assert self . noise noiseresult = self . noise . spectrum ( trunc_back = 2 , trunc_front = 2 ) avg_pulse = self . compute_average_pulse ( pulse_col = pulse_col , use_expr = use_expr ) filter_maker = FilterMaker ( signal_model = avg_pulse , n_pretrigger = self . header . n_presamples , noise_psd = noiseresult . psd , noise_autocorr = noiseresult . autocorr_vec , sample_time_sec = self . header . frametime_s , ) if time_constant_s_of_exp_to_be_orthogonal_to is None : filter5lag = filter_maker . compute_5lag ( f_3db = f_3db ) else : filter5lag = filter_maker . compute_5lag_noexp ( f_3db = f_3db , exp_time_seconds = time_constant_s_of_exp_to_be_orthogonal_to ) step = OptimalFilterStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter5lag , spectrum = noiseresult , filter_maker = filter_maker , transform_raw = self . transform_raw , ) return self . with_step ( step ) def compute_ats_model ( self , pulse_col : str , use_expr : pl . Expr = pl . lit ( True ), limit : int = 2000 ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute the average pulse and arrival-time model for an ATS filter. We use the first `limit` pulses that pass `good_expr` and `use_expr`. Parameters ---------- pulse_col : str _description_ use_expr : pl.Expr, optional _description_, by default pl.lit(True) limit : int, optional _description_, by default 2000 Returns ------- tuple[NDArray, NDArray] _description_ \"\"\" df = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . limit ( limit ) . select ( pulse_col , \"pulse_rms\" , \"promptness\" , \"pretrig_mean\" ) . collect () ) # Adjust promptness: subtract a linear trend with pulse_rms prms = df [ \"pulse_rms\" ] . to_numpy () promptness = df [ \"promptness\" ] . to_numpy () poly = np . poly1d ( np . polyfit ( prms , promptness , 1 )) df = df . with_columns ( promptshifted = ( promptness - poly ( prms ))) # Rescale promptness quadratically to span approximately [-0.5, +0.5], dropping any pulses with abs(t) > 0.45. x , y , z = np . percentile ( df [ \"promptshifted\" ], [ 10 , 50 , 90 ]) A = np . array ([[ x * x , x , 1 ], [ y * y , y , 1 ], [ z * z , z , 1 ]]) param = np . linalg . solve ( A , [ - 0.4 , 0 , + 0.4 ]) ATime = np . poly1d ( param )( df [ \"promptshifted\" ]) df = df . with_columns ( ATime = ATime ) . filter ( np . abs ( ATime ) < 0.45 ) . drop ( \"promptshifted\" ) # Compute mean pulse and dt model as the offset and slope of a linear fit to each pulse sample vs ATime pulse = df [ \"pulse\" ] . to_numpy () avg_pulse = np . zeros ( self . header . n_samples , dtype = float ) dt_model = np . zeros ( self . header . n_samples , dtype = float ) for i in range ( self . header . n_presamples , self . header . n_samples ): slope , offset = np . polyfit ( df [ \"ATime\" ], ( pulse [:, i ] - df [ \"pretrig_mean\" ]), 1 ) dt_model [ i ] = - slope avg_pulse [ i ] = offset return avg_pulse , dt_model def filterATS ( self , pulse_col : str = \"pulse\" , peak_y_col : str = \"ats_y\" , peak_x_col : str = \"ats_x\" , f_3db : float = 25e3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Compute an arrival-time-safe (ATS) optimal filter and apply it. Parameters ---------- pulse_col : str, optional Which column contains raw data, by default \"pulse\" peak_y_col : str, optional Column to contain the optimal filter results, by default \"ats_y\" peak_x_col : str, optional Column to contain the ATS filter's estimate of arrival-time/phase, by default \"ats_x\" f_3db : float, optional A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr : pl.Expr, optional An expression to select pulses for averaging, by default pl.lit(True) Returns ------- Channel This channel with a Filter5LagStep added to the recipe. \"\"\" assert self . noise mprms = self . good_series ( \"pulse_rms\" , use_expr ) . median () use = use_expr . and_ ( np . abs ( pl . col ( \"pulse_rms\" ) / mprms - 1.0 ) < 0.3 ) limit = 4000 avg_pulse , dt_model = self . compute_ats_model ( pulse_col , use , limit ) noiseresult = self . noise . spectrum () filter_maker = FilterMaker ( signal_model = avg_pulse , dt_model = dt_model , n_pretrigger = self . header . n_presamples , noise_psd = noiseresult . psd , noise_autocorr = noiseresult . autocorr_vec , sample_time_sec = self . header . frametime_s , ) filter_ats = filter_maker . compute_ats ( f_3db = f_3db ) step = OptimalFilterStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter_ats , spectrum = noiseresult , filter_maker = filter_maker , transform_raw = self . transform_raw , ) return self . with_step ( step ) def good_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : \"\"\"Return a Polars DataFrame of the given columns, filtered by good_expr and use_expr.\"\"\" good_df = self . df . lazy () . filter ( self . good_expr ) if use_expr is not True : good_df = good_df . filter ( use_expr ) return good_df . select ( cols ) . collect () def bad_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : \"\"\"Return a Polars DataFrame of the given columns, filtered by the inverse of good_expr, and use_expr.\"\"\" bad_df = self . df . lazy () . filter ( self . good_expr . not_ ()) if use_expr is not True : bad_df = bad_df . filter ( use_expr ) return bad_df . select ( cols ) . collect () def good_serieses ( self , cols : list [ str ], use_expr : pl . Expr = pl . lit ( True )) -> list [ pl . Series ]: \"\"\"Return a list of Polars Series of the given columns, filtered by good_expr and use_expr.\"\"\" df2 = self . good_df ( cols , use_expr ) return [ df2 [ col ] for col in cols ] def driftcorrect ( self , indicator_col : str = \"pretrig_mean\" , uncorrected_col : str = \"5lagy\" , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Correct for gain drift correlated with the given indicator column.\"\"\" # by defining a seperate learn method that takes ch as an argument, # we can move all the code for the step outside of Channel step = DriftCorrectStep . learn ( ch = self , indicator_col = indicator_col , uncorrected_col = uncorrected_col , corrected_col = corrected_col , use_expr = use_expr , ) return self . with_step ( step ) def linefit ( # noqa: PLR0917 self , line : GenericLineModel | SpectralLine | str | float , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : \"\"\"Fit a spectral line to the binned data from the given column, optionally filtering by use_expr.\"\"\" model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = self . header . description , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def step_summary ( self ) -> list [ tuple [ str , float ]]: \"\"\"Return a list of (step type name, elapsed time in seconds) for each step in the recipe.\"\"\" return [( type ( a ) . __name__ , b ) for ( a , b ) in zip ( self . steps , self . steps_elapsed_s )] def __hash__ ( self ) -> int : \"\"\"Return a hash based on the object's id.\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other : object ) -> bool : \"\"\"Return True if the other object is the same object (by id).\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results # only checks if the ids match, does not try to be equal if all contents are equal return id ( self ) == id ( other ) @classmethod def from_ljh ( cls , path : str | Path , noise_path : str | Path | None = None , keep_posix_usec : bool = False , transform_raw : Callable | None = None , ) -> \"Channel\" : \"\"\"Load a Channel from an LJH file, optionally with a NoiseChannel from a corresponding noise LJH file.\"\"\" if not noise_path : noise_channel = None else : noise_channel = NoiseChannel . from_ljh ( noise_path ) ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars ( keep_posix_usec ) header = ChannelHeader . from_ljh_header_df ( header_df ) channel = cls ( df , header = header , npulses = ljh . npulses , subframediv = ljh . subframediv , noise = noise_channel , transform_raw = transform_raw ) return channel @classmethod def from_off ( cls , off : OffFile ) -> \"Channel\" : \"\"\"Load a Channel from an OFF file.\"\"\" assert off . _mmap is not None df = pl . from_numpy ( np . asarray ( off . _mmap )) df = ( df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) . select ( pl . exclude ( \"unixnano\" )) ) df_header = pl . DataFrame ( off . header ) df_header = df_header . with_columns ( pl . Series ( \"Filename\" , [ off . filename ])) header = ChannelHeader ( f \" { os . path . split ( off . filename )[ 1 ] } \" , off . header [ \"ChannelNumberMatchingName\" ], off . framePeriodSeconds , off . _mmap [ \"recordPreSamples\" ][ 0 ], off . _mmap [ \"recordSamples\" ][ 0 ], df_header , ) channel = cls ( df , header , off . nRecords , subframediv = off . subframediv ) return channel def with_experiment_state_df ( self , df_es : pl . DataFrame , force_timestamp_monotonic : bool = False ) -> \"Channel\" : \"\"\"Add experiment states from an existing dataframe\"\"\" if not self . df [ \"timestamp\" ] . is_sorted (): df = self . df . select ( pl . col ( \"timestamp\" ) . cum_max () . alias ( \"timestamp\" )) . with_columns ( self . df . select ( pl . exclude ( \"timestamp\" ))) # print(\"WARNING: in with_experiment_state_df, timestamp is not monotonic, forcing it to be\") # print(\"This is likely a BUG in DASTARD.\") else : df = self . df df2 = df . join_asof ( df_es , on = \"timestamp\" , strategy = \"backward\" ) return self . with_replacement_df ( df2 ) def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channel\" : \"\"\"Add external trigger times from an existing dataframe\"\"\" df2 = ( self . df . with_columns ( subframecount = pl . col ( \"framecount\" ) * self . subframediv ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"backward\" , coalesce = False , suffix = \"_prev_ext_trig\" ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"forward\" , coalesce = False , suffix = \"_next_ext_trig\" ) ) return self . with_replacement_df ( df2 ) def with_replacement_df ( self , df2 : pl . DataFrame ) -> \"Channel\" : \"\"\"Replace the dataframe with a new one, keeping all other attributes the same.\"\"\" return dataclasses . replace ( self , df = df2 , ) def with_columns ( self , df2 : pl . DataFrame ) -> \"Channel\" : \"\"\"Append columns from df2 to the existing dataframe, keeping all other attributes the same.\"\"\" df3 = self . df . with_columns ( df2 ) return self . with_replacement_df ( df3 ) def multifit_quadratic_gain_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Fit multiple spectral lines, to create a quadratic gain calibration.\"\"\" step = MultiFitQuadraticGainStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def multifit_mass_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Fit multiple spectral lines, to create a Mass1-style gain calibration.\"\"\" step = MultiFitMassCalibrationStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def concat_df ( self , df : pl . DataFrame ) -> \"Channel\" : \"\"\"Concat the given dataframe to the existing dataframe, keeping all other attributes the same. If the new frame `df` has a history and/or steps, those will be lost\"\"\" ch2 = Channel ( mass2 . core . misc . concat_dfs_with_concat_state ( self . df , df ), self . header , self . npulses , subframediv = self . subframediv , noise = self . noise , good_expr = self . good_expr , ) # we won't copy over df_history and steps. I don't think you should use this when those are filled in? return ch2 def concat_ch ( self , ch : \"Channel\" ) -> \"Channel\" : \"\"\"Concat the given channel's dataframe to the existing dataframe, keeping all other attributes the same. If the new channel `ch` has a history and/or steps, those will be lost\"\"\" ch2 = self . concat_df ( ch . df ) return ch2 def phase_correct_mass_specific_lines ( self , indicator_col : str , uncorrected_col : str , line_names : Iterable [ str | float ], previous_cal_step_index : int , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Apply phase correction to the given uncorrected column, where specific lines are used to judge the correction.\"\"\" if corrected_col is None : corrected_col = uncorrected_col + \"_pc\" step = mass2 . core . phase_correct_steps . phase_correct_mass_specific_lines ( self , indicator_col , uncorrected_col , corrected_col , previous_cal_step_index , line_names , use_expr , ) return self . with_step ( step ) def as_bad ( self , error_type : type | None , error_msg : str , backtrace : str | None ) -> \"BadChannel\" : \"\"\"Return a BadChannel object, which wraps this Channel and includes error information.\"\"\" return BadChannel ( self , error_type , error_msg , backtrace ) def save_recipes ( self , filename : str ) -> dict [ int , Recipe ]: \"\"\"Save the recipe steps to a pickle file, keyed by channel number.\"\"\" steps = { self . header . ch_num : self . steps } misc . pickle_object ( steps , filename ) return steps def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr_in: pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample: int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log: bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) contents , _ , _ = plt . hist ( y , 200 , range = limits , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" ) def fit_pulse ( self , index : int = 0 , col : str = \"pulse\" , verbose : bool = True ) -> LineModelResult : \"\"\"Fit a single pulse to a 2-exponential-with-tail model, returning the fit result.\"\"\" pulse = self . df [ col ][ index ] . to_numpy () result = mass2 . core . pulse_algorithms . fit_pulse_2exp_with_tail ( pulse , npre = self . header . n_presamples , dt = self . header . frametime_s ) if verbose : print ( f \"ch= { self } \" ) print ( f \"pulse index= { index } \" ) print ( result . fit_report ()) return result last_avg_pulse property Return the average pulse stored in the last recipe step that's an optimal filter step Returns: NDArray | None \u2013 The last filtering step's signal model, or None if no such step last_filter property Return the average pulse stored in the last recipe step that's an optimal filter step Returns: NDArray | None \u2013 The last filtering step's signal model, or None if no such step last_noise_autocorrelation property Return the noise autocorrelation stored in the last recipe step that's an optimal filter step Returns: NDArray | None \u2013 The last filtering step's noise autocorrelation, or None if no such step last_noise_psd property Return the noise PSD stored in the last recipe step that's an optimal filter step Returns: tuple [ NDArray , NDArray ] | None \u2013 The last filtering step's (frequencies, noise spectrum), or None if no such step shortname property A short name for this channel, suitable for plot titles. __eq__ ( other ) Return True if the other object is the same object (by id). Source code in mass2/core/channel.py 859 860 861 862 863 864 865 def __eq__ ( self , other : object ) -> bool : \"\"\"Return True if the other object is the same object (by id).\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results # only checks if the ids match, does not try to be equal if all contents are equal return id ( self ) == id ( other ) __hash__ () Return a hash based on the object's id. Source code in mass2/core/channel.py 852 853 854 855 856 857 def __hash__ ( self ) -> int : \"\"\"Return a hash based on the object's id.\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) as_bad ( error_type , error_msg , backtrace ) Return a BadChannel object, which wraps this Channel and includes error information. Source code in mass2/core/channel.py 1020 1021 1022 def as_bad ( self , error_type : type | None , error_msg : str , backtrace : str | None ) -> \"BadChannel\" : \"\"\"Return a BadChannel object, which wraps this Channel and includes error information.\"\"\" return BadChannel ( self , error_type , error_msg , backtrace ) bad_df ( cols = pl . all (), use_expr = pl . lit ( True )) Return a Polars DataFrame of the given columns, filtered by the inverse of good_expr, and use_expr. Source code in mass2/core/channel.py 783 784 785 786 787 788 def bad_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : \"\"\"Return a Polars DataFrame of the given columns, filtered by the inverse of good_expr, and use_expr.\"\"\" bad_df = self . df . lazy () . filter ( self . good_expr . not_ ()) if use_expr is not True : bad_df = bad_df . filter ( use_expr ) return bad_df . select ( cols ) . collect () compute_ats_model ( pulse_col , use_expr = pl . lit ( True ), limit = 2000 ) Compute the average pulse and arrival-time model for an ATS filter. We use the first limit pulses that pass good_expr and use_expr . Parameters: pulse_col ( str ) \u2013 description use_expr ( Expr , default: lit (True) ) \u2013 description , by default pl.lit(True) limit ( int , default: 2000 ) \u2013 description , by default 2000 Returns: tuple [ NDArray , NDArray ] \u2013 description Source code in mass2/core/channel.py 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 def compute_ats_model ( self , pulse_col : str , use_expr : pl . Expr = pl . lit ( True ), limit : int = 2000 ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute the average pulse and arrival-time model for an ATS filter. We use the first `limit` pulses that pass `good_expr` and `use_expr`. Parameters ---------- pulse_col : str _description_ use_expr : pl.Expr, optional _description_, by default pl.lit(True) limit : int, optional _description_, by default 2000 Returns ------- tuple[NDArray, NDArray] _description_ \"\"\" df = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . limit ( limit ) . select ( pulse_col , \"pulse_rms\" , \"promptness\" , \"pretrig_mean\" ) . collect () ) # Adjust promptness: subtract a linear trend with pulse_rms prms = df [ \"pulse_rms\" ] . to_numpy () promptness = df [ \"promptness\" ] . to_numpy () poly = np . poly1d ( np . polyfit ( prms , promptness , 1 )) df = df . with_columns ( promptshifted = ( promptness - poly ( prms ))) # Rescale promptness quadratically to span approximately [-0.5, +0.5], dropping any pulses with abs(t) > 0.45. x , y , z = np . percentile ( df [ \"promptshifted\" ], [ 10 , 50 , 90 ]) A = np . array ([[ x * x , x , 1 ], [ y * y , y , 1 ], [ z * z , z , 1 ]]) param = np . linalg . solve ( A , [ - 0.4 , 0 , + 0.4 ]) ATime = np . poly1d ( param )( df [ \"promptshifted\" ]) df = df . with_columns ( ATime = ATime ) . filter ( np . abs ( ATime ) < 0.45 ) . drop ( \"promptshifted\" ) # Compute mean pulse and dt model as the offset and slope of a linear fit to each pulse sample vs ATime pulse = df [ \"pulse\" ] . to_numpy () avg_pulse = np . zeros ( self . header . n_samples , dtype = float ) dt_model = np . zeros ( self . header . n_samples , dtype = float ) for i in range ( self . header . n_presamples , self . header . n_samples ): slope , offset = np . polyfit ( df [ \"ATime\" ], ( pulse [:, i ] - df [ \"pretrig_mean\" ]), 1 ) dt_model [ i ] = - slope avg_pulse [ i ] = offset return avg_pulse , dt_model compute_average_pulse ( pulse_col = 'pulse' , use_expr = pl . lit ( True ), limit = 2000 ) Compute an average pulse given a use expression. Parameters: pulse_col ( str , default: 'pulse' ) \u2013 Name of the column in self.df containing raw pulses, by default \"pulse\" use_expr ( Expr , default: lit (True) ) \u2013 Selection (in addition to self.good_expr) to use, by default pl.lit(True) limit ( int , default: 2000 ) \u2013 Use no more than this many pulses, by default 2000 Returns: NDArray \u2013 description Source code in mass2/core/channel.py 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 def compute_average_pulse ( self , pulse_col : str = \"pulse\" , use_expr : pl . Expr = pl . lit ( True ), limit : int = 2000 ) -> NDArray : \"\"\"Compute an average pulse given a use expression. Parameters ---------- pulse_col : str, optional Name of the column in self.df containing raw pulses, by default \"pulse\" use_expr : pl.Expr, optional Selection (in addition to self.good_expr) to use, by default pl.lit(True) limit : int, optional Use no more than this many pulses, by default 2000 Returns ------- NDArray _description_ \"\"\" avg_pulse = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( pulse_col ) . limit ( limit ) . collect () . to_series () . to_numpy () . mean ( axis = 0 ) ) avg_pulse -= avg_pulse [: self . header . n_presamples ] . mean () return avg_pulse concat_ch ( ch ) Concat the given channel's dataframe to the existing dataframe, keeping all other attributes the same. If the new channel ch has a history and/or steps, those will be lost Source code in mass2/core/channel.py 991 992 993 994 995 def concat_ch ( self , ch : \"Channel\" ) -> \"Channel\" : \"\"\"Concat the given channel's dataframe to the existing dataframe, keeping all other attributes the same. If the new channel `ch` has a history and/or steps, those will be lost\"\"\" ch2 = self . concat_df ( ch . df ) return ch2 concat_df ( df ) Concat the given dataframe to the existing dataframe, keeping all other attributes the same. If the new frame df has a history and/or steps, those will be lost Source code in mass2/core/channel.py 977 978 979 980 981 982 983 984 985 986 987 988 989 def concat_df ( self , df : pl . DataFrame ) -> \"Channel\" : \"\"\"Concat the given dataframe to the existing dataframe, keeping all other attributes the same. If the new frame `df` has a history and/or steps, those will be lost\"\"\" ch2 = Channel ( mass2 . core . misc . concat_dfs_with_concat_state ( self . df , df ), self . header , self . npulses , subframediv = self . subframediv , noise = self . noise , good_expr = self . good_expr , ) # we won't copy over df_history and steps. I don't think you should use this when those are filled in? return ch2 correct_pretrig_mean_jumps ( uncorrected = 'pretrig_mean' , corrected = 'ptm_jf' , period = 4096 ) Correct pretrigger mean jumps in the raw pulse data, writing to a new column. Source code in mass2/core/channel.py 534 535 536 537 538 539 540 541 542 543 544 545 def correct_pretrig_mean_jumps ( self , uncorrected : str = \"pretrig_mean\" , corrected : str = \"ptm_jf\" , period : int = 4096 ) -> \"Channel\" : \"\"\"Correct pretrigger mean jumps in the raw pulse data, writing to a new column.\"\"\" step = mass2 . core . recipe . PretrigMeanJumpFixStep ( inputs = [ uncorrected ], output = [ corrected ], good_expr = self . good_expr , use_expr = pl . lit ( True ), period = period , ) return self . with_step ( step ) driftcorrect ( indicator_col = 'pretrig_mean' , uncorrected_col = '5lagy' , corrected_col = None , use_expr = pl . lit ( True )) Correct for gain drift correlated with the given indicator column. Source code in mass2/core/channel.py 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 def driftcorrect ( self , indicator_col : str = \"pretrig_mean\" , uncorrected_col : str = \"5lagy\" , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Correct for gain drift correlated with the given indicator column.\"\"\" # by defining a seperate learn method that takes ch as an argument, # we can move all the code for the step outside of Channel step = DriftCorrectStep . learn ( ch = self , indicator_col = indicator_col , uncorrected_col = uncorrected_col , corrected_col = corrected_col , use_expr = use_expr , ) return self . with_step ( step ) filter5lag ( pulse_col = 'pulse' , peak_y_col = '5lagy' , peak_x_col = '5lagx' , f_3db = 25000.0 , use_expr = pl . lit ( True ), time_constant_s_of_exp_to_be_orthogonal_to = None ) Compute a 5-lag optimal filter and apply it. Parameters: pulse_col ( str , default: 'pulse' ) \u2013 Which column contains raw data, by default \"pulse\" peak_y_col ( str , default: '5lagy' ) \u2013 Column to contain the optimal filter results, by default \"5lagy\" peak_x_col ( str , default: '5lagx' ) \u2013 Column to contain the 5-lag filter's estimate of arrival-time/phase, by default \"5lagx\" f_3db ( float , default: 25000.0 ) \u2013 A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr ( Expr , default: lit (True) ) \u2013 An expression to select pulses for averaging, by default pl.lit(True) time_constant_s_of_exp_to_be_orthogonal_to ( float | None , default: None ) \u2013 Optionally an exponential decay time to make the filter insensitive to, by default None Returns: Channel \u2013 This channel with a Filter5LagStep added to the recipe. Source code in mass2/core/channel.py 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 def filter5lag ( self , pulse_col : str = \"pulse\" , peak_y_col : str = \"5lagy\" , peak_x_col : str = \"5lagx\" , f_3db : float = 25e3 , use_expr : pl . Expr = pl . lit ( True ), time_constant_s_of_exp_to_be_orthogonal_to : float | None = None , ) -> \"Channel\" : \"\"\"Compute a 5-lag optimal filter and apply it. Parameters ---------- pulse_col : str, optional Which column contains raw data, by default \"pulse\" peak_y_col : str, optional Column to contain the optimal filter results, by default \"5lagy\" peak_x_col : str, optional Column to contain the 5-lag filter's estimate of arrival-time/phase, by default \"5lagx\" f_3db : float, optional A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr : pl.Expr, optional An expression to select pulses for averaging, by default pl.lit(True) time_constant_s_of_exp_to_be_orthogonal_to : float | None, optional Optionally an exponential decay time to make the filter insensitive to, by default None Returns ------- Channel This channel with a Filter5LagStep added to the recipe. \"\"\" assert self . noise noiseresult = self . noise . spectrum ( trunc_back = 2 , trunc_front = 2 ) avg_pulse = self . compute_average_pulse ( pulse_col = pulse_col , use_expr = use_expr ) filter_maker = FilterMaker ( signal_model = avg_pulse , n_pretrigger = self . header . n_presamples , noise_psd = noiseresult . psd , noise_autocorr = noiseresult . autocorr_vec , sample_time_sec = self . header . frametime_s , ) if time_constant_s_of_exp_to_be_orthogonal_to is None : filter5lag = filter_maker . compute_5lag ( f_3db = f_3db ) else : filter5lag = filter_maker . compute_5lag_noexp ( f_3db = f_3db , exp_time_seconds = time_constant_s_of_exp_to_be_orthogonal_to ) step = OptimalFilterStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter5lag , spectrum = noiseresult , filter_maker = filter_maker , transform_raw = self . transform_raw , ) return self . with_step ( step ) filterATS ( pulse_col = 'pulse' , peak_y_col = 'ats_y' , peak_x_col = 'ats_x' , f_3db = 25000.0 , use_expr = pl . lit ( True )) Compute an arrival-time-safe (ATS) optimal filter and apply it. Parameters: pulse_col ( str , default: 'pulse' ) \u2013 Which column contains raw data, by default \"pulse\" peak_y_col ( str , default: 'ats_y' ) \u2013 Column to contain the optimal filter results, by default \"ats_y\" peak_x_col ( str , default: 'ats_x' ) \u2013 Column to contain the ATS filter's estimate of arrival-time/phase, by default \"ats_x\" f_3db ( float , default: 25000.0 ) \u2013 A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr ( Expr , default: lit (True) ) \u2013 An expression to select pulses for averaging, by default pl.lit(True) Returns: Channel \u2013 This channel with a Filter5LagStep added to the recipe. Source code in mass2/core/channel.py 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 def filterATS ( self , pulse_col : str = \"pulse\" , peak_y_col : str = \"ats_y\" , peak_x_col : str = \"ats_x\" , f_3db : float = 25e3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Compute an arrival-time-safe (ATS) optimal filter and apply it. Parameters ---------- pulse_col : str, optional Which column contains raw data, by default \"pulse\" peak_y_col : str, optional Column to contain the optimal filter results, by default \"ats_y\" peak_x_col : str, optional Column to contain the ATS filter's estimate of arrival-time/phase, by default \"ats_x\" f_3db : float, optional A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr : pl.Expr, optional An expression to select pulses for averaging, by default pl.lit(True) Returns ------- Channel This channel with a Filter5LagStep added to the recipe. \"\"\" assert self . noise mprms = self . good_series ( \"pulse_rms\" , use_expr ) . median () use = use_expr . and_ ( np . abs ( pl . col ( \"pulse_rms\" ) / mprms - 1.0 ) < 0.3 ) limit = 4000 avg_pulse , dt_model = self . compute_ats_model ( pulse_col , use , limit ) noiseresult = self . noise . spectrum () filter_maker = FilterMaker ( signal_model = avg_pulse , dt_model = dt_model , n_pretrigger = self . header . n_presamples , noise_psd = noiseresult . psd , noise_autocorr = noiseresult . autocorr_vec , sample_time_sec = self . header . frametime_s , ) filter_ats = filter_maker . compute_ats ( f_3db = f_3db ) step = OptimalFilterStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter_ats , spectrum = noiseresult , filter_maker = filter_maker , transform_raw = self . transform_raw , ) return self . with_step ( step ) fit_pulse ( index = 0 , col = 'pulse' , verbose = True ) Fit a single pulse to a 2-exponential-with-tail model, returning the fit result. Source code in mass2/core/channel.py 1096 1097 1098 1099 1100 1101 1102 1103 1104 def fit_pulse ( self , index : int = 0 , col : str = \"pulse\" , verbose : bool = True ) -> LineModelResult : \"\"\"Fit a single pulse to a 2-exponential-with-tail model, returning the fit result.\"\"\" pulse = self . df [ col ][ index ] . to_numpy () result = mass2 . core . pulse_algorithms . fit_pulse_2exp_with_tail ( pulse , npre = self . header . n_presamples , dt = self . header . frametime_s ) if verbose : print ( f \"ch= { self } \" ) print ( f \"pulse index= { index } \" ) print ( result . fit_report ()) return result from_ljh ( path , noise_path = None , keep_posix_usec = False , transform_raw = None ) classmethod Load a Channel from an LJH file, optionally with a NoiseChannel from a corresponding noise LJH file. Source code in mass2/core/channel.py 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 @classmethod def from_ljh ( cls , path : str | Path , noise_path : str | Path | None = None , keep_posix_usec : bool = False , transform_raw : Callable | None = None , ) -> \"Channel\" : \"\"\"Load a Channel from an LJH file, optionally with a NoiseChannel from a corresponding noise LJH file.\"\"\" if not noise_path : noise_channel = None else : noise_channel = NoiseChannel . from_ljh ( noise_path ) ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars ( keep_posix_usec ) header = ChannelHeader . from_ljh_header_df ( header_df ) channel = cls ( df , header = header , npulses = ljh . npulses , subframediv = ljh . subframediv , noise = noise_channel , transform_raw = transform_raw ) return channel from_off ( off ) classmethod Load a Channel from an OFF file. Source code in mass2/core/channel.py 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 @classmethod def from_off ( cls , off : OffFile ) -> \"Channel\" : \"\"\"Load a Channel from an OFF file.\"\"\" assert off . _mmap is not None df = pl . from_numpy ( np . asarray ( off . _mmap )) df = ( df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) . select ( pl . exclude ( \"unixnano\" )) ) df_header = pl . DataFrame ( off . header ) df_header = df_header . with_columns ( pl . Series ( \"Filename\" , [ off . filename ])) header = ChannelHeader ( f \" { os . path . split ( off . filename )[ 1 ] } \" , off . header [ \"ChannelNumberMatchingName\" ], off . framePeriodSeconds , off . _mmap [ \"recordPreSamples\" ][ 0 ], off . _mmap [ \"recordSamples\" ][ 0 ], df_header , ) channel = cls ( df , header , off . nRecords , subframediv = off . subframediv ) return channel get_step ( index ) Get the step at the given index, supporting negative indices. Source code in mass2/core/channel.py 112 113 114 115 116 117 118 def get_step ( self , index : int ) -> tuple [ RecipeStep , int ]: \"\"\"Get the step at the given index, supporting negative indices.\"\"\" # normalize the index to a positive index if index < 0 : index = len ( self . steps ) + index step = self . steps [ index ] return step , index good_df ( cols = pl . all (), use_expr = pl . lit ( True )) Return a Polars DataFrame of the given columns, filtered by good_expr and use_expr. Source code in mass2/core/channel.py 776 777 778 779 780 781 def good_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : \"\"\"Return a Polars DataFrame of the given columns, filtered by good_expr and use_expr.\"\"\" good_df = self . df . lazy () . filter ( self . good_expr ) if use_expr is not True : good_df = good_df . filter ( use_expr ) return good_df . select ( cols ) . collect () good_series ( col , use_expr = pl . lit ( True )) Return a Polars Series of the given column, filtered by good_expr and use_expr. Source code in mass2/core/channel.py 280 281 282 def good_series ( self , col : str , use_expr : pl . Expr = pl . lit ( True )) -> pl . Series : \"\"\"Return a Polars Series of the given column, filtered by good_expr and use_expr.\"\"\" return mass2 . misc . good_series ( self . df , col , self . good_expr , use_expr ) good_serieses ( cols , use_expr = pl . lit ( True )) Return a list of Polars Series of the given columns, filtered by good_expr and use_expr. Source code in mass2/core/channel.py 790 791 792 793 def good_serieses ( self , cols : list [ str ], use_expr : pl . Expr = pl . lit ( True )) -> list [ pl . Series ]: \"\"\"Return a list of Polars Series of the given columns, filtered by good_expr and use_expr.\"\"\" df2 = self . good_df ( cols , use_expr ) return [ df2 [ col ] for col in cols ] hist ( col , bin_edges , use_good_expr = True , use_expr = pl . lit ( True )) Compute a histogram of the given column, optionally filtering by good_expr and use_expr. Source code in mass2/core/channel.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def hist ( self , col : str , bin_edges : ArrayLike , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute a histogram of the given column, optionally filtering by good_expr and use_expr.\"\"\" if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col )) . collect () values = df_small [ col ] bin_centers , counts = misc . hist_of_series ( values , bin_edges ) return bin_centers , counts linefit ( line , col , use_expr = pl . lit ( True ), has_linear_background = False , has_tails = False , dlo = 50 , dhi = 50 , binsize = 0.5 , params_update = lmfit . Parameters ()) Fit a spectral line to the binned data from the given column, optionally filtering by use_expr. Source code in mass2/core/channel.py 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 def linefit ( # noqa: PLR0917 self , line : GenericLineModel | SpectralLine | str | float , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : \"\"\"Fit a spectral line to the binned data from the given column, optionally filtering by use_expr.\"\"\" model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = self . header . description , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result mo_stepplots () Marimo UI element to choose and display step plots, with a dropdown to choose channel number. Source code in mass2/core/channel.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def mo_stepplots ( self ) -> mo . ui . dropdown : \"\"\"Marimo UI element to choose and display step plots, with a dropdown to choose channel number.\"\"\" desc_ind = { step . description : i for i , step in enumerate ( self . steps )} first_non_summarize_step = self . steps [ 0 ] for step in self . steps : if isinstance ( step , SummarizeStep ): continue first_non_summarize_step = step break mo_ui = mo . ui . dropdown ( desc_ind , value = first_non_summarize_step . description , label = f \"choose step for ch { self . header . ch_num } \" , ) def show () -> mo . Html : \"\"\"Show the selected step plot.\"\"\" return self . _mo_stepplots_explicit ( mo_ui ) def step_ind () -> Any : \"\"\"Get the selected step index from the dropdown item, if any.\"\"\" return mo_ui . value mo_ui . show = show mo_ui . step_ind = step_ind return mo_ui multifit_mass_cal ( multifit , previous_cal_step_index , calibrated_col , use_expr = pl . lit ( True )) Fit multiple spectral lines, to create a Mass1-style gain calibration. Source code in mass2/core/channel.py 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 def multifit_mass_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Fit multiple spectral lines, to create a Mass1-style gain calibration.\"\"\" step = MultiFitMassCalibrationStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) multifit_quadratic_gain_cal ( multifit , previous_cal_step_index , calibrated_col , use_expr = pl . lit ( True )) Fit multiple spectral lines, to create a quadratic gain calibration. Source code in mass2/core/channel.py 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 def multifit_quadratic_gain_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Fit multiple spectral lines, to create a quadratic gain calibration.\"\"\" step = MultiFitQuadraticGainStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) phase_correct_mass_specific_lines ( indicator_col , uncorrected_col , line_names , previous_cal_step_index , corrected_col = None , use_expr = pl . lit ( True )) Apply phase correction to the given uncorrected column, where specific lines are used to judge the correction. Source code in mass2/core/channel.py 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 def phase_correct_mass_specific_lines ( self , indicator_col : str , uncorrected_col : str , line_names : Iterable [ str | float ], previous_cal_step_index : int , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Apply phase correction to the given uncorrected column, where specific lines are used to judge the correction.\"\"\" if corrected_col is None : corrected_col = uncorrected_col + \"_pc\" step = mass2 . core . phase_correct_steps . phase_correct_mass_specific_lines ( self , indicator_col , uncorrected_col , corrected_col , previous_cal_step_index , line_names , use_expr , ) return self . with_step ( step ) plot_hist ( col , bin_edges , axis = None , use_good_expr = True , use_expr = pl . lit ( True )) Compute and plot a histogram of the given column, optionally filtering by good_expr and use_expr. Source code in mass2/core/channel.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def plot_hist ( self , col : str , bin_edges : ArrayLike , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute and plot a histogram of the given column, optionally filtering by good_expr and use_expr.\"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis bin_centers , counts = self . hist ( col , bin_edges = bin_edges , use_good_expr = use_good_expr , use_expr = use_expr ) _ , step_size = misc . midpoints_and_step_size ( bin_edges ) plt . step ( bin_centers , counts , where = \"mid\" ) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } for { self . shortname } \" ) plt . tight_layout () return bin_centers , counts plot_hists ( col , bin_edges , group_by_col , axis = None , use_good_expr = True , use_expr = pl . lit ( True ), skip_none = True ) Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channel.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : str , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), skip_none : bool = True , ) -> tuple [ NDArray , dict [ str , NDArray ]]: \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group counts_dict : dict [ str , NDArray ] = {} for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] _ , step_size = misc . midpoints_and_step_size ( bin_edges ) bin_centers , counts = misc . hist_of_series ( values , bin_edges ) group_name_str = str ( group_name ) counts_dict [ group_name_str ] = counts plt . step ( bin_centers , counts , where = \"mid\" , label = group_name_str ) # Plot the histogram for the current group # if group_name == \"EBIT\": # ax.hist(values, bins=bin_edges, alpha=0.9, color=\"k\", label=group_name_str) # else: # ax.hist(values, bins=bin_edges, alpha=0.5, label=group_name_str) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () return bin_centers , counts_dict plot_scatter ( x_col , y_col , color_col = None , use_expr = pl . lit ( True ), use_good_expr = True , skip_none = True , ax = None ) Generate a scatter plot of y_col vs x_col , optionally colored by color_col . Source code in mass2/core/channel.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def plot_scatter ( self , x_col : str , y_col : str , color_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), use_good_expr : bool = True , skip_none : bool = True , ax : plt . Axes | None = None , ) -> None : \"\"\"Generate a scatter plot of `y_col` vs `x_col`, optionally colored by `color_col`.\"\"\" if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # set current axis so I can use plt api if use_good_expr : filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = self . good_expr df_small = self . df . lazy () . filter ( filter_expr ) . select ( x_col , y_col , color_col ) . collect () for ( name ,), data in df_small . group_by ( color_col , maintain_order = True ): if name is None and skip_none and color_col is not None : continue plt . plot ( data . select ( x_col ) . to_series (), data . select ( y_col ) . to_series (), \".\" , label = name , ) plt . xlabel ( str ( x_col )) plt . ylabel ( str ( y_col )) title_str = f \"\"\" { self . header . description } use_expr= { str ( use_expr ) } good_expr= { str ( self . good_expr ) } \"\"\" plt . title ( title_str ) if color_col is not None : plt . legend ( title = color_col ) plt . tight_layout () plot_summaries ( use_expr_in = None , downsample = None , log = False ) Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters: use_expr_in ( Expr | None , default: None ) \u2013 A polars expression to determine valid pulses, by default None. If None, use self.good_expr downsample ( int | None , default: None ) \u2013 Plot only every one of downsample pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log ( bool , default: False ) \u2013 Whether to make the histograms have a logarithmic y-scale, by default False. Source code in mass2/core/channel.py 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr_in: pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample: int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log: bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) contents , _ , _ = plt . hist ( y , 200 , range = limits , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" ) rough_cal ( line_names , uncalibrated_col = 'filtValue' , calibrated_col = None , use_expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment = 0.1 , min_gain_fraction_at_ph_30k = 0.25 , fwhm_pulse_height_units = 75 , n_extra_peaks = 10 , acceptable_rms_residual_e = 10 ) Learn a rough calibration by trying to assign the 3 brightest peaks, then fitting a line to those and looking for other peaks that fit that line. Source code in mass2/core/channel.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 def rough_cal ( # noqa: PLR0917 self , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying to assign the 3 brightest peaks, then fitting a line to those and looking for other peaks that fit that line. \"\"\" step = mass2 . core . RoughCalibrationStep . learn_3peak ( self , line_names , uncalibrated_col , calibrated_col , use_expr , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , fwhm_pulse_height_units , n_extra_peaks , acceptable_rms_residual_e , ) return self . with_step ( step ) rough_cal_combinatoric ( line_names , uncalibrated_col , calibrated_col , ph_smoothing_fwhm , n_extra = 3 , use_expr = pl . lit ( True )) Learn a rough calibration by trying all combinatorically possible peak assignments. Source code in mass2/core/channel.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def rough_cal_combinatoric ( self , line_names : list [ str ], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying all combinatorically possible peak assignments.\"\"\" step = mass2 . core . RoughCalibrationStep . learn_combinatoric ( self , line_names , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) rough_cal_combinatoric_height_info ( line_names , line_heights_allowed , uncalibrated_col , calibrated_col , ph_smoothing_fwhm , n_extra = 3 , use_expr = pl . lit ( True )) Learn a rough calibration by trying all combinatorically possible peak assignments, using known relative peak heights to limit the possibilities. Source code in mass2/core/channel.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def rough_cal_combinatoric_height_info ( self , line_names : list [ str ], line_heights_allowed : list [ list [ int ]], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying all combinatorically possible peak assignments, using known relative peak heights to limit the possibilities.\"\"\" step = mass2 . core . RoughCalibrationStep . learn_combinatoric_height_info ( self , line_names , line_heights_allowed , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) save_recipes ( filename ) Save the recipe steps to a pickle file, keyed by channel number. Source code in mass2/core/channel.py 1024 1025 1026 1027 1028 def save_recipes ( self , filename : str ) -> dict [ int , Recipe ]: \"\"\"Save the recipe steps to a pickle file, keyed by channel number.\"\"\" steps = { self . header . ch_num : self . steps } misc . pickle_object ( steps , filename ) return steps step_plot ( step_ind , ** kwargs ) Make a debug plot for the given step index, supporting negative indices. Source code in mass2/core/channel.py 120 121 122 123 124 125 126 127 def step_plot ( self , step_ind : int , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a debug plot for the given step index, supporting negative indices.\"\"\" step , step_ind = self . get_step ( step_ind ) if step_ind + 1 == len ( self . df_history ): df_after = self . df else : df_after = self . df_history [ step_ind + 1 ] return step . dbg_plot ( df_after , ** kwargs ) step_summary () Return a list of (step type name, elapsed time in seconds) for each step in the recipe. Source code in mass2/core/channel.py 848 849 850 def step_summary ( self ) -> list [ tuple [ str , float ]]: \"\"\"Return a list of (step type name, elapsed time in seconds) for each step in the recipe.\"\"\" return [( type ( a ) . __name__ , b ) for ( a , b ) in zip ( self . steps , self . steps_elapsed_s )] summarize_pulses ( col = 'pulse' , pretrigger_ignore_samples = 0 , peak_index = None ) Summarize the pulses, adding columns for pulse height, pretrigger mean, etc. Source code in mass2/core/channel.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 def summarize_pulses ( self , col : str = \"pulse\" , pretrigger_ignore_samples : int = 0 , peak_index : int | None = None ) -> \"Channel\" : \"\"\"Summarize the pulses, adding columns for pulse height, pretrigger mean, etc.\"\"\" if peak_index is None : peak_index = self . typical_peak_ind ( col ) out_names = mass2 . core . pulse_algorithms . result_dtype . names # mypy (incorrectly) thinks `out_names` might be None, and `list(None)` is forbidden. Assertion makes it happy again. assert out_names is not None outputs = list ( out_names ) step = SummarizeStep ( inputs = [ col ], output = outputs , good_expr = self . good_expr , use_expr = pl . lit ( True ), frametime_s = self . header . frametime_s , peak_index = peak_index , pulse_col = col , pretrigger_ignore_samples = pretrigger_ignore_samples , n_presamples = self . header . n_presamples , transform_raw = self . transform_raw , ) return self . with_step ( step ) typical_peak_ind ( col = 'pulse' ) cached Return the typical peak index of the given column, using the median peak index for the first 100 pulses. Source code in mass2/core/channel.py 504 505 506 507 508 509 510 @functools . cache def typical_peak_ind ( self , col : str = \"pulse\" ) -> int : \"\"\"Return the typical peak index of the given column, using the median peak index for the first 100 pulses.\"\"\" raw = self . df . limit ( 100 )[ col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) return int ( np . median ( raw . argmax ( axis = 1 ))) with_categorize_step ( category_condition_dict , output_col = 'category' ) Add a recipe step that categorizes pulses based on the given conditions. Source code in mass2/core/channel.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 def with_categorize_step ( self , category_condition_dict : dict [ str , pl . Expr ], output_col : str = \"category\" ) -> \"Channel\" : \"\"\"Add a recipe step that categorizes pulses based on the given conditions.\"\"\" # ensure the first condition is True, to be used as a fallback first_expr = next ( iter ( category_condition_dict . values ())) if not first_expr . meta . eq ( pl . lit ( True )): category_condition_dict = { \"fallback\" : pl . lit ( True ), ** category_condition_dict } extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in category_condition_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . CategorizeStep ( inputs = list ( inputs ), output = [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), category_condition_dict = category_condition_dict , ) return self . with_step ( step ) with_column_map_step ( input_col , output_col , f ) f should take a numpy array and return a numpy array with the same number of elements Source code in mass2/core/channel.py 445 446 447 448 def with_column_map_step ( self , input_col : str , output_col : str , f : Callable ) -> \"Channel\" : \"\"\"f should take a numpy array and return a numpy array with the same number of elements\"\"\" step = mass2 . core . recipe . ColumnAsNumpyMapStep ([ input_col ], [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), f = f ) return self . with_step ( step ) with_columns ( df2 ) Append columns from df2 to the existing dataframe, keeping all other attributes the same. Source code in mass2/core/channel.py 938 939 940 941 def with_columns ( self , df2 : pl . DataFrame ) -> \"Channel\" : \"\"\"Append columns from df2 to the existing dataframe, keeping all other attributes the same.\"\"\" df3 = self . df . with_columns ( df2 ) return self . with_replacement_df ( df3 ) with_experiment_state_df ( df_es , force_timestamp_monotonic = False ) Add experiment states from an existing dataframe Source code in mass2/core/channel.py 911 912 913 914 915 916 917 918 919 920 def with_experiment_state_df ( self , df_es : pl . DataFrame , force_timestamp_monotonic : bool = False ) -> \"Channel\" : \"\"\"Add experiment states from an existing dataframe\"\"\" if not self . df [ \"timestamp\" ] . is_sorted (): df = self . df . select ( pl . col ( \"timestamp\" ) . cum_max () . alias ( \"timestamp\" )) . with_columns ( self . df . select ( pl . exclude ( \"timestamp\" ))) # print(\"WARNING: in with_experiment_state_df, timestamp is not monotonic, forcing it to be\") # print(\"This is likely a BUG in DASTARD.\") else : df = self . df df2 = df . join_asof ( df_es , on = \"timestamp\" , strategy = \"backward\" ) return self . with_replacement_df ( df2 ) with_external_trigger_df ( df_ext ) Add external trigger times from an existing dataframe Source code in mass2/core/channel.py 922 923 924 925 926 927 928 929 def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channel\" : \"\"\"Add external trigger times from an existing dataframe\"\"\" df2 = ( self . df . with_columns ( subframecount = pl . col ( \"framecount\" ) * self . subframediv ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"backward\" , coalesce = False , suffix = \"_prev_ext_trig\" ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"forward\" , coalesce = False , suffix = \"_next_ext_trig\" ) ) return self . with_replacement_df ( df2 ) with_good_expr ( good_expr , replace = False ) Return a new Channel with the given good_expr, combined with the existing good_expr by \"and\", of by replacing it entirely if replace is True. Source code in mass2/core/channel.py 436 437 438 439 440 441 442 443 def with_good_expr ( self , good_expr : pl . Expr , replace : bool = False ) -> \"Channel\" : \"\"\"Return a new Channel with the given good_expr, combined with the existing good_expr by \"and\", of by replacing it entirely if `replace` is True.\"\"\" # the default value of self.good_expr is pl.lit(True) # and_(True) will just add visual noise when looking at good_expr and not affect behavior if not replace and good_expr is not True and not good_expr . meta . eq ( pl . lit ( True )): good_expr = good_expr . and_ ( self . good_expr ) return dataclasses . replace ( self , good_expr = good_expr ) with_good_expr_below_nsigma_outlier_resistant ( col_nsigma_pairs , replace = False , use_prev_good_expr = True ) Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative Source code in mass2/core/channel.py 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) with_good_expr_nsigma_range_outlier_resistant ( col_nsigma_pairs , replace = False , use_prev_good_expr = True ) Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative Source code in mass2/core/channel.py 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) with_good_expr_pretrig_rms_and_postpeak_deriv ( n_sigma_pretrig_rms = 20 , n_sigma_postpeak_deriv = 20 , replace = False ) Set good_expr to exclude pulses with pretrigger RMS or postpeak derivative above outlier-resistant thresholds. Source code in mass2/core/channel.py 450 451 452 453 454 455 456 457 458 459 def with_good_expr_pretrig_rms_and_postpeak_deriv ( self , n_sigma_pretrig_rms : float = 20 , n_sigma_postpeak_deriv : float = 20 , replace : bool = False ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with pretrigger RMS or postpeak derivative above outlier-resistant thresholds.\"\"\" max_postpeak_deriv = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"postpeak_deriv\" ] . to_numpy (), nsigma = n_sigma_postpeak_deriv ) max_pretrig_rms = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"pretrig_rms\" ] . to_numpy (), nsigma = n_sigma_pretrig_rms ) good_expr = ( pl . col ( \"postpeak_deriv\" ) < max_postpeak_deriv ) . and_ ( pl . col ( \"pretrig_rms\" ) < max_pretrig_rms ) return self . with_good_expr ( good_expr , replace ) with_range_around_median ( col , range_up , range_down ) Set good_expr to exclude pulses with col outside the given range around its median. Source code in mass2/core/channel.py 461 462 463 464 def with_range_around_median ( self , col : str , range_up : float , range_down : float ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with `col` outside the given range around its median.\"\"\" med = np . median ( self . df [ col ] . to_numpy ()) return self . with_good_expr ( pl . col ( col ) . is_between ( med - range_down , med + range_up )) with_replacement_df ( df2 ) Replace the dataframe with a new one, keeping all other attributes the same. Source code in mass2/core/channel.py 931 932 933 934 935 936 def with_replacement_df ( self , df2 : pl . DataFrame ) -> \"Channel\" : \"\"\"Replace the dataframe with a new one, keeping all other attributes the same.\"\"\" return dataclasses . replace ( self , df = df2 , ) with_select_step ( col_expr_dict ) This step is meant for interactive exploration; it's basically like the df.select() method, but it's saved as a step. Source code in mass2/core/channel.py 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 def with_select_step ( self , col_expr_dict : dict [ str , pl . Expr ]) -> \"Channel\" : \"\"\" This step is meant for interactive exploration; it's basically like the df.select() method, but it's saved as a step. \"\"\" extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in col_expr_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . SelectStep ( inputs = list ( inputs ), output = list ( col_expr_dict . keys ()), good_expr = self . good_expr , use_expr = pl . lit ( True ), col_expr_dict = col_expr_dict , ) return self . with_step ( step ) with_step ( step ) Return a new Channel with the given step applied to generate new columns in the dataframe. Source code in mass2/core/channel.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 def with_step ( self , step : RecipeStep ) -> \"Channel\" : \"\"\"Return a new Channel with the given step applied to generate new columns in the dataframe.\"\"\" t_start = time . time () df2 = step . calc_from_df ( self . df ) elapsed_s = time . time () - t_start ch2 = dataclasses . replace ( self , df = df2 , good_expr = step . good_expr , df_history = self . df_history + [ self . df ], steps = self . steps . with_step ( step ), steps_elapsed_s = self . steps_elapsed_s + [ elapsed_s ], ) return ch2 with_steps ( steps ) Return a new Channel with the given steps applied to generate new columns in the dataframe. Source code in mass2/core/channel.py 429 430 431 432 433 434 def with_steps ( self , steps : Recipe ) -> \"Channel\" : \"\"\"Return a new Channel with the given steps applied to generate new columns in the dataframe.\"\"\" ch2 = self for step in steps : ch2 = ch2 . with_step ( step ) return ch2 ChannelHeader dataclass Metadata about a Channel, of the sort read from file header. Source code in mass2/core/channel.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @dataclass ( frozen = True ) class ChannelHeader : \"\"\"Metadata about a Channel, of the sort read from file header.\"\"\" description : str # filename or date/run number, etc ch_num : int frametime_s : float n_presamples : int n_samples : int df : pl . DataFrame = field ( repr = False ) @classmethod def from_ljh_header_df ( cls , df : pl . DataFrame ) -> \"ChannelHeader\" : \"\"\"Construct from the LJH header dataframe as returned by LJHFile.to_polars()\"\"\" return cls ( description = os . path . split ( df [ \"Filename\" ][ 0 ])[ - 1 ], ch_num = df [ \"Channel\" ][ 0 ], frametime_s = df [ \"Timebase\" ][ 0 ], n_presamples = df [ \"Presamples\" ][ 0 ], n_samples = df [ \"Total Samples\" ][ 0 ], df = df , ) from_ljh_header_df ( df ) classmethod Construct from the LJH header dataframe as returned by LJHFile.to_polars() Source code in mass2/core/channel.py 45 46 47 48 49 50 51 52 53 54 55 @classmethod def from_ljh_header_df ( cls , df : pl . DataFrame ) -> \"ChannelHeader\" : \"\"\"Construct from the LJH header dataframe as returned by LJHFile.to_polars()\"\"\" return cls ( description = os . path . split ( df [ \"Filename\" ][ 0 ])[ - 1 ], ch_num = df [ \"Channel\" ][ 0 ], frametime_s = df [ \"Timebase\" ][ 0 ], n_presamples = df [ \"Presamples\" ][ 0 ], n_samples = df [ \"Total Samples\" ][ 0 ], df = df , ) Data structures and methods for handling a group of microcalorimeter channels. Channels dataclass A collection of microcalorimeter channels, with methods to operate in parallel on all channels. Source code in mass2/core/channels.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 @dataclass ( frozen = True ) # noqa: PLR0904 class Channels : \"\"\"A collection of microcalorimeter channels, with methods to operate in parallel on all channels.\"\"\" channels : dict [ int , Channel ] description : str bad_channels : dict [ int , BadChannel ] = field ( default_factory = dict ) @property def ch0 ( self ) -> Channel : \"\"\"Return a representative Channel object for convenient exploration (the one with the lowest channel number).\"\"\" assert len ( self . channels ) > 0 , \"channels must be non-empty\" return next ( iter ( self . channels . values ())) def with_more_channels ( self , more : \"Channels\" ) -> \"Channels\" : \"\"\"Return a Channels object with additional Channels in it. New channels with the same number will overrule existing ones. Parameters ---------- more : Channels Another Channels object, to be added Returns ------- Channels The replacement \"\"\" channels = self . channels . copy () channels . update ( more . channels ) bad = self . bad_channels . copy () bad . update ( more . bad_channels ) descr = self . description + more . description + \" \\n Warning! created by with_more_channels()\" return dataclasses . replace ( self , channels = channels , bad_channels = bad , description = descr ) @functools . cache def dfg ( self , exclude : str = \"pulse\" ) -> pl . DataFrame : \"\"\"Return a DataFrame containing good pulses from each channel. Excludes the given columns (default \"pulse\").\"\"\" # return a dataframe containing good pulses from each channel, # exluding \"pulse\" by default # and including column \"ch_num\" # the more common call should be to wrap this in a convenient plotter dfs = [] for ch_num , channel in self . channels . items (): df = channel . df . select ( pl . exclude ( exclude )) . filter ( channel . good_expr ) # key_series = pl.Series(\"key\", dtype=pl.Int64).extend_constant(key, len(df)) assert ch_num == channel . header . ch_num ch_series = pl . Series ( \"ch_num\" , dtype = pl . Int64 ) . extend_constant ( channel . header . ch_num , len ( df )) dfs . append ( df . with_columns ( ch_series )) return pl . concat ( dfs ) def linefit ( # noqa: PLR0917 self , line : float | str | SpectralLine | GenericLineModel , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : \"\"\"Perform a fit to one spectral line in the coadded histogram of the given column.\"\"\" model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = mass2 . misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = f \" { len ( self . channels ) } channels, { self . description } \" , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def plot_hist ( self , col : str , bin_edges : ArrayLike , use_expr : pl . Expr = pl . lit ( True ), axis : plt . Axes | None = None ) -> None : \"\"\"Plot a histogram for the given column across all channels.\"\"\" df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () ax = mass2 . misc . plot_hist_of_series ( df_small [ col ], bin_edges , axis ) ax . set_title ( f \" { len ( self . channels ) } channels, { self . description } \" ) def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : bool , axis : plt . Axes | None = None , use_expr : pl . Expr | None = None , skip_none : bool = True , ) -> None : \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () def _limited_chan_list ( self , limit : int | None = 20 , channels : list [ int ] | None = None ) -> list [ int ]: \"\"\"A helper to get a list of channel numbers, limited to the given number if needed, and including only channel numbers from `channels` if not None.\"\"\" limited_chan = list ( self . channels . keys ()) if channels is not None : limited_chan = list ( set ( limited_chan ) . intersection ( set ( channels ))) limited_chan . sort () if limit and len ( limited_chan ) > limit : limited_chan = limited_chan [: limit ] return limited_chan def plot_filters ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the optimal filters for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] # The next line _assumes_ a 5-lag filter. Fix as needed. x = np . arange ( ch . header . n_samples - 4 ) - ch . header . n_presamples + 2 y = ch . last_filter if y is not None : plt . plot ( x , y , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . xlabel ( \"Samples after trigger\" ) plt . title ( \"Optimal filters\" ) def plot_avg_pulses ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the average pulses (the signal model) for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] x = np . arange ( ch . header . n_samples ) - ch . header . n_presamples y = ch . last_avg_pulse if y is not None : plt . plot ( x , y , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . xlabel ( \"Samples after trigger\" ) plt . title ( \"Average pulses\" ) def plot_noise_spectrum ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the noise power spectrum for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] freqpsd = ch . last_noise_psd if freqpsd is not None : freq , psd = freqpsd plt . plot ( freq , psd , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . loglog () plt . xlabel ( \"Frequency (Hz)\" ) plt . title ( \"Noise power spectral density\" ) def plot_noise_autocorr ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the noise power autocorrelation for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] ac = ch . last_noise_autocorrelation if ac is not None : color = colormap ( i / n_expected ) plt . plot ( ac , color = color , label = f \"Chan { ch_num } \" ) plt . plot ( 0 , ac [ 0 ], \"o\" , color = color ) plt . legend () plt . xlabel ( \"Lags\" ) plt . title ( \"Noise autocorrelation\" ) def map ( self , f : Callable , allow_throw : bool = False ) -> \"Channels\" : \"\"\"Map function `f` over all channels, returning a new Channels object containing the new Channel objects.\"\"\" new_channels = {} new_bad_channels = {} for key , channel in self . channels . items (): try : new_channels [ key ] = f ( channel ) except KeyboardInterrupt as kint : raise kint except Exception as ex : error_type : type = type ( ex ) error_message : str = str ( ex ) backtrace : str = traceback . format_exc () if allow_throw : raise print ( f \" { key =} { channel =} failed the step { f } \" ) print ( f \" { error_type =} \" ) print ( f \" { error_message =} \" ) new_bad_channels [ key ] = channel . as_bad ( error_type , error_message , backtrace ) new_bad_channels = mass2 . misc . merge_dicts_ordered_by_keys ( self . bad_channels , new_bad_channels ) return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def set_bad ( self , ch_num : int , msg : str , require_ch_num_exists : bool = True ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with the given channel number marked as bad.\"\"\" new_channels = {} new_bad_channels = {} if require_ch_num_exists : assert ch_num in self . channels . keys (), f \" { ch_num } can't be set bad because it does not exist\" for key , channel in self . channels . items (): if key == ch_num : new_bad_channels [ key ] = channel . as_bad ( None , msg , None ) else : new_channels [ key ] = channel return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def linefit_joblib ( self , line : str , col : str , prefer : str = \"threads\" , n_jobs : int = 4 ) -> LineModelResult : \"\"\"No one but Galen understands this function.\"\"\" def work ( key : int ) -> LineModelResult : \"\"\"A unit of parallel work: fit line to one channel.\"\"\" channel = self . channels [ key ] return channel . linefit ( line , col ) parallel = joblib . Parallel ( n_jobs = n_jobs , prefer = prefer ) # its not clear if threads are better.... what blocks the gil? results = parallel ( joblib . delayed ( work )( key ) for key in self . channels . keys ()) return results def __hash__ ( self ) -> int : \"\"\"Hash based on the object's id (identity).\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other : Any ) -> bool : \"\"\"Equality test based on object identity.\"\"\" return id ( self ) == id ( other ) @classmethod def from_ljh_path_pairs ( cls , pulse_noise_pairs : Iterable [ tuple [ str , str ]], description : str ) -> \"Channels\" : \"\"\" Create a :class:`Channels` instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of `(pulse_path, noise_path)` tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class:`Channel` per `(pulse_path, noise_path)` pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth:`Channel.from_ljh`. The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] \"\"\" channels : dict [ int , Channel ] = {} for pulse_path , noise_path in pulse_noise_pairs : channel = Channel . from_ljh ( pulse_path , noise_path ) assert channel . header . ch_num not in channels . keys () channels [ channel . header . ch_num ] = channel return cls ( channels , description ) @classmethod def from_off_paths ( cls , off_paths : Iterable [ str | Path ], description : str ) -> \"Channels\" : \"\"\"Create an instance from a sequence of OFF-file paths\"\"\" channels = {} for path in off_paths : ch = Channel . from_off ( mass2 . core . OffFile ( str ( path ))) channels [ ch . header . ch_num ] = ch return cls ( channels , description ) @classmethod def from_ljh_folder ( cls , pulse_folder : str | Path , noise_folder : str | Path | None = None , limit : int | None = None , exclude_ch_nums : list [ int ] | None = None , ) -> \"Channels\" : \"\"\"Create an instance from a directory of LJH files.\"\"\" assert os . path . isdir ( pulse_folder ), f \" { pulse_folder =} { noise_folder =} \" pulse_folder = str ( pulse_folder ) if exclude_ch_nums is None : exclude_ch_nums = [] if noise_folder is None : paths = ljhutil . find_ljh_files ( pulse_folder , exclude_ch_nums = exclude_ch_nums ) if limit is not None : paths = paths [: limit ] pairs = [( path , \"\" ) for path in paths ] else : assert os . path . isdir ( noise_folder ), f \" { pulse_folder =} { noise_folder =} \" noise_folder = str ( noise_folder ) pairs = ljhutil . match_files_by_channel ( pulse_folder , noise_folder , limit = limit , exclude_ch_nums = exclude_ch_nums ) description = f \"from_ljh_folder { pulse_folder =} { noise_folder =} \" print ( f \" { description } \" ) print ( f \" from_ljh_folder has { len ( pairs ) } pairs\" ) data = cls . from_ljh_path_pairs ( pairs , description ) print ( f \" and the Channels obj has { len ( data . channels ) } pairs\" ) return data def get_an_ljh_path ( self ) -> Path : \"\"\"Return the path to a representative one of the LJH files used to create this Channels object.\"\"\" return pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) def get_path_in_output_folder ( self , filename : str | Path ) -> Path : \"\"\"Return a path in an output folder named like the run number, sibling to the LJH folder.\"\"\" ljh_path = self . get_an_ljh_path () base_name , _ = ljh_path . name . split ( \"_chan\" ) date , run_num = base_name . split ( \"_run\" ) # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") output_dir = ljh_path . parent . parent / f \" { run_num } mass2_output\" output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir / filename def get_experiment_state_df ( self , experiment_state_path : str | Path | None = None ) -> pl . DataFrame : \"\"\"Return a DataFrame containing experiment state information, loading from the given path or (if None) inferring it from an LJH file.\"\"\" if experiment_state_path is None : ljh_path = self . get_an_ljh_path () experiment_state_path = ljhutil . experiment_state_path_from_ljh_path ( ljh_path ) df = pl . read_csv ( experiment_state_path , new_columns = [ \"unixnano\" , \"state_label\" ]) # _col0, _col1 = df.columns df_es = df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) # strip whitespace from state_label column sl_series = df . select ( pl . col ( \"state_label\" ) . str . strip_chars ()) . to_series () df_es = df_es . with_columns ( state_label = pl . Series ( values = sl_series , dtype = pl . Categorical )) return df_es def with_experiment_state_by_path ( self , experiment_state_path : str | None = None ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with experiment state information added, loaded from the given path.\"\"\" df_es = self . get_experiment_state_df ( experiment_state_path ) return self . with_experiment_state_df ( df_es ) def with_external_trigger_by_path ( self , path : str | None = None ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with external trigger information added, loaded from the given path or EVENTUALLY (if None) inferring it from an LJH file (not yet implemented).\"\"\" if path is None : raise NotImplementedError ( \"cannot infer external trigger path yet\" ) with open ( path , \"rb\" ) as _f : _header_line = _f . readline () # read the one header line before opening the binary data external_trigger_subframe_count = np . fromfile ( _f , \"int64\" ) df_ext = pl . DataFrame ({ \"subframecount\" : external_trigger_subframe_count , }) return self . with_external_trigger_df ( df_ext ) def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with external trigger information added to each Channel, found from the given DataFrame.\"\"\" def with_etrig_df ( channel : Channel ) -> Channel : \"\"\"Return a copy of one Channel object with external trigger information added to it\"\"\" return channel . with_external_trigger_df ( df_ext ) return self . map ( with_etrig_df ) def with_experiment_state_df ( self , df_es : pl . DataFrame ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with experiment state information added to each Channel, found from the given DataFrame.\"\"\" # this is not as performant as making use_exprs for states # and using .set_sorted on the timestamp column ch2s = {} for ch_num , ch in self . channels . items (): ch2s [ ch_num ] = ch . with_experiment_state_df ( df_es ) return Channels ( ch2s , self . description ) def with_steps_dict ( self , steps_dict : dict [ int , Recipe ]) -> \"Channels\" : \"\"\"Return a copy of this Channels object with the given Recipe objects added to each Channel.\"\"\" def load_recipes ( channel : Channel ) -> Channel : \"\"\"Return a copy of one Channel object with Recipe steps added to it\"\"\" try : steps = steps_dict [ channel . header . ch_num ] except KeyError : raise Exception ( \"steps dict did not contain steps for this ch_num\" ) return channel . with_steps ( steps ) return self . map ( load_recipes ) def save_recipes ( self , filename : str , required_fields : str | Iterable [ str ] | None = None , drop_debug : bool = True ) -> dict [ int , Recipe ]: \"\"\"Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set `required_fields` to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters ---------- filename : str Filename to store recipe in, typically of the form \"*.pkl\" required_fields : str | Iterable[str] | None The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug: bool Whether to remove debugging-related data from each `RecipeStep`, if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns ------- dict Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. \"\"\" steps = {} for channum , ch in self . channels . items (): steps [ channum ] = ch . steps . trim_dead_ends ( required_fields = required_fields , drop_debug = drop_debug ) mass2 . misc . pickle_object ( steps , filename ) return steps def load_recipes ( self , filename : str ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with Recipe objects loaded from the given pickle file and applied to each Channel.\"\"\" steps = mass2 . misc . unpickle_object ( filename ) return self . with_steps_dict ( steps ) def parent_folder_path ( self ) -> pathlib . Path : \"\"\"Return the parent folder of the LJH files used to create this Channels object. Specifically, the `self.ch0` channel's directory is used (normally the answer would be the same for all channels).\"\"\" parent_folder_path = pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) . parent . parent print ( f \" { parent_folder_path =} \" ) return parent_folder_path def concat_data ( self , other_data : \"Channels\" ) -> \"Channels\" : \"\"\"Return a new Channels object with data from this and the other Channels object concatenated together. Only channels that exist in both objects are included in the result.\"\"\" # sorting here to show intention, but I think set is sorted by insertion order as # an implementation detail so this may not do anything ch_nums = sorted ( list ( set ( self . channels . keys ()) . intersection ( other_data . channels . keys ()))) new_channels = {} for ch_num in ch_nums : ch = self . channels [ ch_num ] other_ch = other_data . channels [ ch_num ] combined_df = mass2 . core . misc . concat_dfs_with_concat_state ( ch . df , other_ch . df ) new_ch = ch . with_replacement_df ( combined_df ) new_channels [ ch_num ] = new_ch return mass2 . Channels ( new_channels , self . description + other_data . description ) @classmethod def from_df ( cls , df_in : pl . DataFrame , frametime_s : float , n_presamples : int , n_samples : int , description : str = \"from Channels.channels_from_df\" , ) -> \"Channels\" : \"\"\"Create a Channels object from a single DataFrame that holds data from multiple channels.\"\"\" # requres a column named \"ch_num\" containing the channel number keys_df : dict [ tuple , pl . DataFrame ] = df_in . partition_by ( by = [ \"ch_num\" ], as_dict = True ) dfs : dict [ int , pl . DataFrame ] = { keys [ 0 ]: df for ( keys , df ) in keys_df . items ()} channels : dict [ int , Channel ] = {} for ch_num , df in dfs . items (): channels [ ch_num ] = Channel ( df , header = ChannelHeader ( description = \"from df\" , ch_num = ch_num , frametime_s = frametime_s , n_presamples = n_presamples , n_samples = n_samples , df = df , ), npulses = len ( df ), ) return Channels ( channels , description ) ch0 property Return a representative Channel object for convenient exploration (the one with the lowest channel number). __eq__ ( other ) Equality test based on object identity. Source code in mass2/core/channels.py 406 407 408 def __eq__ ( self , other : Any ) -> bool : \"\"\"Equality test based on object identity.\"\"\" return id ( self ) == id ( other ) __hash__ () Hash based on the object's id (identity). Source code in mass2/core/channels.py 399 400 401 402 403 404 def __hash__ ( self ) -> int : \"\"\"Hash based on the object's id (identity).\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) concat_data ( other_data ) Return a new Channels object with data from this and the other Channels object concatenated together. Only channels that exist in both objects are included in the result. Source code in mass2/core/channels.py 613 614 615 616 617 618 619 620 621 622 623 624 625 626 def concat_data ( self , other_data : \"Channels\" ) -> \"Channels\" : \"\"\"Return a new Channels object with data from this and the other Channels object concatenated together. Only channels that exist in both objects are included in the result.\"\"\" # sorting here to show intention, but I think set is sorted by insertion order as # an implementation detail so this may not do anything ch_nums = sorted ( list ( set ( self . channels . keys ()) . intersection ( other_data . channels . keys ()))) new_channels = {} for ch_num in ch_nums : ch = self . channels [ ch_num ] other_ch = other_data . channels [ ch_num ] combined_df = mass2 . core . misc . concat_dfs_with_concat_state ( ch . df , other_ch . df ) new_ch = ch . with_replacement_df ( combined_df ) new_channels [ ch_num ] = new_ch return mass2 . Channels ( new_channels , self . description + other_data . description ) dfg ( exclude = 'pulse' ) cached Return a DataFrame containing good pulses from each channel. Excludes the given columns (default \"pulse\"). Source code in mass2/core/channels.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @functools . cache def dfg ( self , exclude : str = \"pulse\" ) -> pl . DataFrame : \"\"\"Return a DataFrame containing good pulses from each channel. Excludes the given columns (default \"pulse\").\"\"\" # return a dataframe containing good pulses from each channel, # exluding \"pulse\" by default # and including column \"ch_num\" # the more common call should be to wrap this in a convenient plotter dfs = [] for ch_num , channel in self . channels . items (): df = channel . df . select ( pl . exclude ( exclude )) . filter ( channel . good_expr ) # key_series = pl.Series(\"key\", dtype=pl.Int64).extend_constant(key, len(df)) assert ch_num == channel . header . ch_num ch_series = pl . Series ( \"ch_num\" , dtype = pl . Int64 ) . extend_constant ( channel . header . ch_num , len ( df )) dfs . append ( df . with_columns ( ch_series )) return pl . concat ( dfs ) from_df ( df_in , frametime_s , n_presamples , n_samples , description = 'from Channels.channels_from_df' ) classmethod Create a Channels object from a single DataFrame that holds data from multiple channels. Source code in mass2/core/channels.py 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 @classmethod def from_df ( cls , df_in : pl . DataFrame , frametime_s : float , n_presamples : int , n_samples : int , description : str = \"from Channels.channels_from_df\" , ) -> \"Channels\" : \"\"\"Create a Channels object from a single DataFrame that holds data from multiple channels.\"\"\" # requres a column named \"ch_num\" containing the channel number keys_df : dict [ tuple , pl . DataFrame ] = df_in . partition_by ( by = [ \"ch_num\" ], as_dict = True ) dfs : dict [ int , pl . DataFrame ] = { keys [ 0 ]: df for ( keys , df ) in keys_df . items ()} channels : dict [ int , Channel ] = {} for ch_num , df in dfs . items (): channels [ ch_num ] = Channel ( df , header = ChannelHeader ( description = \"from df\" , ch_num = ch_num , frametime_s = frametime_s , n_presamples = n_presamples , n_samples = n_samples , df = df , ), npulses = len ( df ), ) return Channels ( channels , description ) from_ljh_folder ( pulse_folder , noise_folder = None , limit = None , exclude_ch_nums = None ) classmethod Create an instance from a directory of LJH files. Source code in mass2/core/channels.py 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 @classmethod def from_ljh_folder ( cls , pulse_folder : str | Path , noise_folder : str | Path | None = None , limit : int | None = None , exclude_ch_nums : list [ int ] | None = None , ) -> \"Channels\" : \"\"\"Create an instance from a directory of LJH files.\"\"\" assert os . path . isdir ( pulse_folder ), f \" { pulse_folder =} { noise_folder =} \" pulse_folder = str ( pulse_folder ) if exclude_ch_nums is None : exclude_ch_nums = [] if noise_folder is None : paths = ljhutil . find_ljh_files ( pulse_folder , exclude_ch_nums = exclude_ch_nums ) if limit is not None : paths = paths [: limit ] pairs = [( path , \"\" ) for path in paths ] else : assert os . path . isdir ( noise_folder ), f \" { pulse_folder =} { noise_folder =} \" noise_folder = str ( noise_folder ) pairs = ljhutil . match_files_by_channel ( pulse_folder , noise_folder , limit = limit , exclude_ch_nums = exclude_ch_nums ) description = f \"from_ljh_folder { pulse_folder =} { noise_folder =} \" print ( f \" { description } \" ) print ( f \" from_ljh_folder has { len ( pairs ) } pairs\" ) data = cls . from_ljh_path_pairs ( pairs , description ) print ( f \" and the Channels obj has { len ( data . channels ) } pairs\" ) return data from_ljh_path_pairs ( pulse_noise_pairs , description ) classmethod Create a :class: Channels instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of (pulse_path, noise_path) tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class: Channel per (pulse_path, noise_path) pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth: Channel.from_ljh . The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] Source code in mass2/core/channels.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 @classmethod def from_ljh_path_pairs ( cls , pulse_noise_pairs : Iterable [ tuple [ str , str ]], description : str ) -> \"Channels\" : \"\"\" Create a :class:`Channels` instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of `(pulse_path, noise_path)` tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class:`Channel` per `(pulse_path, noise_path)` pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth:`Channel.from_ljh`. The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] \"\"\" channels : dict [ int , Channel ] = {} for pulse_path , noise_path in pulse_noise_pairs : channel = Channel . from_ljh ( pulse_path , noise_path ) assert channel . header . ch_num not in channels . keys () channels [ channel . header . ch_num ] = channel return cls ( channels , description ) from_off_paths ( off_paths , description ) classmethod Create an instance from a sequence of OFF-file paths Source code in mass2/core/channels.py 451 452 453 454 455 456 457 458 @classmethod def from_off_paths ( cls , off_paths : Iterable [ str | Path ], description : str ) -> \"Channels\" : \"\"\"Create an instance from a sequence of OFF-file paths\"\"\" channels = {} for path in off_paths : ch = Channel . from_off ( mass2 . core . OffFile ( str ( path ))) channels [ ch . header . ch_num ] = ch return cls ( channels , description ) get_an_ljh_path () Return the path to a representative one of the LJH files used to create this Channels object. Source code in mass2/core/channels.py 489 490 491 def get_an_ljh_path ( self ) -> Path : \"\"\"Return the path to a representative one of the LJH files used to create this Channels object.\"\"\" return pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) get_experiment_state_df ( experiment_state_path = None ) Return a DataFrame containing experiment state information, loading from the given path or (if None) inferring it from an LJH file. Source code in mass2/core/channels.py 502 503 504 505 506 507 508 509 510 511 512 513 514 def get_experiment_state_df ( self , experiment_state_path : str | Path | None = None ) -> pl . DataFrame : \"\"\"Return a DataFrame containing experiment state information, loading from the given path or (if None) inferring it from an LJH file.\"\"\" if experiment_state_path is None : ljh_path = self . get_an_ljh_path () experiment_state_path = ljhutil . experiment_state_path_from_ljh_path ( ljh_path ) df = pl . read_csv ( experiment_state_path , new_columns = [ \"unixnano\" , \"state_label\" ]) # _col0, _col1 = df.columns df_es = df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) # strip whitespace from state_label column sl_series = df . select ( pl . col ( \"state_label\" ) . str . strip_chars ()) . to_series () df_es = df_es . with_columns ( state_label = pl . Series ( values = sl_series , dtype = pl . Categorical )) return df_es get_path_in_output_folder ( filename ) Return a path in an output folder named like the run number, sibling to the LJH folder. Source code in mass2/core/channels.py 493 494 495 496 497 498 499 500 def get_path_in_output_folder ( self , filename : str | Path ) -> Path : \"\"\"Return a path in an output folder named like the run number, sibling to the LJH folder.\"\"\" ljh_path = self . get_an_ljh_path () base_name , _ = ljh_path . name . split ( \"_chan\" ) date , run_num = base_name . split ( \"_run\" ) # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") output_dir = ljh_path . parent . parent / f \" { run_num } mass2_output\" output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir / filename linefit ( line , col , use_expr = pl . lit ( True ), has_linear_background = False , has_tails = False , dlo = 50 , dhi = 50 , binsize = 0.5 , params_update = lmfit . Parameters ()) Perform a fit to one spectral line in the coadded histogram of the given column. Source code in mass2/core/channels.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def linefit ( # noqa: PLR0917 self , line : float | str | SpectralLine | GenericLineModel , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : \"\"\"Perform a fit to one spectral line in the coadded histogram of the given column.\"\"\" model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = mass2 . misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = f \" { len ( self . channels ) } channels, { self . description } \" , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result linefit_joblib ( line , col , prefer = 'threads' , n_jobs = 4 ) No one but Galen understands this function. Source code in mass2/core/channels.py 387 388 389 390 391 392 393 394 395 396 397 def linefit_joblib ( self , line : str , col : str , prefer : str = \"threads\" , n_jobs : int = 4 ) -> LineModelResult : \"\"\"No one but Galen understands this function.\"\"\" def work ( key : int ) -> LineModelResult : \"\"\"A unit of parallel work: fit line to one channel.\"\"\" channel = self . channels [ key ] return channel . linefit ( line , col ) parallel = joblib . Parallel ( n_jobs = n_jobs , prefer = prefer ) # its not clear if threads are better.... what blocks the gil? results = parallel ( joblib . delayed ( work )( key ) for key in self . channels . keys ()) return results load_recipes ( filename ) Return a copy of this Channels object with Recipe objects loaded from the given pickle file and applied to each Channel. Source code in mass2/core/channels.py 600 601 602 603 604 def load_recipes ( self , filename : str ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with Recipe objects loaded from the given pickle file and applied to each Channel.\"\"\" steps = mass2 . misc . unpickle_object ( filename ) return self . with_steps_dict ( steps ) map ( f , allow_throw = False ) Map function f over all channels, returning a new Channels object containing the new Channel objects. Source code in mass2/core/channels.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def map ( self , f : Callable , allow_throw : bool = False ) -> \"Channels\" : \"\"\"Map function `f` over all channels, returning a new Channels object containing the new Channel objects.\"\"\" new_channels = {} new_bad_channels = {} for key , channel in self . channels . items (): try : new_channels [ key ] = f ( channel ) except KeyboardInterrupt as kint : raise kint except Exception as ex : error_type : type = type ( ex ) error_message : str = str ( ex ) backtrace : str = traceback . format_exc () if allow_throw : raise print ( f \" { key =} { channel =} failed the step { f } \" ) print ( f \" { error_type =} \" ) print ( f \" { error_message =} \" ) new_bad_channels [ key ] = channel . as_bad ( error_type , error_message , backtrace ) new_bad_channels = mass2 . misc . merge_dicts_ordered_by_keys ( self . bad_channels , new_bad_channels ) return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) parent_folder_path () Return the parent folder of the LJH files used to create this Channels object. Specifically, the self.ch0 channel's directory is used (normally the answer would be the same for all channels). Source code in mass2/core/channels.py 606 607 608 609 610 611 def parent_folder_path ( self ) -> pathlib . Path : \"\"\"Return the parent folder of the LJH files used to create this Channels object. Specifically, the `self.ch0` channel's directory is used (normally the answer would be the same for all channels).\"\"\" parent_folder_path = pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) . parent . parent print ( f \" { parent_folder_path =} \" ) return parent_folder_path plot_avg_pulses ( limit = 20 , channels = None , colormap = plt . cm . viridis , axis = None ) Plot the average pulses (the signal model) for the channels in this Channels object. Parameters: limit ( int | None , default: 20 ) \u2013 Plot at most this many filters if not None, by default 20 channels ( list [ int ] | None , default: None ) \u2013 Plot only channels with numbers in this list if not None, by default None colormap ( Colormap , default: viridis ) \u2013 The color scale to use, by default plt.cm.viridis axis ( Axes | None , default: None ) \u2013 A plt.Axes to plot on, or if None a new one, by default None Returns: Axes \u2013 The plt.Axes containing the plot. Source code in mass2/core/channels.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def plot_avg_pulses ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the average pulses (the signal model) for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] x = np . arange ( ch . header . n_samples ) - ch . header . n_presamples y = ch . last_avg_pulse if y is not None : plt . plot ( x , y , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . xlabel ( \"Samples after trigger\" ) plt . title ( \"Average pulses\" ) plot_filters ( limit = 20 , channels = None , colormap = plt . cm . viridis , axis = None ) Plot the optimal filters for the channels in this Channels object. Parameters: limit ( int | None , default: 20 ) \u2013 Plot at most this many filters if not None, by default 20 channels ( list [ int ] | None , default: None ) \u2013 Plot only channels with numbers in this list if not None, by default None colormap ( Colormap , default: viridis ) \u2013 The color scale to use, by default plt.cm.viridis axis ( Axes | None , default: None ) \u2013 A plt.Axes to plot on, or if None a new one, by default None Returns: Axes \u2013 The plt.Axes containing the plot. Source code in mass2/core/channels.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def plot_filters ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the optimal filters for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] # The next line _assumes_ a 5-lag filter. Fix as needed. x = np . arange ( ch . header . n_samples - 4 ) - ch . header . n_presamples + 2 y = ch . last_filter if y is not None : plt . plot ( x , y , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . xlabel ( \"Samples after trigger\" ) plt . title ( \"Optimal filters\" ) plot_hist ( col , bin_edges , use_expr = pl . lit ( True ), axis = None ) Plot a histogram for the given column across all channels. Source code in mass2/core/channels.py 115 116 117 118 119 def plot_hist ( self , col : str , bin_edges : ArrayLike , use_expr : pl . Expr = pl . lit ( True ), axis : plt . Axes | None = None ) -> None : \"\"\"Plot a histogram for the given column across all channels.\"\"\" df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () ax = mass2 . misc . plot_hist_of_series ( df_small [ col ], bin_edges , axis ) ax . set_title ( f \" { len ( self . channels ) } channels, { self . description } \" ) plot_hists ( col , bin_edges , group_by_col , axis = None , use_expr = None , skip_none = True ) Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channels.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : bool , axis : plt . Axes | None = None , use_expr : pl . Expr | None = None , skip_none : bool = True , ) -> None : \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () plot_noise_autocorr ( limit = 20 , channels = None , colormap = plt . cm . viridis , axis = None ) Plot the noise power autocorrelation for the channels in this Channels object. Parameters: limit ( int | None , default: 20 ) \u2013 Plot at most this many filters if not None, by default 20 channels ( list [ int ] | None , default: None ) \u2013 Plot only channels with numbers in this list if not None, by default None colormap ( Colormap , default: viridis ) \u2013 The color scale to use, by default plt.cm.viridis axis ( Axes | None , default: None ) \u2013 A plt.Axes to plot on, or if None a new one, by default None Returns: Axes \u2013 The plt.Axes containing the plot. Source code in mass2/core/channels.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def plot_noise_autocorr ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the noise power autocorrelation for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] ac = ch . last_noise_autocorrelation if ac is not None : color = colormap ( i / n_expected ) plt . plot ( ac , color = color , label = f \"Chan { ch_num } \" ) plt . plot ( 0 , ac [ 0 ], \"o\" , color = color ) plt . legend () plt . xlabel ( \"Lags\" ) plt . title ( \"Noise autocorrelation\" ) plot_noise_spectrum ( limit = 20 , channels = None , colormap = plt . cm . viridis , axis = None ) Plot the noise power spectrum for the channels in this Channels object. Parameters: limit ( int | None , default: 20 ) \u2013 Plot at most this many filters if not None, by default 20 channels ( list [ int ] | None , default: None ) \u2013 Plot only channels with numbers in this list if not None, by default None colormap ( Colormap , default: viridis ) \u2013 The color scale to use, by default plt.cm.viridis axis ( Axes | None , default: None ) \u2013 A plt.Axes to plot on, or if None a new one, by default None Returns: Axes \u2013 The plt.Axes containing the plot. Source code in mass2/core/channels.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def plot_noise_spectrum ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the noise power spectrum for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] freqpsd = ch . last_noise_psd if freqpsd is not None : freq , psd = freqpsd plt . plot ( freq , psd , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . loglog () plt . xlabel ( \"Frequency (Hz)\" ) plt . title ( \"Noise power spectral density\" ) save_recipes ( filename , required_fields = None , drop_debug = True ) Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set required_fields to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters: filename ( str ) \u2013 Filename to store recipe in, typically of the form \"*.pkl\" required_fields ( str | Iterable [ str ] | None , default: None ) \u2013 The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug ( bool , default: True ) \u2013 Whether to remove debugging-related data from each RecipeStep , if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns: dict \u2013 Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. Source code in mass2/core/channels.py 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 def save_recipes ( self , filename : str , required_fields : str | Iterable [ str ] | None = None , drop_debug : bool = True ) -> dict [ int , Recipe ]: \"\"\"Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set `required_fields` to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters ---------- filename : str Filename to store recipe in, typically of the form \"*.pkl\" required_fields : str | Iterable[str] | None The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug: bool Whether to remove debugging-related data from each `RecipeStep`, if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns ------- dict Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. \"\"\" steps = {} for channum , ch in self . channels . items (): steps [ channum ] = ch . steps . trim_dead_ends ( required_fields = required_fields , drop_debug = drop_debug ) mass2 . misc . pickle_object ( steps , filename ) return steps set_bad ( ch_num , msg , require_ch_num_exists = True ) Return a copy of this Channels object with the given channel number marked as bad. Source code in mass2/core/channels.py 374 375 376 377 378 379 380 381 382 383 384 385 def set_bad ( self , ch_num : int , msg : str , require_ch_num_exists : bool = True ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with the given channel number marked as bad.\"\"\" new_channels = {} new_bad_channels = {} if require_ch_num_exists : assert ch_num in self . channels . keys (), f \" { ch_num } can't be set bad because it does not exist\" for key , channel in self . channels . items (): if key == ch_num : new_bad_channels [ key ] = channel . as_bad ( None , msg , None ) else : new_channels [ key ] = channel return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) with_experiment_state_by_path ( experiment_state_path = None ) Return a copy of this Channels object with experiment state information added, loaded from the given path. Source code in mass2/core/channels.py 516 517 518 519 520 def with_experiment_state_by_path ( self , experiment_state_path : str | None = None ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with experiment state information added, loaded from the given path.\"\"\" df_es = self . get_experiment_state_df ( experiment_state_path ) return self . with_experiment_state_df ( df_es ) with_experiment_state_df ( df_es ) Return a copy of this Channels object with experiment state information added to each Channel, found from the given DataFrame. Source code in mass2/core/channels.py 545 546 547 548 549 550 551 552 553 def with_experiment_state_df ( self , df_es : pl . DataFrame ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with experiment state information added to each Channel, found from the given DataFrame.\"\"\" # this is not as performant as making use_exprs for states # and using .set_sorted on the timestamp column ch2s = {} for ch_num , ch in self . channels . items (): ch2s [ ch_num ] = ch . with_experiment_state_df ( df_es ) return Channels ( ch2s , self . description ) with_external_trigger_by_path ( path = None ) Return a copy of this Channels object with external trigger information added, loaded from the given path or EVENTUALLY (if None) inferring it from an LJH file (not yet implemented). Source code in mass2/core/channels.py 522 523 524 525 526 527 528 529 530 531 532 533 def with_external_trigger_by_path ( self , path : str | None = None ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with external trigger information added, loaded from the given path or EVENTUALLY (if None) inferring it from an LJH file (not yet implemented).\"\"\" if path is None : raise NotImplementedError ( \"cannot infer external trigger path yet\" ) with open ( path , \"rb\" ) as _f : _header_line = _f . readline () # read the one header line before opening the binary data external_trigger_subframe_count = np . fromfile ( _f , \"int64\" ) df_ext = pl . DataFrame ({ \"subframecount\" : external_trigger_subframe_count , }) return self . with_external_trigger_df ( df_ext ) with_external_trigger_df ( df_ext ) Return a copy of this Channels object with external trigger information added to each Channel, found from the given DataFrame. Source code in mass2/core/channels.py 535 536 537 538 539 540 541 542 543 def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with external trigger information added to each Channel, found from the given DataFrame.\"\"\" def with_etrig_df ( channel : Channel ) -> Channel : \"\"\"Return a copy of one Channel object with external trigger information added to it\"\"\" return channel . with_external_trigger_df ( df_ext ) return self . map ( with_etrig_df ) with_more_channels ( more ) Return a Channels object with additional Channels in it. New channels with the same number will overrule existing ones. Parameters: more ( Channels ) \u2013 Another Channels object, to be added Returns: Channels \u2013 The replacement Source code in mass2/core/channels.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def with_more_channels ( self , more : \"Channels\" ) -> \"Channels\" : \"\"\"Return a Channels object with additional Channels in it. New channels with the same number will overrule existing ones. Parameters ---------- more : Channels Another Channels object, to be added Returns ------- Channels The replacement \"\"\" channels = self . channels . copy () channels . update ( more . channels ) bad = self . bad_channels . copy () bad . update ( more . bad_channels ) descr = self . description + more . description + \" \\n Warning! created by with_more_channels()\" return dataclasses . replace ( self , channels = channels , bad_channels = bad , description = descr ) with_steps_dict ( steps_dict ) Return a copy of this Channels object with the given Recipe objects added to each Channel. Source code in mass2/core/channels.py 555 556 557 558 559 560 561 562 563 564 565 566 def with_steps_dict ( self , steps_dict : dict [ int , Recipe ]) -> \"Channels\" : \"\"\"Return a copy of this Channels object with the given Recipe objects added to each Channel.\"\"\" def load_recipes ( channel : Channel ) -> Channel : \"\"\"Return a copy of one Channel object with Recipe steps added to it\"\"\" try : steps = steps_dict [ channel . header . ch_num ] except KeyError : raise Exception ( \"steps dict did not contain steps for this ch_num\" ) return channel . with_steps ( steps ) return self . map ( load_recipes ) mass2.core.analysis_algorithms - main algorithms used in data analysis Designed to abstract certain key algorithms out of the class MicrocalDataSet and be able to run them fast. Created on Jun 9, 2014 @author: fowlerj HistogramSmoother Object that can repeatedly smooth histograms with the same bin count and width to the same Gaussian width. By pre-computing the smoothing kernel for that histogram, we can smooth multiple histograms with the same geometry. Source code in mass2/core/analysis_algorithms.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 class HistogramSmoother : \"\"\"Object that can repeatedly smooth histograms with the same bin count and width to the same Gaussian width. By pre-computing the smoothing kernel for that histogram, we can smooth multiple histograms with the same geometry. \"\"\" def __init__ ( self , smooth_sigma : float , limits : ArrayLike ): \"\"\"Give the smoothing Gaussian's width as <smooth_sigma> and the [lower,upper] histogram limits as <limits>.\"\"\" self . limits = tuple ( np . asarray ( limits , dtype = float )) self . smooth_sigma = smooth_sigma # Choose a reasonable # of bins, at least 1024 and a power of 2 stepsize = 0.4 * smooth_sigma dlimits = self . limits [ 1 ] - self . limits [ 0 ] nbins_guess = int ( dlimits / stepsize + 0.5 ) min_nbins = 1024 max_nbins = 32768 # 32k bins, 2**15 # Clamp nbins_guess to at least min_nbins clamped_nbins = np . clip ( nbins_guess , min_nbins , max_nbins ) nbins_forced_to_power_of_2 = int ( 2 ** np . ceil ( np . log2 ( clamped_nbins ))) # if nbins_forced_to_power_of_2 == max_nbins: # print(f\"Warning: HistogramSmoother (for drift correct) Limiting histogram bins to {max_nbins} (requested {nbins_guess})\") self . nbins = nbins_forced_to_power_of_2 self . stepsize = dlimits / self . nbins # Compute the Fourier-space smoothing kernel kernel = np . exp ( - 0.5 * ( np . arange ( self . nbins ) * self . stepsize / self . smooth_sigma ) ** 2 ) kernel [ 1 :] += kernel [ - 1 : 0 : - 1 ] # Handle the negative frequencies kernel /= kernel . sum () self . kernel_ft = np . fft . rfft ( kernel ) def __call__ ( self , values : ArrayLike ) -> NDArray : \"\"\"Return a smoothed histogram of the data vector <values>\"\"\" contents , _ = np . histogram ( values , self . nbins , self . limits ) ftc = np . fft . rfft ( contents ) csmooth = np . fft . irfft ( self . kernel_ft * ftc ) csmooth [ csmooth < 0 ] = 0 return csmooth __call__ ( values ) Return a smoothed histogram of the data vector Source code in mass2/core/analysis_algorithms.py 215 216 217 218 219 220 221 def __call__ ( self , values : ArrayLike ) -> NDArray : \"\"\"Return a smoothed histogram of the data vector <values>\"\"\" contents , _ = np . histogram ( values , self . nbins , self . limits ) ftc = np . fft . rfft ( contents ) csmooth = np . fft . irfft ( self . kernel_ft * ftc ) csmooth [ csmooth < 0 ] = 0 return csmooth __init__ ( smooth_sigma , limits ) Give the smoothing Gaussian's width as and the [lower,upper] histogram limits as . Source code in mass2/core/analysis_algorithms.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def __init__ ( self , smooth_sigma : float , limits : ArrayLike ): \"\"\"Give the smoothing Gaussian's width as <smooth_sigma> and the [lower,upper] histogram limits as <limits>.\"\"\" self . limits = tuple ( np . asarray ( limits , dtype = float )) self . smooth_sigma = smooth_sigma # Choose a reasonable # of bins, at least 1024 and a power of 2 stepsize = 0.4 * smooth_sigma dlimits = self . limits [ 1 ] - self . limits [ 0 ] nbins_guess = int ( dlimits / stepsize + 0.5 ) min_nbins = 1024 max_nbins = 32768 # 32k bins, 2**15 # Clamp nbins_guess to at least min_nbins clamped_nbins = np . clip ( nbins_guess , min_nbins , max_nbins ) nbins_forced_to_power_of_2 = int ( 2 ** np . ceil ( np . log2 ( clamped_nbins ))) # if nbins_forced_to_power_of_2 == max_nbins: # print(f\"Warning: HistogramSmoother (for drift correct) Limiting histogram bins to {max_nbins} (requested {nbins_guess})\") self . nbins = nbins_forced_to_power_of_2 self . stepsize = dlimits / self . nbins # Compute the Fourier-space smoothing kernel kernel = np . exp ( - 0.5 * ( np . arange ( self . nbins ) * self . stepsize / self . smooth_sigma ) ** 2 ) kernel [ 1 :] += kernel [ - 1 : 0 : - 1 ] # Handle the negative frequencies kernel /= kernel . sum () self . kernel_ft = np . fft . rfft ( kernel ) compute_max_deriv ( pulse_data , ignore_leading , spike_reject = True , kernel = None ) Computes the maximum derivative in timeseries . can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in . Args: pulse_data: ignore_leading: spike_reject: (default True) kernel: the linear filter against which the signals will be convolved (CONVOLED, not correlated, so reverse the filter as needed). If None, then the default kernel of [+.2 +.1 0 -.1 -.2] will be used. If \"SG\", then the cubic 5-point Savitzky-Golay filter will be used (see below). Otherwise, kernel needs to be a (short) array which will be converted to a 1xN 2-dimensional np.ndarray. (default None) Returns: An np.ndarray, dimension 1: the value of the maximum derivative (units of per sample). When kernel==\"SG\", then we estimate the derivative by Savitzky-Golay filtering (with 1 point before/3 points after the point in question and fitting polynomial of order 3). Find the right general area by first doing a simple difference. Source code in mass2/core/analysis_algorithms.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def compute_max_deriv ( pulse_data : ArrayLike , ignore_leading : int , spike_reject : bool = True , kernel : ArrayLike | str | None = None ) -> NDArray : \"\"\"Computes the maximum derivative in timeseries <pulse_data>. <pulse_data> can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in <pulse_data>. Args: pulse_data: ignore_leading: spike_reject: (default True) kernel: the linear filter against which the signals will be convolved (CONVOLED, not correlated, so reverse the filter as needed). If None, then the default kernel of [+.2 +.1 0 -.1 -.2] will be used. If \"SG\", then the cubic 5-point Savitzky-Golay filter will be used (see below). Otherwise, kernel needs to be a (short) array which will be converted to a 1xN 2-dimensional np.ndarray. (default None) Returns: An np.ndarray, dimension 1: the value of the maximum derivative (units of <pulse_data units> per sample). When kernel==\"SG\", then we estimate the derivative by Savitzky-Golay filtering (with 1 point before/3 points after the point in question and fitting polynomial of order 3). Find the right general area by first doing a simple difference. \"\"\" # If pulse_data is a 1D array, turn it into 2 pulse_data = np . asarray ( pulse_data ) ndim = len ( pulse_data . shape ) if ndim > 2 or ndim < 1 : raise ValueError ( \"input pulse_data should be a 1d or 2d array.\" ) if ndim == 1 : pulse_data . shape = ( 1 , pulse_data . shape [ 0 ]) pulse_view = pulse_data [:, ignore_leading :] NPulse = pulse_view . shape [ 0 ] NSamp = pulse_view . shape [ 1 ] # The default filter: filter_coef = np . array ([ + 0.2 , + 0.1 , 0 , - 0.1 , - 0.2 ]) if kernel == \"SG\" : # This filter is the Savitzky-Golay filter of n_L=1, n_R=3 and M=3, to use the # language of Numerical Recipes 3rd edition. It amounts to least-squares fitting # of an M=3rd order polynomial to the five points [-1,+3] and # finding the slope of the polynomial at 0. # Note that we reverse the order of coefficients because convolution will re-reverse filter_coef = np . array ([ - 0.45238 , - 0.02381 , 0.28571 , 0.30952 , - 0.11905 ])[:: - 1 ] elif kernel is not None : filter_coef = np . array ( kernel ) . ravel () f0 , f1 , f2 , f3 , f4 = filter_coef max_deriv = np . zeros ( NPulse , dtype = np . float64 ) if spike_reject : for i in range ( NPulse ): pulses = pulse_view [ i ] t0 = f4 * pulses [ 0 ] + f3 * pulses [ 1 ] + f2 * pulses [ 2 ] + f1 * pulses [ 3 ] + f0 * pulses [ 4 ] t1 = f4 * pulses [ 1 ] + f3 * pulses [ 2 ] + f2 * pulses [ 3 ] + f1 * pulses [ 4 ] + f0 * pulses [ 5 ] t2 = f4 * pulses [ 2 ] + f3 * pulses [ 3 ] + f2 * pulses [ 4 ] + f1 * pulses [ 5 ] + f0 * pulses [ 6 ] t_max_deriv = t2 if t2 < t0 else t0 for j in range ( 7 , NSamp ): t3 = f4 * pulses [ j - 4 ] + f3 * pulses [ j - 3 ] + f2 * pulses [ j - 2 ] + f1 * pulses [ j - 1 ] + f0 * pulses [ j ] t4 = t3 if t3 < t1 else t1 t_max_deriv = max ( t4 , t_max_deriv ) t0 , t1 , t2 = t1 , t2 , t3 max_deriv [ i ] = t_max_deriv else : for i in range ( NPulse ): pulses = pulse_view [ i ] t0 = f4 * pulses [ 0 ] + f3 * pulses [ 1 ] + f2 * pulses [ 2 ] + f1 * pulses [ 3 ] + f0 * pulses [ 4 ] t_max_deriv = t0 for j in range ( 5 , NSamp ): t0 = f4 * pulses [ j - 4 ] + f3 * pulses [ j - 3 ] + f2 * pulses [ j - 2 ] + f1 * pulses [ j - 1 ] + f0 * pulses [ j ] t_max_deriv = max ( t0 , t_max_deriv ) max_deriv [ i ] = t_max_deriv return np . asarray ( max_deriv , dtype = np . float32 ) correct_flux_jumps ( vals , mask , flux_quant ) Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected Source code in mass2/core/analysis_algorithms.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 def correct_flux_jumps ( vals : ArrayLike , mask : ArrayLike , flux_quant : float ) -> NDArray : \"\"\"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected \"\"\" return unwrap_n ( vals , flux_quant , mask ) correct_flux_jumps_original ( vals , mask , flux_quant ) Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected Source code in mass2/core/analysis_algorithms.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def correct_flux_jumps_original ( vals : ArrayLike , mask : ArrayLike , flux_quant : float ) -> NDArray : \"\"\"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected \"\"\" # The naive thing is to simply replace each value with its value mod # the flux quantum. But of the baseline value turns out to fluctuate # about an integer number of flux quanta, this will introduce new # jumps. I don't know the best way to handle this in general. For now, # if there are still jumps after the mod, I add 1/4 of a flux quanta # before modding, then mod, then subtract the 1/4 flux quantum and then # *add* a single flux quantum so that the values never go negative. # # To determine whether there are \"still jumps after the mod\" I look at the # difference between the largest and smallest values for \"good\" pulses. If # you don't exclude \"bad\" pulses, this check can be tricked in cases where # the pretrigger section contains a (sufficiently large) tail. vals = np . asarray ( vals ) mask = np . asarray ( mask ) if ( np . amax ( vals ) - np . amin ( vals )) >= flux_quant : corrected = vals % flux_quant if ( np . amax ( corrected [ mask ]) - np . amin ( corrected [ mask ])) > 0.75 * flux_quant : corrected = ( vals + flux_quant / 4 ) % ( flux_quant ) corrected = corrected - flux_quant / 4 + flux_quant corrected -= corrected [ 0 ] - vals [ 0 ] return corrected else : return vals drift_correct ( indicator , uncorrected , limit = None ) Compute a drift correction that minimizes the spectral entropy. Args: indicator: The \"x-axis\", which indicates the size of the correction. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. limit: The upper limit of uncorrected values over which entropy is computed (default None). Generally indicator will be the pretrigger mean of the pulses, but you can experiment with other choices. The entropy will be computed on corrected values only in the range [0, limit], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then in will be compute as 25% larger than the 99%ile point of uncorrected. The model is that the filtered pulse height PH should be scaled by (1 + a*PTM) where a is an arbitrary parameter computed here, and PTM is the difference between each record's pretrigger mean and the median value of all pretrigger means. (Or replace \"pretrigger mean\" with whatever quantity you passed in as .) Source code in mass2/core/analysis_algorithms.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def drift_correct ( indicator : ArrayLike , uncorrected : ArrayLike , limit : float | None = None ) -> tuple [ float , dict ]: \"\"\"Compute a drift correction that minimizes the spectral entropy. Args: indicator: The \"x-axis\", which indicates the size of the correction. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. limit: The upper limit of uncorrected values over which entropy is computed (default None). Generally indicator will be the pretrigger mean of the pulses, but you can experiment with other choices. The entropy will be computed on corrected values only in the range [0, limit], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then in will be compute as 25% larger than the 99%ile point of uncorrected. The model is that the filtered pulse height PH should be scaled by (1 + a*PTM) where a is an arbitrary parameter computed here, and PTM is the difference between each record's pretrigger mean and the median value of all pretrigger means. (Or replace \"pretrigger mean\" with whatever quantity you passed in as <indicator>.) \"\"\" uncorrected = np . asarray ( uncorrected ) indicator = np . array ( indicator ) # make a copy ptm_offset = np . median ( indicator ) indicator -= ptm_offset if limit is None : pct99 = np . percentile ( uncorrected , 99 ) limit = 1.25 * pct99 smoother = HistogramSmoother ( 0.5 , [ 0 , limit ]) assert smoother . nbins < 1e6 , \"will be crazy slow, should not be possible\" def entropy ( param : NDArray , indicator : NDArray , uncorrected : NDArray , smoother : HistogramSmoother ) -> float : \"\"\"Return the entropy of the drift-corrected values\"\"\" corrected = uncorrected * ( 1 + indicator * param ) hsmooth = smoother ( corrected ) w = hsmooth > 0 return - ( np . log ( hsmooth [ w ]) * hsmooth [ w ]) . sum () drift_corr_param = sp . optimize . brent ( entropy , ( indicator , uncorrected , smoother ), brack = [ 0 , 0.001 ]) drift_correct_info = { \"type\" : \"ptmean_gain\" , \"slope\" : drift_corr_param , \"median_pretrig_mean\" : ptm_offset } return drift_corr_param , drift_correct_info estimateRiseTime ( pulse_data , timebase , nPretrig ) Computes the rise time of timeseries , where the time steps are . can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in . If nPretrig >= 4, then the samples pulse_data[:nPretrig] are averaged to estimate the baseline. Otherwise, the minimum of pulse_data is assumed to be the baseline. Specifically, take the first and last of the rising points in the range of 10% to 90% of the peak value, interpolate a line between the two, and use its slope to find the time to rise from 0 to the peak. Args: pulse_data: An np.ndarray of dimension 1 (a single pulse record) or 2 (an array with each row being a pulse record). timebase: The sampling time. nPretrig: The number of samples that are recorded before the trigger. Returns: An ndarray of dimension 1, giving the rise times. Source code in mass2/core/analysis_algorithms.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @njit def estimateRiseTime ( pulse_data : ArrayLike , timebase : float , nPretrig : int ) -> NDArray : \"\"\"Computes the rise time of timeseries <pulse_data>, where the time steps are <timebase>. <pulse_data> can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in <pulse_data>. If nPretrig >= 4, then the samples pulse_data[:nPretrig] are averaged to estimate the baseline. Otherwise, the minimum of pulse_data is assumed to be the baseline. Specifically, take the first and last of the rising points in the range of 10% to 90% of the peak value, interpolate a line between the two, and use its slope to find the time to rise from 0 to the peak. Args: pulse_data: An np.ndarray of dimension 1 (a single pulse record) or 2 (an array with each row being a pulse record). timebase: The sampling time. nPretrig: The number of samples that are recorded before the trigger. Returns: An ndarray of dimension 1, giving the rise times. \"\"\" MINTHRESH , MAXTHRESH = 0.1 , 0.9 # If pulse_data is a 1D array, turn it into 2 pulse_data = np . asarray ( pulse_data ) ndim = len ( pulse_data . shape ) if ndim > 2 or ndim < 1 : raise ValueError ( \"input pulse_data should be a 1d or 2d array.\" ) if ndim == 1 : pulse_data . shape = ( 1 , pulse_data . shape [ 0 ]) # The following requires a lot of numpy foo to read. Sorry! if nPretrig >= 4 : baseline_value = pulse_data [:, 0 : nPretrig ] . mean ( axis = 1 ) else : baseline_value = pulse_data . min ( axis = 1 ) nPretrig = 0 value_at_peak = pulse_data . max ( axis = 1 ) - baseline_value idx_last_pk = pulse_data . argmax ( axis = 1 ) . max () npulses = pulse_data . shape [ 0 ] try : rising_data = ( pulse_data [:, nPretrig : idx_last_pk + 1 ] - baseline_value [:, np . newaxis ]) / value_at_peak [:, np . newaxis ] # Find the last and first indices at which the data are in (0.1, 0.9] times the # peak value. Then make sure last is at least 1 past first. last_idx = ( rising_data > MAXTHRESH ) . argmax ( axis = 1 ) - 1 first_idx = ( rising_data > MINTHRESH ) . argmax ( axis = 1 ) last_idx [ last_idx < first_idx ] = first_idx [ last_idx < first_idx ] + 1 last_idx [ last_idx == rising_data . shape [ 1 ]] = rising_data . shape [ 1 ] - 1 pulsenum = np . arange ( npulses ) y_diff = np . asarray ( rising_data [ pulsenum , last_idx ] - rising_data [ pulsenum , first_idx ], dtype = float ) y_diff [ y_diff < timebase ] = timebase time_diff = timebase * ( last_idx - first_idx ) rise_time = time_diff / y_diff rise_time [ y_diff <= 0 ] = - 9.9e-6 return rise_time except ValueError : return - 9.9e-6 + np . zeros ( npulses , dtype = float ) filter_signal_lowpass ( sig , fs , fcut ) Tophat lowpass filter using an FFT Args: sig - the signal to be filtered fs - the sampling frequency of the signal fcut - the frequency at which to cutoff the signal Returns: the filtered signal Source code in mass2/core/analysis_algorithms.py 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 @njit def filter_signal_lowpass ( sig : NDArray , fs : float , fcut : float ) -> NDArray : \"\"\"Tophat lowpass filter using an FFT Args: sig - the signal to be filtered fs - the sampling frequency of the signal fcut - the frequency at which to cutoff the signal Returns: the filtered signal \"\"\" N = sig . shape [ 0 ] SIG = np . fft . fft ( sig ) freqs = ( fs / N ) * np . concatenate (( np . arange ( 0 , N / 2 + 1 ), np . arange ( N / 2 - 1 , 0 , - 1 ))) filt = np . zeros_like ( SIG ) filt [ freqs < fcut ] = 1.0 sig_filt = np . fft . ifft ( SIG * filt ) return sig_filt make_smooth_histogram ( values , smooth_sigma , limit , upper_limit = None ) Convert a vector of arbitrary info a smoothed histogram by histogramming it and smoothing. This is a convenience function using the HistogramSmoother class. Args: values: The vector of data to be histogrammed. smooth_sigma: The smoothing Gaussian's width (FWHM) limit, upper_limit: The histogram limits are [limit,upper_limit] or [0,limit] if upper_limit is None. Returns: The smoothed histogram as an array. Source code in mass2/core/analysis_algorithms.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 @njit def make_smooth_histogram ( values : ArrayLike , smooth_sigma : float , limit : float , upper_limit : float | None = None ) -> NDArray : \"\"\"Convert a vector of arbitrary <values> info a smoothed histogram by histogramming it and smoothing. This is a convenience function using the HistogramSmoother class. Args: values: The vector of data to be histogrammed. smooth_sigma: The smoothing Gaussian's width (FWHM) limit, upper_limit: The histogram limits are [limit,upper_limit] or [0,limit] if upper_limit is None. Returns: The smoothed histogram as an array. \"\"\" if upper_limit is None : limit , upper_limit = 0 , limit return HistogramSmoother ( smooth_sigma , [ limit , upper_limit ])( values ) nearest_arrivals ( reference_times , other_times ) Find the external trigger time immediately before and after each pulse timestamp Args: pulse_timestamps - 1d array of pulse timestamps whose nearest neighbors need to be found. external_trigger_timestamps - 1d array of possible nearest neighbors. Returns: (before_times, after_times) before_times is an ndarray of the same size as pulse_timestamps. before_times[i] contains the difference between the closest lesser time contained in external_trigger_timestamps and pulse_timestamps[i] or inf if there was no earlier time in other_times Note that before_times is always a positive number even though the time difference it represents is negative. after_times is an ndarray of the same size as pulse_timestamps. after_times[i] contains the difference between pulse_timestamps[i] and the closest greater time contained in other_times or a inf number if there was no later time in external_trigger_timestamps. Source code in mass2/core/analysis_algorithms.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 @njit def nearest_arrivals ( reference_times : ArrayLike , other_times : ArrayLike ) -> tuple [ NDArray , NDArray ]: \"\"\"Find the external trigger time immediately before and after each pulse timestamp Args: pulse_timestamps - 1d array of pulse timestamps whose nearest neighbors need to be found. external_trigger_timestamps - 1d array of possible nearest neighbors. Returns: (before_times, after_times) before_times is an ndarray of the same size as pulse_timestamps. before_times[i] contains the difference between the closest lesser time contained in external_trigger_timestamps and pulse_timestamps[i] or inf if there was no earlier time in other_times Note that before_times is always a positive number even though the time difference it represents is negative. after_times is an ndarray of the same size as pulse_timestamps. after_times[i] contains the difference between pulse_timestamps[i] and the closest greater time contained in other_times or a inf number if there was no later time in external_trigger_timestamps. \"\"\" other_times = np . asarray ( other_times ) nearest_after_index = np . searchsorted ( other_times , reference_times ) # because both sets of arrival times should be sorted, there are faster algorithms than searchsorted # for example: https://github.com/kwgoodman/bottleneck/issues/47 # we could use one if performance becomes an issue last_index = np . searchsorted ( nearest_after_index , other_times . size , side = \"left\" ) first_index = np . searchsorted ( nearest_after_index , 1 ) nearest_before_index = np . copy ( nearest_after_index ) nearest_before_index [: first_index ] = 1 nearest_before_index -= 1 before_times = reference_times - other_times [ nearest_before_index ] before_times [: first_index ] = np . inf nearest_after_index [ last_index :] = other_times . size - 1 after_times = other_times [ nearest_after_index ] - reference_times after_times [ last_index :] = np . inf return before_times , after_times time_drift_correct ( time , uncorrected , w , sec_per_degree = 2000 , pulses_per_degree = 2000 , max_degrees = 20 , ndeg = None , limit = None ) Compute a time-based drift correction that minimizes the spectral entropy. Args: time: The \"time-axis\". Correction will be a low-order polynomial in this. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. w: the kernel width for the Laplace KDE density estimator sec_per_degree: assign as many as one polynomial degree per this many seconds pulses_per_degree: assign as many as one polynomial degree per this many pulses max_degrees: never use more than this many degrees of Legendre polynomial. n_deg: If not None, use this many degrees, regardless of the values of sec_per_degree, pulses_per_degree, and max_degress. In this case, never downsample. limit: The [lower,upper] limit of uncorrected values over which entropy is computed (default None). The entropy will be computed on corrected values only in the range [limit[0], limit[1]], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then it will be computed as 25%% larger than the 99%%ile point of uncorrected. Possible improvements in the future: * Use Numba to speed up. * Allow the parameters to be function arguments with defaults: photons per degree of freedom, seconds per degree of freedom, and max degrees of freedom. * Figure out how to span the available time with more than one set of legendre polynomials, so that we can have more than 20 d.o.f. eventually, for long runs. Source code in mass2/core/analysis_algorithms.py 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 def time_drift_correct ( # noqa: PLR0914 time : ArrayLike , uncorrected : ArrayLike , w : float , sec_per_degree : float = 2000 , pulses_per_degree : int = 2000 , max_degrees : int = 20 , ndeg : int | None = None , limit : tuple [ float , float ] | None = None , ) -> dict [ str , Any ]: \"\"\"Compute a time-based drift correction that minimizes the spectral entropy. Args: time: The \"time-axis\". Correction will be a low-order polynomial in this. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. w: the kernel width for the Laplace KDE density estimator sec_per_degree: assign as many as one polynomial degree per this many seconds pulses_per_degree: assign as many as one polynomial degree per this many pulses max_degrees: never use more than this many degrees of Legendre polynomial. n_deg: If not None, use this many degrees, regardless of the values of sec_per_degree, pulses_per_degree, and max_degress. In this case, never downsample. limit: The [lower,upper] limit of uncorrected values over which entropy is computed (default None). The entropy will be computed on corrected values only in the range [limit[0], limit[1]], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then it will be computed as 25%% larger than the 99%%ile point of uncorrected. Possible improvements in the future: * Use Numba to speed up. * Allow the parameters to be function arguments with defaults: photons per degree of freedom, seconds per degree of freedom, and max degrees of freedom. * Figure out how to span the available time with more than one set of legendre polynomials, so that we can have more than 20 d.o.f. eventually, for long runs. \"\"\" time = np . asarray ( time ) uncorrected = np . asarray ( uncorrected ) if limit is None : pct99 = np . percentile ( uncorrected , 99 ) limit = ( 0 , 1.25 * pct99 ) use = np . logical_and ( uncorrected > limit [ 0 ], uncorrected < limit [ 1 ]) time = np . asarray ( time [ use ]) uncorrected = np . asarray ( uncorrected [ use ]) tmin , tmax = np . min ( time ), np . max ( time ) def normalize ( t : NDArray ) -> NDArray : \"\"\"Rescale time to the range [-1,1]\"\"\" return ( t - tmin ) / ( tmax - tmin ) * 2 - 1 info = { \"tmin\" : tmin , \"tmax\" : tmax , \"normalize\" : normalize , } dtime = tmax - tmin N = len ( time ) if ndeg is None : ndeg = int ( np . minimum ( dtime / sec_per_degree , N / pulses_per_degree )) ndeg = min ( ndeg , max_degrees ) ndeg = max ( ndeg , 1 ) phot_per_degree = N / float ( ndeg ) if phot_per_degree >= 2 * pulses_per_degree : downsample = int ( phot_per_degree / pulses_per_degree ) time = time [:: downsample ] uncorrected = uncorrected [:: downsample ] N = len ( time ) else : downsample = 1 else : downsample = 1 LOG . info ( \"Using %2d degrees for %6d photons (after %d downsample)\" , ndeg , N , downsample ) LOG . info ( \"That's %6.1f photons per degree, and %6.1f seconds per degree.\" , N / float ( ndeg ), dtime / ndeg ) def model1 ( pi : NDArray , i : int , param : NDArray , basis : NDArray ) -> NDArray : \"The model function, with one parameter pi varied, others fixed.\" pcopy = np . array ( param ) pcopy [ i ] = pi return 1 + np . dot ( basis . T , pcopy ) def cost1 ( pi : NDArray , i : int , param : NDArray , y : NDArray , w : float , basis : NDArray ) -> float : \"The cost function (spectral entropy), with one parameter pi varied, others fixed.\" return laplace_entropy ( y * model1 ( pi , i , param , basis ), w = w ) param = np . zeros ( ndeg , dtype = float ) xnorm = np . asarray ( normalize ( time ), dtype = float ) basis = np . vstack ([ sp . special . legendre ( i + 1 )( xnorm ) for i in range ( ndeg )]) fc = 0 model : Callable = np . poly1d ([ 0 ]) info [ \"coefficients\" ] = np . zeros ( ndeg , dtype = float ) for i in range ( ndeg ): result , _fval , _iter , funcalls = sp . optimize . brent ( cost1 , ( i , param , uncorrected , w , basis ), [ - 0.001 , 0.001 ], tol = 1e-5 , full_output = True ) param [ i ] = result fc += funcalls model += sp . special . legendre ( i + 1 ) * result info [ \"coefficients\" ][ i ] = result info [ \"funccalls\" ] = fc xk = np . linspace ( - 1 , 1 , 1 + 2 * ndeg ) model2 = CubicSpline ( xk , model ( xk )) H1 = laplace_entropy ( uncorrected , w = w ) H2 = laplace_entropy ( uncorrected * ( 1 + model ( xnorm )), w = w ) H3 = laplace_entropy ( uncorrected * ( 1 + model2 ( xnorm )), w = w ) if H2 <= 0 or H2 - H1 > 0.0 : model = np . poly1d ([ 0 ]) elif H3 <= 0 or H3 - H2 > 0.00001 : model = model2 info [ \"entropies\" ] = ( H1 , H2 , H3 ) info [ \"model\" ] = model return info unwrap_n ( data , period , mask , n = 3 ) Unwrap data that has been restricted to a given period. The algorithm iterates through each data point and compares it to the average of the previous n data points. It then offsets the data point by the multiple of the period that will minimize the difference from that n-point running average. For the first n data points, there are not enough preceding points to average n of them, so the algorithm will average fewer points. This code was written by Thomas Baker; integrated into MASS by Dan Becker. Sped up 300x by @njit. Parameters: data ( array of data values ) \u2013 period ( the range over which the data loops ) \u2013 n ( how many preceding points to average , default: 3 ) \u2013 mask ( ArrayLike ) \u2013 Source code in mass2/core/analysis_algorithms.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 @njit def unwrap_n ( data : NDArray [ np . uint16 ], period : float , mask : ArrayLike , n : int = 3 ) -> NDArray : \"\"\"Unwrap data that has been restricted to a given period. The algorithm iterates through each data point and compares it to the average of the previous n data points. It then offsets the data point by the multiple of the period that will minimize the difference from that n-point running average. For the first n data points, there are not enough preceding points to average n of them, so the algorithm will average fewer points. This code was written by Thomas Baker; integrated into MASS by Dan Becker. Sped up 300x by @njit. Parameters ---------- data : array of data values period : the range over which the data loops n : how many preceding points to average mask: mask indentifying \"good\" pulses \"\"\" mask = np . asarray ( mask ) udata = np . copy ( data ) # make a copy for output if n <= 0 : return udata # Iterate through each data point and offset it by # an amount that will minimize the difference from the # rolling average nprior = 0 firstgoodidx = np . argmax ( mask ) priorvalues = np . full ( n , udata [ firstgoodidx ]) for i in range ( len ( data )): # Take the average of the previous n data points (only those with mask[i]==True). # Offset the data point by the most reasonable multiple of period (make this point closest to the running average). if mask [ i ]: avg = np . mean ( priorvalues ) if nprior == 0 : avg = float ( priorvalues [ 0 ]) elif nprior < n : avg = np . mean ( priorvalues [: nprior ]) udata [ i ] -= np . round (( udata [ i ] - avg ) / period ) * period if mask [ i ]: priorvalues [ nprior % n ] = udata [ i ] nprior += 1 return udata Provide DriftCorrectStep and DriftCorrection for correcting gain drifts that correlate with pretrigger mean. DriftCorrectStep dataclass Bases: RecipeStep A RecipeStep to apply a linear drift correction to pulse data in a DataFrame. Source code in mass2/core/drift_correction.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @dataclass ( frozen = True ) class DriftCorrectStep ( RecipeStep ): \"\"\"A RecipeStep to apply a linear drift correction to pulse data in a DataFrame.\"\"\" dc : typing . Any def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Apply the drift correction to the input DataFrame and return a new DataFrame with results.\"\"\" indicator_col , uncorrected_col = self . inputs slope , offset = self . dc . slope , self . dc . offset df2 = df . select (( pl . col ( uncorrected_col ) * ( 1 + slope * ( pl . col ( indicator_col ) - offset ))) . alias ( self . output [ 0 ])) . with_columns ( df ) return df2 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the uncorrected and corrected values against the indicator for debugging purposes.\"\"\" indicator_col , uncorrected_col = self . inputs # breakpoint() df_small = df_after . lazy () . filter ( self . good_expr ) . filter ( self . use_expr ) . select ( self . inputs + self . output ) . collect () mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ uncorrected_col ]) mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ self . output [ 0 ]], plt . gca (), ) plt . legend () plt . tight_layout () return plt . gca () @classmethod def learn ( cls , ch : \"Channel\" , indicator_col : str , uncorrected_col : str , corrected_col : str | None , use_expr : pl . Expr ) -> \"DriftCorrectStep\" : \"\"\"Create a DriftCorrectStep by learning the correction from data in the given Channel.\"\"\" if corrected_col is None : corrected_col = uncorrected_col + \"_dc\" indicator_s , uncorrected_s = ch . good_serieses ([ indicator_col , uncorrected_col ], use_expr ) dc = mass2 . core . drift_correct ( indicator = indicator_s . to_numpy (), uncorrected = uncorrected_s . to_numpy (), ) step = cls ( inputs = [ indicator_col , uncorrected_col ], output = [ corrected_col ], good_expr = ch . good_expr , use_expr = use_expr , dc = dc , ) return step calc_from_df ( df ) Apply the drift correction to the input DataFrame and return a new DataFrame with results. Source code in mass2/core/drift_correction.py 47 48 49 50 51 52 53 54 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Apply the drift correction to the input DataFrame and return a new DataFrame with results.\"\"\" indicator_col , uncorrected_col = self . inputs slope , offset = self . dc . slope , self . dc . offset df2 = df . select (( pl . col ( uncorrected_col ) * ( 1 + slope * ( pl . col ( indicator_col ) - offset ))) . alias ( self . output [ 0 ])) . with_columns ( df ) return df2 dbg_plot ( df_after , ** kwargs ) Plot the uncorrected and corrected values against the indicator for debugging purposes. Source code in mass2/core/drift_correction.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the uncorrected and corrected values against the indicator for debugging purposes.\"\"\" indicator_col , uncorrected_col = self . inputs # breakpoint() df_small = df_after . lazy () . filter ( self . good_expr ) . filter ( self . use_expr ) . select ( self . inputs + self . output ) . collect () mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ uncorrected_col ]) mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ self . output [ 0 ]], plt . gca (), ) plt . legend () plt . tight_layout () return plt . gca () learn ( ch , indicator_col , uncorrected_col , corrected_col , use_expr ) classmethod Create a DriftCorrectStep by learning the correction from data in the given Channel. Source code in mass2/core/drift_correction.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @classmethod def learn ( cls , ch : \"Channel\" , indicator_col : str , uncorrected_col : str , corrected_col : str | None , use_expr : pl . Expr ) -> \"DriftCorrectStep\" : \"\"\"Create a DriftCorrectStep by learning the correction from data in the given Channel.\"\"\" if corrected_col is None : corrected_col = uncorrected_col + \"_dc\" indicator_s , uncorrected_s = ch . good_serieses ([ indicator_col , uncorrected_col ], use_expr ) dc = mass2 . core . drift_correct ( indicator = indicator_s . to_numpy (), uncorrected = uncorrected_s . to_numpy (), ) step = cls ( inputs = [ indicator_col , uncorrected_col ], output = [ corrected_col ], good_expr = ch . good_expr , use_expr = use_expr , dc = dc , ) return step DriftCorrection dataclass A linear correction used to attempt remove any correlation between pretrigger mean and pulse height; will work with other quantities instead. Source code in mass2/core/drift_correction.py 93 94 95 96 97 98 99 100 101 102 103 104 105 @dataclass class DriftCorrection : \"\"\"A linear correction used to attempt remove any correlation between pretrigger mean and pulse height; will work with other quantities instead.\"\"\" offset : float slope : float def __call__ ( self , indicator : ArrayLike , uncorrected : ArrayLike ) -> NDArray : \"\"\"Apply the drift correction to the given uncorrected values using the given indicator values.\"\"\" indicator = np . asarray ( indicator ) uncorrected = np . asarray ( uncorrected ) return uncorrected * ( 1 + ( indicator - self . offset ) * self . slope ) __call__ ( indicator , uncorrected ) Apply the drift correction to the given uncorrected values using the given indicator values. Source code in mass2/core/drift_correction.py 101 102 103 104 105 def __call__ ( self , indicator : ArrayLike , uncorrected : ArrayLike ) -> NDArray : \"\"\"Apply the drift correction to the given uncorrected values using the given indicator values.\"\"\" indicator = np . asarray ( indicator ) uncorrected = np . asarray ( uncorrected ) return uncorrected * ( 1 + ( indicator - self . offset ) * self . slope ) drift_correct_mass ( indicator , uncorrected ) Determine drift correction parameters using mass2.core.analysis_algorithms.drift_correct. Source code in mass2/core/drift_correction.py 20 21 22 23 24 def drift_correct_mass ( indicator : ArrayLike , uncorrected : ArrayLike ) -> \"DriftCorrection\" : \"\"\"Determine drift correction parameters using mass2.core.analysis_algorithms.drift_correct.\"\"\" slope , dc_info = mass2 . core . analysis_algorithms . drift_correct ( indicator , uncorrected ) offset = dc_info [ \"median_pretrig_mean\" ] return DriftCorrection ( slope = slope , offset = offset ) drift_correct_wip ( indicator , uncorrected ) Work in progress to Determine drift correction parameters directly (??). Source code in mass2/core/drift_correction.py 27 28 29 30 31 32 33 34 35 def drift_correct_wip ( indicator : ArrayLike , uncorrected : ArrayLike ) -> \"DriftCorrection\" : \"\"\"Work in progress to Determine drift correction parameters directly (??).\"\"\" opt_result , offset = mass2 . core . rough_cal . minimize_entropy_linear ( np . asarray ( indicator ), np . asarray ( uncorrected ), bin_edges = np . arange ( 0 , 60000 , 1 ), fwhm_in_bin_number_units = 5 , ) return DriftCorrection ( offset = float ( offset ), slope = opt_result . x . astype ( np . float64 )) Provide OptimalFilterStep , a step to apply an optimal filter to pulse data in a DataFrame. OptimalFilterStep dataclass Bases: RecipeStep A step to apply an optimal filter to pulse data in a DataFrame. Source code in mass2/core/filter_steps.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @dataclass ( frozen = True ) class OptimalFilterStep ( RecipeStep ): \"\"\"A step to apply an optimal filter to pulse data in a DataFrame.\"\"\" filter : Filter spectrum : NoiseResult | None filter_maker : \"FilterMaker\" transform_raw : Callable | None = None def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Apply the optimal filter to the input DataFrame and return a new DataFrame with results.\"\"\" dfs = [] for df_iter in df . iter_slices ( 10000 ): raw = df_iter [ self . inputs [ 0 ]] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) peak_y , peak_x = self . filter . filter_records ( raw ) dfs . append ( pl . DataFrame ({ \"peak_x\" : peak_x , \"peak_y\" : peak_y })) df2 = pl . concat ( dfs ) . with_columns ( df ) df2 = df2 . rename ({ \"peak_x\" : self . output [ 0 ], \"peak_y\" : self . output [ 1 ]}) return df2 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the filter shape for debugging purposes.\"\"\" self . filter . plot () return plt . gca () def drop_debug ( self ) -> \"OptimalFilterStep\" : \"\"\"Return a copy of this step with debugging information (the NoiseResult) removed.\"\"\" return dataclasses . replace ( self , spectrum = None ) calc_from_df ( df ) Apply the optimal filter to the input DataFrame and return a new DataFrame with results. Source code in mass2/core/filter_steps.py 25 26 27 28 29 30 31 32 33 34 35 36 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Apply the optimal filter to the input DataFrame and return a new DataFrame with results.\"\"\" dfs = [] for df_iter in df . iter_slices ( 10000 ): raw = df_iter [ self . inputs [ 0 ]] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) peak_y , peak_x = self . filter . filter_records ( raw ) dfs . append ( pl . DataFrame ({ \"peak_x\" : peak_x , \"peak_y\" : peak_y })) df2 = pl . concat ( dfs ) . with_columns ( df ) df2 = df2 . rename ({ \"peak_x\" : self . output [ 0 ], \"peak_y\" : self . output [ 1 ]}) return df2 dbg_plot ( df_after , ** kwargs ) Plot the filter shape for debugging purposes. Source code in mass2/core/filter_steps.py 38 39 40 41 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the filter shape for debugging purposes.\"\"\" self . filter . plot () return plt . gca () drop_debug () Return a copy of this step with debugging information (the NoiseResult) removed. Source code in mass2/core/filter_steps.py 43 44 45 def drop_debug ( self ) -> \"OptimalFilterStep\" : \"\"\"Return a copy of this step with debugging information (the NoiseResult) removed.\"\"\" return dataclasses . replace ( self , spectrum = None ) Classes and functions for reading and handling LJH files. LJHFile dataclass Bases: ABC Represents the header and binary information of a single LJH file. Includes the complete ASCII header stored both as a dictionary and a string, and key attributes including the number of pulses, number of samples (and presamples) in each pulse record, client information stored by the LJH writer, and the filename. Also includes a np.memmap to the raw binary data. This memmap always starts with pulse zero and extends to the last full pulse given the file size at the time of object creation. To extend the memmap for files that are growing, use LJHFile.reopen_binary() to return a new object with a possibly longer memmap. Source code in mass2/core/ljhfiles.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @dataclass ( frozen = True ) class LJHFile ( ABC ): \"\"\"Represents the header and binary information of a single LJH file. Includes the complete ASCII header stored both as a dictionary and a string, and key attributes including the number of pulses, number of samples (and presamples) in each pulse record, client information stored by the LJH writer, and the filename. Also includes a `np.memmap` to the raw binary data. This memmap always starts with pulse zero and extends to the last full pulse given the file size at the time of object creation. To extend the memmap for files that are growing, use `LJHFile.reopen_binary()` to return a new object with a possibly longer memmap. \"\"\" filename : str channum : int dtype : np . dtype npulses : int timebase : float nsamples : int npresamples : int subframediv : int | None client : str header : dict header_string : str header_size : int binary_size : int _mmap : np . memmap ljh_version : Version max_pulses : int | None = None OVERLONG_HEADER : ClassVar [ int ] = 100 def __repr__ ( self ) -> str : \"\"\"A string that can be evaluated to re-open this LJH file.\"\"\" return f \"\"\"mass2.core.ljhfiles.LJHFile.open(\" { self . filename } \")\"\"\" @classmethod def open ( cls , filename : str | Path , max_pulses : int | None = None ) -> \"LJHFile\" : \"\"\"Open an LJH file, parsing its header information and returning an LJHFile object.\"\"\" filename = str ( filename ) header_dict , header_string , header_size = cls . read_header ( filename ) channum = header_dict [ \"Channel\" ] timebase = header_dict [ \"Timebase\" ] nsamples = header_dict [ \"Total Samples\" ] npresamples = header_dict [ \"Presamples\" ] client = header_dict . get ( \"Software Version\" , \"UNKNOWN\" ) subframediv : int | None = None if \"Subframe divisions\" in header_dict : subframediv = header_dict [ \"Subframe divisions\" ] elif \"Number of rows\" in header_dict : subframediv = header_dict [ \"Number of rows\" ] ljh_version = Version ( header_dict [ \"Save File Format Version\" ]) if ljh_version < Version ( \"2.0.0\" ): raise NotImplementedError ( \"LJH files version 1 are not supported\" ) if ljh_version < Version ( \"2.1.0\" ): dtype = np . dtype ([ ( \"internal_unused\" , np . uint16 ), ( \"internal_ms\" , np . uint32 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type : type [ LJHFile ] = LJHFile_2_0 elif ljh_version < Version ( \"2.2.0\" ): dtype = np . dtype ([ ( \"internal_us\" , np . uint8 ), ( \"internal_unused\" , np . uint8 ), ( \"internal_ms\" , np . uint32 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type = LJHFile_2_1 else : dtype = np . dtype ([ ( \"subframecount\" , np . int64 ), ( \"posix_usec\" , np . int64 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type = LJHFile_2_2 pulse_size_bytes = dtype . itemsize binary_size = os . path . getsize ( filename ) - header_size # Fix long-standing bug in LJH files made by MATTER or XCALDAQ_client: # It adds 3 to the \"true value\" of nPresamples. Assume only DASTARD clients have value correct. if \"DASTARD\" not in client : npresamples += 3 npulses = binary_size // pulse_size_bytes if max_pulses is not None : npulses = min ( max_pulses , npulses ) mmap = np . memmap ( filename , dtype , mode = \"r\" , offset = header_size , shape = ( npulses ,)) return concrete_LJHFile_type ( filename , channum , dtype , npulses , timebase , nsamples , npresamples , subframediv , client , header_dict , header_string , header_size , binary_size , mmap , ljh_version , max_pulses , ) @classmethod def read_header ( cls , filename : str ) -> tuple [ dict , str , int ]: \"\"\"Read in the text header of an LJH file. Return the header parsed into a dictionary, the complete header string (in case you want to generate a new LJH file from this one), and the size of the header in bytes. The file does not remain open after this method. Returns: (header_dict, header_string, header_size) Args: filename: path to the file to be opened. \"\"\" # parse header into a dictionary header_dict : dict [ str , Any ] = {} with open ( filename , \"rb\" ) as fp : i = 0 lines = [] while True : line = fp . readline () . decode () lines . append ( line ) i += 1 if line . startswith ( \"#End of Header\" ): break elif not line : raise Exception ( \"reached EOF before #End of Header\" ) elif i > cls . OVERLONG_HEADER : raise Exception ( f \"header is too long--seems not to contain '#End of Header' \\n in file { filename } \" ) # ignore lines without \":\" elif \":\" in line : a , b = line . split ( \":\" , maxsplit = 1 ) a = a . strip () b = b . strip () header_dict [ a ] = b header_size = fp . tell () header_string = \"\" . join ( lines ) # Convert values from header_dict into numeric types, when appropriate header_dict [ \"Filename\" ] = filename for name , datatype in ( ( \"Channel\" , int ), ( \"Timebase\" , float ), ( \"Total Samples\" , int ), ( \"Presamples\" , int ), ( \"Number of columns\" , int ), ( \"Number of rows\" , int ), ( \"Subframe divisions\" , int ), ( \"Timestamp offset (s)\" , float ), ): # Have to convert to float first, as some early LJH have \"Channel: 1.0\" header_dict [ name ] = datatype ( float ( header_dict . get ( name , - 1 ))) return header_dict , header_string , header_size @property def pulse_size_bytes ( self ) -> int : \"\"\"The size in bytes of each binary pulse record (including the timestamps)\"\"\" return self . dtype . itemsize def reopen_binary ( self , max_pulses : int | None = None ) -> \"LJHFile\" : \"\"\"Reopen the underlying binary section of the LJH file, in case its size has changed, without re-reading the LJH header section. Parameters ---------- max_pulses : Optional[int], optional A limit to the number of pulses to memory map or None for no limit, by default None Returns ------- Self A new `LJHFile` object with the same header but a new memmap and number of pulses. \"\"\" current_binary_size = os . path . getsize ( self . filename ) - self . header_size npulses = current_binary_size // self . pulse_size_bytes if max_pulses is not None : npulses = min ( max_pulses , npulses ) mmap = np . memmap ( self . filename , self . dtype , mode = \"r\" , offset = self . header_size , shape = ( npulses ,), ) return dataclasses . replace ( self , npulses = npulses , _mmap = mmap , max_pulses = max_pulses , binary_size = current_binary_size , ) @property def subframecount ( self ) -> NDArray : \"\"\"Return a copy of the subframecount memory map. Old LJH versions don't have this: return zeros, unless overridden by derived class (LJHFile_2_2 will be the only one). Returns ------- np.ndarray An array of subframecount values for each pulse record. \"\"\" return np . zeros ( self . npulses , dtype = np . int64 ) @property @abstractmethod def datatimes_raw ( self ) -> NDArray : \"\"\"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of timestamp values for each pulse record, in microseconds since the epoh (1970). \"\"\" raise NotImplementedError ( \"illegal: this is an abstract base class\" ) @property def datatimes_float ( self ) -> NDArray : \"\"\"Compute pulse record times in floating-point (seconds since the 1970 epoch). In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, compute on chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of pulse record times in floating-point (seconds since the 1970 epoch). \"\"\" return self . datatimes_raw / 1e6 def read_trace ( self , i : int ) -> NDArray : \"\"\"Return a single pulse record from an LJH file. Parameters ---------- i : int Pulse record number (0-indexed) Returns ------- ArrayLike A view into the pulse record. \"\"\" return self . _mmap [ \"data\" ][ i ] def read_trace_with_timing ( self , i : int ) -> tuple [ int , int , NDArray ]: \"\"\"Return a single data trace as (subframecount, posix_usec, pulse_record).\"\"\" pulse_record = self . read_trace ( i ) return ( self . subframecount [ i ], self . datatimes_raw [ i ], pulse_record ) def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Convert this LJH file to two Polars dataframes: one for the binary data, one for the header. Parameters ---------- first_pulse : int, optional The pulse dataframe starts with this pulse record number, by default 0 keep_posix_usec : bool, optional Whether to keep the raw `posix_usec` field in the pulse dataframe, by default False force_continuous: bool Whether to claim that the data stream is actually continuous (because it cannot be learned from data for LJH files before version 2.2.0). Only relevant for noise data files. Returns ------- tuple[pl.DataFrame, pl.DataFrame] (df, header_df) df: the dataframe containing raw pulse information, one row per pulse header_df: a one-row dataframe containing the information from the LJH file header \"\"\" data = { \"pulse\" : self . _mmap [ \"data\" ][ first_pulse :], \"posix_usec\" : self . datatimes_raw [ first_pulse :], \"subframecount\" : self . subframecount [ first_pulse :], } schema : pl . _typing . SchemaDict = { \"pulse\" : pl . Array ( pl . UInt16 , self . nsamples ), \"posix_usec\" : pl . UInt64 , \"subframecount\" : pl . UInt64 , } df = pl . DataFrame ( data , schema = schema ) df = df . select ( pl . from_epoch ( \"posix_usec\" , time_unit = \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) if not keep_posix_usec : df = df . select ( pl . exclude ( \"posix_usec\" )) continuous = self . is_continuous or force_continuous header_df = pl . DataFrame ( self . header ) . with_columns ( continuous = continuous ) return df , header_df def write_truncated_ljh ( self , filename : str , npulses : int ) -> None : \"\"\"Write an LJH copy of this file, with a limited number of pulses. Parameters ---------- filename : str The path where a new LJH file will be created (or replaced). npulses : int Number of pulse records to write \"\"\" npulses = max ( npulses , self . npulses ) with open ( filename , \"wb\" ) as f : f . write ( self . header_string . encode ( \"utf-8\" )) f . write ( self . _mmap [: npulses ] . tobytes ()) @property def is_continuous ( self ) -> bool : \"\"\"Is this LJH file made of a perfectly continuous data stream? For pre-version 2.2 LJH files, we cannot discern from the data. So just claim False. (LJH_2_2 subtype will override by actually computing.) Returns ------- bool Whether every record is strictly continuous with the ones before and after \"\"\" return False datatimes_float property Compute pulse record times in floating-point (seconds since the 1970 epoch). In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, compute on chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of pulse record times in floating-point (seconds since the 1970 epoch). datatimes_raw abstractmethod property Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of timestamp values for each pulse record, in microseconds since the epoh (1970). is_continuous property Is this LJH file made of a perfectly continuous data stream? For pre-version 2.2 LJH files, we cannot discern from the data. So just claim False. (LJH_2_2 subtype will override by actually computing.) Returns: bool \u2013 Whether every record is strictly continuous with the ones before and after pulse_size_bytes property The size in bytes of each binary pulse record (including the timestamps) subframecount property Return a copy of the subframecount memory map. Old LJH versions don't have this: return zeros, unless overridden by derived class (LJHFile_2_2 will be the only one). Returns: ndarray \u2013 An array of subframecount values for each pulse record. __repr__ () A string that can be evaluated to re-open this LJH file. Source code in mass2/core/ljhfiles.py 50 51 52 def __repr__ ( self ) -> str : \"\"\"A string that can be evaluated to re-open this LJH file.\"\"\" return f \"\"\"mass2.core.ljhfiles.LJHFile.open(\" { self . filename } \")\"\"\" open ( filename , max_pulses = None ) classmethod Open an LJH file, parsing its header information and returning an LJHFile object. Source code in mass2/core/ljhfiles.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @classmethod def open ( cls , filename : str | Path , max_pulses : int | None = None ) -> \"LJHFile\" : \"\"\"Open an LJH file, parsing its header information and returning an LJHFile object.\"\"\" filename = str ( filename ) header_dict , header_string , header_size = cls . read_header ( filename ) channum = header_dict [ \"Channel\" ] timebase = header_dict [ \"Timebase\" ] nsamples = header_dict [ \"Total Samples\" ] npresamples = header_dict [ \"Presamples\" ] client = header_dict . get ( \"Software Version\" , \"UNKNOWN\" ) subframediv : int | None = None if \"Subframe divisions\" in header_dict : subframediv = header_dict [ \"Subframe divisions\" ] elif \"Number of rows\" in header_dict : subframediv = header_dict [ \"Number of rows\" ] ljh_version = Version ( header_dict [ \"Save File Format Version\" ]) if ljh_version < Version ( \"2.0.0\" ): raise NotImplementedError ( \"LJH files version 1 are not supported\" ) if ljh_version < Version ( \"2.1.0\" ): dtype = np . dtype ([ ( \"internal_unused\" , np . uint16 ), ( \"internal_ms\" , np . uint32 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type : type [ LJHFile ] = LJHFile_2_0 elif ljh_version < Version ( \"2.2.0\" ): dtype = np . dtype ([ ( \"internal_us\" , np . uint8 ), ( \"internal_unused\" , np . uint8 ), ( \"internal_ms\" , np . uint32 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type = LJHFile_2_1 else : dtype = np . dtype ([ ( \"subframecount\" , np . int64 ), ( \"posix_usec\" , np . int64 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type = LJHFile_2_2 pulse_size_bytes = dtype . itemsize binary_size = os . path . getsize ( filename ) - header_size # Fix long-standing bug in LJH files made by MATTER or XCALDAQ_client: # It adds 3 to the \"true value\" of nPresamples. Assume only DASTARD clients have value correct. if \"DASTARD\" not in client : npresamples += 3 npulses = binary_size // pulse_size_bytes if max_pulses is not None : npulses = min ( max_pulses , npulses ) mmap = np . memmap ( filename , dtype , mode = \"r\" , offset = header_size , shape = ( npulses ,)) return concrete_LJHFile_type ( filename , channum , dtype , npulses , timebase , nsamples , npresamples , subframediv , client , header_dict , header_string , header_size , binary_size , mmap , ljh_version , max_pulses , ) read_header ( filename ) classmethod Read in the text header of an LJH file. Return the header parsed into a dictionary, the complete header string (in case you want to generate a new LJH file from this one), and the size of the header in bytes. The file does not remain open after this method. Returns: (header_dict, header_string, header_size) Args: filename: path to the file to be opened. Source code in mass2/core/ljhfiles.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 @classmethod def read_header ( cls , filename : str ) -> tuple [ dict , str , int ]: \"\"\"Read in the text header of an LJH file. Return the header parsed into a dictionary, the complete header string (in case you want to generate a new LJH file from this one), and the size of the header in bytes. The file does not remain open after this method. Returns: (header_dict, header_string, header_size) Args: filename: path to the file to be opened. \"\"\" # parse header into a dictionary header_dict : dict [ str , Any ] = {} with open ( filename , \"rb\" ) as fp : i = 0 lines = [] while True : line = fp . readline () . decode () lines . append ( line ) i += 1 if line . startswith ( \"#End of Header\" ): break elif not line : raise Exception ( \"reached EOF before #End of Header\" ) elif i > cls . OVERLONG_HEADER : raise Exception ( f \"header is too long--seems not to contain '#End of Header' \\n in file { filename } \" ) # ignore lines without \":\" elif \":\" in line : a , b = line . split ( \":\" , maxsplit = 1 ) a = a . strip () b = b . strip () header_dict [ a ] = b header_size = fp . tell () header_string = \"\" . join ( lines ) # Convert values from header_dict into numeric types, when appropriate header_dict [ \"Filename\" ] = filename for name , datatype in ( ( \"Channel\" , int ), ( \"Timebase\" , float ), ( \"Total Samples\" , int ), ( \"Presamples\" , int ), ( \"Number of columns\" , int ), ( \"Number of rows\" , int ), ( \"Subframe divisions\" , int ), ( \"Timestamp offset (s)\" , float ), ): # Have to convert to float first, as some early LJH have \"Channel: 1.0\" header_dict [ name ] = datatype ( float ( header_dict . get ( name , - 1 ))) return header_dict , header_string , header_size read_trace ( i ) Return a single pulse record from an LJH file. Parameters: i ( int ) \u2013 Pulse record number (0-indexed) Returns: ArrayLike \u2013 A view into the pulse record. Source code in mass2/core/ljhfiles.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def read_trace ( self , i : int ) -> NDArray : \"\"\"Return a single pulse record from an LJH file. Parameters ---------- i : int Pulse record number (0-indexed) Returns ------- ArrayLike A view into the pulse record. \"\"\" return self . _mmap [ \"data\" ][ i ] read_trace_with_timing ( i ) Return a single data trace as (subframecount, posix_usec, pulse_record). Source code in mass2/core/ljhfiles.py 276 277 278 279 def read_trace_with_timing ( self , i : int ) -> tuple [ int , int , NDArray ]: \"\"\"Return a single data trace as (subframecount, posix_usec, pulse_record).\"\"\" pulse_record = self . read_trace ( i ) return ( self . subframecount [ i ], self . datatimes_raw [ i ], pulse_record ) reopen_binary ( max_pulses = None ) Reopen the underlying binary section of the LJH file, in case its size has changed, without re-reading the LJH header section. Parameters: max_pulses ( Optional [ int ] , default: None ) \u2013 A limit to the number of pulses to memory map or None for no limit, by default None Returns: Self \u2013 A new LJHFile object with the same header but a new memmap and number of pulses. Source code in mass2/core/ljhfiles.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def reopen_binary ( self , max_pulses : int | None = None ) -> \"LJHFile\" : \"\"\"Reopen the underlying binary section of the LJH file, in case its size has changed, without re-reading the LJH header section. Parameters ---------- max_pulses : Optional[int], optional A limit to the number of pulses to memory map or None for no limit, by default None Returns ------- Self A new `LJHFile` object with the same header but a new memmap and number of pulses. \"\"\" current_binary_size = os . path . getsize ( self . filename ) - self . header_size npulses = current_binary_size // self . pulse_size_bytes if max_pulses is not None : npulses = min ( max_pulses , npulses ) mmap = np . memmap ( self . filename , self . dtype , mode = \"r\" , offset = self . header_size , shape = ( npulses ,), ) return dataclasses . replace ( self , npulses = npulses , _mmap = mmap , max_pulses = max_pulses , binary_size = current_binary_size , ) to_polars ( first_pulse = 0 , keep_posix_usec = False , force_continuous = False ) Convert this LJH file to two Polars dataframes: one for the binary data, one for the header. Parameters: first_pulse ( int , default: 0 ) \u2013 The pulse dataframe starts with this pulse record number, by default 0 keep_posix_usec ( bool , default: False ) \u2013 Whether to keep the raw posix_usec field in the pulse dataframe, by default False force_continuous ( bool , default: False ) \u2013 Whether to claim that the data stream is actually continuous (because it cannot be learned from data for LJH files before version 2.2.0). Only relevant for noise data files. Returns: tuple [ DataFrame , DataFrame ] \u2013 (df, header_df) df: the dataframe containing raw pulse information, one row per pulse header_df: a one-row dataframe containing the information from the LJH file header Source code in mass2/core/ljhfiles.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Convert this LJH file to two Polars dataframes: one for the binary data, one for the header. Parameters ---------- first_pulse : int, optional The pulse dataframe starts with this pulse record number, by default 0 keep_posix_usec : bool, optional Whether to keep the raw `posix_usec` field in the pulse dataframe, by default False force_continuous: bool Whether to claim that the data stream is actually continuous (because it cannot be learned from data for LJH files before version 2.2.0). Only relevant for noise data files. Returns ------- tuple[pl.DataFrame, pl.DataFrame] (df, header_df) df: the dataframe containing raw pulse information, one row per pulse header_df: a one-row dataframe containing the information from the LJH file header \"\"\" data = { \"pulse\" : self . _mmap [ \"data\" ][ first_pulse :], \"posix_usec\" : self . datatimes_raw [ first_pulse :], \"subframecount\" : self . subframecount [ first_pulse :], } schema : pl . _typing . SchemaDict = { \"pulse\" : pl . Array ( pl . UInt16 , self . nsamples ), \"posix_usec\" : pl . UInt64 , \"subframecount\" : pl . UInt64 , } df = pl . DataFrame ( data , schema = schema ) df = df . select ( pl . from_epoch ( \"posix_usec\" , time_unit = \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) if not keep_posix_usec : df = df . select ( pl . exclude ( \"posix_usec\" )) continuous = self . is_continuous or force_continuous header_df = pl . DataFrame ( self . header ) . with_columns ( continuous = continuous ) return df , header_df write_truncated_ljh ( filename , npulses ) Write an LJH copy of this file, with a limited number of pulses. Parameters: filename ( str ) \u2013 The path where a new LJH file will be created (or replaced). npulses ( int ) \u2013 Number of pulse records to write Source code in mass2/core/ljhfiles.py 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def write_truncated_ljh ( self , filename : str , npulses : int ) -> None : \"\"\"Write an LJH copy of this file, with a limited number of pulses. Parameters ---------- filename : str The path where a new LJH file will be created (or replaced). npulses : int Number of pulse records to write \"\"\" npulses = max ( npulses , self . npulses ) with open ( filename , \"wb\" ) as f : f . write ( self . header_string . encode ( \"utf-8\" )) f . write ( self . _mmap [: npulses ] . tobytes ()) LJHFile_2_0 dataclass Bases: LJHFile LJH files version 2.0, which have internal_ms fields only, but no subframecount and no \u00b5s information. Source code in mass2/core/ljhfiles.py 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 class LJHFile_2_0 ( LJHFile ): \"\"\"LJH files version 2.0, which have internal_ms fields only, but no subframecount and no \u00b5s information.\"\"\" @property def datatimes_raw ( self ) -> NDArray : \"\"\"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of timestamp values for each pulse record, in microseconds since the epoh (1970). \"\"\" usec = np . zeros ( self . npulses , dtype = np . int64 ) mmap = self . _mmap [ \"internal_ms\" ] scale = 1000 offset = round ( self . header [ \"Timestamp offset (s)\" ] * 1e6 ) MAXSEGMENT = 4096 first = 0 while first < self . npulses : last = min ( first + MAXSEGMENT , self . npulses ) usec [ first : last ] = mmap [ first : last ] first = last usec = usec * scale + offset return usec def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Generate two Polars dataframes from this LJH file: one for the binary data, one for the header.\"\"\" df , df_header = super () . to_polars ( first_pulse , keep_posix_usec , force_continuous = force_continuous ) return df . select ( pl . exclude ( \"subframecount\" )), df_header datatimes_raw property Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of timestamp values for each pulse record, in microseconds since the epoh (1970). to_polars ( first_pulse = 0 , keep_posix_usec = False , force_continuous = False ) Generate two Polars dataframes from this LJH file: one for the binary data, one for the header. Source code in mass2/core/ljhfiles.py 500 501 502 503 504 505 def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Generate two Polars dataframes from this LJH file: one for the binary data, one for the header.\"\"\" df , df_header = super () . to_polars ( first_pulse , keep_posix_usec , force_continuous = force_continuous ) return df . select ( pl . exclude ( \"subframecount\" )), df_header LJHFile_2_1 dataclass Bases: LJHFile LJH files version 2.1, which have internal_us and internal_ms fields, but no subframecount. Source code in mass2/core/ljhfiles.py 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 class LJHFile_2_1 ( LJHFile ): \"\"\"LJH files version 2.1, which have internal_us and internal_ms fields, but no subframecount.\"\"\" @property def datatimes_raw ( self ) -> NDArray : \"\"\"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of timestamp values for each pulse record, in microseconds since the epoh (1970). \"\"\" usec = np . zeros ( self . npulses , dtype = np . int64 ) mmap = self . _mmap [ \"internal_ms\" ] scale = 1000 offset = round ( self . header [ \"Timestamp offset (s)\" ] * 1e6 ) MAXSEGMENT = 4096 first = 0 while first < self . npulses : last = min ( first + MAXSEGMENT , self . npulses ) usec [ first : last ] = mmap [ first : last ] first = last usec = usec * scale + offset # Add the 4 \u00b5s units found in LJH version 2.1 assert self . dtype . names is not None and \"internal_us\" in self . dtype . names first = 0 mmap = self . _mmap [ \"internal_us\" ] while first < self . npulses : last = min ( first + MAXSEGMENT , self . npulses ) usec [ first : last ] += mmap [ first : last ] * 4 first = last return usec def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Generate two Polars dataframes from this LJH file: one for the binary data, one for the header.\"\"\" df , df_header = super () . to_polars ( first_pulse , keep_posix_usec , force_continuous = force_continuous ) return df . select ( pl . exclude ( \"subframecount\" )), df_header datatimes_raw property Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of timestamp values for each pulse record, in microseconds since the epoh (1970). to_polars ( first_pulse = 0 , keep_posix_usec = False , force_continuous = False ) Generate two Polars dataframes from this LJH file: one for the binary data, one for the header. Source code in mass2/core/ljhfiles.py 461 462 463 464 465 466 def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Generate two Polars dataframes from this LJH file: one for the binary data, one for the header.\"\"\" df , df_header = super () . to_polars ( first_pulse , keep_posix_usec , force_continuous = force_continuous ) return df . select ( pl . exclude ( \"subframecount\" )), df_header LJHFile_2_2 dataclass Bases: LJHFile LJH files version 2.2 and later, which have subframecount and posix_usec fields. Source code in mass2/core/ljhfiles.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 class LJHFile_2_2 ( LJHFile ): \"\"\"LJH files version 2.2 and later, which have subframecount and posix_usec fields.\"\"\" @property def subframecount ( self ) -> NDArray : \"\"\"Return a copy of the subframecount memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of subframecount values for each pulse record. \"\"\" subframecount = np . zeros ( self . npulses , dtype = np . int64 ) mmap = self . _mmap [ \"subframecount\" ] MAXSEGMENT = 4096 first = 0 while first < self . npulses : last = min ( first + MAXSEGMENT , self . npulses ) subframecount [ first : last ] = mmap [ first : last ] first = last return subframecount @property def datatimes_raw ( self ) -> NDArray : \"\"\"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of timestamp values for each pulse record, in microseconds since the epoh (1970). \"\"\" usec = np . zeros ( self . npulses , dtype = np . int64 ) assert self . dtype . names is not None and \"posix_usec\" in self . dtype . names mmap = self . _mmap [ \"posix_usec\" ] MAXSEGMENT = 4096 first = 0 while first < self . npulses : last = min ( first + MAXSEGMENT , self . npulses ) usec [ first : last ] = mmap [ first : last ] first = last return usec @property def is_continuous ( self ) -> bool : \"\"\"Is this LJH file made of a perfectly continuous data stream? We generally do take noise data in this mode, and it's useful to analyze the noise data by gluing many records together. This property says whether such gluing is valid. Returns ------- bool Whether every record is strictly continuous with the ones before and after \"\"\" if self . subframediv is None or self . npulses <= 1 : return False expected_subframe_diff = self . nsamples * self . subframediv subframe = self . _mmap [ \"subframecount\" ] return np . max ( np . diff ( subframe )) <= expected_subframe_diff datatimes_raw property Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of timestamp values for each pulse record, in microseconds since the epoh (1970). is_continuous property Is this LJH file made of a perfectly continuous data stream? We generally do take noise data in this mode, and it's useful to analyze the noise data by gluing many records together. This property says whether such gluing is valid. Returns: bool \u2013 Whether every record is strictly continuous with the ones before and after subframecount property Return a copy of the subframecount memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of subframecount values for each pulse record. Utility functions for handling and finding LJH files, and opening them as Channel or Channels objects. experiment_state_path_from_ljh_path ( ljh_path ) Find the experiment_state.txt file in the directory of the given ljh file. Source code in mass2/core/ljhutil.py 131 132 133 134 135 136 137 138 def experiment_state_path_from_ljh_path ( ljh_path : str | pathlib . Path , ) -> pathlib . Path : \"\"\"Find the experiment_state.txt file in the directory of the given ljh file.\"\"\" ljh_path = pathlib . Path ( ljh_path ) # Convert to Path if it's a string base_name = ljh_path . name . split ( \"_chan\" )[ 0 ] new_file_name = f \" { base_name } _experiment_state.txt\" return ljh_path . parent / new_file_name external_trigger_bin_path_from_ljh_path ( ljh_path ) Find the external_trigger.bin file in the directory of the given ljh file. Source code in mass2/core/ljhutil.py 141 142 143 144 145 146 147 148 def external_trigger_bin_path_from_ljh_path ( ljh_path : str | pathlib . Path , ) -> pathlib . Path : \"\"\"Find the external_trigger.bin file in the directory of the given ljh file.\"\"\" ljh_path = pathlib . Path ( ljh_path ) # Convert to Path if it's a string base_name = ljh_path . name . split ( \"_chan\" )[ 0 ] new_file_name = f \" { base_name } _external_trigger.bin\" return ljh_path . parent / new_file_name extract_channel_number ( file_path ) Extracts the channel number from the .ljh file name. Args: - file_path (str): The path to the .ljh file. Returns: - int: The channel number. Source code in mass2/core/ljhutil.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def extract_channel_number ( file_path : str ) -> int : \"\"\" Extracts the channel number from the .ljh file name. Args: - file_path (str): The path to the .ljh file. Returns: - int: The channel number. \"\"\" match = re . search ( r \"_chan(\\d+)\\..*$\" , file_path ) if match : return int ( match . group ( 1 )) else : raise ValueError ( f \"File path does not match expected pattern: { file_path } \" ) filename_glob_expand ( pattern ) Return the result of glob-expansion on the input pattern. :param pattern: Aglob pattern and return the glob-result as a list. :type pattern: str :return: filenames; the result is sorted first by str.sort, then by ljh_sort_filenames_numerically() :rtype: list Source code in mass2/core/ljhutil.py 179 180 181 182 183 184 185 186 187 188 def filename_glob_expand ( pattern : str ) -> list [ str ]: \"\"\"Return the result of glob-expansion on the input pattern. :param pattern: Aglob pattern and return the glob-result as a list. :type pattern: str :return: filenames; the result is sorted first by str.sort, then by ljh_sort_filenames_numerically() :rtype: list \"\"\" result = glob . glob ( pattern ) return ljh_sort_filenames_numerically ( result ) find_folders_with_extension ( root_path , extensions ) Finds all folders within the root_path that contain at least one file with the given extension. Args: - root_path (str): The root directory to start the search from. - extension (str): The file extension to search for (e.g., '.txt'). Returns: - list[str]: A list of paths to directories containing at least one file with the given extension. Source code in mass2/core/ljhutil.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def find_folders_with_extension ( root_path : str , extensions : list [ str ]) -> list [ str ]: \"\"\" Finds all folders within the root_path that contain at least one file with the given extension. Args: - root_path (str): The root directory to start the search from. - extension (str): The file extension to search for (e.g., '.txt'). Returns: - list[str]: A list of paths to directories containing at least one file with the given extension. \"\"\" matching_folders = set () # Walk through the directory tree for dirpath , _ , filenames in os . walk ( root_path ): # Check if any file in the current directory has the given extension for filename in filenames : for extension in extensions : if filename . endswith ( extension ): matching_folders . add ( dirpath ) break # No need to check further, move to the next directory return list ( matching_folders ) find_ljh_files ( folder , ext = '.ljh' , search_subdirectories = False , exclude_ch_nums = []) Finds all .ljh files in the given folder and its subfolders. Args: - folder (str): The root directory to start the search from. Returns: - list[str]: A list of paths to .ljh files. Source code in mass2/core/ljhutil.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def find_ljh_files ( folder : str , ext : str = \".ljh\" , search_subdirectories : bool = False , exclude_ch_nums : list [ int ] = []) -> list [ str ]: \"\"\" Finds all .ljh files in the given folder and its subfolders. Args: - folder (str): The root directory to start the search from. Returns: - list[str]: A list of paths to .ljh files. \"\"\" ljh_files = [] if search_subdirectories : pathgen = os . walk ( folder ) else : pathgen = zip ([ folder ], [[ \"\" ]], [ os . listdir ( folder )]) for dirpath , _ , filenames in pathgen : for filename in filenames : if filename . endswith ( ext ): if extract_channel_number ( filename ) in exclude_ch_nums : continue ljh_files . append ( os . path . join ( dirpath , filename )) return ljh_files helper_write_pulse ( dest , src , i ) Write a single pulse from one LJHFile to another open file. Source code in mass2/core/ljhutil.py 191 192 193 194 195 196 197 198 def helper_write_pulse ( dest : BinaryIO , src : LJHFile , i : int ) -> None : \"\"\"Write a single pulse from one LJHFile to another open file.\"\"\" subframecount , timestamp_usec , trace = src . read_trace_with_timing ( i ) prefix = struct . pack ( \"<Q\" , int ( subframecount )) dest . write ( prefix ) prefix = struct . pack ( \"<Q\" , int ( timestamp_usec )) dest . write ( prefix ) trace . tofile ( dest , sep = \"\" ) ljh_append_traces ( src_name , dest_name , pulses = None ) Append traces from one LJH file onto another. The destination file is assumed to be version 2.2.0. Can be used to grab specific traces from some other ljh file, and append them onto an existing ljh file. Args: src_name: the name of the source file dest_name: the name of the destination file pulses: indices of the pulses to copy (default: None, meaning copy all) Source code in mass2/core/ljhutil.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def ljh_append_traces ( src_name : str , dest_name : str , pulses : range | None = None ) -> None : \"\"\"Append traces from one LJH file onto another. The destination file is assumed to be version 2.2.0. Can be used to grab specific traces from some other ljh file, and append them onto an existing ljh file. Args: src_name: the name of the source file dest_name: the name of the destination file pulses: indices of the pulses to copy (default: None, meaning copy all) \"\"\" src = LJHFile . open ( src_name ) if pulses is None : pulses = range ( src . npulses ) with open ( dest_name , \"ab\" ) as dest_fp : for i in pulses : helper_write_pulse ( dest_fp , src , i ) ljh_merge ( out_path , filenames , overwrite ) Merge a set of LJH files to a single output file. Source code in mass2/core/ljhutil.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def ljh_merge ( out_path : str , filenames : list [ str ], overwrite : bool ) -> None : \"\"\"Merge a set of LJH files to a single output file.\"\"\" if not overwrite and os . path . isfile ( out_path ): raise OSError ( f \"To overwrite destination { out_path } , use the --force flag\" ) shutil . copy ( filenames [ 0 ], out_path ) f = LJHFile . open ( out_path ) channum = f . channum print ( f \"Combining { len ( filenames ) } LJH files from channel { channum } \" ) print ( f \"<-- { filenames [ 0 ] } \" ) for in_fname in filenames [ 1 :]: f = LJHFile . open ( in_fname ) if f . channum != channum : raise RuntimeError ( f \"file ' { in_fname } ' channel= { f . channum } , but want { channum } \" ) print ( f \"<-- { in_fname } \" ) ljh_append_traces ( in_fname , out_path ) size = os . stat ( out_path ) . st_size print ( f \"--> { out_path } size: { size } bytes. \\n \" ) ljh_sort_filenames_numerically ( fnames , inclusion_list = None ) Sort filenames of the form ' _chanXXX. ', according to the numerical value of channel number XXX. Filenames are first sorted by the usual string comparisons, then by channel number. In this way, the standard sort is applied to all files with the same channel number. :param fnames: A sequence of filenames of the form ' _chan .*' :type fnames: list of str :param inclusion_list: If not None, a container with channel numbers. All files whose channel numbers are not on this list will be omitted from the output, defaults to None :type inclusion_list: sequence of int, optional :return: A list containg the same filenames, sorted according to the numerical value of channel number. :rtype: list Source code in mass2/core/ljhutil.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def ljh_sort_filenames_numerically ( fnames : list [ str ], inclusion_list : list [ int ] | None = None ) -> list [ str ]: \"\"\"Sort filenames of the form '*_chanXXX.*', according to the numerical value of channel number XXX. Filenames are first sorted by the usual string comparisons, then by channel number. In this way, the standard sort is applied to all files with the same channel number. :param fnames: A sequence of filenames of the form '*_chan*.*' :type fnames: list of str :param inclusion_list: If not None, a container with channel numbers. All files whose channel numbers are not on this list will be omitted from the output, defaults to None :type inclusion_list: sequence of int, optional :return: A list containg the same filenames, sorted according to the numerical value of channel number. :rtype: list \"\"\" if fnames is None or len ( fnames ) == 0 : return [] if inclusion_list is not None : fnames = list ( filter ( lambda n : extract_channel_number ( n ) in inclusion_list , fnames )) # Sort the results first by raw filename, then sort numerically by LJH channel number. # Because string sort and the builtin `sorted` are both stable, we ensure that the first # sort is used to break ties in channel number. fnames . sort () return sorted ( fnames , key = extract_channel_number ) ljh_truncate ( input_filename , output_filename , n_pulses = None , timestamp = None ) Truncate an LJH file. Writes a new copy of an LJH file, with the same header but fewer raw data pulses. Arguments: input_filename -- name of file to truncate output_filename -- filename for truncated file n_pulses -- truncate to include only this many pulses (default None) timestamp -- truncate to include only pulses with timestamp earlier than this number (default None) Exactly one of n_pulses and timestamp must be specified. Source code in mass2/core/ljhutil.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def ljh_truncate ( input_filename : str , output_filename : str , n_pulses : int | None = None , timestamp : float | None = None ) -> None : \"\"\"Truncate an LJH file. Writes a new copy of an LJH file, with the same header but fewer raw data pulses. Arguments: input_filename -- name of file to truncate output_filename -- filename for truncated file n_pulses -- truncate to include only this many pulses (default None) timestamp -- truncate to include only pulses with timestamp earlier than this number (default None) Exactly one of n_pulses and timestamp must be specified. \"\"\" if ( n_pulses is None and timestamp is None ) or ( n_pulses is not None and timestamp is not None ): msg = \"Must specify exactly one of n_pulses, timestamp.\" msg += f \" Values were { str ( n_pulses ) } , { str ( timestamp ) } \" raise Exception ( msg ) # Check for file problems, then open the input and output LJH files. if os . path . exists ( output_filename ): if os . path . samefile ( input_filename , output_filename ): msg = f \"Input ' { input_filename } ' and output ' { output_filename } ' are the same file, which is not allowed\" raise ValueError ( msg ) infile = LJHFile . open ( input_filename ) if infile . ljh_version < Version ( \"2.2.0\" ): raise Exception ( f \"Don't know how to truncate this LJH version [ { infile . ljh_version } ]\" ) with open ( output_filename , \"wb\" ) as outfile : # write the header as a single string. for k , v in infile . header . items (): outfile . write ( bytes ( f \" { k } : { v } \\n \" , encoding = \"utf-8\" )) outfile . write ( b \"#End of Header \\n \" ) # Write pulses. if n_pulses is None : n_pulses = infile . npulses for i in range ( n_pulses ): if timestamp is not None and infile . datatimes_float [ i ] > timestamp : break prefix = struct . pack ( \"<Q\" , np . uint64 ( infile . subframecount [ i ])) outfile . write ( prefix ) prefix = struct . pack ( \"<Q\" , np . uint64 ( infile . datatimes_raw [ i ])) outfile . write ( prefix ) trace = infile . read_trace ( i ) trace . tofile ( outfile , sep = \"\" ) main_ljh_merge () Merge all LJH files that match a pattern to a single output file. The idea is that all such files come from a single TES and could have been (but were not) written as a single continuous file. The pattern should be of the form \"blah_blah_*_chan1.ljh\" or something. The output will then be \"merged_chan1.ljh\" in the directory of the first file found (or alter the directory with the --outdir argument). It is not (currently) possible to merge data from LJH files that represent channels with different numbers. Source code in mass2/core/ljhutil.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def main_ljh_merge () -> None : \"\"\" Merge all LJH files that match a pattern to a single output file. The idea is that all such files come from a single TES and could have been (but were not) written as a single continuous file. The pattern should be of the form \"blah_blah_*_chan1.ljh\" or something. The output will then be \"merged_chan1.ljh\" in the directory of the first file found (or alter the directory with the --outdir argument). It is not (currently) possible to merge data from LJH files that represent channels with different numbers. \"\"\" parser = argparse . ArgumentParser ( description = \"Merge a set of LJH files\" , epilog = \"Beware! Python glob does not perform brace-expansion, so braces must be expanded by the shell.\" , ) parser . add_argument ( \"patterns\" , type = str , nargs = \"+\" , help = 'glob pattern of files to process, e.g. \"20171116_*_chan1.ljh\" (suggest double quotes)' ) parser . add_argument ( \"-d\" , \"--outdir\" , type = str , default = \"\" , help = \"directory to place output file (default: same as directory of first file to be merged\" , ) # TODO: add way to control the output _filename_ parser . add_argument ( \"-F\" , \"--force\" , action = \"store_true\" , help = \"force overwrite of existing target? (default: False)\" ) parser . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" , help = \"list files found before merging (default: False)\" ) parser . add_argument ( \"-n\" , \"--dry-run\" , action = \"store_true\" , help = \"list files found, then quit without merging (default: False)\" ) args = parser . parse_args () filenames : list [ str ] = [] for pattern in args . patterns : filenames . extend ( filename_glob_expand ( pattern )) assert len ( filenames ) > 0 if args . verbose or args . dry_run : print ( f \"Will expand the following { len ( filenames ) } files:\" ) for f in filenames : print ( \" - \" , f ) if args . dry_run : return ljh = LJHFile . open ( filenames [ 0 ]) channum = ljh . channum out_dir = args . outdir if not out_dir : out_dir = os . path . split ( filenames [ 0 ])[ 0 ] out_path = os . path . join ( out_dir , f \"merged_chan { channum } .ljh\" ) overwrite : bool = args . force ljh_merge ( out_path , filenames , overwrite = overwrite ) main_ljh_truncate () A convenience script to truncate all LJH files that match a pattern, writing a new LJH file for each that contains only the first N pulse records. Source code in mass2/core/ljhutil.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def main_ljh_truncate () -> None : \"\"\" A convenience script to truncate all LJH files that match a pattern, writing a new LJH file for each that contains only the first N pulse records. \"\"\" parser = argparse . ArgumentParser ( description = \"Truncate a set of LJH files\" ) parser . add_argument ( \"pattern\" , type = str , help = \"basename of files to process, e.g. 20171116_152922\" ) parser . add_argument ( \"out\" , type = str , help = \"string to append to basename when creating output filename\" ) group = parser . add_mutually_exclusive_group ( required = True ) group . add_argument ( \"--npulses\" , type = int , help = \"Number of pulses to keep\" ) group . add_argument ( \"--timestamp\" , type = float , help = \"Keep only pulses before this timestamp\" ) args = parser . parse_args () pattern = f \" { args . pattern } _chan*.ljh\" filenames = filename_glob_expand ( pattern ) for in_fname in filenames : matches = re . search ( r \"chan(\\d+)\\.ljh\" , in_fname ) if matches : ch = matches . groups ()[ 0 ] out_fname = f \" { args . pattern } _ { args . out } _chan { ch } .ljh\" ljh_truncate ( in_fname , out_fname , n_pulses = args . npulses , timestamp = args . timestamp ) match_files_by_channel ( folder1 , folder2 , limit = None , exclude_ch_nums = []) Matches .ljh files from two folders by channel number. Args: - folder1 (str): The first root directory. - folder2 (str): The second root directory. Returns: - list[Iterator[tuple[str, str]]]: A list of iterators, each containing pairs of paths with matching channel numbers. Source code in mass2/core/ljhutil.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def match_files_by_channel ( folder1 : str , folder2 : str , limit : int | None = None , exclude_ch_nums : list [ int ] = [] ) -> list [ tuple [ str , str ]]: \"\"\" Matches .ljh files from two folders by channel number. Args: - folder1 (str): The first root directory. - folder2 (str): The second root directory. Returns: - list[Iterator[tuple[str, str]]]: A list of iterators, each containing pairs of paths with matching channel numbers. \"\"\" files1 = find_ljh_files ( folder1 ) files2 = find_ljh_files ( folder2 ) # print(f\"in folder {folder1} found {len(files1)} files\") # print(f\"in folder {folder2} found {len(files2)} files\") def collect_to_dict_error_on_repeat_channel ( files : list [ str ]) -> dict : \"\"\" Collects files into a dictionary by channel number, raising an error if a channel number is repeated. \"\"\" files_by_channel : dict [ int , str ] = {} for file in files : channel = extract_channel_number ( file ) if channel in files_by_channel . keys (): existing_file = files_by_channel [ channel ] raise ValueError ( f \"Duplicate channel number found: { channel } in file { file } and already in { existing_file } \" ) files_by_channel [ channel ] = file return files_by_channel # we could have repeat channels even in the same folder, so we should error on that files1_by_channel = collect_to_dict_error_on_repeat_channel ( files1 ) files2_by_channel = collect_to_dict_error_on_repeat_channel ( files2 ) matching_pairs = [] for channel in sorted ( files1_by_channel . keys ()): if channel in files2_by_channel . keys () and channel not in exclude_ch_nums : matching_pairs . append (( files1_by_channel [ channel ], files2_by_channel [ channel ])) if limit is not None : matching_pairs = matching_pairs [: limit ] return matching_pairs Miscellaneous utility functions used in mass2 for plotting, pickling, statistics, and DataFrame manipulation. alwaysTrue () alwaysTrue: a factory function to generate a new copy of polars literal True for class construction Returns: Expr \u2013 Literal True Source code in mass2/core/misc.py 209 210 211 212 213 214 215 216 217 def alwaysTrue () -> pl . Expr : \"\"\"alwaysTrue: a factory function to generate a new copy of polars literal True for class construction Returns ------- pl.Expr Literal True \"\"\" return pl . lit ( True ) concat_dfs_with_concat_state ( df1 , df2 , concat_state_col = 'concat_state' ) Concatenate two DataFrames vertically, adding a column concat_state (or named according to concat_state_col ) to indicate which DataFrame each row came from. Source code in mass2/core/misc.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def concat_dfs_with_concat_state ( df1 : pl . DataFrame , df2 : pl . DataFrame , concat_state_col : str = \"concat_state\" ) -> pl . DataFrame : \"\"\"Concatenate two DataFrames vertically, adding a column `concat_state` (or named according to `concat_state_col`) to indicate which DataFrame each row came from.\"\"\" if concat_state_col in df1 . columns : # Continue incrementing from the last known concat_state max_state = df1 [ concat_state_col ][ - 1 ] df2 = df2 . with_columns ( pl . lit ( max_state + 1 ) . alias ( concat_state_col )) else : # Fresh concat: label first as 0, second as 1 df1 = df1 . with_columns ( pl . lit ( 0 ) . alias ( concat_state_col )) df2 = df2 . with_columns ( pl . lit ( 1 ) . alias ( concat_state_col )) df_out = pl . concat ([ df1 , df2 ], how = \"vertical\" ) return df_out extract_column_names_from_polars_expr ( expr ) Recursively extract all column names from a Polars expression. Source code in mass2/core/misc.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def extract_column_names_from_polars_expr ( expr : pl . Expr ) -> list [ str ]: \"\"\"Recursively extract all column names from a Polars expression.\"\"\" names = set () if hasattr ( expr , \"meta\" ): meta = expr . meta if hasattr ( meta , \"root_names\" ): # For polars >=0.19.0 names . update ( meta . root_names ()) elif hasattr ( meta , \"output_name\" ): # For older polars names . add ( meta . output_name ()) if hasattr ( meta , \"inputs\" ): for subexpr in meta . inputs (): names . update ( extract_column_names_from_polars_expr ( subexpr )) return list ( names ) good_series ( df , col , good_expr , use_expr ) Return a Series from the given DataFrame column, filtered by the given good_expr and use_expr. Source code in mass2/core/misc.py 49 50 51 52 53 54 55 56 def good_series ( df : pl . DataFrame , col : str , good_expr : pl . Expr , use_expr : bool | pl . Expr ) -> pl . Series : \"\"\"Return a Series from the given DataFrame column, filtered by the given good_expr and use_expr.\"\"\" # This uses lazy before filtering. We hope this will allow polars to only access the data needed to filter # and the data needed to output what we want. good_df = df . lazy () . filter ( good_expr ) if use_expr is not True : good_df = good_df . filter ( use_expr ) return good_df . select ( pl . col ( col )) . collect () . to_series () hist_of_series ( series , bin_edges ) Return the bin centers and counts of a histogram of the given Series using the given bin edges. Source code in mass2/core/misc.py 95 96 97 98 99 100 def hist_of_series ( series : pl . Series , bin_edges : ArrayLike ) -> tuple [ NDArray , NDArray ]: \"\"\"Return the bin centers and counts of a histogram of the given Series using the given bin edges.\"\"\" bin_edges = np . asarray ( bin_edges ) bin_centers , _ = midpoints_and_step_size ( bin_edges ) counts = series . rename ( \"count\" ) . hist ( list ( bin_edges ), include_category = False , include_breakpoint = False ) return bin_centers , counts . to_numpy () . T [ 0 ] launch_examples () Launch marimo edit in the examples folder. Source code in mass2/core/misc.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def launch_examples () -> None : \"\"\"Launch marimo edit in the examples folder.\"\"\" examples_folder = pathlib . Path ( __file__ ) . parent . parent . parent / \"examples\" # use relative path to avoid this bug: https://github.com/marimo-team/marimo/issues/1895 examples_folder_relative = str ( examples_folder . relative_to ( pathlib . Path . cwd ())) # Prepare the command command = [ \"marimo\" , \"edit\" , examples_folder_relative ] + sys . argv [ 1 :] # Execute the command print ( f \"launching marimo edit in { examples_folder_relative } \" ) try : # Execute the command and directly forward stdout and stderr process = subprocess . Popen ( command , stdout = sys . stdout , stderr = sys . stderr ) process . communicate () except KeyboardInterrupt : # Handle cleanup on Ctrl-C try : process . terminate () except OSError : pass process . wait () sys . exit ( 1 ) # Check if the command was successful if process . returncode != 0 : sys . exit ( process . returncode ) median_absolute_deviation ( x ) Return the median absolute deviation of the input, unnormalized. Source code in mass2/core/misc.py 59 60 61 62 def median_absolute_deviation ( x : ArrayLike ) -> float : \"\"\"Return the median absolute deviation of the input, unnormalized.\"\"\" x = np . asarray ( x ) return float ( np . median ( np . abs ( x - np . median ( x )))) merge_dicts_ordered_by_keys ( dict1 , dict2 ) Merge two dictionaries and return a new dictionary with items ordered by key. Source code in mass2/core/misc.py 162 163 164 165 166 167 168 169 170 171 172 173 def merge_dicts_ordered_by_keys ( dict1 : dict [ int , Any ], dict2 : dict [ int , Any ]) -> dict [ int , Any ]: \"\"\"Merge two dictionaries and return a new dictionary with items ordered by key.\"\"\" # Combine both dictionaries' items (key, value) into a list of tuples combined_items = list ( dict1 . items ()) + list ( dict2 . items ()) # Sort the combined list of tuples by key combined_items . sort ( key = lambda item : item [ 0 ]) # Convert the sorted list of tuples back into a dictionary merged_dict : dict [ int , Any ] = { key : value for key , value in combined_items } return merged_dict midpoints_and_step_size ( x ) return midpoints, step_size for bin edges x Source code in mass2/core/misc.py 86 87 88 89 90 91 92 def midpoints_and_step_size ( x : ArrayLike ) -> tuple [ NDArray , float ]: \"\"\"return midpoints, step_size for bin edges x\"\"\" x = np . asarray ( x ) d = np . diff ( x ) step_size = float ( d [ 0 ]) assert np . allclose ( d , step_size , atol = 1e-9 ), f \" { d =} \" return x [: - 1 ] + step_size , step_size outlier_resistant_nsigma_above_mid ( x , nsigma = 5 ) RReturn the value that is nsigma median absolute deviations (MADs) above the median of the input. Source code in mass2/core/misc.py 71 72 73 74 75 def outlier_resistant_nsigma_above_mid ( x : ArrayLike , nsigma : float = 5 ) -> float : \"\"\"RReturn the value that is `nsigma` median absolute deviations (MADs) above the median of the input.\"\"\" x = np . asarray ( x ) mid = np . median ( x ) return mid + nsigma * sigma_mad ( x ) outlier_resistant_nsigma_range_from_mid ( x , nsigma = 5 ) Return the values that are nsigma median absolute deviations (MADs) below and above the median of the input Source code in mass2/core/misc.py 78 79 80 81 82 83 def outlier_resistant_nsigma_range_from_mid ( x : ArrayLike , nsigma : float = 5 ) -> tuple [ float , float ]: \"\"\"Return the values that are `nsigma` median absolute deviations (MADs) below and above the median of the input\"\"\" x = np . asarray ( x ) mid = np . median ( x ) smad = sigma_mad ( x ) return mid - nsigma * smad , mid + nsigma * smad pickle_object ( obj , filename ) Pickle the given object to the given filename using dill. Mass2 Recipe objects are compatible with dill but not with the standard pickle module. Source code in mass2/core/misc.py 24 25 26 27 28 def pickle_object ( obj : Any , filename : str ) -> None : \"\"\"Pickle the given object to the given filename using dill. Mass2 Recipe objects are compatible with `dill` but _not_ with the standard `pickle` module.\"\"\" with open ( filename , \"wb\" ) as file : dill . dump ( obj , file ) plot_a_vs_b_series ( a , b , axis = None , ** plotkwarg ) Plot the two given Series as a scatterplot on the given axis (or a new one if None). Source code in mass2/core/misc.py 117 118 119 120 121 122 123 124 def plot_a_vs_b_series ( a : pl . Series , b : pl . Series , axis : plt . Axes | None = None , ** plotkwarg : dict ) -> None : \"\"\"Plot the two given Series as a scatterplot on the given axis (or a new one if None).\"\"\" if axis is None : plt . figure () axis = plt . gca () axis . plot ( a , b , \".\" , label = b . name , ** plotkwarg ) axis . set_xlabel ( a . name ) axis . set_ylabel ( b . name ) plot_hist_of_series ( series , bin_edges , axis = None , ** plotkwarg ) Plot a histogram of the given Series using the given bin edges on the given axis (or a new one if None). Source code in mass2/core/misc.py 103 104 105 106 107 108 109 110 111 112 113 114 def plot_hist_of_series ( series : pl . Series , bin_edges : ArrayLike , axis : plt . Axes | None = None , ** plotkwarg : dict ) -> plt . Axes : \"\"\"Plot a histogram of the given Series using the given bin edges on the given axis (or a new one if None).\"\"\" if axis is None : plt . figure () axis = plt . gca () bin_edges = np . asarray ( bin_edges ) bin_centers , step_size = midpoints_and_step_size ( bin_edges ) hist = series . rename ( \"count\" ) . hist ( list ( bin_edges ), include_category = False , include_breakpoint = False ) axis . plot ( bin_centers , hist , label = series . name , ** plotkwarg ) axis . set_xlabel ( series . name ) axis . set_ylabel ( f \"counts per { step_size : .2f } unit bin\" ) return axis root_mean_squared ( x , axis = None ) Return the root mean square of the input along the given axis or axes. Does not subtract the mean first. Source code in mass2/core/misc.py 156 157 158 159 def root_mean_squared ( x : ArrayLike , axis : int | tuple [ int ] | None = None ) -> float : \"\"\"Return the root mean square of the input along the given axis or axes. Does _not_ subtract the mean first.\"\"\" return np . sqrt ( np . mean ( np . asarray ( x ) ** 2 , axis )) show ( fig = None ) Create a Marimo interactive view of the given Matplotlib figure (or the current figure if None). Source code in mass2/core/misc.py 17 18 19 20 21 def show ( fig : plt . Figure | None = None ) -> mo . Html : \"\"\"Create a Marimo interactive view of the given Matplotlib figure (or the current figure if None).\"\"\" if fig is None : fig = plt . gcf () return mo . mpl . interactive ( fig ) sigma_mad ( x ) Return the nomrlized median absolute deviation of the input, rescaled to give the standard deviation if distribution is Gaussian. This method is more robust to outliers than calculating the standard deviation directly. Source code in mass2/core/misc.py 65 66 67 68 def sigma_mad ( x : ArrayLike ) -> float : \"\"\"Return the nomrlized median absolute deviation of the input, rescaled to give the standard deviation if distribution is Gaussian. This method is more robust to outliers than calculating the standard deviation directly.\"\"\" return median_absolute_deviation ( x ) * 1.4826 smallest_positive_real ( arr ) Return the smallest positive real number in the given array-like object. Source code in mass2/core/misc.py 38 39 40 41 42 43 44 45 46 def smallest_positive_real ( arr : ArrayLike ) -> float : \"\"\"Return the smallest positive real number in the given array-like object.\"\"\" def is_positive_real ( x : Any ) -> bool : \"Is `x` a positive real number?\" return x > 0 and np . isreal ( x ) positive_real_numbers = np . array ( list ( filter ( is_positive_real , np . asarray ( arr )))) return np . min ( positive_real_numbers ) unpickle_object ( filename ) Unpickle an object from the given filename using dill. Source code in mass2/core/misc.py 31 32 33 34 35 def unpickle_object ( filename : str ) -> Any : \"\"\"Unpickle an object from the given filename using dill.\"\"\" with open ( filename , \"rb\" ) as file : obj = dill . load ( file ) return obj Tools for fitting multiple spectral lines in a single pass. FitSpec dataclass Specification of a single line fit within a MultiFit. Source code in mass2/core/multifit.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @dataclass ( frozen = True ) class FitSpec : \"\"\"Specification of a single line fit within a MultiFit.\"\"\" model : GenericLineModel bin_edges : np . ndarray use_expr : pl . Expr params_update : lmfit . parameter . Parameters def params ( self , bin_centers : NDArray , counts : NDArray ) -> lmfit . Parameters : \"\"\"Return a reasonable guess at the parameters given the spectrum to be fit.\"\"\" params = self . model . make_params () params = self . model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) params = params . update ( self . params_update ) return params def fit_series_without_use_expr ( self , series : pl . Series ) -> LineModelResult : \"\"\"Fit the given Series without applying a use_expr filter.\"\"\" bin_centers , counts = mass2 . misc . hist_of_series ( series , self . bin_edges ) params = self . params ( bin_centers , counts ) bin_centers , bin_size = mass2 . misc . midpoints_and_step_size ( self . bin_edges ) result = self . model . fit ( counts , params , bin_centers = bin_centers ) result . set_label_hints ( binsize = bin_size , ds_shortname = \"??\" , unit_str = \"eV\" , attr_str = series . name , states_hint = f \" { self . use_expr } \" , cut_hint = \"\" , ) return result def fit_df ( self , df : pl . DataFrame , col : str , good_expr : pl . Expr ) -> LineModelResult : \"\"\"Fit the given DataFrame column `col` after applying good_expr and use_expr filters.\"\"\" series = mass2 . misc . good_series ( df , col , good_expr , use_expr = self . use_expr ) return self . fit_series_without_use_expr ( series ) def fit_ch ( self , ch : \"Channel\" , col : str ) -> LineModelResult : \"\"\"Fit the given Channel's DataFrame column `col` after applying the Channel's good_expr and this FitSpec's use_expr filters.\"\"\" return self . fit_df ( ch . df , col , ch . good_expr ) fit_ch ( ch , col ) Fit the given Channel's DataFrame column col after applying the Channel's good_expr and this FitSpec's use_expr filters. Source code in mass2/core/multifit.py 76 77 78 79 def fit_ch ( self , ch : \"Channel\" , col : str ) -> LineModelResult : \"\"\"Fit the given Channel's DataFrame column `col` after applying the Channel's good_expr and this FitSpec's use_expr filters.\"\"\" return self . fit_df ( ch . df , col , ch . good_expr ) fit_df ( df , col , good_expr ) Fit the given DataFrame column col after applying good_expr and use_expr filters. Source code in mass2/core/multifit.py 71 72 73 74 def fit_df ( self , df : pl . DataFrame , col : str , good_expr : pl . Expr ) -> LineModelResult : \"\"\"Fit the given DataFrame column `col` after applying good_expr and use_expr filters.\"\"\" series = mass2 . misc . good_series ( df , col , good_expr , use_expr = self . use_expr ) return self . fit_series_without_use_expr ( series ) fit_series_without_use_expr ( series ) Fit the given Series without applying a use_expr filter. Source code in mass2/core/multifit.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def fit_series_without_use_expr ( self , series : pl . Series ) -> LineModelResult : \"\"\"Fit the given Series without applying a use_expr filter.\"\"\" bin_centers , counts = mass2 . misc . hist_of_series ( series , self . bin_edges ) params = self . params ( bin_centers , counts ) bin_centers , bin_size = mass2 . misc . midpoints_and_step_size ( self . bin_edges ) result = self . model . fit ( counts , params , bin_centers = bin_centers ) result . set_label_hints ( binsize = bin_size , ds_shortname = \"??\" , unit_str = \"eV\" , attr_str = series . name , states_hint = f \" { self . use_expr } \" , cut_hint = \"\" , ) return result params ( bin_centers , counts ) Return a reasonable guess at the parameters given the spectrum to be fit. Source code in mass2/core/multifit.py 47 48 49 50 51 52 53 def params ( self , bin_centers : NDArray , counts : NDArray ) -> lmfit . Parameters : \"\"\"Return a reasonable guess at the parameters given the spectrum to be fit.\"\"\" params = self . model . make_params () params = self . model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) params = params . update ( self . params_update ) return params MultiFit dataclass Specification of multiple emission-line fits to be done in one pass. Source code in mass2/core/multifit.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 @dataclass ( frozen = True ) class MultiFit : \"\"\"Specification of multiple emission-line fits to be done in one pass.\"\"\" default_fit_width : float = 50 default_bin_size : float = 0.5 default_use_expr : pl . Expr = field ( default_factory = alwaysTrue ) default_params_update : dict = field ( default_factory = lmfit . Parameters ) fitspecs : list [ FitSpec ] = field ( default_factory = list ) results : list | None = None def with_line ( self , line : GenericLineModel | SpectralLine | str | float , dlo : float | None = None , dhi : float | None = None , bin_size : float | None = None , use_expr : pl . Expr | None = None , params_update : lmfit . Parameters | None = None , ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with an additional FitSpec for the given line.\"\"\" model = get_model ( line ) peak_energy = model . spect . peak_energy dlo = handle_none ( dlo , self . default_fit_width / 2 ) dhi = handle_none ( dhi , self . default_fit_width / 2 ) bin_size = handle_none ( bin_size , self . default_bin_size ) params_update = handle_none ( params_update , self . default_params_update ) use_expr = handle_none ( use_expr , self . default_use_expr ) bin_edges = np . arange ( - dlo , dhi + bin_size , bin_size ) + peak_energy fitspec = FitSpec ( model , bin_edges , use_expr , params_update ) return self . with_fitspec ( fitspec ) def with_fitspec ( self , fitspec : FitSpec ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with a new FitSpec added.\"\"\" # make sure they're always sorted by energy newfitspecs = sorted ( self . fitspecs + [ fitspec ], key = lambda x : x . model . spect . peak_energy ) return dataclasses . replace ( self , fitspecs = newfitspecs ) def with_results ( self , results : list ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with the given results added.\"\"\" return dataclasses . replace ( self , results = results ) def results_params_as_df ( self ) -> pl . DataFrame : \"\"\"Return a DataFrame made from the fit parameters from the results.\"\"\" assert self . results is not None result = self . results [ 0 ] param_names = result . params . keys () d : dict [ str , list ] = {} d [ \"line\" ] = [ fitspec . model . spect . shortname for fitspec in self . fitspecs ] d [ \"peak_energy_ref\" ] = [ fitspec . model . spect . peak_energy for fitspec in self . fitspecs ] d [ \"peak_energy_ref_err\" ] = [] # for quickline, position_uncertainty is a string # translate that into a large value for uncertainty so we can proceed without crashing for fitspec in self . fitspecs : if isinstance ( fitspec . model . spect . position_uncertainty , str ): v = 0.1 * fitspec . model . spect . peak_energy # 10% error is large! else : v = fitspec . model . spect . position_uncertainty d [ \"peak_energy_ref_err\" ] . append ( v ) for param_name in param_names : d [ param_name ] = [ result . params [ param_name ] . value for result in self . results ] d [ param_name + \"_stderr\" ] = [ result . params [ param_name ] . stderr for result in self . results ] return pl . DataFrame ( d ) def fit_series_without_use_expr ( self , series : pl . Series ) -> \"MultiFit\" : \"Fit all the FitSpecs in this MultiFit to the given Series without applying any use_expr filter.\" results = [ fitspec . fit_series_without_use_expr ( series ) for fitspec in self . fitspecs ] return self . with_results ( results ) def fit_df ( self , df : pl . DataFrame , col : str , good_expr : pl . Expr ) -> \"MultiFit\" : \"\"\"Fit all the FitSpecs in this MultiFit to the given DataFrame column `col` after applying good_expr filter.\"\"\" results = [] for fitspec in self . fitspecs : result = fitspec . fit_df ( df , col , good_expr ) results . append ( result ) return self . with_results ( results ) def fit_ch ( self , ch : \"Channel\" , col : str ) -> \"MultiFit\" : \"\"\"Fit all the FitSpecs in this MultiFit to the given Channel's DataFrame column `col` after applying the Channel's good_expr filter.\"\"\" return self . fit_df ( ch . df , col , ch . good_expr ) def plot_results ( self , n_extra_axes : int = 0 ) -> tuple [ plt . Figure , plt . Axes ]: \"\"\"Plot all the fit results in subplots, with n_extra_axes empty subplots included at the end.\"\"\" assert self . results is not None n = len ( self . results ) + n_extra_axes cols = min ( 3 , n ) rows = math . ceil ( n / cols ) fig , axes = plt . subplots ( rows , cols , figsize = ( cols * 4 , rows * 4 )) # Adjust figure size as needed # If there's only one subplot, axes is not a list but a single Axes object. if rows == 1 and cols == 1 : axes = [ axes ] elif rows == 1 or cols == 1 : axes = axes . flatten () else : axes = axes . ravel () for result , ax in zip ( self . results , axes ): result . plotm ( ax = ax ) # Hide any remaining empty subplots for ax in axes [ n :]: ax . axis ( \"off\" ) plt . tight_layout () return fig , axes def plot_results_and_pfit ( self , uncalibrated_name : str , previous_energy2ph : Callable , n_extra_axes : int = 0 ) -> plt . Axes : \"\"\"Plot all the fit results in subplots, and also plot the gain curve on an extra axis.\"\"\" assert self . results is not None _fig , axes = self . plot_results ( n_extra_axes = 1 + n_extra_axes ) ax = axes [ len ( self . results )] multifit_df = self . results_params_as_df () peaks_in_energy_rough_cal = multifit_df [ \"peak_ph\" ] . to_numpy () peaks_uncalibrated = previous_energy2ph ( peaks_in_energy_rough_cal ) peaks_in_energy_reference = multifit_df [ \"peak_energy_ref\" ] . to_numpy () pfit_gain , rms_residual_energy = self . to_pfit_gain ( previous_energy2ph ) plt . sca ( ax ) x = np . linspace ( 0 , np . amax ( peaks_uncalibrated ), 100 ) plt . plot ( x , pfit_gain ( x ), \"k\" , label = \"fit\" ) gain = peaks_uncalibrated / peaks_in_energy_reference plt . plot ( peaks_uncalibrated , gain , \"o\" ) plt . xlabel ( uncalibrated_name ) plt . ylabel ( \"gain\" ) plt . title ( f \" { rms_residual_energy =: .3f } \" ) for name , x , y in zip ( multifit_df [ \"line\" ], peaks_uncalibrated , gain ): ax . annotate ( str ( name ), ( x , y )) return axes def to_pfit_gain ( self , previous_energy2ph : Callable ) -> tuple [ np . polynomial . Polynomial , float ]: \"\"\"Return a best-fit 2nd degree polynomial for gain (ph/energy) vs uncalibrated ph, and the rms residual in energy after applying that gain correction.\"\"\" multifit_df = self . results_params_as_df () peaks_in_energy_rough_cal = multifit_df [ \"peak_ph\" ] . to_numpy () peaks_uncalibrated = np . array ([ previous_energy2ph ( e ) for e in peaks_in_energy_rough_cal ]) . ravel () peaks_in_energy_reference = multifit_df [ \"peak_energy_ref\" ] . to_numpy () gain = peaks_uncalibrated / peaks_in_energy_reference pfit_gain = np . polynomial . Polynomial . fit ( peaks_uncalibrated , gain , deg = 2 ) def ph2energy ( ph : NDArray ) -> NDArray : \"Given an array `ph` of pulse heights, return the corresponding energies as an array.\" gain = pfit_gain ( ph ) return ph / gain e_predicted = ph2energy ( peaks_uncalibrated ) rms_residual_energy = mass2 . misc . root_mean_squared ( e_predicted - peaks_in_energy_reference ) return pfit_gain , rms_residual_energy def to_mass_cal ( self , previous_energy2ph : Callable , curvetype : Curvetypes = Curvetypes . GAIN , approximate : bool = False ) -> EnergyCalibration : \"\"\"Return a calibration object made from the fit results in this MultiFit.\"\"\" df = self . results_params_as_df () maker = EnergyCalibrationMaker ( ph = np . array ([ previous_energy2ph ( x ) for x in df [ \"peak_ph\" ] . to_numpy ()]), energy = df [ \"peak_energy_ref\" ] . to_numpy (), dph = df [ \"peak_ph_stderr\" ] . to_numpy (), de = df [ \"peak_energy_ref_err\" ] . to_numpy (), names = [ name for name in df [ \"line\" ]], ) cal = maker . make_calibration ( curvename = curvetype , approximate = approximate ) return cal fit_ch ( ch , col ) Fit all the FitSpecs in this MultiFit to the given Channel's DataFrame column col after applying the Channel's good_expr filter. Source code in mass2/core/multifit.py 159 160 161 162 def fit_ch ( self , ch : \"Channel\" , col : str ) -> \"MultiFit\" : \"\"\"Fit all the FitSpecs in this MultiFit to the given Channel's DataFrame column `col` after applying the Channel's good_expr filter.\"\"\" return self . fit_df ( ch . df , col , ch . good_expr ) fit_df ( df , col , good_expr ) Fit all the FitSpecs in this MultiFit to the given DataFrame column col after applying good_expr filter. Source code in mass2/core/multifit.py 151 152 153 154 155 156 157 def fit_df ( self , df : pl . DataFrame , col : str , good_expr : pl . Expr ) -> \"MultiFit\" : \"\"\"Fit all the FitSpecs in this MultiFit to the given DataFrame column `col` after applying good_expr filter.\"\"\" results = [] for fitspec in self . fitspecs : result = fitspec . fit_df ( df , col , good_expr ) results . append ( result ) return self . with_results ( results ) fit_series_without_use_expr ( series ) Fit all the FitSpecs in this MultiFit to the given Series without applying any use_expr filter. Source code in mass2/core/multifit.py 146 147 148 149 def fit_series_without_use_expr ( self , series : pl . Series ) -> \"MultiFit\" : \"Fit all the FitSpecs in this MultiFit to the given Series without applying any use_expr filter.\" results = [ fitspec . fit_series_without_use_expr ( series ) for fitspec in self . fitspecs ] return self . with_results ( results ) plot_results ( n_extra_axes = 0 ) Plot all the fit results in subplots, with n_extra_axes empty subplots included at the end. Source code in mass2/core/multifit.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def plot_results ( self , n_extra_axes : int = 0 ) -> tuple [ plt . Figure , plt . Axes ]: \"\"\"Plot all the fit results in subplots, with n_extra_axes empty subplots included at the end.\"\"\" assert self . results is not None n = len ( self . results ) + n_extra_axes cols = min ( 3 , n ) rows = math . ceil ( n / cols ) fig , axes = plt . subplots ( rows , cols , figsize = ( cols * 4 , rows * 4 )) # Adjust figure size as needed # If there's only one subplot, axes is not a list but a single Axes object. if rows == 1 and cols == 1 : axes = [ axes ] elif rows == 1 or cols == 1 : axes = axes . flatten () else : axes = axes . ravel () for result , ax in zip ( self . results , axes ): result . plotm ( ax = ax ) # Hide any remaining empty subplots for ax in axes [ n :]: ax . axis ( \"off\" ) plt . tight_layout () return fig , axes plot_results_and_pfit ( uncalibrated_name , previous_energy2ph , n_extra_axes = 0 ) Plot all the fit results in subplots, and also plot the gain curve on an extra axis. Source code in mass2/core/multifit.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def plot_results_and_pfit ( self , uncalibrated_name : str , previous_energy2ph : Callable , n_extra_axes : int = 0 ) -> plt . Axes : \"\"\"Plot all the fit results in subplots, and also plot the gain curve on an extra axis.\"\"\" assert self . results is not None _fig , axes = self . plot_results ( n_extra_axes = 1 + n_extra_axes ) ax = axes [ len ( self . results )] multifit_df = self . results_params_as_df () peaks_in_energy_rough_cal = multifit_df [ \"peak_ph\" ] . to_numpy () peaks_uncalibrated = previous_energy2ph ( peaks_in_energy_rough_cal ) peaks_in_energy_reference = multifit_df [ \"peak_energy_ref\" ] . to_numpy () pfit_gain , rms_residual_energy = self . to_pfit_gain ( previous_energy2ph ) plt . sca ( ax ) x = np . linspace ( 0 , np . amax ( peaks_uncalibrated ), 100 ) plt . plot ( x , pfit_gain ( x ), \"k\" , label = \"fit\" ) gain = peaks_uncalibrated / peaks_in_energy_reference plt . plot ( peaks_uncalibrated , gain , \"o\" ) plt . xlabel ( uncalibrated_name ) plt . ylabel ( \"gain\" ) plt . title ( f \" { rms_residual_energy =: .3f } \" ) for name , x , y in zip ( multifit_df [ \"line\" ], peaks_uncalibrated , gain ): ax . annotate ( str ( name ), ( x , y )) return axes results_params_as_df () Return a DataFrame made from the fit parameters from the results. Source code in mass2/core/multifit.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def results_params_as_df ( self ) -> pl . DataFrame : \"\"\"Return a DataFrame made from the fit parameters from the results.\"\"\" assert self . results is not None result = self . results [ 0 ] param_names = result . params . keys () d : dict [ str , list ] = {} d [ \"line\" ] = [ fitspec . model . spect . shortname for fitspec in self . fitspecs ] d [ \"peak_energy_ref\" ] = [ fitspec . model . spect . peak_energy for fitspec in self . fitspecs ] d [ \"peak_energy_ref_err\" ] = [] # for quickline, position_uncertainty is a string # translate that into a large value for uncertainty so we can proceed without crashing for fitspec in self . fitspecs : if isinstance ( fitspec . model . spect . position_uncertainty , str ): v = 0.1 * fitspec . model . spect . peak_energy # 10% error is large! else : v = fitspec . model . spect . position_uncertainty d [ \"peak_energy_ref_err\" ] . append ( v ) for param_name in param_names : d [ param_name ] = [ result . params [ param_name ] . value for result in self . results ] d [ param_name + \"_stderr\" ] = [ result . params [ param_name ] . stderr for result in self . results ] return pl . DataFrame ( d ) to_mass_cal ( previous_energy2ph , curvetype = Curvetypes . GAIN , approximate = False ) Return a calibration object made from the fit results in this MultiFit. Source code in mass2/core/multifit.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def to_mass_cal ( self , previous_energy2ph : Callable , curvetype : Curvetypes = Curvetypes . GAIN , approximate : bool = False ) -> EnergyCalibration : \"\"\"Return a calibration object made from the fit results in this MultiFit.\"\"\" df = self . results_params_as_df () maker = EnergyCalibrationMaker ( ph = np . array ([ previous_energy2ph ( x ) for x in df [ \"peak_ph\" ] . to_numpy ()]), energy = df [ \"peak_energy_ref\" ] . to_numpy (), dph = df [ \"peak_ph_stderr\" ] . to_numpy (), de = df [ \"peak_energy_ref_err\" ] . to_numpy (), names = [ name for name in df [ \"line\" ]], ) cal = maker . make_calibration ( curvename = curvetype , approximate = approximate ) return cal to_pfit_gain ( previous_energy2ph ) Return a best-fit 2nd degree polynomial for gain (ph/energy) vs uncalibrated ph, and the rms residual in energy after applying that gain correction. Source code in mass2/core/multifit.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def to_pfit_gain ( self , previous_energy2ph : Callable ) -> tuple [ np . polynomial . Polynomial , float ]: \"\"\"Return a best-fit 2nd degree polynomial for gain (ph/energy) vs uncalibrated ph, and the rms residual in energy after applying that gain correction.\"\"\" multifit_df = self . results_params_as_df () peaks_in_energy_rough_cal = multifit_df [ \"peak_ph\" ] . to_numpy () peaks_uncalibrated = np . array ([ previous_energy2ph ( e ) for e in peaks_in_energy_rough_cal ]) . ravel () peaks_in_energy_reference = multifit_df [ \"peak_energy_ref\" ] . to_numpy () gain = peaks_uncalibrated / peaks_in_energy_reference pfit_gain = np . polynomial . Polynomial . fit ( peaks_uncalibrated , gain , deg = 2 ) def ph2energy ( ph : NDArray ) -> NDArray : \"Given an array `ph` of pulse heights, return the corresponding energies as an array.\" gain = pfit_gain ( ph ) return ph / gain e_predicted = ph2energy ( peaks_uncalibrated ) rms_residual_energy = mass2 . misc . root_mean_squared ( e_predicted - peaks_in_energy_reference ) return pfit_gain , rms_residual_energy with_fitspec ( fitspec ) Return a copy of this MultiFit with a new FitSpec added. Source code in mass2/core/multifit.py 114 115 116 117 118 def with_fitspec ( self , fitspec : FitSpec ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with a new FitSpec added.\"\"\" # make sure they're always sorted by energy newfitspecs = sorted ( self . fitspecs + [ fitspec ], key = lambda x : x . model . spect . peak_energy ) return dataclasses . replace ( self , fitspecs = newfitspecs ) with_line ( line , dlo = None , dhi = None , bin_size = None , use_expr = None , params_update = None ) Return a copy of this MultiFit with an additional FitSpec for the given line. Source code in mass2/core/multifit.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def with_line ( self , line : GenericLineModel | SpectralLine | str | float , dlo : float | None = None , dhi : float | None = None , bin_size : float | None = None , use_expr : pl . Expr | None = None , params_update : lmfit . Parameters | None = None , ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with an additional FitSpec for the given line.\"\"\" model = get_model ( line ) peak_energy = model . spect . peak_energy dlo = handle_none ( dlo , self . default_fit_width / 2 ) dhi = handle_none ( dhi , self . default_fit_width / 2 ) bin_size = handle_none ( bin_size , self . default_bin_size ) params_update = handle_none ( params_update , self . default_params_update ) use_expr = handle_none ( use_expr , self . default_use_expr ) bin_edges = np . arange ( - dlo , dhi + bin_size , bin_size ) + peak_energy fitspec = FitSpec ( model , bin_edges , use_expr , params_update ) return self . with_fitspec ( fitspec ) with_results ( results ) Return a copy of this MultiFit with the given results added. Source code in mass2/core/multifit.py 120 121 122 def with_results ( self , results : list ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with the given results added.\"\"\" return dataclasses . replace ( self , results = results ) MultiFitMassCalibrationStep dataclass Bases: RecipeStep A RecipeStep to apply a mass-style calibration derived from a MultiFit. Source code in mass2/core/multifit.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 @dataclass ( frozen = True ) class MultiFitMassCalibrationStep ( RecipeStep ): \"\"\"A RecipeStep to apply a mass-style calibration derived from a MultiFit.\"\"\" cal : EnergyCalibration multifit : MultiFit | None def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calibrate energy and return a new DataFrame with results.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2 def drop_debug ( self ) -> \"MultiFitMassCalibrationStep\" : \"For slimmer object pickling, return a copy of self with the fat debugging info removed\" return dataclasses . replace ( self , multifit = None ) def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"Plot the fit results and gain curve for debugging purposes.\" assert self . multifit is not None axes = self . multifit . plot_results_and_pfit ( uncalibrated_name = self . inputs [ 0 ], previous_energy2ph = self . energy2ph , ) return axes def ph2energy ( self , ph : ArrayLike ) -> NDArray : \"The quadratic gain calibration curve: ph -> energy\" ph = np . asarray ( ph ) return self . cal . ph2energy ( ph ) def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"The inverse of the quadratic gain calibration curve: energy -> ph\"\"\" energy = np . asarray ( energy ) return self . cal . energy2ph ( energy ) @classmethod def learn ( cls , ch : \"Channel\" , multifit_spec : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"MultiFitMassCalibrationStep\" : \"\"\"multifit then make a mass calibration object with curve_type=Curvetypes.GAIN and approx=False TODO: support more options\"\"\" previous_cal_step = ch . steps [ previous_cal_step_index ] assert hasattr ( previous_cal_step , \"energy2ph\" ) rough_energy_col = previous_cal_step . output [ 0 ] uncalibrated_col = previous_cal_step . inputs [ 0 ] multifit_with_results = multifit_spec . fit_ch ( ch , col = rough_energy_col ) cal = multifit_with_results . to_mass_cal ( previous_cal_step . energy2ph ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr , cal , multifit_with_results , ) return step calc_from_df ( df ) Calibrate energy and return a new DataFrame with results. Source code in mass2/core/multifit.py 333 334 335 336 337 338 339 340 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calibrate energy and return a new DataFrame with results.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2 dbg_plot ( df_after , ** kwargs ) Plot the fit results and gain curve for debugging purposes. Source code in mass2/core/multifit.py 346 347 348 349 350 351 352 353 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"Plot the fit results and gain curve for debugging purposes.\" assert self . multifit is not None axes = self . multifit . plot_results_and_pfit ( uncalibrated_name = self . inputs [ 0 ], previous_energy2ph = self . energy2ph , ) return axes drop_debug () For slimmer object pickling, return a copy of self with the fat debugging info removed Source code in mass2/core/multifit.py 342 343 344 def drop_debug ( self ) -> \"MultiFitMassCalibrationStep\" : \"For slimmer object pickling, return a copy of self with the fat debugging info removed\" return dataclasses . replace ( self , multifit = None ) energy2ph ( energy ) The inverse of the quadratic gain calibration curve: energy -> ph Source code in mass2/core/multifit.py 360 361 362 363 def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"The inverse of the quadratic gain calibration curve: energy -> ph\"\"\" energy = np . asarray ( energy ) return self . cal . energy2ph ( energy ) learn ( ch , multifit_spec , previous_cal_step_index , calibrated_col , use_expr = pl . lit ( True )) classmethod multifit then make a mass calibration object with curve_type=Curvetypes.GAIN and approx=False TODO: support more options Source code in mass2/core/multifit.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 @classmethod def learn ( cls , ch : \"Channel\" , multifit_spec : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"MultiFitMassCalibrationStep\" : \"\"\"multifit then make a mass calibration object with curve_type=Curvetypes.GAIN and approx=False TODO: support more options\"\"\" previous_cal_step = ch . steps [ previous_cal_step_index ] assert hasattr ( previous_cal_step , \"energy2ph\" ) rough_energy_col = previous_cal_step . output [ 0 ] uncalibrated_col = previous_cal_step . inputs [ 0 ] multifit_with_results = multifit_spec . fit_ch ( ch , col = rough_energy_col ) cal = multifit_with_results . to_mass_cal ( previous_cal_step . energy2ph ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr , cal , multifit_with_results , ) return step ph2energy ( ph ) The quadratic gain calibration curve: ph -> energy Source code in mass2/core/multifit.py 355 356 357 358 def ph2energy ( self , ph : ArrayLike ) -> NDArray : \"The quadratic gain calibration curve: ph -> energy\" ph = np . asarray ( ph ) return self . cal . ph2energy ( ph ) MultiFitQuadraticGainStep dataclass Bases: RecipeStep A RecipeStep to apply a quadratic gain curve, after fitting multiple emission lines. Source code in mass2/core/multifit.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 @dataclass ( frozen = True ) class MultiFitQuadraticGainStep ( RecipeStep ): \"\"\"A RecipeStep to apply a quadratic gain curve, after fitting multiple emission lines.\"\"\" pfit_gain : np . polynomial . Polynomial multifit : MultiFit | None rms_residual_energy : float def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calibrate energy and return a new DataFrame with results.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the fit results and gain curve for debugging purposes.\"\"\" if self . multifit is not None : self . multifit . plot_results_and_pfit ( uncalibrated_name = self . inputs [ 0 ], previous_energy2ph = self . energy2ph ) return plt . gca () def drop_debug ( self ) -> \"MultiFitQuadraticGainStep\" : \"For slimmer object pickling, return a copy of self with the fat debugging info removed\" return dataclasses . replace ( self , multifit = None ) def ph2energy ( self , ph : ArrayLike ) -> NDArray : \"The quadratic gain calibration curve: ph -> energy\" ph = np . asarray ( ph ) gain = self . pfit_gain ( ph ) return ph / gain def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"The inverse of the quadratic gain calibration curve: energy -> ph\"\"\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(c + b*x + a*x^2) # so # y*c + (y*b-1)*x + a*x^2 = 0 # and given that we've selected for well formed calibrations, # we know which root we want energy = np . asarray ( energy ) cba = self . pfit_gain . convert () . coef c , bb , a = cba * energy b = bb - 1 ph = ( - b - np . sqrt ( b ** 2 - 4 * a * c )) / ( 2 * a ) assert math . isclose ( self . ph2energy ( ph ), energy , rel_tol = 1e-6 , abs_tol = 1e-3 ) return ph @classmethod def learn ( cls , ch : \"Channel\" , multifit_spec : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"MultiFitQuadraticGainStep\" : \"\"\"Perform a multifit then make a quadratic gain calibration object.\"\"\" previous_cal_step = ch . steps [ previous_cal_step_index ] assert hasattr ( previous_cal_step , \"energy2ph\" ) rough_energy_col = previous_cal_step . output [ 0 ] uncalibrated_col = previous_cal_step . inputs [ 0 ] multifit_with_results = multifit_spec . fit_ch ( ch , col = rough_energy_col ) # multifit_df = multifit_with_results.results_params_as_df() pfit_gain , rms_residual_energy = multifit_with_results . to_pfit_gain ( previous_cal_step . energy2ph ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr , pfit_gain , multifit_with_results , rms_residual_energy , ) return step calc_from_df ( df ) Calibrate energy and return a new DataFrame with results. Source code in mass2/core/multifit.py 255 256 257 258 259 260 261 262 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calibrate energy and return a new DataFrame with results.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2 dbg_plot ( df_after , ** kwargs ) Plot the fit results and gain curve for debugging purposes. Source code in mass2/core/multifit.py 264 265 266 267 268 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the fit results and gain curve for debugging purposes.\"\"\" if self . multifit is not None : self . multifit . plot_results_and_pfit ( uncalibrated_name = self . inputs [ 0 ], previous_energy2ph = self . energy2ph ) return plt . gca () drop_debug () For slimmer object pickling, return a copy of self with the fat debugging info removed Source code in mass2/core/multifit.py 270 271 272 def drop_debug ( self ) -> \"MultiFitQuadraticGainStep\" : \"For slimmer object pickling, return a copy of self with the fat debugging info removed\" return dataclasses . replace ( self , multifit = None ) energy2ph ( energy ) The inverse of the quadratic gain calibration curve: energy -> ph Source code in mass2/core/multifit.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"The inverse of the quadratic gain calibration curve: energy -> ph\"\"\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(c + b*x + a*x^2) # so # y*c + (y*b-1)*x + a*x^2 = 0 # and given that we've selected for well formed calibrations, # we know which root we want energy = np . asarray ( energy ) cba = self . pfit_gain . convert () . coef c , bb , a = cba * energy b = bb - 1 ph = ( - b - np . sqrt ( b ** 2 - 4 * a * c )) / ( 2 * a ) assert math . isclose ( self . ph2energy ( ph ), energy , rel_tol = 1e-6 , abs_tol = 1e-3 ) return ph learn ( ch , multifit_spec , previous_cal_step_index , calibrated_col , use_expr = pl . lit ( True )) classmethod Perform a multifit then make a quadratic gain calibration object. Source code in mass2/core/multifit.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 @classmethod def learn ( cls , ch : \"Channel\" , multifit_spec : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"MultiFitQuadraticGainStep\" : \"\"\"Perform a multifit then make a quadratic gain calibration object.\"\"\" previous_cal_step = ch . steps [ previous_cal_step_index ] assert hasattr ( previous_cal_step , \"energy2ph\" ) rough_energy_col = previous_cal_step . output [ 0 ] uncalibrated_col = previous_cal_step . inputs [ 0 ] multifit_with_results = multifit_spec . fit_ch ( ch , col = rough_energy_col ) # multifit_df = multifit_with_results.results_params_as_df() pfit_gain , rms_residual_energy = multifit_with_results . to_pfit_gain ( previous_cal_step . energy2ph ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr , pfit_gain , multifit_with_results , rms_residual_energy , ) return step ph2energy ( ph ) The quadratic gain calibration curve: ph -> energy Source code in mass2/core/multifit.py 274 275 276 277 278 def ph2energy ( self , ph : ArrayLike ) -> NDArray : \"The quadratic gain calibration curve: ph -> energy\" ph = np . asarray ( ph ) gain = self . pfit_gain ( ph ) return ph / gain handle_none ( val , default ) If val is None, return a copy of default, else return val. Source code in mass2/core/multifit.py 31 32 33 34 35 def handle_none ( val : T | None , default : T ) -> T : \"If val is None, return a copy of default, else return val.\" if val is None : return copy . copy ( default ) return val Algorithms to analyze noise data. NoiseResult dataclass A dataclass to hold the results of noise analysis, both power-spectral density and (optionally) autocorrelation. Source code in mass2/core/noise_algorithms.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 @dataclass class NoiseResult : \"\"\"A dataclass to hold the results of noise analysis, both power-spectral density and (optionally) autocorrelation.\"\"\" psd : np . ndarray autocorr_vec : np . ndarray | None frequencies : np . ndarray def plot ( self , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , loglog : bool = True , ** plotkwarg : dict , ) -> None : \"\"\"Plot the power spectral density.\"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . psd [ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies [ 1 :] if sqrt_psd : axis . plot ( freq , np . sqrt ( psd ), ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( freq , psd , ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) if loglog : plt . loglog () axis . grid () axis . set_xlabel ( \"Frequency (Hz)\" ) plt . title ( f \"noise from records of length { len ( self . frequencies ) * 2 - 2 } \" ) axis . figure . tight_layout () def plot_log_rebinned ( self , bins_per_decade : int = 10 , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , ** plotkwarg : dict , ) -> None : \"\"\"Plot PSD rebinned into logarithmically spaced frequency bins.\"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . psd [ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies [ 1 :] # define logarithmically spaced bin edges fmin , fmax = freq [ 0 ], freq [ - 1 ] n_decades = np . log10 ( fmax / fmin ) n_bins = int ( bins_per_decade * n_decades ) bin_edges = np . logspace ( np . log10 ( fmin ), np . log10 ( fmax ), n_bins + 1 ) # digitize frequencies into bins inds = np . digitize ( freq , bin_edges ) # average PSD per bin binned_freqs = np . zeros ( n_bins , dtype = float ) binned_psd = np . zeros ( n_bins , dtype = float ) for i in range ( 1 , len ( bin_edges )): mask = inds == i if np . any ( mask ): binned_freqs [ i - 1 ] = np . exp ( np . mean ( np . log ( freq [ mask ]))) # geometric mean binned_psd [ i - 1 ] = np . mean ( psd [ mask ]) if sqrt_psd : axis . plot ( binned_freqs , np . sqrt ( binned_psd ), ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( binned_freqs , binned_psd , ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) axis . set_xscale ( \"log\" ) axis . set_yscale ( \"log\" ) axis . grid ( True , which = \"both\" ) axis . set_xlabel ( \"Frequency (Hz)\" ) axis . set_title ( f \"Log-rebinned noise from { len ( self . frequencies ) * 2 - 2 } samples\" ) axis . figure . tight_layout () plot ( axis = None , arb_to_unit_scale_and_label = ( 1 , 'arb' ), sqrt_psd = True , loglog = True , ** plotkwarg ) Plot the power spectral density. Source code in mass2/core/noise_algorithms.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def plot ( self , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , loglog : bool = True , ** plotkwarg : dict , ) -> None : \"\"\"Plot the power spectral density.\"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . psd [ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies [ 1 :] if sqrt_psd : axis . plot ( freq , np . sqrt ( psd ), ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( freq , psd , ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) if loglog : plt . loglog () axis . grid () axis . set_xlabel ( \"Frequency (Hz)\" ) plt . title ( f \"noise from records of length { len ( self . frequencies ) * 2 - 2 } \" ) axis . figure . tight_layout () plot_log_rebinned ( bins_per_decade = 10 , axis = None , arb_to_unit_scale_and_label = ( 1 , 'arb' ), sqrt_psd = True , ** plotkwarg ) Plot PSD rebinned into logarithmically spaced frequency bins. Source code in mass2/core/noise_algorithms.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def plot_log_rebinned ( self , bins_per_decade : int = 10 , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , ** plotkwarg : dict , ) -> None : \"\"\"Plot PSD rebinned into logarithmically spaced frequency bins.\"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . psd [ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies [ 1 :] # define logarithmically spaced bin edges fmin , fmax = freq [ 0 ], freq [ - 1 ] n_decades = np . log10 ( fmax / fmin ) n_bins = int ( bins_per_decade * n_decades ) bin_edges = np . logspace ( np . log10 ( fmin ), np . log10 ( fmax ), n_bins + 1 ) # digitize frequencies into bins inds = np . digitize ( freq , bin_edges ) # average PSD per bin binned_freqs = np . zeros ( n_bins , dtype = float ) binned_psd = np . zeros ( n_bins , dtype = float ) for i in range ( 1 , len ( bin_edges )): mask = inds == i if np . any ( mask ): binned_freqs [ i - 1 ] = np . exp ( np . mean ( np . log ( freq [ mask ]))) # geometric mean binned_psd [ i - 1 ] = np . mean ( psd [ mask ]) if sqrt_psd : axis . plot ( binned_freqs , np . sqrt ( binned_psd ), ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( binned_freqs , binned_psd , ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) axis . set_xscale ( \"log\" ) axis . set_yscale ( \"log\" ) axis . grid ( True , which = \"both\" ) axis . set_xlabel ( \"Frequency (Hz)\" ) axis . set_title ( f \"Log-rebinned noise from { len ( self . frequencies ) * 2 - 2 } samples\" ) axis . figure . tight_layout () calc_autocorrelation_times ( n , dt ) Compute the timesteps for an autocorrelation function Parameters: n ( int ) \u2013 Number of lags dt ( float ) \u2013 Sample time Returns: NDArray \u2013 The time delays for each lag Source code in mass2/core/noise_algorithms.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def calc_autocorrelation_times ( n : int , dt : float ) -> NDArray : \"\"\"Compute the timesteps for an autocorrelation function Parameters ---------- n : int Number of lags dt : float Sample time Returns ------- NDArray The time delays for each lag \"\"\" return np . arange ( n ) * dt calc_continuous_autocorrelation ( data , n_lags , max_excursion = 1000 ) Calculate the autocorrelation of the input data, assuming the entire array is continuous. Parameters: data ( ArrayLike ) \u2013 Data to be autocorrelated. Arrays of 2+ dimensions will be converted to a 1D array via .ravel() . n_lags ( int ) \u2013 Compute the autocorrelation for lags in the range [0, n_lags-1] . max_excursion ( int , default: 1000 ) \u2013 Chunks of data with max absolute excursion from the mean this large will be excluded from the calculation, by default 1000 Returns: NDArray \u2013 The autocorrelation array Raises: ValueError \u2013 If the data are too short to provide the requested number of lags, or the data contain apparent pulses. Source code in mass2/core/noise_algorithms.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def calc_continuous_autocorrelation ( data : ArrayLike , n_lags : int , max_excursion : int = 1000 ) -> NDArray : \"\"\"Calculate the autocorrelation of the input data, assuming the entire array is continuous. Parameters ---------- data : ArrayLike Data to be autocorrelated. Arrays of 2+ dimensions will be converted to a 1D array via `.ravel()`. n_lags : int Compute the autocorrelation for lags in the range `[0, n_lags-1]`. max_excursion : int, optional Chunks of data with max absolute excursion from the mean this large will be excluded from the calculation, by default 1000 Returns ------- NDArray The autocorrelation array Raises ------ ValueError If the data are too short to provide the requested number of lags, or the data contain apparent pulses. \"\"\" data = np . asarray ( data ) . ravel () n_data = len ( data ) assert n_lags < n_data def padded_length ( n : int ) -> int : \"\"\"Return a sensible number in the range [n, 2n] which is not too much larger than n, yet is good for FFTs. Returns: A number: (1, 3, or 5)*(a power of two), whichever is smallest. \"\"\" pow2 = np . round ( 2 ** np . ceil ( np . log2 ( n ))) if n == pow2 : return int ( n ) elif n > 0.75 * pow2 : return int ( pow2 ) elif n > 0.625 * pow2 : return int ( np . round ( 0.75 * pow2 )) else : return int ( np . round ( 0.625 * pow2 )) # When there are 10 million data points and only 10,000 lags wanted, # it's hugely inefficient to compute the full autocorrelation, especially # in memory. Instead, compute it on chunks several times the length of the desired # correlation, and average. CHUNK_MULTIPLE = 15 if n_data < CHUNK_MULTIPLE * n_lags : msg = f \"There are too few data values ( { n_data =} ) to compute at least { n_lags } lags.\" raise ValueError ( msg ) # Be sure to pad chunksize samples by AT LEAST n_lags zeros, to prevent # unwanted wraparound in the autocorrelation. # padded_data is what we do DFT/InvDFT on; ac is the unnormalized output. chunksize = CHUNK_MULTIPLE * n_lags padsize = n_lags padded_data = np . zeros ( padded_length ( padsize + chunksize ), dtype = float ) ac = np . zeros ( n_lags , dtype = float ) entries = 0 Nchunks = n_data // chunksize datachunks = data [: Nchunks * chunksize ] . reshape ( Nchunks , chunksize ) for data in datachunks : padded_data [: chunksize ] = data - np . asarray ( data ) . mean () if np . abs ( padded_data ) . max () > max_excursion : continue ft = np . fft . rfft ( padded_data ) ft [ 0 ] = 0 # this redundantly removes the mean of the data set power = ( ft * ft . conj ()) . real acsum = np . fft . irfft ( power ) ac += acsum [: n_lags ] entries += 1 if entries == 0 : raise ValueError ( \"Apparently all 'noise' chunks had large excursions from baseline, so no autocorrelation was computed\" ) ac /= entries ac /= np . arange ( chunksize , chunksize - n_lags + 0.5 , - 1.0 , dtype = float ) return ac calc_discontinuous_autocorrelation ( data , max_excursion = 1000 ) Calculate the autocorrelation of the input data, assuming the rows of the array are NOT continuous in time. Parameters: data ( ArrayLike ) \u2013 A 2D array of noise data. Shape is (ntraces, nsamples) . max_excursion ( int , default: 1000 ) \u2013 description , by default 1000 Returns: NDArray \u2013 The mean autocorrelation of the rows (\"traces\") in the input data , from lags [0, nsamples-1] . Source code in mass2/core/noise_algorithms.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def calc_discontinuous_autocorrelation ( data : ArrayLike , max_excursion : int = 1000 ) -> NDArray : \"\"\"Calculate the autocorrelation of the input data, assuming the rows of the array are NOT continuous in time. Parameters ---------- data : ArrayLike A 2D array of noise data. Shape is `(ntraces, nsamples)`. max_excursion : int, optional _description_, by default 1000 Returns ------- NDArray The mean autocorrelation of the rows (\"traces\") in the input `data`, from lags `[0, nsamples-1]`. \"\"\" data = np . asarray ( data ) ntraces , nsamples = data . shape ac = np . zeros ( nsamples , dtype = float ) traces_used = 0 for i in range ( ntraces ): pulse = data [ i , :] - data [ i , :] . mean () if np . abs ( pulse ) . max () > max_excursion : continue ac += np . correlate ( pulse , pulse , \"full\" )[ nsamples - 1 :] traces_used += 1 ac /= traces_used ac /= nsamples - np . arange ( nsamples , dtype = float ) return ac calc_noise_result ( data , dt , continuous , window = None , skip_autocorr_if_length_over = 10000 ) Analyze the noise as Mass has always done. Compute autocorrelation with a lower noise at longer lags when data are known to be continuous Subtract the mean before computing the power spectrum Parameters: data ( ArrayLike ) \u2013 A 2d array of noise data, of shape (npulses, nsamples) dt ( float ) \u2013 Periodic sampling time, in seconds continuous ( bool ) \u2013 Whether the \"pulses\" in the data array are continuous in time window ( callable , default: None ) \u2013 A function to compute a data window (or if None, no windowing), by default None Returns: NoiseResult \u2013 The derived noise spectrum and autocorrelation Source code in mass2/core/noise_algorithms.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def calc_noise_result ( data : ArrayLike , dt : float , continuous : bool , window : Callable | None = None , skip_autocorr_if_length_over : int = 10000 ) -> \"NoiseResult\" : \"\"\"Analyze the noise as Mass has always done. * Compute autocorrelation with a lower noise at longer lags when data are known to be continuous * Subtract the mean before computing the power spectrum Parameters ---------- data : ArrayLike A 2d array of noise data, of shape `(npulses, nsamples)` dt : float Periodic sampling time, in seconds continuous : bool Whether the \"pulses\" in the `data` array are continuous in time window : callable, optional A function to compute a data window (or if None, no windowing), by default None Returns ------- NoiseResult The derived noise spectrum and autocorrelation \"\"\" data = np . asarray ( data ) data_zeromean = data - np . mean ( data ) ( n_pulses , nsamples ) = data_zeromean . shape # see test_ravel_behavior to be sure this is written correctly f_mass , psd_mass = mass2 . mathstat . power_spectrum . computeSpectrum ( data_zeromean . ravel (), segfactor = n_pulses , dt = dt , window = window ) if nsamples <= skip_autocorr_if_length_over : if continuous : autocorr_vec = calc_continuous_autocorrelation ( data_zeromean . ravel (), n_lags = nsamples ) else : autocorr_vec = calc_discontinuous_autocorrelation ( data_zeromean ) else : print ( \"\"\"warning: noise_psd_mass skipping autocorrelation calculation for long traces, use skip_autocorr_if_length_over argument to override this\"\"\" ) autocorr_vec = None return NoiseResult ( psd = psd_mass , autocorr_vec = autocorr_vec , frequencies = f_mass ) noise_psd_periodogram ( data , dt , window = 'boxcar' , detrend = False ) Compute the noise power spectral density using scipy's periodogram function and the autocorrelation. Source code in mass2/core/noise_algorithms.py 151 152 153 154 155 156 157 158 159 def noise_psd_periodogram ( data : ndarray , dt : float , window : ArrayLike | str = \"boxcar\" , detrend : bool = False ) -> \"NoiseResult\" : \"\"\"Compute the noise power spectral density using scipy's periodogram function and the autocorrelation.\"\"\" f , Pxx = sp . signal . periodogram ( data , fs = 1 / dt , window = window , axis =- 1 , detrend = detrend ) # len(f) = data.shape[1]//2+1 # Pxx[i, j] is the PSD at frequency f[j] for the i\u2011th trace data[i, :] Pxx_mean = np . mean ( Pxx , axis = 0 ) # Pxx_mean[j] is the averaged PSD at frequency f[j] over all traces autocorr_vec = calc_discontinuous_autocorrelation ( data ) return NoiseResult ( psd = Pxx_mean , autocorr_vec = autocorr_vec , frequencies = f ) Hold a class to represent a channel with noise data only, and to analyze its noise characteristics. NoiseChannel dataclass A class to represent a channel with noise data only, and to analyze its noise characteristics. Source code in mass2/core/noise_channel.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @dataclass ( frozen = True ) class NoiseChannel : \"\"\"A class to represent a channel with noise data only, and to analyze its noise characteristics.\"\"\" df : pl . DataFrame # DO NOT MUTATE THIS!!! header_df : pl . DataFrame # DO NOT MUTATE THIS!! frametime_s : float # @functools.cache def calc_max_excursion ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 ) -> tuple [ pl . DataFrame , float ]: \"\"\"Compute the maximum excursion from the median for each noise record, and store in dataframe.\"\"\" def excursion2d ( noise_trace : NDArray ) -> float : \"\"\"Return the excursion (max - min) for each trace in a 2D array of traces.\"\"\" return np . amax ( noise_trace , axis = 1 ) - np . amin ( noise_trace , axis = 1 ) noise_traces = self . df . limit ( n_limit )[ trace_col_name ] . to_numpy () excursion = excursion2d ( noise_traces ) max_excursion = mass2 . misc . outlier_resistant_nsigma_above_mid ( excursion , nsigma = excursion_nsigma ) df_noise2 = self . df . limit ( n_limit ) . with_columns ( excursion = excursion ) return df_noise2 , max_excursion def get_records_2d ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 , trunc_front : int = 0 , trunc_back : int = 0 , ) -> NDArray : \"\"\" Return a 2D NumPy array of cleaned noise traces from the specified column. This method identifies noise traces with excursions below a threshold and optionally truncates the beginning and/or end of each trace. Parameters: ---------- trace_col_name : str, optional Name of the column containing trace data. Default is \"pulse\". n_limit : int, optional Maximum number of traces to analyze. Default is 10000. excursion_nsigma : float, optional Threshold for maximum excursion in units of noise sigma. Default is 5. trunc_front : int, optional Number of samples to truncate from the front of each trace. Default is 0. trunc_back : int, optional Number of samples to truncate from the back of each trace. Must be >= 0. Default is 0. Returns: ------- np.ndarray A 2D array of cleaned and optionally truncated noise traces. Shape: (n_pulses, len(pulse)) \"\"\" df_noise2 , max_excursion = self . calc_max_excursion ( trace_col_name , n_limit , excursion_nsigma ) noise_traces_clean = df_noise2 . filter ( pl . col ( \"excursion\" ) <= max_excursion )[ \"pulse\" ] . to_numpy () if trunc_back == 0 : noise_traces_clean2 = noise_traces_clean [:, trunc_front :] elif trunc_back > 0 : noise_traces_clean2 = noise_traces_clean [:, trunc_front : - trunc_back ] else : raise ValueError ( \"trunc_back must be >= 0\" ) assert noise_traces_clean2 . shape [ 0 ] > 0 return noise_traces_clean2 # @functools.cache def spectrum ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 , trunc_front : int = 0 , trunc_back : int = 0 , skip_autocorr_if_length_over : int = 10000 , ) -> NoiseResult : \"\"\"Compute and return the noise result from the noise traces.\"\"\" records = self . get_records_2d ( trace_col_name , n_limit , excursion_nsigma , trunc_front , trunc_back ) spectrum = mass2 . core . noise_algorithms . calc_noise_result ( records , continuous = self . is_continuous , dt = self . frametime_s , skip_autocorr_if_length_over = skip_autocorr_if_length_over ) return spectrum def __hash__ ( self ) -> int : \"\"\"A hash function based on the object's id.\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other : Any ) -> bool : \"\"\"Equality based on object identity.\"\"\" return id ( self ) == id ( other ) @property def is_continuous ( self ) -> bool : \"Whether this channel is continuous data (True) or triggered records with arbitrary gaps (False).\" if \"continuous\" in self . header_df : return self . header_df [ \"continuous\" ][ 0 ] return False @classmethod def from_ljh ( cls , path : str | Path ) -> \"NoiseChannel\" : \"\"\"Create a NoiseChannel by loading data from the given LJH file path.\"\"\" ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars () noise_channel = cls ( df , header_df , header_df [ \"Timebase\" ][ 0 ]) return noise_channel is_continuous property Whether this channel is continuous data (True) or triggered records with arbitrary gaps (False). __eq__ ( other ) Equality based on object identity. Source code in mass2/core/noise_channel.py 108 109 110 def __eq__ ( self , other : Any ) -> bool : \"\"\"Equality based on object identity.\"\"\" return id ( self ) == id ( other ) __hash__ () A hash function based on the object's id. Source code in mass2/core/noise_channel.py 101 102 103 104 105 106 def __hash__ ( self ) -> int : \"\"\"A hash function based on the object's id.\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) calc_max_excursion ( trace_col_name = 'pulse' , n_limit = 10000 , excursion_nsigma = 5 ) Compute the maximum excursion from the median for each noise record, and store in dataframe. Source code in mass2/core/noise_channel.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def calc_max_excursion ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 ) -> tuple [ pl . DataFrame , float ]: \"\"\"Compute the maximum excursion from the median for each noise record, and store in dataframe.\"\"\" def excursion2d ( noise_trace : NDArray ) -> float : \"\"\"Return the excursion (max - min) for each trace in a 2D array of traces.\"\"\" return np . amax ( noise_trace , axis = 1 ) - np . amin ( noise_trace , axis = 1 ) noise_traces = self . df . limit ( n_limit )[ trace_col_name ] . to_numpy () excursion = excursion2d ( noise_traces ) max_excursion = mass2 . misc . outlier_resistant_nsigma_above_mid ( excursion , nsigma = excursion_nsigma ) df_noise2 = self . df . limit ( n_limit ) . with_columns ( excursion = excursion ) return df_noise2 , max_excursion from_ljh ( path ) classmethod Create a NoiseChannel by loading data from the given LJH file path. Source code in mass2/core/noise_channel.py 119 120 121 122 123 124 125 @classmethod def from_ljh ( cls , path : str | Path ) -> \"NoiseChannel\" : \"\"\"Create a NoiseChannel by loading data from the given LJH file path.\"\"\" ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars () noise_channel = cls ( df , header_df , header_df [ \"Timebase\" ][ 0 ]) return noise_channel get_records_2d ( trace_col_name = 'pulse' , n_limit = 10000 , excursion_nsigma = 5 , trunc_front = 0 , trunc_back = 0 ) Return a 2D NumPy array of cleaned noise traces from the specified column. This method identifies noise traces with excursions below a threshold and optionally truncates the beginning and/or end of each trace. Parameters: trace_col_name : str, optional Name of the column containing trace data. Default is \"pulse\". n_limit : int, optional Maximum number of traces to analyze. Default is 10000. excursion_nsigma : float, optional Threshold for maximum excursion in units of noise sigma. Default is 5. trunc_front : int, optional Number of samples to truncate from the front of each trace. Default is 0. trunc_back : int, optional Number of samples to truncate from the back of each trace. Must be >= 0. Default is 0. Returns: np.ndarray A 2D array of cleaned and optionally truncated noise traces. Shape: (n_pulses, len(pulse)) Source code in mass2/core/noise_channel.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def get_records_2d ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 , trunc_front : int = 0 , trunc_back : int = 0 , ) -> NDArray : \"\"\" Return a 2D NumPy array of cleaned noise traces from the specified column. This method identifies noise traces with excursions below a threshold and optionally truncates the beginning and/or end of each trace. Parameters: ---------- trace_col_name : str, optional Name of the column containing trace data. Default is \"pulse\". n_limit : int, optional Maximum number of traces to analyze. Default is 10000. excursion_nsigma : float, optional Threshold for maximum excursion in units of noise sigma. Default is 5. trunc_front : int, optional Number of samples to truncate from the front of each trace. Default is 0. trunc_back : int, optional Number of samples to truncate from the back of each trace. Must be >= 0. Default is 0. Returns: ------- np.ndarray A 2D array of cleaned and optionally truncated noise traces. Shape: (n_pulses, len(pulse)) \"\"\" df_noise2 , max_excursion = self . calc_max_excursion ( trace_col_name , n_limit , excursion_nsigma ) noise_traces_clean = df_noise2 . filter ( pl . col ( \"excursion\" ) <= max_excursion )[ \"pulse\" ] . to_numpy () if trunc_back == 0 : noise_traces_clean2 = noise_traces_clean [:, trunc_front :] elif trunc_back > 0 : noise_traces_clean2 = noise_traces_clean [:, trunc_front : - trunc_back ] else : raise ValueError ( \"trunc_back must be >= 0\" ) assert noise_traces_clean2 . shape [ 0 ] > 0 return noise_traces_clean2 spectrum ( trace_col_name = 'pulse' , n_limit = 10000 , excursion_nsigma = 5 , trunc_front = 0 , trunc_back = 0 , skip_autocorr_if_length_over = 10000 ) Compute and return the noise result from the noise traces. Source code in mass2/core/noise_channel.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def spectrum ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 , trunc_front : int = 0 , trunc_back : int = 0 , skip_autocorr_if_length_over : int = 10000 , ) -> NoiseResult : \"\"\"Compute and return the noise result from the noise traces.\"\"\" records = self . get_records_2d ( trace_col_name , n_limit , excursion_nsigma , trunc_front , trunc_back ) spectrum = mass2 . core . noise_algorithms . calc_noise_result ( records , continuous = self . is_continuous , dt = self . frametime_s , skip_autocorr_if_length_over = skip_autocorr_if_length_over ) return spectrum Code copied from Mass version 1. Not up to date with latest style. Sorry. Supported off versions: 0.1.0 has projectors and basis in json with base64 encoding 0.2.0 has projectors and basis after json as binary 0.3.0 adds pretrigDelta field OffFile Working with an OFF file: off = OffFile(\"filename\") print off.dtype # show the fields available off[0] # get record 0 off[0][\"coefs\"] # get the model coefs for record 0 x,y = off.recordXY(0) plot(x,y) # plot record 0 Source code in mass2/core/offfiles.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 class OffFile : \"\"\" Working with an OFF file: off = OffFile(\"filename\") print off.dtype # show the fields available off[0] # get record 0 off[0][\"coefs\"] # get the model coefs for record 0 x,y = off.recordXY(0) plot(x,y) # plot record 0 \"\"\" def __init__ ( self , filename : str ): self . filename = filename with open ( self . filename , \"rb\" ) as f : self . headerString = readJsonString ( f ) # self.headerStringLength = f.tell() # doesn't work on windows because readline uses a readahead buffer self . headerStringLength = len ( self . headerString ) self . header = json . loads ( self . headerString ) self . dtype = recordDtype ( self . header [ \"FileFormatVersion\" ], self . header [ \"NumberOfBases\" ]) self . _dtype_non_descriptive = recordDtype ( self . header [ \"FileFormatVersion\" ], self . header [ \"NumberOfBases\" ], descriptive_coefs_names = False ) self . framePeriodSeconds = float ( self . header [ \"FramePeriodSeconds\" ]) # Estimate subframe division rate. If explicitly given in ReadoutInfo, use that. # Otherwise, if it's a Lancero-TDM system, then we know it's equal to the # of rows. # Otherwise, we don't know any way to estimate subframe divisions. (But add it if you think of any!) self . subframediv : int | None = None try : self . subframediv = self . header [ \"ReadoutInfo\" ][ \"Subframedivisions\" ] except KeyError : try : if self . header [ \"CreationInfo\" ][ \"SourceName\" ] == \"Lancero\" : self . subframediv = self . header [ \"ReadoutInfo\" ][ \"NumberOfRows\" ] except KeyError : self . subframediv = None self . validateHeader () self . _mmap : np . memmap | None = None self . projectors : NDArray | None = None self . basis : NDArray | None = None self . _decodeModelInfo () # calculates afterHeaderPos used by _updateMmap self . _updateMmap () def close ( self ) -> None : \"\"\"Close the memory map and projectors and basis memmaps\"\"\" del self . _mmap del self . projectors del self . basis def validateHeader ( self ) -> None : \"Check that the header looks like we expect, with a valid version code\" with open ( self . filename , \"rb\" ) as f : f . seek ( self . headerStringLength - 2 ) if not f . readline () . decode ( \"utf-8\" ) == \"} \\n \" : raise Exception ( \"failed to find end of header\" ) if self . header [ \"FileFormat\" ] != \"OFF\" : raise Exception ( \"FileFormatVersion is {} , want OFF\" . format ( self . header [ \"FileFormatVersion\" ])) def _updateMmap ( self , _nRecords : int | None = None ) -> None : \"\"\"Memory map an OFF file's data. `_nRecords` maps only a subset--designed for testing only \"\"\" fileSize = os . path . getsize ( self . filename ) recordSize = fileSize - self . afterHeaderPos if _nRecords is None : self . nRecords = recordSize // self . dtype . itemsize else : # for testing only self . nRecords = _nRecords self . _mmap = np . memmap ( self . filename , self . dtype , mode = \"r\" , offset = self . afterHeaderPos , shape = ( self . nRecords ,)) self . shape = self . _mmap . shape def __getitem__ ( self , * args : Any , ** kwargs : Any ) -> Any : \"Make indexing into the off the same as indexing into the memory mapped array\" assert self . _mmap is not None return self . _mmap . __getitem__ ( * args , ** kwargs ) def __len__ ( self ) -> int : \"\"\"Number of records in the OFF file\"\"\" assert self . _mmap is not None return len ( self . _mmap ) def __sizeof__ ( self ) -> int : \"\"\"Size of the memory mapped array in bytes\"\"\" assert self . _mmap is not None return self . _mmap . __sizeof__ () def _decodeModelInfo ( self ) -> None : \"\"\"Decode the model info (projectors and basis) from the OFF file, either from base64 in json or a later proprietary, binary format\"\"\" if ( \"RowMajorFloat64ValuesBase64\" in self . header [ \"ModelInfo\" ][ \"Projectors\" ] and \"RowMajorFloat64ValuesBase64\" in self . header [ \"ModelInfo\" ][ \"Basis\" ] ): # should only be in version 0.1.0 files self . _decodeModelInfoBase64 () else : self . _decodeModelInfoMmap () def _decodeModelInfoBase64 ( self ) -> None : \"\"\"Decode the model info (projectors and basis) from the OFF file, from base64 in json.\"\"\" projectorsData = decodebytes ( self . header [ \"ModelInfo\" ][ \"Projectors\" ][ \"RowMajorFloat64ValuesBase64\" ] . encode ()) projectorsRows = int ( self . header [ \"ModelInfo\" ][ \"Projectors\" ][ \"Rows\" ]) projectorsCols = int ( self . header [ \"ModelInfo\" ][ \"Projectors\" ][ \"Cols\" ]) self . projectors = np . frombuffer ( projectorsData , np . float64 ) self . projectors = self . projectors . reshape (( projectorsRows , projectorsCols )) basisData = decodebytes ( self . header [ \"ModelInfo\" ][ \"Basis\" ][ \"RowMajorFloat64ValuesBase64\" ] . encode ()) basisRows = int ( self . header [ \"ModelInfo\" ][ \"Basis\" ][ \"Rows\" ]) basisCols = int ( self . header [ \"ModelInfo\" ][ \"Basis\" ][ \"Cols\" ]) self . basis = np . frombuffer ( basisData , np . float64 ) self . basis = self . basis . reshape (( basisRows , basisCols )) if basisRows != projectorsCols or basisCols != projectorsRows or self . header [ \"NumberOfBases\" ] != projectorsRows : raise Exception ( \"basis shape should be transpose of projectors shape. have basis \" f \"( { basisCols } , { basisRows } ), projectors ( { projectorsCols } , { projectorsRows } ), \" f \"NumberOfBases { self . header [ 'NumberOfBases' ] } \" ) self . afterHeaderPos = self . headerStringLength def _decodeModelInfoMmap ( self ) -> None : \"\"\"Decode the model info (projectors and basis) from the OFF file, from binary.\"\"\" projectorsRows = int ( self . header [ \"ModelInfo\" ][ \"Projectors\" ][ \"Rows\" ]) projectorsCols = int ( self . header [ \"ModelInfo\" ][ \"Projectors\" ][ \"Cols\" ]) basisRows = int ( self . header [ \"ModelInfo\" ][ \"Basis\" ][ \"Rows\" ]) basisCols = int ( self . header [ \"ModelInfo\" ][ \"Basis\" ][ \"Cols\" ]) # 8 for float64, basis and projectors have the same number of elements and therefore of bytes nBytes = basisCols * basisRows * 8 projectorsPos = self . headerStringLength basisPos = projectorsPos + nBytes self . afterHeaderPos = basisPos + nBytes self . projectors = np . memmap ( self . filename , np . float64 , mode = \"r\" , offset = projectorsPos , shape = ( projectorsRows , projectorsCols )) self . basis = np . memmap ( self . filename , np . float64 , mode = \"r\" , offset = basisPos , shape = ( basisRows , basisCols )) if basisRows != projectorsCols or basisCols != projectorsRows or self . header [ \"NumberOfBases\" ] != projectorsRows : raise Exception ( \"basis shape should be transpose of projectors shape. have basis \" f \"( { basisCols } , { basisRows } ), projectors ( { projectorsCols } , { projectorsRows } ), \" f \"NumberOfBases { self . header [ 'NumberOfBases' ] } \" ) def __repr__ ( self ) -> str : \"\"\"Return a string representation of the OffFile object.\"\"\" return \"<OFF file> {} , {} records, {} length basis \\n \" . format ( self . filename , self . nRecords , self . header [ \"NumberOfBases\" ]) def sampleTimes ( self , i : int ) -> NDArray : \"\"\"return a vector of sample times for record i, approriate for plotting\"\"\" recordSamples = self [ i ][ \"recordSamples\" ] recordPreSamples = self [ i ][ \"recordPreSamples\" ] return np . arange ( - recordPreSamples , recordSamples - recordPreSamples ) * self . framePeriodSeconds def modeledPulse ( self , i : int ) -> NDArray : \"\"\"return a vector of the modeled pulse samples, the best available value of the actual raw samples\"\"\" # projectors has size (n,z) where it is (rows,cols) # basis has size (z,n) # coefs has size (n,1) # coefs (n,1) = projectors (n,z) * data (z,1) # modelData (z,1) = basis (z,n) * coefs (n,1) # n = number of basis (eg 3) # z = record length (eg 4) # .view(self._dtype_non_descriptive) should be a copy-free way of changing # the dtype so we can access the coefs all together assert self . basis is not None allVals = np . matmul ( self . basis , self . _mmap_with_coefs [ i ][ \"coefs\" ]) return np . asarray ( allVals ) def recordXY ( self , i : int ) -> tuple [ NDArray , NDArray ]: \"\"\"return (x,y) for record i, where x is time and y is modeled pulse\"\"\" return self . sampleTimes ( i ), self . modeledPulse ( i ) @property def _mmap_with_coefs ( self ) -> NDArray : \"\"\"Return a view of the memmap with the coefs all together in one field\"\"\" assert self . _mmap is not None return self . _mmap . view ( self . _dtype_non_descriptive ) def view ( self , * args : Any ) -> NDArray : \"\"\"Return a view of the memmap with the given args\"\"\" assert self . _mmap is not None return self . _mmap . view ( * args ) __getitem__ ( * args , ** kwargs ) Make indexing into the off the same as indexing into the memory mapped array Source code in mass2/core/offfiles.py 143 144 145 146 def __getitem__ ( self , * args : Any , ** kwargs : Any ) -> Any : \"Make indexing into the off the same as indexing into the memory mapped array\" assert self . _mmap is not None return self . _mmap . __getitem__ ( * args , ** kwargs ) __len__ () Number of records in the OFF file Source code in mass2/core/offfiles.py 148 149 150 151 def __len__ ( self ) -> int : \"\"\"Number of records in the OFF file\"\"\" assert self . _mmap is not None return len ( self . _mmap ) __repr__ () Return a string representation of the OffFile object. Source code in mass2/core/offfiles.py 210 211 212 def __repr__ ( self ) -> str : \"\"\"Return a string representation of the OffFile object.\"\"\" return \"<OFF file> {} , {} records, {} length basis \\n \" . format ( self . filename , self . nRecords , self . header [ \"NumberOfBases\" ]) __sizeof__ () Size of the memory mapped array in bytes Source code in mass2/core/offfiles.py 153 154 155 156 def __sizeof__ ( self ) -> int : \"\"\"Size of the memory mapped array in bytes\"\"\" assert self . _mmap is not None return self . _mmap . __sizeof__ () close () Close the memory map and projectors and basis memmaps Source code in mass2/core/offfiles.py 115 116 117 118 119 def close ( self ) -> None : \"\"\"Close the memory map and projectors and basis memmaps\"\"\" del self . _mmap del self . projectors del self . basis modeledPulse ( i ) return a vector of the modeled pulse samples, the best available value of the actual raw samples Source code in mass2/core/offfiles.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def modeledPulse ( self , i : int ) -> NDArray : \"\"\"return a vector of the modeled pulse samples, the best available value of the actual raw samples\"\"\" # projectors has size (n,z) where it is (rows,cols) # basis has size (z,n) # coefs has size (n,1) # coefs (n,1) = projectors (n,z) * data (z,1) # modelData (z,1) = basis (z,n) * coefs (n,1) # n = number of basis (eg 3) # z = record length (eg 4) # .view(self._dtype_non_descriptive) should be a copy-free way of changing # the dtype so we can access the coefs all together assert self . basis is not None allVals = np . matmul ( self . basis , self . _mmap_with_coefs [ i ][ \"coefs\" ]) return np . asarray ( allVals ) recordXY ( i ) return (x,y) for record i, where x is time and y is modeled pulse Source code in mass2/core/offfiles.py 236 237 238 def recordXY ( self , i : int ) -> tuple [ NDArray , NDArray ]: \"\"\"return (x,y) for record i, where x is time and y is modeled pulse\"\"\" return self . sampleTimes ( i ), self . modeledPulse ( i ) sampleTimes ( i ) return a vector of sample times for record i, approriate for plotting Source code in mass2/core/offfiles.py 214 215 216 217 218 def sampleTimes ( self , i : int ) -> NDArray : \"\"\"return a vector of sample times for record i, approriate for plotting\"\"\" recordSamples = self [ i ][ \"recordSamples\" ] recordPreSamples = self [ i ][ \"recordPreSamples\" ] return np . arange ( - recordPreSamples , recordSamples - recordPreSamples ) * self . framePeriodSeconds validateHeader () Check that the header looks like we expect, with a valid version code Source code in mass2/core/offfiles.py 121 122 123 124 125 126 127 128 def validateHeader ( self ) -> None : \"Check that the header looks like we expect, with a valid version code\" with open ( self . filename , \"rb\" ) as f : f . seek ( self . headerStringLength - 2 ) if not f . readline () . decode ( \"utf-8\" ) == \"} \\n \" : raise Exception ( \"failed to find end of header\" ) if self . header [ \"FileFormat\" ] != \"OFF\" : raise Exception ( \"FileFormatVersion is {} , want OFF\" . format ( self . header [ \"FileFormatVersion\" ])) view ( * args ) Return a view of the memmap with the given args Source code in mass2/core/offfiles.py 246 247 248 249 def view ( self , * args : Any ) -> NDArray : \"\"\"Return a view of the memmap with the given args\"\"\" assert self . _mmap is not None return self . _mmap . view ( * args ) readJsonString ( f ) look in file f for a line \"}\\n\" and return all contents up to that point for an OFF file this can be parsed by json.dumps and all remaining data is records Source code in mass2/core/offfiles.py 57 58 59 60 61 62 63 64 65 66 67 68 def readJsonString ( f : io . BufferedReader ) -> str : \"\"\"look in file f for a line \"}\\\\n\" and return all contents up to that point for an OFF file this can be parsed by json.dumps and all remaining data is records\"\"\" lines : list [ str ] = [] while True : line = f . readline () . decode ( \"utf-8\" ) lines += line if line == \"} \\n \" : return \"\" . join ( lines ) if len ( line ) == 0 : raise Exception ( \"\"\"reached end of file without finding an end-of-JSON line \"} \\\\ n\" \"\"\" ) recordDtype ( offVersion , nBasis , descriptive_coefs_names = True ) return a np.dtype matching the record datatype for the given offVersion and nBasis descriptive_coefs_names - determines how the modeled pulse coefficients are name, you usually want True For True, the names will be derivLike , pulseLike , and if nBasis>3, also extraCoefs For False, they will all have the single name coefs . False is to make implementing recordXY and other methods that want access to all coefs simultaneously easier Source code in mass2/core/offfiles.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def recordDtype ( offVersion : str , nBasis : int , descriptive_coefs_names : bool = True ) -> np . dtype : \"\"\"return a np.dtype matching the record datatype for the given offVersion and nBasis descriptive_coefs_names - determines how the modeled pulse coefficients are name, you usually want True For True, the names will be `derivLike`, `pulseLike`, and if nBasis>3, also `extraCoefs` For False, they will all have the single name `coefs`. False is to make implementing recordXY and other methods that want access to all coefs simultaneously easier\"\"\" if offVersion in { \"0.1.0\" , \"0.2.0\" }: # start of the dtype is identical for all cases dt_list : list [ tuple [ str , type ] | tuple [ str , type , int ]] = [ ( \"recordSamples\" , np . int32 ), ( \"recordPreSamples\" , np . int32 ), ( \"framecount\" , np . int64 ), ( \"unixnano\" , np . int64 ), ( \"pretriggerMean\" , np . float32 ), ( \"residualStdDev\" , np . float32 ), ] elif offVersion == \"0.3.0\" : dt_list = [ ( \"recordSamples\" , np . int32 ), ( \"recordPreSamples\" , np . int32 ), ( \"framecount\" , np . int64 ), ( \"unixnano\" , np . int64 ), ( \"pretriggerMean\" , np . float32 ), ( \"pretriggerDelta\" , np . float32 ), ( \"residualStdDev\" , np . float32 ), ] else : raise Exception ( f \"dtype for OFF version { offVersion } not implemented\" ) if descriptive_coefs_names : dt_list += [( \"pulseMean\" , np . float32 ), ( \"derivativeLike\" , np . float32 ), ( \"filtValue\" , np . float32 )] if nBasis > 3 : dt_list += [( \"extraCoefs\" , np . float32 , ( nBasis - 3 ))] else : dt_list += [( \"coefs\" , np . float32 , ( nBasis ))] return np . dtype ( dt_list ) Classes to create time-domain and Fourier-domain optimal filters. Filter dataclass Bases: ABC A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level. Returns: Filter \u2013 A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted variance due to noise and the resulting predicted_v_over_dv , the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: nominal_peak . Source code in mass2/core/optimal_filtering.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 @dataclass ( frozen = True ) class Filter ( ABC ): \"\"\"A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level. Returns ------- Filter A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted `variance` due to noise and the resulting `predicted_v_over_dv`, the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: `nominal_peak`. \"\"\" values : np . ndarray nominal_peak : float variance : float predicted_v_over_dv : float dt_values : np . ndarray | None const_values : np . ndarray | None signal_model : np . ndarray | None dt_model : np . ndarray | None convolution_lags : int = 1 fmax : float | None = None f_3db : float | None = None cut_pre : int = 0 cut_post : int = 0 @property @abstractmethod def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property @abstractmethod def _filter_type ( self ) -> str : \"\"\"The name for this filter type\"\"\" return \"illegal: this is supposed to be an abstract base class\" def plot ( self , axis : plt . Axes | None = None , ** kwargs : Any ) -> None : \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout () def report ( self , std_energy : float = 5898.8 ) -> None : \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" ) @abstractmethod def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records.\"\"\" pass is_arrival_time_safe abstractmethod property Is this an arrival-time-safe filter? filter_records ( x ) abstractmethod Filter one microcalorimeter record or an array of records. Source code in mass2/core/optimal_filtering.py 316 317 318 319 @abstractmethod def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records.\"\"\" pass plot ( axis = None , ** kwargs ) Make a plot of the filter Parameters: axis ( Axes , default: None ) \u2013 A pre-existing axis to plot on, by default None Source code in mass2/core/optimal_filtering.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def plot ( self , axis : plt . Axes | None = None , ** kwargs : Any ) -> None : \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout () report ( std_energy = 5898.8 ) Report on estimated V/dV for the filter. Parameters: std_energy ( float , default: 5898.8 ) \u2013 Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 Source code in mass2/core/optimal_filtering.py 302 303 304 305 306 307 308 309 310 311 312 313 314 def report ( self , std_energy : float = 5898.8 ) -> None : \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" ) Filter5Lag dataclass Bases: Filter Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015. Returns: Filter5Lag \u2013 An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 @dataclass ( frozen = True ) class Filter5Lag ( Filter ): \"\"\"Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015. Returns ------- Filter5Lag An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ) -> None : \"\"\"Post-init checks that this filter, indeed, is a 5-lag one\"\"\" assert self . convolution_lags == 5 @property def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property def _filter_type ( self ) -> str : \"\"\"Name for this filter type\"\"\" return \"5lag\" # These parameters fit a parabola to any 5 evenly-spaced points FIVELAG_FITTER = ( np . array ( ( ( - 6 , 24 , 34 , 24 , - 6 ), ( - 14 , - 7 , 0 , 7 , 14 ), ( 10 , - 5 , - 10 , - 5 , 10 ), ), dtype = float , ) / 70.0 ) def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x is_arrival_time_safe property Is this an arrival-time-safe filter? __post_init__ () Post-init checks that this filter, indeed, is a 5-lag one Source code in mass2/core/optimal_filtering.py 334 335 336 def __post_init__ ( self ) -> None : \"\"\"Post-init checks that this filter, indeed, is a 5-lag one\"\"\" assert self . convolution_lags == 5 filter_records ( x ) Filter one microcalorimeter record or an array of records. Parameters: x ( ArrayLike ) \u2013 A 1-d array, a single pulse record, or a 2-d array, where x[i, :] is pulse record number i . Returns: tuple [ ndarray , ndarray ] \u2013 The optimally filtered value, or an array (one per row) if the input is a 2-d array. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises: AssertionError \u2013 If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x FilterATS dataclass Bases: Filter Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015. Returns: FilterATS \u2013 An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 @dataclass ( frozen = True ) class FilterATS ( Filter ): \"\"\"Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015. Returns ------- FilterATS An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ) -> None : \"\"\"Post-init checks that this filter aexpects one lag, and has a dt_values array\"\"\" assert self . convolution_lags == 1 assert self . dt_values is not None @property def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return True @property def _filter_type ( self ) -> str : \"\"\"Return the name for this filter type\"\"\" return \"ats\" def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values ) is_arrival_time_safe property Is this an arrival-time-safe filter? __post_init__ () Post-init checks that this filter aexpects one lag, and has a dt_values array Source code in mass2/core/optimal_filtering.py 412 413 414 415 def __post_init__ ( self ) -> None : \"\"\"Post-init checks that this filter aexpects one lag, and has a dt_values array\"\"\" assert self . convolution_lags == 1 assert self . dt_values is not None filter_records ( x ) Filter one microcalorimeter record or an array of records. Parameters: x ( ArrayLike ) \u2013 A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns: tuple [ ndarray , ndarray ] \u2013 The optimally filtered value, or an array (one per row) if the input is a 2-d array. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises: AssertionError \u2013 If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values ) FilterMaker dataclass An object capable of creating optimal filter based on a single signal and noise set. Parameters: signal_model ( ArrayLike ) \u2013 The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the peak value of this filter (that is, peak value relative to the baseline level). n_pretrigger ( int ) \u2013 The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only n_pretrigger samples at the start of a record. noise_autocorr ( Optional [ ArrayLike ] , default: None ) \u2013 The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of avg_signal . noise_psd ( Optional [ ArrayLike ] , default: None ) \u2013 The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of avg_signal , and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method compute_fourier() will not work. whitener ( Optional [ ToeplitzWhitener ] , default: None ) \u2013 An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes noise_autocorr if both are given. sample_time_sec ( float , default: 0.0 ) \u2013 The time step between samples in avg_signal and noise_autocorr (in seconds). This must be given if fmax or f_3db are ever to be used. peak ( float , default: 0.0 ) \u2013 The peak amplitude of the standard signal Notes If both noise_autocorr and whitener are None, then methods compute_5lag and compute_ats will both fail, as they require a time-domain characterization of the noise. The units of noise_autocorr are the square of the units used in signal_model and/or peak . The units of whitener are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. The units of noise_psd are square signal units, per Hertz. Returns: FilterMaker \u2013 An object that can make a variety of optimal filters, assuming a single signal and noise analysis. Source code in mass2/core/optimal_filtering.py 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 @dataclass ( frozen = True ) class FilterMaker : \"\"\"An object capable of creating optimal filter based on a single signal and noise set. Parameters --------- signal_model : ArrayLike The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the *peak value* of this filter (that is, peak value relative to the baseline level). n_pretrigger : int The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only `n_pretrigger` samples at the start of a record. noise_autocorr : Optional[ArrayLike] The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of `avg_signal`. noise_psd : Optional[ArrayLike] The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of `avg_signal`, and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method `compute_fourier()` will not work. whitener : Optional[ToeplitzWhitener] An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes `noise_autocorr` if both are given. sample_time_sec : float The time step between samples in `avg_signal` and `noise_autocorr` (in seconds). This must be given if `fmax` or `f_3db` are ever to be used. peak : float The peak amplitude of the standard signal Notes ----- * If both `noise_autocorr` and `whitener` are None, then methods `compute_5lag` and `compute_ats` will both fail, as they require a time-domain characterization of the noise. * The units of `noise_autocorr` are the square of the units used in `signal_model` and/or `peak`. The units of `whitener` are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. * The units of `noise_psd` are square signal units, per Hertz. Returns ------- FilterMaker An object that can make a variety of optimal filters, assuming a single signal and noise analysis. \"\"\" signal_model : NDArray n_pretrigger : int noise_autocorr : NDArray | None = None noise_psd : NDArray | None = None dt_model : NDArray | None = None whitener : ToeplitzWhitener | None = None sample_time_sec : float = 0.0 peak : float = 0.0 def compute_constrained_5lag ( self , constraints : ArrayLike | None = None , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 , ) -> Filter : \"\"\"Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- constraints: ndarray, optional The vector or vectors to which the filter should be orthogonal. If a 2d array, each _row_ is a constraint, and the number of columns should be equal to the len(self.signal_model) minus `(cut_pre+cut_post)`. fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) shorten = 2 # for 5-lag convolution truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" pulse_model = np . vstack (( truncated_signal , np . ones_like ( truncated_signal ))) if constraints is not None : pulse_model = np . vstack (( pulse_model , constraints )) assert pulse_model . shape [ 1 ] == n noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_model = np . vstack ([ TS ( r ) for r in pulse_model ]) A = pulse_model . dot ( Rinv_model . T ) all_filters = np . linalg . solve ( A , Rinv_model ) filt_noconst = all_filters [ 0 ] band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) if variance <= 0 : vdv = np . inf else : vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post ) def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" return self . compute_constrained_5lag ( None , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) def compute_5lag_noexp ( self , exp_time_seconds : float , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- exp_time_seconds: float Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" assert exp_time_seconds > 0 n = len ( self . signal_model ) - 4 - ( cut_pre + cut_post ) log_per_sample = self . sample_time_sec / exp_time_seconds constraint = np . exp ( - np . arange ( n ) * log_per_sample ) return self . compute_constrained_5lag ( constraint , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , ) def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post ) def _compute_autocorr ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> np . ndarray : \"\"\"Return the noise autocorrelation, if any, cut down by the requested number of values at the start and end. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- np.ndarray The noise autocorrelation of the appropriate length. Or a length-0 array if not known. \"\"\" # If there's an autocorrelation, cut it down to length. if self . noise_autocorr is None : return np . array ([], dtype = float ) N = len ( np . asarray ( self . signal_model )) return np . asarray ( self . noise_autocorr )[: N - ( cut_pre + cut_post )] def _normalize_signal ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> tuple [ np . ndarray , float , np . ndarray ]: \"\"\"Compute the normalized signal, peak value, and first-order arrival-time model. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- tuple[np.ndarray, float, np.ndarray] (sig, pk, dsig), where `sig` is the nominal signal model (normalized to have unit amplitude), `pk` is the peak values of the nominal signal, and `dsig` is the delta between `sig` that differ by one sample in arrival time. The `dsig` will be an empty array if no arrival-time model is known. Raises ------ ValueError If negative numbers of samples are to be cut, or the entire record is to be cut. \"\"\" avg_signal = np . array ( self . signal_model ) ns = len ( avg_signal ) pre_avg = avg_signal [ cut_pre : self . n_pretrigger - 1 ] . mean () if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) # Unless passed in, find the signal's peak value. This is normally peak=(max-pretrigger). # If signal is negative-going, however, then peak=(pretrigger-min). if self . peak > 0.0 : peak_signal = self . peak else : a = avg_signal [ cut_pre : ns - cut_post ] . min () b = avg_signal [ cut_pre : ns - cut_post ] . max () is_negative = pre_avg - a > b - pre_avg if is_negative : peak_signal = a - pre_avg else : peak_signal = b - pre_avg # avg_signal: normalize to have unit peak avg_signal -= pre_avg rescale = 1 / np . max ( avg_signal ) avg_signal *= rescale avg_signal [: self . n_pretrigger ] = 0.0 avg_signal = avg_signal [ cut_pre : ns - cut_post ] if self . dt_model is None : dt_model = np . array ([], dtype = float ) else : dt_model = self . dt_model * rescale dt_model = dt_model [ cut_pre : ns - cut_post ] return avg_signal , peak_signal , dt_model @staticmethod def _normalize_5lag_filter ( f : np . ndarray , avg_signal : np . ndarray ) -> None : \"\"\"Rescale 5-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) <= len ( avg_signal ) - 4 conv = np . zeros ( 5 , dtype = float ) for i in range ( 5 ): conv [ i ] = np . dot ( f , avg_signal [ i : i + len ( f )]) x = np . linspace ( - 2 , 2 , 5 ) fit = np . polyfit ( x , conv , 2 ) fit_ctr = - 0.5 * fit [ 1 ] / fit [ 0 ] fit_peak = np . polyval ( fit , fit_ctr ) f *= 1.0 / fit_peak @staticmethod def _normalize_filter ( f : np . ndarray , avg_signal : np . ndarray ) -> None : \"\"\"Rescale single-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) == len ( avg_signal ) f *= 1 / np . dot ( f , avg_signal ) compute_5lag ( fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" return self . compute_constrained_5lag ( None , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) compute_5lag_noexp ( exp_time_seconds , fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: exp_time_seconds ( float ) \u2013 Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 def compute_5lag_noexp ( self , exp_time_seconds : float , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- exp_time_seconds: float Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" assert exp_time_seconds > 0 n = len ( self . signal_model ) - 4 - ( cut_pre + cut_post ) log_per_sample = self . sample_time_sec / exp_time_seconds constraint = np . exp ( - np . arange ( n ) * log_per_sample ) return self . compute_constrained_5lag ( constraint , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) compute_ats ( fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 An arrival-time-safe optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post ) compute_constrained_5lag ( constraints = None , fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: constraints ( ArrayLike | None , default: None ) \u2013 The vector or vectors to which the filter should be orthogonal. If a 2d array, each row is a constraint, and the number of columns should be equal to the len(self.signal_model) minus (cut_pre+cut_post) . fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 def compute_constrained_5lag ( self , constraints : ArrayLike | None = None , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 , ) -> Filter : \"\"\"Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- constraints: ndarray, optional The vector or vectors to which the filter should be orthogonal. If a 2d array, each _row_ is a constraint, and the number of columns should be equal to the len(self.signal_model) minus `(cut_pre+cut_post)`. fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) shorten = 2 # for 5-lag convolution truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" pulse_model = np . vstack (( truncated_signal , np . ones_like ( truncated_signal ))) if constraints is not None : pulse_model = np . vstack (( pulse_model , constraints )) assert pulse_model . shape [ 1 ] == n noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_model = np . vstack ([ TS ( r ) for r in pulse_model ]) A = pulse_model . dot ( Rinv_model . T ) all_filters = np . linalg . solve ( A , Rinv_model ) filt_noconst = all_filters [ 0 ] band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) if variance <= 0 : vdv = np . inf else : vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post ) compute_fourier ( fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter, computed in the Fourier domain. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , ) ToeplitzWhitener dataclass An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: tw.whiten(v) returns Wv; it is equivalent to tw(v) tw.solveWT(v) returns inv(W')*v tw.applyWT(v) returns W'v tw.solveW(v) returns inv(W)*v Arguments theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients Returns: ToeplitzWhitener \u2013 Object that can perform approximate, time-invariant noise whitening. Raises: ValueError \u2013 If the operative methods are passed an array of dimension higher than 2. Source code in mass2/core/optimal_filtering.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @dataclass ( frozen = True ) class ToeplitzWhitener : \"\"\"An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: * `tw.whiten(v)` returns Wv; it is equivalent to `tw(v)` * `tw.solveWT(v)` returns inv(W')*v * `tw.applyWT(v)` returns W'v * `tw.solveW(v)` returns inv(W)*v Arguments --------- theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients Returns ------- ToeplitzWhitener Object that can perform approximate, time-invariant noise whitening. Raises ------ ValueError If the operative methods are passed an array of dimension higher than 2. \"\"\" theta : np . ndarray phi : np . ndarray @property def p ( self ) -> int : \"\"\"Return the autoregressive order\"\"\" return len ( self . phi ) - 1 @property def q ( self ) -> int : \"\"\"Return the moving-average order\"\"\" return len ( self . theta ) - 1 def whiten ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v ) def __call__ ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y def solveW ( self , v : ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y def solveWT ( self , v : ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :] def applyWT ( self , v : ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :] def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR ) p property Return the autoregressive order q property Return the moving-average order W ( N ) Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. Source code in mass2/core/optimal_filtering.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR ) __call__ ( v ) Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __call__ ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y applyWT ( v ) Return vector (or matrix of column vectors) W'v Source code in mass2/core/optimal_filtering.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def applyWT ( self , v : ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :] solveW ( v ) Return unwhitened vector (or matrix of column vectors) inv(W)*v Source code in mass2/core/optimal_filtering.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def solveW ( self , v : ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y solveWT ( v ) Return vector (or matrix of column vectors) inv(W')*v Source code in mass2/core/optimal_filtering.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def solveWT ( self , v : ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :] whiten ( v ) Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 68 69 70 def whiten ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v ) band_limit ( modelmatrix , sample_time_sec , fmax , f_3db ) Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input modelmatrix in-place. No effect if both fmax and f_3db are None . Parameters: modelmatrix ( ndarray ) \u2013 The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec ( float ) \u2013 The sampling period, normally in seconds. fmax ( Optional [ float ] ) \u2013 The hard maximum frequency (units are inverse of sample_time_sec units, or Hz) f_3db ( Optional [ float ] ) \u2013 The 1-pole low-pass filter's 3 dB point (same units as fmax ) Source code in mass2/core/optimal_filtering.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def band_limit ( modelmatrix : np . ndarray , sample_time_sec : float , fmax : float | None , f_3db : float | None ) -> None : \"\"\"Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input `modelmatrix` in-place. No effect if both `fmax` and `f_3db` are `None`. Parameters ---------- modelmatrix : np.ndarray The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec : float The sampling period, normally in seconds. fmax : Optional[float] The hard maximum frequency (units are inverse of `sample_time_sec` units, or Hz) f_3db : Optional[float] The 1-pole low-pass filter's 3 dB point (same units as `fmax`) \"\"\" if fmax is None and f_3db is None : return # Handle the 2D case by calling this function once per column. assert len ( modelmatrix . shape ) <= 2 if len ( modelmatrix . shape ) == 2 : for i in range ( modelmatrix . shape [ 1 ]): band_limit ( modelmatrix [:, i ], sample_time_sec , fmax , f_3db ) return vector = modelmatrix filt_length = len ( vector ) sig_ft = np . fft . rfft ( vector ) freq = np . fft . fftfreq ( filt_length , d = sample_time_sec ) freq = np . abs ( freq [: len ( sig_ft )]) if fmax is not None : sig_ft [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft /= 1.0 + ( 1.0 * freq / f_3db ) ** 2 # n=filt_length is needed when filt_length is ODD vector [:] = np . fft . irfft ( sig_ft , n = filt_length ) bracketR ( q , noise ) Return the dot product (q^T R q) for vector and matrix R constructed from the vector by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). Source code in mass2/core/optimal_filtering.py 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 def bracketR ( q : NDArray , noise : NDArray ) -> float : \"\"\"Return the dot product (q^T R q) for vector <q> and matrix R constructed from the vector <noise> by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). \"\"\" if len ( noise ) < len ( q ): raise ValueError ( f \"Vector q (length { len ( q ) } ) cannot be longer than the noise (length { len ( noise ) } )\" ) n = len ( q ) r = np . zeros ( 2 * n - 1 , dtype = float ) r [ n - 1 :] = noise [: n ] r [ n - 1 :: - 1 ] = noise [: n ] dot = 0.0 for i in range ( n ): dot += q [ i ] * r [ n - i - 1 : 2 * n - i - 1 ] . dot ( q ) return dot Classes and functions to correct for arrival-time bias in optimal filtering. PhaseCorrector A class to correct for arrival-time bias in optimal filtering. Source code in mass2/core/phase_correct.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class PhaseCorrector : \"\"\"A class to correct for arrival-time bias in optimal filtering.\"\"\" version = 1 def __init__ ( self , phase_uniformifier_x : ArrayLike , phase_uniformifier_y : ArrayLike , corrections : list [ CubicSpline ], indicatorName : str , uncorrectedName : str , ): self . corrections = corrections self . phase_uniformifier_x = np . array ( phase_uniformifier_x ) self . phase_uniformifier_y = np . array ( phase_uniformifier_y ) self . indicatorName = tostr ( indicatorName ) self . uncorrectedName = tostr ( uncorrectedName ) self . phase_uniformifier = CubicSpline ( self . phase_uniformifier_x , self . phase_uniformifier_y ) def toHDF5 ( self , hdf5_group : h5py . Group , name : str = \"phase_correction\" , overwrite : bool = False ) -> None : \"\"\"Write to the given HDF5 group for later recovery from disk (by fromHDF5 class method).\"\"\" group = hdf5_group . require_group ( name ) def h5group_update ( name : str , vector : ArrayLike ) -> None : \"Overwrite or create a dataset in the given group.\" if name in group : if overwrite : del group [ name ] else : raise AttributeError ( f \"Cannot overwrite phase correction dataset ' { name } '\" ) group [ name ] = vector h5group_update ( \"phase_uniformifier_x\" , self . phase_uniformifier_x ) h5group_update ( \"phase_uniformifier_y\" , self . phase_uniformifier_y ) h5group_update ( \"uncorrected_name\" , self . uncorrectedName ) h5group_update ( \"indicator_name\" , self . indicatorName ) h5group_update ( \"version\" , self . version ) for i , correction in enumerate ( self . corrections ): h5group_update ( f \"correction_ { i } _x\" , correction . _x ) h5group_update ( f \"correction_ { i } _y\" , correction . _y ) def correct ( self , phase : ArrayLike , ph : ArrayLike ) -> NDArray : \"\"\"Apply the phase correction to the given data `ph`.\"\"\" ph = np . asarray ( ph ) # attempt to force phases to fall between X and X phase_uniformified = np . asarray ( phase ) - self . phase_uniformifier ( ph ) # Compute a correction for each pulse for each correction-line energy # For the actual correction, don't let |ph| > 0.6 sample phase_clipped = np . clip ( phase_uniformified , - 0.6 , 0.6 ) pheight_corrected = _phase_corrected_filtvals ( phase_clipped , ph , self . corrections ) return pheight_corrected def __call__ ( self , phase_indicator : ArrayLike , ph : ArrayLike ) -> NDArray : \"Equivalent to self.correct()\" return self . correct ( phase_indicator , ph ) @classmethod def fromHDF5 ( cls , hdf5_group : h5py . Group , name : str = \"phase_correction\" ) -> \"PhaseCorrector\" : \"\"\"Recover a PhaseCorrector object from the given HDF5 group.\"\"\" x = hdf5_group [ f \" { name } /phase_uniformifier_x\" ][()] y = hdf5_group [ f \" { name } /phase_uniformifier_y\" ][()] uncorrectedName = tostr ( hdf5_group [ f \" { name } /uncorrected_name\" ][()]) indicatorName = tostr ( hdf5_group [ f \" { name } /indicator_name\" ][()]) version = hdf5_group [ f \" { name } /version\" ][()] i = 0 corrections = [] while f \" { name } /correction_ { i } _x\" in hdf5_group : _x = hdf5_group [ f \" { name } /correction_ { i } _x\" ][()] _y = hdf5_group [ f \" { name } /correction_ { i } _y\" ][()] corrections . append ( CubicSpline ( _x , _y )) i += 1 assert version == cls . version return cls ( x , y , corrections , indicatorName , uncorrectedName ) def __repr__ ( self ) -> str : \"\"\"String representation of this object.\"\"\" s = f \"\"\"PhaseCorrector with splines at this many levels: { len ( self . corrections ) } phase_uniformifier_x: { self . phase_uniformifier_x } phase_uniformifier_y: { self . phase_uniformifier_y } uncorrectedName: { self . uncorrectedName } \"\"\" return s __call__ ( phase_indicator , ph ) Equivalent to self.correct() Source code in mass2/core/phase_correct.py 71 72 73 def __call__ ( self , phase_indicator : ArrayLike , ph : ArrayLike ) -> NDArray : \"Equivalent to self.correct()\" return self . correct ( phase_indicator , ph ) __repr__ () String representation of this object. Source code in mass2/core/phase_correct.py 93 94 95 96 97 98 99 100 101 def __repr__ ( self ) -> str : \"\"\"String representation of this object.\"\"\" s = f \"\"\"PhaseCorrector with splines at this many levels: { len ( self . corrections ) } phase_uniformifier_x: { self . phase_uniformifier_x } phase_uniformifier_y: { self . phase_uniformifier_y } uncorrectedName: { self . uncorrectedName } \"\"\" return s correct ( phase , ph ) Apply the phase correction to the given data ph . Source code in mass2/core/phase_correct.py 60 61 62 63 64 65 66 67 68 69 def correct ( self , phase : ArrayLike , ph : ArrayLike ) -> NDArray : \"\"\"Apply the phase correction to the given data `ph`.\"\"\" ph = np . asarray ( ph ) # attempt to force phases to fall between X and X phase_uniformified = np . asarray ( phase ) - self . phase_uniformifier ( ph ) # Compute a correction for each pulse for each correction-line energy # For the actual correction, don't let |ph| > 0.6 sample phase_clipped = np . clip ( phase_uniformified , - 0.6 , 0.6 ) pheight_corrected = _phase_corrected_filtvals ( phase_clipped , ph , self . corrections ) return pheight_corrected fromHDF5 ( hdf5_group , name = 'phase_correction' ) classmethod Recover a PhaseCorrector object from the given HDF5 group. Source code in mass2/core/phase_correct.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 @classmethod def fromHDF5 ( cls , hdf5_group : h5py . Group , name : str = \"phase_correction\" ) -> \"PhaseCorrector\" : \"\"\"Recover a PhaseCorrector object from the given HDF5 group.\"\"\" x = hdf5_group [ f \" { name } /phase_uniformifier_x\" ][()] y = hdf5_group [ f \" { name } /phase_uniformifier_y\" ][()] uncorrectedName = tostr ( hdf5_group [ f \" { name } /uncorrected_name\" ][()]) indicatorName = tostr ( hdf5_group [ f \" { name } /indicator_name\" ][()]) version = hdf5_group [ f \" { name } /version\" ][()] i = 0 corrections = [] while f \" { name } /correction_ { i } _x\" in hdf5_group : _x = hdf5_group [ f \" { name } /correction_ { i } _x\" ][()] _y = hdf5_group [ f \" { name } /correction_ { i } _y\" ][()] corrections . append ( CubicSpline ( _x , _y )) i += 1 assert version == cls . version return cls ( x , y , corrections , indicatorName , uncorrectedName ) toHDF5 ( hdf5_group , name = 'phase_correction' , overwrite = False ) Write to the given HDF5 group for later recovery from disk (by fromHDF5 class method). Source code in mass2/core/phase_correct.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def toHDF5 ( self , hdf5_group : h5py . Group , name : str = \"phase_correction\" , overwrite : bool = False ) -> None : \"\"\"Write to the given HDF5 group for later recovery from disk (by fromHDF5 class method).\"\"\" group = hdf5_group . require_group ( name ) def h5group_update ( name : str , vector : ArrayLike ) -> None : \"Overwrite or create a dataset in the given group.\" if name in group : if overwrite : del group [ name ] else : raise AttributeError ( f \"Cannot overwrite phase correction dataset ' { name } '\" ) group [ name ] = vector h5group_update ( \"phase_uniformifier_x\" , self . phase_uniformifier_x ) h5group_update ( \"phase_uniformifier_y\" , self . phase_uniformifier_y ) h5group_update ( \"uncorrected_name\" , self . uncorrectedName ) h5group_update ( \"indicator_name\" , self . indicatorName ) h5group_update ( \"version\" , self . version ) for i , correction in enumerate ( self . corrections ): h5group_update ( f \"correction_ { i } _x\" , correction . _x ) h5group_update ( f \"correction_ { i } _y\" , correction . _y ) phase_correct ( phase , pheight , ph_peaks = None , method2017 = True , kernel_width = None , indicatorName = '' , uncorrectedName = '' ) Create a PhaseCorrector object to correct for arrival-time bias in optimal filtering. Source code in mass2/core/phase_correct.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def phase_correct ( phase : ArrayLike , pheight : ArrayLike , ph_peaks : ArrayLike | None = None , method2017 : bool = True , kernel_width : float | None = None , indicatorName : str = \"\" , uncorrectedName : str = \"\" , ) -> PhaseCorrector : \"\"\"Create a PhaseCorrector object to correct for arrival-time bias in optimal filtering.\"\"\" phase = np . asarray ( phase ) pheight = np . asarray ( pheight ) if ph_peaks is None : ph_peaks = _find_peaks_heuristic ( pheight ) ph_peaks = np . asarray ( ph_peaks ) if len ( ph_peaks ) <= 0 : raise ValueError ( \"Could not phase_correct because no peaks found\" ) ph_peaks . sort () # Compute a correction function at each line in ph_peaks corrections = [] median_phase = [] if kernel_width is None : kernel_width = np . max ( ph_peaks ) / 1000.0 for pk in ph_peaks : nextcorr , mphase = _phasecorr_find_alignment ( phase , pheight , pk , 0.012 * np . mean ( ph_peaks ), method2017 = method2017 , kernel_width = kernel_width ) corrections . append ( nextcorr ) median_phase . append ( mphase ) NC = len ( corrections ) if NC > 3 : phase_uniformifier_x = ph_peaks phase_uniformifier_y = np . array ( median_phase ) else : # Too few peaks to spline, so just bin and take the median per bin, then # interpolated (approximating) spline through/near these points. NBINS = 10 top = min ( pheight . max (), 1.2 * np . percentile ( pheight , 98 )) bin = np . digitize ( pheight , np . linspace ( 0 , top , 1 + NBINS )) - 1 x = np . zeros ( NBINS , dtype = float ) y = np . zeros ( NBINS , dtype = float ) w = np . zeros ( NBINS , dtype = float ) for i in range ( NBINS ): w [ i ] = ( bin == i ) . sum () if w [ i ] == 0 : continue x [ i ] = np . median ( pheight [ bin == i ]) y [ i ] = np . median ( phase [ bin == i ]) nonempty = w > 0 # Use sp.interpolate.UnivariateSpline because it can make an approximating # spline. But then use its x/y data and knots to create a Mass CubicSpline, # because that one can have natural boundary conditions instead of insane # cubic functions in the extrapolation. if nonempty . sum () > 1 : spline_order = min ( 3 , nonempty . sum () - 1 ) crazy_spline = sp . interpolate . UnivariateSpline ( x [ nonempty ], y [ nonempty ], w = w [ nonempty ] * ( 12 **- 0.5 ), k = spline_order ) phase_uniformifier_x = crazy_spline . _data [ 0 ] phase_uniformifier_y = crazy_spline . _data [ 1 ] else : phase_uniformifier_x = np . array ([ 0 , 0 , 0 , 0 ]) phase_uniformifier_y = np . array ([ 0 , 0 , 0 , 0 ]) return PhaseCorrector ( phase_uniformifier_x , phase_uniformifier_y , corrections , indicatorName , uncorrectedName ) Phase correction step, Mass-style. PhaseCorrectMassStep dataclass Bases: RecipeStep Perform a Mass-style phase correction step. Source code in mass2/core/phase_correct_steps.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @dataclass ( frozen = True ) class PhaseCorrectMassStep ( RecipeStep ): \"\"\"Perform a Mass-style phase correction step.\"\"\" line_names : list [ str ] line_energies : list [ float ] previous_step_index : int phase_corrector : PhaseCorrector def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the phase-corrected pulse height and return a new DataFrame.\"\"\" # since we only need to load two columns I'm assuming we can fit them in memory and just # loading them whole # if it becomes an issues, use iter_slices or # add a user defined funciton in rust indicator_col , uncorrected_col = self . inputs corrected_col = self . output [ 0 ] indicator = df [ indicator_col ] . to_numpy () uncorrected = df [ uncorrected_col ] . to_numpy () corrected = self . phase_corrector ( indicator , uncorrected ) series = pl . Series ( corrected_col , corrected ) df2 = df . with_columns ( series ) return df2 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a diagnostic plot of the phase correction.\"\"\" indicator_col , uncorrected_col = self . inputs df_small = df_after . lazy () . filter ( self . good_expr ) . filter ( self . use_expr ) . select ( self . inputs + self . output ) . collect () mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ uncorrected_col ]) mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ self . output [ 0 ]], plt . gca (), ) plt . legend () plt . tight_layout () return plt . gca () calc_from_df ( df ) Calculate the phase-corrected pulse height and return a new DataFrame. Source code in mass2/core/phase_correct_steps.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the phase-corrected pulse height and return a new DataFrame.\"\"\" # since we only need to load two columns I'm assuming we can fit them in memory and just # loading them whole # if it becomes an issues, use iter_slices or # add a user defined funciton in rust indicator_col , uncorrected_col = self . inputs corrected_col = self . output [ 0 ] indicator = df [ indicator_col ] . to_numpy () uncorrected = df [ uncorrected_col ] . to_numpy () corrected = self . phase_corrector ( indicator , uncorrected ) series = pl . Series ( corrected_col , corrected ) df2 = df . with_columns ( series ) return df2 dbg_plot ( df_after , ** kwargs ) Make a diagnostic plot of the phase correction. Source code in mass2/core/phase_correct_steps.py 40 41 42 43 44 45 46 47 48 49 50 51 52 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a diagnostic plot of the phase correction.\"\"\" indicator_col , uncorrected_col = self . inputs df_small = df_after . lazy () . filter ( self . good_expr ) . filter ( self . use_expr ) . select ( self . inputs + self . output ) . collect () mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ uncorrected_col ]) mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ self . output [ 0 ]], plt . gca (), ) plt . legend () plt . tight_layout () return plt . gca () phase_correct_mass_specific_lines ( ch , indicator_col , uncorrected_col , corrected_col , previous_step_index , line_names , use_expr ) Perform a Mass-style phase correction step using specific lines. Source code in mass2/core/phase_correct_steps.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def phase_correct_mass_specific_lines ( ch : Channel , indicator_col : str , uncorrected_col : str , corrected_col : str , previous_step_index : int , line_names : Iterable [ str | float ], use_expr : pl . Expr , ) -> PhaseCorrectMassStep : \"\"\"Perform a Mass-style phase correction step using specific lines.\"\"\" previous_step , previous_step_index = ch . get_step ( previous_step_index ) assert hasattr ( previous_step , \"energy2ph\" ) ( line_names , line_energies ) = mass2 . calibration . algorithms . line_names_and_energies ( line_names ) line_positions = [ previous_step . energy2ph ( line_energy ) for line_energy in line_energies ] [ indicator , uncorrected ] = ch . good_serieses ([ indicator_col , uncorrected_col ], use_expr = use_expr ) phase_corrector = mass2 . core . phase_correct . phase_correct ( indicator . to_numpy (), uncorrected . to_numpy (), line_positions , indicatorName = indicator_col , uncorrectedName = uncorrected_col , ) return PhaseCorrectMassStep ( inputs = [ indicator_col , uncorrected_col ], output = [ corrected_col ], good_expr = ch . good_expr , use_expr = use_expr , line_names = line_names , line_energies = line_energies , previous_step_index = previous_step_index , phase_corrector = phase_corrector , ) Pulse summarizing algorithms. fit_pulse_2exp_with_tail ( data , npre , dt = 1 , guess_tau = None ) Fit a pulse shape to data using two exponentials plus an exponential tail. Source code in mass2/core/pulse_algorithms.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def fit_pulse_2exp_with_tail ( data : ArrayLike , npre : int , dt : float = 1 , guess_tau : float | None = None ) -> LineModelResult : \"\"\"Fit a pulse shape to data using two exponentials plus an exponential tail.\"\"\" data = np . asarray ( data ) if guess_tau is None : guess_tau = dt * len ( data ) / 5 model = lmfit . Model ( pulse_2exp_with_tail ) baseline = np . amin ( data ) params = model . make_params ( t0 = npre * dt , a_tail = data [ 0 ] - baseline , baseline = baseline , a = np . amax ( data ) - baseline , tau_tail = guess_tau , tau_rise = guess_tau , tau_fall_factor = 2.0 , ) params [ \"a_tail\" ] . set ( min = 0 ) params [ \"a\" ] . set ( min = 0 ) params [ \"tau_tail\" ] . set ( min = dt / 5 ) params [ \"tau_rise\" ] . set ( min = dt / 5 ) params [ \"tau_fall_factor\" ] . set ( min = 1 ) params . add ( \"tau_fall\" , expr = \"tau_rise*tau_fall_factor\" ) result = model . fit ( data , params , t = np . arange ( len ( data )) * dt ) return result pulse_2exp_with_tail ( t , t0 , a_tail , tau_tail , a , tau_rise , tau_fall_factor , baseline ) Create a pulse shape from two exponentials plus an exponential tail. Source code in mass2/core/pulse_algorithms.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def pulse_2exp_with_tail ( t : ArrayLike , t0 : float , a_tail : float , tau_tail : float , a : float , tau_rise : float , tau_fall_factor : float , baseline : float ) -> NDArray : \"\"\"Create a pulse shape from two exponentials plus an exponential tail.\"\"\" tt = np . asarray ( t ) - t0 tau_fall = tau_rise * tau_fall_factor assert tau_fall_factor >= 1 if tau_fall_factor > 1 : # location of peak t_peak = ( tau_rise * tau_fall ) / ( tau_fall - tau_rise ) * np . log ( tau_fall / tau_rise ) # value at peak max_val = np . exp ( - t_peak / tau_fall ) - np . exp ( - t_peak / tau_rise ) else : # tau_fall == tau_rise max_val = 1 / np . e return ( a_tail * np . exp ( - tt / tau_tail ) / np . exp ( - tt [ 0 ] / tau_tail ) # normalized tail + a * ( np . exp ( - tt / tau_fall ) - np . exp ( - tt / tau_rise )) * np . greater ( tt , 0 ) / max_val + baseline ) summarize_data_numba ( rawdata , timebase , peak_samplenumber , pretrigger_ignore_samples , nPresamples ) Summarize one segment of the data file, loading it into cache. Source code in mass2/core/pulse_algorithms.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 @njit def summarize_data_numba ( # noqa: PLR0914 rawdata : NDArray [ np . uint16 ], timebase : float , peak_samplenumber : int , pretrigger_ignore_samples : int , nPresamples : int , ) -> ResultArrayType : \"\"\"Summarize one segment of the data file, loading it into cache.\"\"\" nPulses = rawdata . shape [ 0 ] nSamples = rawdata . shape [ 1 ] e_nPresamples = nPresamples - pretrigger_ignore_samples # Create the structured array for results results = np . zeros ( nPulses , dtype = result_dtype ) for j in range ( nPulses ): pulse = rawdata [ j , :] pretrig_sum = 0.0 pretrig_rms_sum = 0.0 pulse_sum = 0.0 pulse_rms_sum = 0.0 promptness_sum = 0.0 peak_value = 0 peak_index = 0 min_value = np . iinfo ( np . uint16 ) . max s_prompt = nPresamples + 2 e_prompt = nPresamples + 8 for k in range ( nSamples ): signal = pulse [ k ] if signal > peak_value : peak_value = signal peak_index = k min_value = min ( min_value , signal ) if k < e_nPresamples : pretrig_sum += signal pretrig_rms_sum += signal ** 2 if s_prompt <= k < e_prompt : promptness_sum += signal if k == nPresamples - 1 : ptm = pretrig_sum / e_nPresamples ptrms = np . sqrt ( pretrig_rms_sum / e_nPresamples - ptm ** 2 ) if signal - ptm > 4.3 * ptrms : e_prompt -= 1 s_prompt -= 1 results [ \"shift1\" ][ j ] = 1 else : results [ \"shift1\" ][ j ] = 0 if k >= nPresamples - 1 : pulse_sum += signal pulse_rms_sum += signal ** 2 results [ \"pretrig_mean\" ][ j ] = ptm results [ \"pretrig_rms\" ][ j ] = ptrms if ptm < peak_value : peak_value -= int ( ptm + 0.5 ) results [ \"promptness\" ][ j ] = ( promptness_sum / 6.0 - ptm ) / peak_value results [ \"peak_value\" ][ j ] = peak_value results [ \"peak_index\" ][ j ] = peak_index else : results [ \"promptness\" ][ j ] = 0.0 results [ \"peak_value\" ][ j ] = 0 results [ \"peak_index\" ][ j ] = 0 results [ \"min_value\" ][ j ] = min_value pulse_avg = pulse_sum / ( nSamples - nPresamples + 1 ) - ptm results [ \"pulse_average\" ][ j ] = pulse_avg results [ \"pulse_rms\" ][ j ] = np . sqrt ( pulse_rms_sum / ( nSamples - nPresamples + 1 ) - ptm * pulse_avg * 2 - ptm ** 2 ) low_th = int ( 0.1 * peak_value + ptm ) high_th = int ( 0.9 * peak_value + ptm ) k = nPresamples low_value = high_value = pulse [ k ] while k < nSamples : signal = pulse [ k ] if signal > low_th : low_idx = k low_value = signal break k += 1 high_value = low_value high_idx = low_idx while k < nSamples : signal = pulse [ k ] if signal > high_th : high_idx = k - 1 high_value = pulse [ high_idx ] break k += 1 if high_value > low_value : results [ \"rise_time\" ][ j ] = timebase * ( high_idx - low_idx ) * peak_value / ( high_value - low_value ) else : results [ \"rise_time\" ][ j ] = timebase # The following is quite confusing, but it appears to be equivalent to # slope = -2 * pulse[peak_samplenumber:-4] # slope -= pulse[peak_samplenumber+1:-3] # slope += pulse[peak_samplenumber+3:-1] # slope += 2*pulse[peak_samplenumber+4:] # slope = np.minimum(slope[2:], slope[:-2]) # results[\"postpeak_deriv\"][j] = 0.1 * np.max(slope) # TODO: consider replacing, if the above is not slower? f0 , f1 , f3 , f4 = 2 , 1 , - 1 , - 2 s0 , s1 , s2 , s3 = ( pulse [ peak_samplenumber ], pulse [ peak_samplenumber + 1 ], pulse [ peak_samplenumber + 2 ], pulse [ peak_samplenumber + 3 ], ) s4 = pulse [ peak_samplenumber + 4 ] t0 = f4 * s0 + f3 * s1 + f1 * s3 + f0 * s4 s0 , s1 , s2 , s3 = s1 , s2 , s3 , s4 s4 = pulse [ peak_samplenumber + 5 ] t1 = f4 * s0 + f3 * s1 + f1 * s3 + f0 * s4 t_max_deriv = np . iinfo ( np . int32 ) . min for k in range ( peak_samplenumber + 6 , nSamples ): s0 , s1 , s2 , s3 = s1 , s2 , s3 , s4 s4 = pulse [ k ] t2 = f4 * s0 + f3 * s1 + f1 * s3 + f0 * s4 t3 = min ( t2 , t0 ) t_max_deriv = max ( t_max_deriv , t3 ) t0 , t1 = t1 , t2 results [ \"postpeak_deriv\" ][ j ] = 0.1 * t_max_deriv return results Pulse model object, to hold a low-dimensional linear basis able to express all normal pulses, PulseModel Object to hold a \"pulse model\", meaning a low-dimensional linear basis to express \"all\" pulses, along with a projector such that projector.dot(basis) is the identity matrix. Also has the capacity to store to and restore from HDF5, and the ability to compute additional basis elements and corresponding projectors with method _additional_projectors_tsvd Source code in mass2/core/pulse_model.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class PulseModel : \"\"\"Object to hold a \"pulse model\", meaning a low-dimensional linear basis to express \"all\" pulses, along with a projector such that projector.dot(basis) is the identity matrix. Also has the capacity to store to and restore from HDF5, and the ability to compute additional basis elements and corresponding projectors with method _additional_projectors_tsvd\"\"\" version = 2 def __init__ ( # noqa: PLR0917 self , projectors_so_far : NDArray , basis_so_far : NDArray , n_basis : int , pulses_for_svd : NDArray , v_dv : float , pretrig_rms_median : float , pretrig_rms_sigma : float , file_name : str , extra_n_basis_5lag : int , f_5lag : NDArray , average_pulse_for_5lag : NDArray , noise_psd : NDArray , noise_psd_delta_f : NDArray , noise_autocorr : NDArray , _from_hdf5 : bool = False , ): self . pulses_for_svd = pulses_for_svd self . n_basis = n_basis dn = n_basis - extra_n_basis_5lag if projectors_so_far . shape [ 0 ] < dn : self . projectors , self . basis = self . _additional_projectors_tsvd ( projectors_so_far , basis_so_far , dn , pulses_for_svd ) elif ( projectors_so_far . shape [ 0 ] == dn ) or _from_hdf5 : self . projectors , self . basis = projectors_so_far , basis_so_far else : # don't throw error on s = f \"n_basis-extra_n_basis_5lag= { dn } < projectors_so_far.shape[0] = { projectors_so_far . shape [ 0 ] } \" s += f \", extra_n_basis_5lag= { extra_n_basis_5lag } \" raise Exception ( s ) if ( not _from_hdf5 ) and ( extra_n_basis_5lag > 0 ): filters_5lag = np . zeros (( len ( f_5lag ) + 4 , 5 )) for i in range ( 5 ): if i < 4 : filters_5lag [ i : - 4 + i , i ] = projectors_so_far [ 2 , 2 : - 2 ] else : filters_5lag [ i :, i ] = projectors_so_far [ 2 , 2 : - 2 ] self . projectors , self . basis = self . _additional_projectors_tsvd ( self . projectors , self . basis , n_basis , filters_5lag ) self . v_dv = v_dv self . pretrig_rms_median = pretrig_rms_median self . pretrig_rms_sigma = pretrig_rms_sigma self . file_name = str ( file_name ) self . extra_n_basis_5lag = extra_n_basis_5lag self . f_5lag = f_5lag self . average_pulse_for_5lag = average_pulse_for_5lag self . noise_psd = noise_psd self . noise_psd_delta_f = noise_psd_delta_f self . noise_autocorr = noise_autocorr def toHDF5 ( self , hdf5_group : h5py . Group , save_inverted : bool ) -> None : \"\"\"Save the pulse model to an HDF5 group.\"\"\" projectors , basis = self . projectors [()], self . basis [()] if save_inverted : # flip every component except the mean component if data is being inverted basis [:, 1 :] *= - 1 projectors [ 1 :, :] *= - 1 # projectors is MxN, where N is samples/record and M the number of basis elements # basis is NxM hdf5_group [ \"svdbasis/projectors\" ] = projectors hdf5_group [ \"svdbasis/basis\" ] = basis hdf5_group [ \"svdbasis/v_dv\" ] = self . v_dv hdf5_group [ \"svdbasis/training_pulses_for_plots\" ] = self . pulses_for_svd hdf5_group [ \"svdbasis/was_saved_inverted\" ] = save_inverted hdf5_group [ \"svdbasis/pretrig_rms_median\" ] = self . pretrig_rms_median hdf5_group [ \"svdbasis/pretrig_rms_sigma\" ] = self . pretrig_rms_sigma hdf5_group [ \"svdbasis/version\" ] = self . version hdf5_group [ \"svdbasis/file_name\" ] = self . file_name hdf5_group [ \"svdbasis/extra_n_basis_5lag\" ] = self . extra_n_basis_5lag hdf5_group [ \"svdbasis/5lag_filter\" ] = self . f_5lag hdf5_group [ \"svdbasis/average_pulse_for_5lag\" ] = self . average_pulse_for_5lag hdf5_group [ \"svdbasis/noise_psd\" ] = self . noise_psd hdf5_group [ \"svdbasis/noise_psd_delta_f\" ] = self . noise_psd_delta_f hdf5_group [ \"svdbasis/noise_autocorr\" ] = self . noise_autocorr @classmethod def fromHDF5 ( cls , hdf5_group : h5py . Group ) -> \"PulseModel\" : \"\"\"Restore a pulse model from an HDF5 group.\"\"\" projectors = hdf5_group [ \"svdbasis/projectors\" ][()] n_basis = projectors . shape [ 0 ] basis = hdf5_group [ \"svdbasis/basis\" ][()] v_dv = hdf5_group [ \"svdbasis/v_dv\" ][()] pulses_for_svd = hdf5_group [ \"svdbasis/training_pulses_for_plots\" ][()] pretrig_rms_median = hdf5_group [ \"svdbasis/pretrig_rms_median\" ][()] pretrig_rms_sigma = hdf5_group [ \"svdbasis/pretrig_rms_sigma\" ][()] version = hdf5_group [ \"svdbasis/version\" ][()] file_name = tostr ( hdf5_group [ \"svdbasis/file_name\" ][()]) extra_n_basis_5lag = hdf5_group [ \"svdbasis/extra_n_basis_5lag\" ][()] f_5lag = hdf5_group [ \"svdbasis/5lag_filter\" ][()] average_pulse_for_5lag = hdf5_group [ \"svdbasis/average_pulse_for_5lag\" ][()] noise_psd = hdf5_group [ \"svdbasis/noise_psd\" ][()] noise_psd_delta_f = hdf5_group [ \"svdbasis/noise_psd_delta_f\" ][()] noise_autocorr = hdf5_group [ \"svdbasis/noise_autocorr\" ][()] if version != cls . version : raise Exception ( f \"loading not implemented for other versions, version= { version } \" ) return cls ( projectors , basis , n_basis , pulses_for_svd , v_dv , pretrig_rms_median , pretrig_rms_sigma , file_name , extra_n_basis_5lag , f_5lag , average_pulse_for_5lag , noise_psd , noise_psd_delta_f , noise_autocorr , _from_hdf5 = True , ) @staticmethod def _additional_projectors_tsvd ( projectors : NDArray , basis : NDArray , n_basis : int , pulses_for_svd : NDArray ) -> tuple [ NDArray , NDArray ]: \"\"\" Given an existing basis with projectors, compute a basis with n_basis elements by randomized SVD of the residual elements of the training data in pulses_for_svd. It should be the case that projectors.dot(basis) is approximately the identity matrix. It is assumed that the projectors will have been computed from the basis in some noise-optimal way, say, from optimal filtering. However, the additional basis elements will be computed from a standard (non-noise-weighted) SVD, and the additional projectors will be computed without noise optimization. The projectors and basis will be ordered as: mean, deriv_ike, pulse_like, any svd components... \"\"\" # Check sanity of inputs n_samples , n_existing = basis . shape assert ( n_existing , n_samples ) == projectors . shape assert n_basis >= n_existing if n_basis == n_existing : return projectors , basis mpc = np . matmul ( projectors , pulses_for_svd ) # modeled pulse coefs mp = np . matmul ( basis , mpc ) # modeled pulse residuals = pulses_for_svd - mp Q = mass2 . mathstat . utilities . find_range_randomly ( residuals , n_basis - n_existing ) projectors2 = np . linalg . pinv ( Q ) # = Q.T, perhaps?? projectors2 -= projectors2 . dot ( basis ) . dot ( projectors ) basis = np . hstack ([ basis , Q ]) projectors = np . vstack ([ projectors , projectors2 ]) return projectors , basis def labels ( self ) -> list [ str ]: \"\"\"Return a list of labels for the basis elements.\"\"\" labels = [ \"const\" , \"deriv\" , \"pulse\" ] for i in range ( self . n_basis - 3 ): if i > self . n_basis - 3 - self . extra_n_basis_5lag : labels += [ f \"5lag { i + 2 - self . extra_n_basis_5lag } \" ] else : labels += [ f \"svd { i } \" ] return labels def plot ( self , fig1 : plt . Axes | None = None , fig2 : plt . Axes | None = None ) -> None : \"\"\"Plot a pulse model\"\"\" # plots information about a pulse model # fig1 and fig2 are optional matplotlib.pyplot (plt) figures if you need to embed the plots. # you can pass in the reference like fig=plt.figure() call or the figure's number, e.g. fig.number # fig1 has modeled pulse vs true pulse # fig2 has projectors, basis, \"from ljh\", residuals, and a measure of \"wrongness\" labels = self . labels () mpc = np . matmul ( self . projectors , self . pulses_for_svd ) mp = np . matmul ( self . basis , mpc ) residuals = self . pulses_for_svd - mp if fig1 is None : fig = plt . figure ( figsize = ( 10 , 14 )) else : fig = plt . figure ( fig1 ) plt . subplot ( 511 ) plt . plot ( self . projectors [:: - 1 , :] . T ) plt . title ( \"projectors\" ) # projector_scale = np.amax(np.abs(self.projectors[2, :])) # plt.ylim(-2*projector_scale, 2*projector_scale) plt . legend ( labels [:: - 1 ]) plt . grid ( True ) plt . subplot ( 512 ) plt . plot ( self . basis [:, :: - 1 ]) plt . title ( \"basis\" ) plt . legend ( labels [:: - 1 ]) plt . grid ( True ) plt . subplot ( 513 ) plt . plot ( self . pulses_for_svd [:, : 10 ]) plt . title ( \"from ljh\" ) plt . legend ([ f \" { i } \" for i in range ( 10 )]) plt . grid ( True ) plt . subplot ( 514 ) plt . plot ( residuals [:, : 10 ]) plt . title ( \"residuals\" ) plt . legend ([ f \" { i } \" for i in range ( 10 )]) plt . grid ( True ) should_be_identity = np . matmul ( self . projectors , self . basis ) identity = np . identity ( self . n_basis ) wrongness = np . abs ( should_be_identity - identity ) wrongness [ wrongness < 1e-20 ] = 1e-20 # avoid warnings plt . subplot ( 515 ) plt . imshow ( np . log10 ( wrongness )) plt . title ( \"log10(abs(projectors*basis-identity))\" ) plt . colorbar () fig . suptitle ( self . file_name ) if fig2 is None : plt . figure ( figsize = ( 10 , 14 )) else : plt . figure ( fig2 ) plt . plot ( self . pulses_for_svd [:, 0 ], label = \"from ljh index 0\" ) plt . plot ( mp [:, 0 ], label = \"modeled pulse index 0\" ) plt . legend () plt . title ( \"modeled pulse vs true pulse\" ) fromHDF5 ( hdf5_group ) classmethod Restore a pulse model from an HDF5 group. Source code in mass2/core/pulse_model.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @classmethod def fromHDF5 ( cls , hdf5_group : h5py . Group ) -> \"PulseModel\" : \"\"\"Restore a pulse model from an HDF5 group.\"\"\" projectors = hdf5_group [ \"svdbasis/projectors\" ][()] n_basis = projectors . shape [ 0 ] basis = hdf5_group [ \"svdbasis/basis\" ][()] v_dv = hdf5_group [ \"svdbasis/v_dv\" ][()] pulses_for_svd = hdf5_group [ \"svdbasis/training_pulses_for_plots\" ][()] pretrig_rms_median = hdf5_group [ \"svdbasis/pretrig_rms_median\" ][()] pretrig_rms_sigma = hdf5_group [ \"svdbasis/pretrig_rms_sigma\" ][()] version = hdf5_group [ \"svdbasis/version\" ][()] file_name = tostr ( hdf5_group [ \"svdbasis/file_name\" ][()]) extra_n_basis_5lag = hdf5_group [ \"svdbasis/extra_n_basis_5lag\" ][()] f_5lag = hdf5_group [ \"svdbasis/5lag_filter\" ][()] average_pulse_for_5lag = hdf5_group [ \"svdbasis/average_pulse_for_5lag\" ][()] noise_psd = hdf5_group [ \"svdbasis/noise_psd\" ][()] noise_psd_delta_f = hdf5_group [ \"svdbasis/noise_psd_delta_f\" ][()] noise_autocorr = hdf5_group [ \"svdbasis/noise_autocorr\" ][()] if version != cls . version : raise Exception ( f \"loading not implemented for other versions, version= { version } \" ) return cls ( projectors , basis , n_basis , pulses_for_svd , v_dv , pretrig_rms_median , pretrig_rms_sigma , file_name , extra_n_basis_5lag , f_5lag , average_pulse_for_5lag , noise_psd , noise_psd_delta_f , noise_autocorr , _from_hdf5 = True , ) labels () Return a list of labels for the basis elements. Source code in mass2/core/pulse_model.py 174 175 176 177 178 179 180 181 182 def labels ( self ) -> list [ str ]: \"\"\"Return a list of labels for the basis elements.\"\"\" labels = [ \"const\" , \"deriv\" , \"pulse\" ] for i in range ( self . n_basis - 3 ): if i > self . n_basis - 3 - self . extra_n_basis_5lag : labels += [ f \"5lag { i + 2 - self . extra_n_basis_5lag } \" ] else : labels += [ f \"svd { i } \" ] return labels plot ( fig1 = None , fig2 = None ) Plot a pulse model Source code in mass2/core/pulse_model.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def plot ( self , fig1 : plt . Axes | None = None , fig2 : plt . Axes | None = None ) -> None : \"\"\"Plot a pulse model\"\"\" # plots information about a pulse model # fig1 and fig2 are optional matplotlib.pyplot (plt) figures if you need to embed the plots. # you can pass in the reference like fig=plt.figure() call or the figure's number, e.g. fig.number # fig1 has modeled pulse vs true pulse # fig2 has projectors, basis, \"from ljh\", residuals, and a measure of \"wrongness\" labels = self . labels () mpc = np . matmul ( self . projectors , self . pulses_for_svd ) mp = np . matmul ( self . basis , mpc ) residuals = self . pulses_for_svd - mp if fig1 is None : fig = plt . figure ( figsize = ( 10 , 14 )) else : fig = plt . figure ( fig1 ) plt . subplot ( 511 ) plt . plot ( self . projectors [:: - 1 , :] . T ) plt . title ( \"projectors\" ) # projector_scale = np.amax(np.abs(self.projectors[2, :])) # plt.ylim(-2*projector_scale, 2*projector_scale) plt . legend ( labels [:: - 1 ]) plt . grid ( True ) plt . subplot ( 512 ) plt . plot ( self . basis [:, :: - 1 ]) plt . title ( \"basis\" ) plt . legend ( labels [:: - 1 ]) plt . grid ( True ) plt . subplot ( 513 ) plt . plot ( self . pulses_for_svd [:, : 10 ]) plt . title ( \"from ljh\" ) plt . legend ([ f \" { i } \" for i in range ( 10 )]) plt . grid ( True ) plt . subplot ( 514 ) plt . plot ( residuals [:, : 10 ]) plt . title ( \"residuals\" ) plt . legend ([ f \" { i } \" for i in range ( 10 )]) plt . grid ( True ) should_be_identity = np . matmul ( self . projectors , self . basis ) identity = np . identity ( self . n_basis ) wrongness = np . abs ( should_be_identity - identity ) wrongness [ wrongness < 1e-20 ] = 1e-20 # avoid warnings plt . subplot ( 515 ) plt . imshow ( np . log10 ( wrongness )) plt . title ( \"log10(abs(projectors*basis-identity))\" ) plt . colorbar () fig . suptitle ( self . file_name ) if fig2 is None : plt . figure ( figsize = ( 10 , 14 )) else : plt . figure ( fig2 ) plt . plot ( self . pulses_for_svd [:, 0 ], label = \"from ljh index 0\" ) plt . plot ( mp [:, 0 ], label = \"modeled pulse index 0\" ) plt . legend () plt . title ( \"modeled pulse vs true pulse\" ) toHDF5 ( hdf5_group , save_inverted ) Save the pulse model to an HDF5 group. Source code in mass2/core/pulse_model.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def toHDF5 ( self , hdf5_group : h5py . Group , save_inverted : bool ) -> None : \"\"\"Save the pulse model to an HDF5 group.\"\"\" projectors , basis = self . projectors [()], self . basis [()] if save_inverted : # flip every component except the mean component if data is being inverted basis [:, 1 :] *= - 1 projectors [ 1 :, :] *= - 1 # projectors is MxN, where N is samples/record and M the number of basis elements # basis is NxM hdf5_group [ \"svdbasis/projectors\" ] = projectors hdf5_group [ \"svdbasis/basis\" ] = basis hdf5_group [ \"svdbasis/v_dv\" ] = self . v_dv hdf5_group [ \"svdbasis/training_pulses_for_plots\" ] = self . pulses_for_svd hdf5_group [ \"svdbasis/was_saved_inverted\" ] = save_inverted hdf5_group [ \"svdbasis/pretrig_rms_median\" ] = self . pretrig_rms_median hdf5_group [ \"svdbasis/pretrig_rms_sigma\" ] = self . pretrig_rms_sigma hdf5_group [ \"svdbasis/version\" ] = self . version hdf5_group [ \"svdbasis/file_name\" ] = self . file_name hdf5_group [ \"svdbasis/extra_n_basis_5lag\" ] = self . extra_n_basis_5lag hdf5_group [ \"svdbasis/5lag_filter\" ] = self . f_5lag hdf5_group [ \"svdbasis/average_pulse_for_5lag\" ] = self . average_pulse_for_5lag hdf5_group [ \"svdbasis/noise_psd\" ] = self . noise_psd hdf5_group [ \"svdbasis/noise_psd_delta_f\" ] = self . noise_psd_delta_f hdf5_group [ \"svdbasis/noise_autocorr\" ] = self . noise_autocorr Define RecipeStep and Recipe classes for processing pulse data in a sequence of steps. CategorizeStep dataclass Bases: RecipeStep A step to categorize pulses into discrete categories based on conditions given in a dictionary mapping category names to polars expressions. The first condition must be True, to be used as a fallback. Source code in mass2/core/recipe.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @dataclass ( frozen = True ) class CategorizeStep ( RecipeStep ): \"\"\"A step to categorize pulses into discrete categories based on conditions given in a dictionary mapping category names to polars expressions. The first condition must be True, to be used as a fallback.\"\"\" category_condition_dict : dict [ str , pl . Expr ] def __post_init__ ( self ) -> None : \"\"\"Verify that the first condition is always True.\"\"\" err_msg = \"The first condition must be True, to be used as a fallback\" first_condition = next ( iter ( self . category_condition_dict . values ())) assert first_condition is True or first_condition . meta . eq ( pl . lit ( True )), err_msg def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the category for each pulse and return a new DataFrame with a column for the category names.\"\"\" output_col = self . output [ 0 ] def categorize_df ( df : pl . DataFrame , category_condition_dict : dict [ str , pl . Expr ], output_col : str ) -> pl . DataFrame : \"\"\"returns a series showing which category each pulse is in pulses will be assigned to the last category for which the condition evaluates to True\"\"\" dtype = pl . Enum ( category_condition_dict . keys ()) physical = np . zeros ( len ( df ), dtype = int ) for category_int , ( category_str , condition_expr ) in enumerate ( category_condition_dict . items ()): if condition_expr is True or condition_expr . meta . eq ( pl . lit ( True )): in_category = np . ones ( len ( df ), dtype = bool ) else : in_category = df . select ( condition_expr ) . fill_null ( False ) . to_numpy () . flatten () assert in_category . dtype == bool physical [ in_category ] = category_int series = pl . Series ( name = output_col , values = physical ) . cast ( dtype ) df = pl . DataFrame ({ output_col : series }) return df df2 = categorize_df ( df , self . category_condition_dict , output_col ) . with_columns ( df ) return df2 __post_init__ () Verify that the first condition is always True. Source code in mass2/core/recipe.py 167 168 169 170 171 def __post_init__ ( self ) -> None : \"\"\"Verify that the first condition is always True.\"\"\" err_msg = \"The first condition must be True, to be used as a fallback\" first_condition = next ( iter ( self . category_condition_dict . values ())) assert first_condition is True or first_condition . meta . eq ( pl . lit ( True )), err_msg calc_from_df ( df ) Calculate the category for each pulse and return a new DataFrame with a column for the category names. Source code in mass2/core/recipe.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the category for each pulse and return a new DataFrame with a column for the category names.\"\"\" output_col = self . output [ 0 ] def categorize_df ( df : pl . DataFrame , category_condition_dict : dict [ str , pl . Expr ], output_col : str ) -> pl . DataFrame : \"\"\"returns a series showing which category each pulse is in pulses will be assigned to the last category for which the condition evaluates to True\"\"\" dtype = pl . Enum ( category_condition_dict . keys ()) physical = np . zeros ( len ( df ), dtype = int ) for category_int , ( category_str , condition_expr ) in enumerate ( category_condition_dict . items ()): if condition_expr is True or condition_expr . meta . eq ( pl . lit ( True )): in_category = np . ones ( len ( df ), dtype = bool ) else : in_category = df . select ( condition_expr ) . fill_null ( False ) . to_numpy () . flatten () assert in_category . dtype == bool physical [ in_category ] = category_int series = pl . Series ( name = output_col , values = physical ) . cast ( dtype ) df = pl . DataFrame ({ output_col : series }) return df df2 = categorize_df ( df , self . category_condition_dict , output_col ) . with_columns ( df ) return df2 ColumnAsNumpyMapStep dataclass Bases: RecipeStep This step is meant for interactive exploration, it takes a column and applies a function to it, and makes a new column with the result. It makes it easy to test functions on a column without having to write a whole new step class, while maintaining the benefit of being able to use the step in a Recipe chain, like replaying steps on another channel. example usage: def my_function(x): ... return x * 2 step = ColumnAsNumpyMapStep(inputs=[\"my_column\"], output=[\"my_new_column\"], f=my_function) ch2 = ch.with_step(step) Source code in mass2/core/recipe.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 @dataclass ( frozen = True ) class ColumnAsNumpyMapStep ( RecipeStep ): \"\"\" This step is meant for interactive exploration, it takes a column and applies a function to it, and makes a new column with the result. It makes it easy to test functions on a column without having to write a whole new step class, while maintaining the benefit of being able to use the step in a Recipe chain, like replaying steps on another channel. example usage: >>> def my_function(x): ... return x * 2 >>> step = ColumnAsNumpyMapStep(inputs=[\"my_column\"], output=[\"my_new_column\"], f=my_function) >>> ch2 = ch.with_step(step) \"\"\" f : Callable [[ np . ndarray ], np . ndarray ] def __post_init__ ( self ) -> None : \"\"\"Check that inputs and outputs are valid (single column each) and that `f` is a callable object.\"\"\" assert len ( self . inputs ) == 1 , \"ColumnMapStep expects exactly one input\" assert len ( self . output ) == 1 , \"ColumnMapStep expects exactly one output\" if not callable ( self . f ): raise ValueError ( f \"f must be a callable, got { self . f } \" ) def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the new column by applying `f` to the input column, returning a new DataFrame.\"\"\" output_col = self . output [ 0 ] output_segments = [] for df_iter in df . select ( self . inputs ) . iter_slices (): series1 = df_iter [ self . inputs [ 0 ]] # Have to apply the function differently when series elements are arrays vs scalars if series1 . dtype . base_type () is pl . Array : output_numpy = np . array ([ self . f ( v . to_numpy ()) for v in series1 ]) else : output_numpy = self . f ( series1 . to_numpy ()) this_output_segment = pl . Series ( output_col , output_numpy ) output_segments . append ( this_output_segment ) combined = pl . concat ( output_segments ) # Put into a DataFrame with one column df2 = pl . DataFrame ({ output_col : combined }) . with_columns ( df ) return df2 __post_init__ () Check that inputs and outputs are valid (single column each) and that f is a callable object. Source code in mass2/core/recipe.py 133 134 135 136 137 138 def __post_init__ ( self ) -> None : \"\"\"Check that inputs and outputs are valid (single column each) and that `f` is a callable object.\"\"\" assert len ( self . inputs ) == 1 , \"ColumnMapStep expects exactly one input\" assert len ( self . output ) == 1 , \"ColumnMapStep expects exactly one output\" if not callable ( self . f ): raise ValueError ( f \"f must be a callable, got { self . f } \" ) calc_from_df ( df ) Calculate the new column by applying f to the input column, returning a new DataFrame. Source code in mass2/core/recipe.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the new column by applying `f` to the input column, returning a new DataFrame.\"\"\" output_col = self . output [ 0 ] output_segments = [] for df_iter in df . select ( self . inputs ) . iter_slices (): series1 = df_iter [ self . inputs [ 0 ]] # Have to apply the function differently when series elements are arrays vs scalars if series1 . dtype . base_type () is pl . Array : output_numpy = np . array ([ self . f ( v . to_numpy ()) for v in series1 ]) else : output_numpy = self . f ( series1 . to_numpy ()) this_output_segment = pl . Series ( output_col , output_numpy ) output_segments . append ( this_output_segment ) combined = pl . concat ( output_segments ) # Put into a DataFrame with one column df2 = pl . DataFrame ({ output_col : combined }) . with_columns ( df ) return df2 PretrigMeanJumpFixStep dataclass Bases: RecipeStep A step to fix jumps in the pretrigger mean by unwrapping the phase angle, a periodic quantity. Source code in mass2/core/recipe.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @dataclass ( frozen = True ) class PretrigMeanJumpFixStep ( RecipeStep ): \"\"\"A step to fix jumps in the pretrigger mean by unwrapping the phase angle, a periodic quantity.\"\"\" period : float def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the jump-corrected pretrigger mean and return a new DataFrame.\"\"\" ptm1 = df [ self . inputs [ 0 ]] . to_numpy () ptm2 = np . unwrap ( ptm1 % self . period , period = self . period ) df2 = pl . DataFrame ({ self . output [ 0 ]: ptm2 }) . with_columns ( df ) return df2 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a diagnostic plot of the pretrigger mean before and after the jump fix.\"\"\" plt . figure () plt . plot ( df_after [ \"timestamp\" ], df_after [ self . inputs [ 0 ]], \".\" , label = self . inputs [ 0 ], ** kwargs ) plt . plot ( df_after [ \"timestamp\" ], df_after [ self . output [ 0 ]], \".\" , label = self . output [ 0 ], ** kwargs ) plt . legend () plt . xlabel ( \"timestamp\" ) plt . ylabel ( \"pretrig mean\" ) plt . tight_layout () return plt . gca () calc_from_df ( df ) Calculate the jump-corrected pretrigger mean and return a new DataFrame. Source code in mass2/core/recipe.py 62 63 64 65 66 67 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the jump-corrected pretrigger mean and return a new DataFrame.\"\"\" ptm1 = df [ self . inputs [ 0 ]] . to_numpy () ptm2 = np . unwrap ( ptm1 % self . period , period = self . period ) df2 = pl . DataFrame ({ self . output [ 0 ]: ptm2 }) . with_columns ( df ) return df2 dbg_plot ( df_after , ** kwargs ) Make a diagnostic plot of the pretrigger mean before and after the jump fix. Source code in mass2/core/recipe.py 69 70 71 72 73 74 75 76 77 78 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a diagnostic plot of the pretrigger mean before and after the jump fix.\"\"\" plt . figure () plt . plot ( df_after [ \"timestamp\" ], df_after [ self . inputs [ 0 ]], \".\" , label = self . inputs [ 0 ], ** kwargs ) plt . plot ( df_after [ \"timestamp\" ], df_after [ self . output [ 0 ]], \".\" , label = self . output [ 0 ], ** kwargs ) plt . legend () plt . xlabel ( \"timestamp\" ) plt . ylabel ( \"pretrig mean\" ) plt . tight_layout () return plt . gca () Recipe dataclass Bases: Sequence [ RecipeStep ] A sequence of RecipeStep objects to be applied in order to a DataFrame. Source code in mass2/core/recipe.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 @dataclass ( frozen = True ) class Recipe ( Sequence [ RecipeStep ]): \"\"\"A sequence of RecipeStep objects to be applied in order to a DataFrame.\"\"\" steps : list [ RecipeStep ] # TODO: leaves many optimizations on the table, but is very simple # 1. we could calculate filt_value_5lag and filt_phase_5lag at the same time # 2. we could calculate intermediate quantities optionally and not materialize all of them def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"return a dataframe with all the newly calculated info\" for step in self . steps : df = step . calc_from_df ( df ) . with_columns ( df ) return df @classmethod def new_empty ( cls ) -> \"Recipe\" : \"\"\"Create a new empty Recipe.\"\"\" return cls ([]) @overload def __getitem__ ( self , key : int ) -> RecipeStep : \"\"\"Return the step at a given index.\"\"\" ... @overload def __getitem__ ( self , key : slice ) -> Sequence [ RecipeStep ]: \"\"\"Return the steps at a given slice of indices.\"\"\" ... def __getitem__ ( self , key : int | slice ) -> RecipeStep | Sequence [ RecipeStep ]: \"\"\"Return the step at the given index, or the steps at a slice of steps.\"\"\" return self . steps [ key ] def __len__ ( self ) -> int : \"\"\"Return the number of steps in the recipe.\"\"\" return len ( self . steps ) def with_step ( self , step : RecipeStep ) -> \"Recipe\" : \"\"\"Create a new Recipe with the given step added to the end.\"\"\" # return a new Recipe with the step added, no mutation! return Recipe ( self . steps + [ step ]) def trim_dead_ends ( self , required_fields : Iterable [ str ] | str | None , drop_debug : bool = True ) -> \"Recipe\" : \"\"\"Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with `drop_debug=True`). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in `required_fields`. The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the `required_fields` (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters ---------- required_fields : Iterable[str] | str | None Steps will be preserved if any of their outputs are among `required_fields`, or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug : bool Whether to run `step.drop_debug()` to remove debugging information from the preserved steps. Returns ------- Recipe A copy of `self`, except that any steps not required to compute any of `required_fields` are omitted. \"\"\" if isinstance ( required_fields , str ): required_fields = [ required_fields ] nsteps = len ( self ) required = np . zeros ( nsteps , dtype = bool ) # The easiest approach is to traverse the steps from last to first to build our list of required # fields, because necessarily no later step can produce the inputs needed by an earlier step. if required_fields is None : required [:] = True else : all_fields_out : set [ str ] = set ( required_fields ) for istep in range ( nsteps - 1 , - 1 , - 1 ): step = self [ istep ] for field in step . output : if field in all_fields_out : required [ istep ] = True all_fields_out . update ( step . inputs ) break if not np . any ( required ): # If this error ever because a problem, where user _acutally_ wants an empty series of steps # to be a non-err, then add argument `error_on_empty_output=True` to this method. raise ValueError ( \"trim_dead_ends found no steps to be preserved\" ) steps = [] for i in range ( nsteps ): if required [ i ]: if drop_debug : steps . append ( self [ i ] . drop_debug ()) else : steps . append ( self [ i ]) return Recipe ( steps ) __getitem__ ( key ) __getitem__ ( key : int ) -> RecipeStep __getitem__ ( key : slice ) -> Sequence [ RecipeStep ] Return the step at the given index, or the steps at a slice of steps. Source code in mass2/core/recipe.py 242 243 244 def __getitem__ ( self , key : int | slice ) -> RecipeStep | Sequence [ RecipeStep ]: \"\"\"Return the step at the given index, or the steps at a slice of steps.\"\"\" return self . steps [ key ] __len__ () Return the number of steps in the recipe. Source code in mass2/core/recipe.py 246 247 248 def __len__ ( self ) -> int : \"\"\"Return the number of steps in the recipe.\"\"\" return len ( self . steps ) calc_from_df ( df ) return a dataframe with all the newly calculated info Source code in mass2/core/recipe.py 221 222 223 224 225 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"return a dataframe with all the newly calculated info\" for step in self . steps : df = step . calc_from_df ( df ) . with_columns ( df ) return df new_empty () classmethod Create a new empty Recipe. Source code in mass2/core/recipe.py 227 228 229 230 @classmethod def new_empty ( cls ) -> \"Recipe\" : \"\"\"Create a new empty Recipe.\"\"\" return cls ([]) trim_dead_ends ( required_fields , drop_debug = True ) Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with drop_debug=True ). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in required_fields . The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the required_fields (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters: required_fields ( Iterable [ str ] | str | None ) \u2013 Steps will be preserved if any of their outputs are among required_fields , or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug ( bool , default: True ) \u2013 Whether to run step.drop_debug() to remove debugging information from the preserved steps. Returns: Recipe \u2013 A copy of self , except that any steps not required to compute any of required_fields are omitted. Source code in mass2/core/recipe.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def trim_dead_ends ( self , required_fields : Iterable [ str ] | str | None , drop_debug : bool = True ) -> \"Recipe\" : \"\"\"Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with `drop_debug=True`). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in `required_fields`. The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the `required_fields` (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters ---------- required_fields : Iterable[str] | str | None Steps will be preserved if any of their outputs are among `required_fields`, or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug : bool Whether to run `step.drop_debug()` to remove debugging information from the preserved steps. Returns ------- Recipe A copy of `self`, except that any steps not required to compute any of `required_fields` are omitted. \"\"\" if isinstance ( required_fields , str ): required_fields = [ required_fields ] nsteps = len ( self ) required = np . zeros ( nsteps , dtype = bool ) # The easiest approach is to traverse the steps from last to first to build our list of required # fields, because necessarily no later step can produce the inputs needed by an earlier step. if required_fields is None : required [:] = True else : all_fields_out : set [ str ] = set ( required_fields ) for istep in range ( nsteps - 1 , - 1 , - 1 ): step = self [ istep ] for field in step . output : if field in all_fields_out : required [ istep ] = True all_fields_out . update ( step . inputs ) break if not np . any ( required ): # If this error ever because a problem, where user _acutally_ wants an empty series of steps # to be a non-err, then add argument `error_on_empty_output=True` to this method. raise ValueError ( \"trim_dead_ends found no steps to be preserved\" ) steps = [] for i in range ( nsteps ): if required [ i ]: if drop_debug : steps . append ( self [ i ] . drop_debug ()) else : steps . append ( self [ i ]) return Recipe ( steps ) with_step ( step ) Create a new Recipe with the given step added to the end. Source code in mass2/core/recipe.py 250 251 252 253 def with_step ( self , step : RecipeStep ) -> \"Recipe\" : \"\"\"Create a new Recipe with the given step added to the end.\"\"\" # return a new Recipe with the step added, no mutation! return Recipe ( self . steps + [ step ]) RecipeStep dataclass Represent one step in a data processing recipe. A step has inputs, outputs, and a calculation method. It also has a good_expr and use_expr that can be used to filter the data before processing. This is an abstract base class, subclasses should implement calc_from_df and dbg_plot. Source code in mass2/core/recipe.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @dataclass ( frozen = True ) class RecipeStep : \"\"\"Represent one step in a data processing recipe. A step has inputs, outputs, and a calculation method. It also has a good_expr and use_expr that can be used to filter the data before processing. This is an abstract base class, subclasses should implement calc_from_df and dbg_plot. \"\"\" inputs : list [ str ] output : list [ str ] good_expr : pl . Expr use_expr : pl . Expr @property def name ( self ) -> str : \"\"\"The name of this step, usually the class name.\"\"\" return str ( type ( self )) @property def description ( self ) -> str : \"\"\"A short description of this step, including its inputs and outputs.\"\"\" return f \" { type ( self ) . __name__ } inputs= { self . inputs } outputs= { self . output } \" def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the outputs from the inputs in the given DataFrame, returning a new DataFrame.\"\"\" # TODO: should this be an abstract method? return df . filter ( self . good_expr ) def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Generate a diagnostic plot of the results after this step.\"\"\" # this is a no-op, subclasses can override this to plot something plt . figure () plt . text ( 0.0 , 0.5 , f \"No plot defined for: { self . description } \" ) return plt . gca () def drop_debug ( self ) -> \"RecipeStep\" : \"Return self, or a copy of it with debug information removed\" return self description property A short description of this step, including its inputs and outputs. name property The name of this step, usually the class name. calc_from_df ( df ) Calculate the outputs from the inputs in the given DataFrame, returning a new DataFrame. Source code in mass2/core/recipe.py 39 40 41 42 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the outputs from the inputs in the given DataFrame, returning a new DataFrame.\"\"\" # TODO: should this be an abstract method? return df . filter ( self . good_expr ) dbg_plot ( df_after , ** kwargs ) Generate a diagnostic plot of the results after this step. Source code in mass2/core/recipe.py 44 45 46 47 48 49 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Generate a diagnostic plot of the results after this step.\"\"\" # this is a no-op, subclasses can override this to plot something plt . figure () plt . text ( 0.0 , 0.5 , f \"No plot defined for: { self . description } \" ) return plt . gca () drop_debug () Return self, or a copy of it with debug information removed Source code in mass2/core/recipe.py 51 52 53 def drop_debug ( self ) -> \"RecipeStep\" : \"Return self, or a copy of it with debug information removed\" return self SelectStep dataclass Bases: RecipeStep This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. Source code in mass2/core/recipe.py 197 198 199 200 201 202 203 204 205 206 207 208 @dataclass ( frozen = True ) class SelectStep ( RecipeStep ): \"\"\" This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. \"\"\" col_expr_dict : dict [ str , pl . Expr ] def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Select the given columns and return a new DataFrame.\"\"\" df2 = df . select ( ** self . col_expr_dict ) . with_columns ( df ) return df2 calc_from_df ( df ) Select the given columns and return a new DataFrame. Source code in mass2/core/recipe.py 205 206 207 208 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Select the given columns and return a new DataFrame.\"\"\" df2 = df . select ( ** self . col_expr_dict ) . with_columns ( df ) return df2 SummarizeStep dataclass Bases: RecipeStep Summarize raw pulse data into summary statistics using numba-accelerated code. Source code in mass2/core/recipe.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 @dataclass ( frozen = True ) class SummarizeStep ( RecipeStep ): \"\"\"Summarize raw pulse data into summary statistics using numba-accelerated code.\"\"\" frametime_s : float peak_index : int pulse_col : str pretrigger_ignore_samples : int n_presamples : int transform_raw : Callable | None = None def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the summary statistics and return a new DataFrame.\"\"\" summaries = [] for df_iter in df . select ( self . inputs ) . iter_slices (): raw = df_iter [ self . pulse_col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) s = pl . from_numpy ( pulse_algorithms . summarize_data_numba ( raw , self . frametime_s , peak_samplenumber = self . peak_index , pretrigger_ignore_samples = self . pretrigger_ignore_samples , nPresamples = self . n_presamples , ) ) summaries . append ( s ) df2 = pl . concat ( summaries ) . with_columns ( df ) return df2 calc_from_df ( df ) Calculate the summary statistics and return a new DataFrame. Source code in mass2/core/recipe.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the summary statistics and return a new DataFrame.\"\"\" summaries = [] for df_iter in df . select ( self . inputs ) . iter_slices (): raw = df_iter [ self . pulse_col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) s = pl . from_numpy ( pulse_algorithms . summarize_data_numba ( raw , self . frametime_s , peak_samplenumber = self . peak_index , pretrigger_ignore_samples = self . pretrigger_ignore_samples , nPresamples = self . n_presamples , ) ) summaries . append ( s ) df2 = pl . concat ( summaries ) . with_columns ( df ) return df2 Tools for rough calibration of pulse heights to energies BestAssignmentPfitGainResult dataclass Result of finding the best assignment of pulse heights to energies and fitting a polynomial gain curve. Source code in mass2/core/rough_cal.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 @dataclass ( frozen = True ) class BestAssignmentPfitGainResult : \"\"\"Result of finding the best assignment of pulse heights to energies and fitting a polynomial gain curve.\"\"\" rms_residual : float ph_assigned : np . ndarray residual_e : np . ndarray | None assignment_inds : np . ndarray | None pfit_gain : np . polynomial . Polynomial energy_target : np . ndarray names_target : list [ str ] # list of strings with names for the energies in energy_target ph_target : np . ndarray # longer than energy target by 0-3 def ph_unassigned ( self ) -> ndarray : \"\"\"Which pulse heights were not assigned to any energy.\"\"\" return np . array ( list ( set ( self . ph_target ) - set ( self . ph_assigned ))) def plot ( self , ax : Axes | None = None ) -> None : \"\"\"Make a diagnostic plot of the gain fit.\"\"\" if ax is None : plt . figure () ax = plt . gca () gain = self . ph_assigned / self . energy_target ax . plot ( self . ph_assigned , self . ph_assigned / self . energy_target , \"o\" ) ph_large_range = np . linspace ( 0 , self . ph_assigned [ - 1 ] * 1.1 , 51 ) ax . plot ( ph_large_range , self . pfit_gain ( ph_large_range )) ax . set_xlabel ( \"pulse_height\" ) ax . set_ylabel ( \"gain\" ) ax . set_title ( f \"BestAssignmentPfitGainResult rms_residual= { self . rms_residual : .2f } eV\" ) assert len ( self . names_target ) == len ( self . ph_assigned ) for name , x , y in zip ( self . names_target , self . ph_assigned , gain ): ax . annotate ( str ( name ), ( x , y )) def phzerogain ( self ) -> float : \"\"\"Find the pulse height where the gain goes to zero. Quadratic fits should have two roots, we want the positive one; if they are complex, choose the real part.\"\"\" # the pulse height at which the gain is zero # for now I'm counting on the roots being ordered, we want the positive root where gain goes zero # since our function is invalid outside that range if self . pfit_gain . degree () == 2 : return np . real ( self . pfit_gain . roots ()[ 1 ]) elif self . pfit_gain . degree () == 1 : return self . pfit_gain . roots ()[ 0 ] else : raise ValueError () def ph2energy ( self , ph : ndarray | float ) -> float | ndarray : \"\"\"Convert pulse height to energy using the fitted gain curve.\"\"\" return ph / self . pfit_gain ( ph ) def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"Invert the gain curve to convert energy to pulse height.\"\"\" if self . pfit_gain . degree () == 2 : return self . _energy2ph_deg2 ( energy ) elif self . pfit_gain . degree () == 1 : return self . _energy2ph_deg1 ( energy ) elif self . pfit_gain . degree () == 0 : return self . _energy2ph_deg0 ( energy ) else : raise Exception ( \"degree out of range\" ) def _energy2ph_deg2 ( self , energy : ArrayLike ) -> NDArray : \"\"\"Invert a 2nd degree polynomial gain curve to convert energy to pulse height.\"\"\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(c + b*x + a*x^2) # so # y*c + (y*b-1)*x + a*x^2 = 0 # and given that we've selected for well formed calibrations, # we know which root we want cba = self . pfit_gain . convert () . coef c , bb , a = cba * np . asarray ( energy ) b = bb - 1 ph = ( - b - np . sqrt ( b ** 2 - 4 * a * c )) / ( 2 * a ) return ph def _energy2ph_deg1 ( self , energy : ArrayLike ) -> NDArray : \"\"\"Invert a 1st degree polynomial gain curve to convert energy to pulse height.\"\"\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(b + a*x) # so # x = y*b/(1-y*a) # and given that we've selected for well formed calibrations, # we know which root we want b , a = self . pfit_gain . convert () . coef y = energy ph = y * b / ( 1 - y * a ) return ph def _energy2ph_deg0 ( self , energy : ArrayLike ) -> NDArray : \"\"\"Invert a 0th degree polynomial gain curve to convert energy to pulse height.\"\"\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(a) # so # x = y*a ( a ,) = self . pfit_gain . convert () . coef y = energy ph = y * a return ph def predicted_energies ( self ) -> NDArray | float : \"\"\"Convert the assigned pulse heights to energies using the fitted gain curve.\"\"\" return self . ph2energy ( self . ph_assigned ) energy2ph ( energy ) Invert the gain curve to convert energy to pulse height. Source code in mass2/core/rough_cal.py 135 136 137 138 139 140 141 142 143 144 def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"Invert the gain curve to convert energy to pulse height.\"\"\" if self . pfit_gain . degree () == 2 : return self . _energy2ph_deg2 ( energy ) elif self . pfit_gain . degree () == 1 : return self . _energy2ph_deg1 ( energy ) elif self . pfit_gain . degree () == 0 : return self . _energy2ph_deg0 ( energy ) else : raise Exception ( \"degree out of range\" ) ph2energy ( ph ) Convert pulse height to energy using the fitted gain curve. Source code in mass2/core/rough_cal.py 131 132 133 def ph2energy ( self , ph : ndarray | float ) -> float | ndarray : \"\"\"Convert pulse height to energy using the fitted gain curve.\"\"\" return ph / self . pfit_gain ( ph ) ph_unassigned () Which pulse heights were not assigned to any energy. Source code in mass2/core/rough_cal.py 98 99 100 def ph_unassigned ( self ) -> ndarray : \"\"\"Which pulse heights were not assigned to any energy.\"\"\" return np . array ( list ( set ( self . ph_target ) - set ( self . ph_assigned ))) phzerogain () Find the pulse height where the gain goes to zero. Quadratic fits should have two roots, we want the positive one; if they are complex, choose the real part. Source code in mass2/core/rough_cal.py 118 119 120 121 122 123 124 125 126 127 128 129 def phzerogain ( self ) -> float : \"\"\"Find the pulse height where the gain goes to zero. Quadratic fits should have two roots, we want the positive one; if they are complex, choose the real part.\"\"\" # the pulse height at which the gain is zero # for now I'm counting on the roots being ordered, we want the positive root where gain goes zero # since our function is invalid outside that range if self . pfit_gain . degree () == 2 : return np . real ( self . pfit_gain . roots ()[ 1 ]) elif self . pfit_gain . degree () == 1 : return self . pfit_gain . roots ()[ 0 ] else : raise ValueError () plot ( ax = None ) Make a diagnostic plot of the gain fit. Source code in mass2/core/rough_cal.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def plot ( self , ax : Axes | None = None ) -> None : \"\"\"Make a diagnostic plot of the gain fit.\"\"\" if ax is None : plt . figure () ax = plt . gca () gain = self . ph_assigned / self . energy_target ax . plot ( self . ph_assigned , self . ph_assigned / self . energy_target , \"o\" ) ph_large_range = np . linspace ( 0 , self . ph_assigned [ - 1 ] * 1.1 , 51 ) ax . plot ( ph_large_range , self . pfit_gain ( ph_large_range )) ax . set_xlabel ( \"pulse_height\" ) ax . set_ylabel ( \"gain\" ) ax . set_title ( f \"BestAssignmentPfitGainResult rms_residual= { self . rms_residual : .2f } eV\" ) assert len ( self . names_target ) == len ( self . ph_assigned ) for name , x , y in zip ( self . names_target , self . ph_assigned , gain ): ax . annotate ( str ( name ), ( x , y )) predicted_energies () Convert the assigned pulse heights to energies using the fitted gain curve. Source code in mass2/core/rough_cal.py 184 185 186 def predicted_energies ( self ) -> NDArray | float : \"\"\"Convert the assigned pulse heights to energies using the fitted gain curve.\"\"\" return self . ph2energy ( self . ph_assigned ) RoughCalibrationStep dataclass Bases: RecipeStep A step to perform a rough calibration of pulse heights to energies. Source code in mass2/core/rough_cal.py 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 @dataclass ( frozen = True ) class RoughCalibrationStep ( RecipeStep ): \"\"\"A step to perform a rough calibration of pulse heights to energies.\"\"\" pfresult : SmoothedLocalMaximaResult | None assignment_result : BestAssignmentPfitGainResult | None ph2energy : Callable success : bool def calc_from_df ( self , df : DataFrame ) -> DataFrame : \"\"\"Apply the rough calibration to a dataframe.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2 def drop_debug ( self ) -> \"RoughCalibrationStep\" : \"\"\"Return a copy of this step with debug information removed.\"\"\" return dataclasses . replace ( self , pfresult = None , assignment_result = None ) def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> None : \"\"\"Create diagnostic plots of the rough calibration step.\"\"\" if self . success : self . dbg_plot_success ( df_after , ** kwargs ) else : self . dbg_plot_failure ( df_after , ** kwargs ) def dbg_plot_success ( self , df : DataFrame , ** kwargs : Any ) -> None : \"\"\"Create diagnostic plots of the rough calibration step, if it succeeded.\"\"\" _ , axs = plt . subplots ( 2 , 1 , figsize = ( 11 , 6 )) if self . assignment_result : self . assignment_result . plot ( ax = axs [ 0 ]) if self . pfresult : self . pfresult . plot ( self . assignment_result , ax = axs [ 1 ]) plt . tight_layout () def dbg_plot_failure ( self , df : DataFrame , ** kwargs : None ) -> None : \"\"\"Create diagnostic plots of the rough calibration step, if it failed.\"\"\" _ , axs = plt . subplots ( 2 , 1 , figsize = ( 11 , 6 )) if self . pfresult : self . pfresult . plot ( self . assignment_result , ax = axs [ 1 ]) plt . tight_layout () def energy2ph ( self , energy : ArrayLike ) -> NDArray | float : \"\"\"Convert energy to pulse height using the fitted gain curve.\"\"\" if self . assignment_result : return self . assignment_result . energy2ph ( energy ) return 0.0 @classmethod def learn_combinatoric ( cls , ch : Channel , line_names : list [ str ], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test.\"\"\" ( names , ee ) = line_names_and_energies ( line_names ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () assert len ( uncalibrated ) > 10 , \"not enough pulses\" pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = ph_smoothing_fwhm ) assignment_result = find_optimal_assignment2 ( pfresult . ph_sorted_by_prominence ()[: len ( ee ) + n_extra ], ee , names ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr . and_ ( pl . col ( uncalibrated_col ) < assignment_result . phzerogain ()), use_expr = use_expr , pfresult = pfresult , assignment_result = assignment_result , ph2energy = assignment_result . ph2energy , success = True , ) return step @classmethod def learn_combinatoric_height_info ( cls , ch : Channel , line_names : list [ str ], line_heights_allowed : list [ list [ int ]], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies.\"\"\" ( names , ee ) = line_names_and_energies ( line_names ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () assert len ( uncalibrated ) > 10 , \"not enough pulses\" pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = ph_smoothing_fwhm ) assignment_result = find_optimal_assignment2_height_info ( pfresult . ph_sorted_by_prominence ()[: len ( ee ) + n_extra ], ee , names , line_heights_allowed , ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr = use_expr , pfresult = pfresult , assignment_result = assignment_result , ph2energy = assignment_result . ph2energy , success = True , ) return step @classmethod def learn_3peak ( # noqa: PLR0917 PLR0914, cls , ch : Channel , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning 3 of them to known energies in a way that minimizes the RMS error in a local linearity test, and then evaluating that assignment by fitting a 2nd degree polynomial gain curve to all possible pulse heights and returning the RMS error in energy after applying that gain curve to all possible pulse heights. If no good assignment is found, the step will be marked as unsuccessful.\"\"\" if calibrated_col is None : calibrated_col = f \"energy_ { uncalibrated_col } \" ( line_names_str , line_energies_list ) = line_names_and_energies ( line_names ) line_energies = np . asarray ( line_energies_list ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = fwhm_pulse_height_units ) possible_phs = pfresult . ph_sorted_by_prominence ()[: len ( line_names_str ) + n_extra_peaks ] df3peak , _dfe = rank_3peak_assignments ( possible_phs , line_energies , line_names_str , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , ) best_rms_residual = np . inf best_assignment_result = None for assignment_row in df3peak . select ( \"e0\" , \"ph0\" , \"e1\" , \"ph1\" , \"e2\" , \"ph2\" , \"e_err_at_ph2\" ) . iter_rows (): e0 , ph0 , e1 , ph1 , e2 , ph2 , _e_err_at_ph2 = assignment_row pharray = np . array ([ ph0 , ph1 , ph2 ]) earray = np . array ([ e0 , e1 , e2 ]) rms_residual , assignment_result = eval_3peak_assignment_pfit_gain ( pharray , earray , possible_phs , line_energies , line_names_str ) if rms_residual < best_rms_residual : best_rms_residual = rms_residual best_assignment_result = assignment_result if rms_residual < acceptable_rms_residual_e : break if ( best_assignment_result and isinstance ( best_assignment_result , BestAssignmentPfitGainResult ) and not np . isinf ( best_rms_residual ) ): success = True ph2energy = best_assignment_result . ph2energy # df3peak_on_failure = None else : success = False def nanenergy ( ph : NDArray | float ) -> NDArray | float : \"Return NaN for all pulse heights, indicating failure to calibrate.\" return ph * np . nan ph2energy = nanenergy # df3peak_on_failure = df3peak # df3peak_on_failure = df3peak if isinstance ( best_assignment_result , str ): best_assignment_result = None step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr = use_expr , pfresult = pfresult , assignment_result = best_assignment_result , ph2energy = ph2energy , success = success , ) return step calc_from_df ( df ) Apply the rough calibration to a dataframe. Source code in mass2/core/rough_cal.py 756 757 758 759 760 761 762 763 def calc_from_df ( self , df : DataFrame ) -> DataFrame : \"\"\"Apply the rough calibration to a dataframe.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2 dbg_plot ( df_after , ** kwargs ) Create diagnostic plots of the rough calibration step. Source code in mass2/core/rough_cal.py 769 770 771 772 773 774 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> None : \"\"\"Create diagnostic plots of the rough calibration step.\"\"\" if self . success : self . dbg_plot_success ( df_after , ** kwargs ) else : self . dbg_plot_failure ( df_after , ** kwargs ) dbg_plot_failure ( df , ** kwargs ) Create diagnostic plots of the rough calibration step, if it failed. Source code in mass2/core/rough_cal.py 785 786 787 788 789 790 def dbg_plot_failure ( self , df : DataFrame , ** kwargs : None ) -> None : \"\"\"Create diagnostic plots of the rough calibration step, if it failed.\"\"\" _ , axs = plt . subplots ( 2 , 1 , figsize = ( 11 , 6 )) if self . pfresult : self . pfresult . plot ( self . assignment_result , ax = axs [ 1 ]) plt . tight_layout () dbg_plot_success ( df , ** kwargs ) Create diagnostic plots of the rough calibration step, if it succeeded. Source code in mass2/core/rough_cal.py 776 777 778 779 780 781 782 783 def dbg_plot_success ( self , df : DataFrame , ** kwargs : Any ) -> None : \"\"\"Create diagnostic plots of the rough calibration step, if it succeeded.\"\"\" _ , axs = plt . subplots ( 2 , 1 , figsize = ( 11 , 6 )) if self . assignment_result : self . assignment_result . plot ( ax = axs [ 0 ]) if self . pfresult : self . pfresult . plot ( self . assignment_result , ax = axs [ 1 ]) plt . tight_layout () drop_debug () Return a copy of this step with debug information removed. Source code in mass2/core/rough_cal.py 765 766 767 def drop_debug ( self ) -> \"RoughCalibrationStep\" : \"\"\"Return a copy of this step with debug information removed.\"\"\" return dataclasses . replace ( self , pfresult = None , assignment_result = None ) energy2ph ( energy ) Convert energy to pulse height using the fitted gain curve. Source code in mass2/core/rough_cal.py 792 793 794 795 796 def energy2ph ( self , energy : ArrayLike ) -> NDArray | float : \"\"\"Convert energy to pulse height using the fitted gain curve.\"\"\" if self . assignment_result : return self . assignment_result . energy2ph ( energy ) return 0.0 learn_3peak ( ch , line_names , uncalibrated_col = 'filtValue' , calibrated_col = None , use_expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment = 0.1 , min_gain_fraction_at_ph_30k = 0.25 , fwhm_pulse_height_units = 75 , n_extra_peaks = 10 , acceptable_rms_residual_e = 10 ) classmethod Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning 3 of them to known energies in a way that minimizes the RMS error in a local linearity test, and then evaluating that assignment by fitting a 2nd degree polynomial gain curve to all possible pulse heights and returning the RMS error in energy after applying that gain curve to all possible pulse heights. If no good assignment is found, the step will be marked as unsuccessful. Source code in mass2/core/rough_cal.py 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 @classmethod def learn_3peak ( # noqa: PLR0917 PLR0914, cls , ch : Channel , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning 3 of them to known energies in a way that minimizes the RMS error in a local linearity test, and then evaluating that assignment by fitting a 2nd degree polynomial gain curve to all possible pulse heights and returning the RMS error in energy after applying that gain curve to all possible pulse heights. If no good assignment is found, the step will be marked as unsuccessful.\"\"\" if calibrated_col is None : calibrated_col = f \"energy_ { uncalibrated_col } \" ( line_names_str , line_energies_list ) = line_names_and_energies ( line_names ) line_energies = np . asarray ( line_energies_list ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = fwhm_pulse_height_units ) possible_phs = pfresult . ph_sorted_by_prominence ()[: len ( line_names_str ) + n_extra_peaks ] df3peak , _dfe = rank_3peak_assignments ( possible_phs , line_energies , line_names_str , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , ) best_rms_residual = np . inf best_assignment_result = None for assignment_row in df3peak . select ( \"e0\" , \"ph0\" , \"e1\" , \"ph1\" , \"e2\" , \"ph2\" , \"e_err_at_ph2\" ) . iter_rows (): e0 , ph0 , e1 , ph1 , e2 , ph2 , _e_err_at_ph2 = assignment_row pharray = np . array ([ ph0 , ph1 , ph2 ]) earray = np . array ([ e0 , e1 , e2 ]) rms_residual , assignment_result = eval_3peak_assignment_pfit_gain ( pharray , earray , possible_phs , line_energies , line_names_str ) if rms_residual < best_rms_residual : best_rms_residual = rms_residual best_assignment_result = assignment_result if rms_residual < acceptable_rms_residual_e : break if ( best_assignment_result and isinstance ( best_assignment_result , BestAssignmentPfitGainResult ) and not np . isinf ( best_rms_residual ) ): success = True ph2energy = best_assignment_result . ph2energy # df3peak_on_failure = None else : success = False def nanenergy ( ph : NDArray | float ) -> NDArray | float : \"Return NaN for all pulse heights, indicating failure to calibrate.\" return ph * np . nan ph2energy = nanenergy # df3peak_on_failure = df3peak # df3peak_on_failure = df3peak if isinstance ( best_assignment_result , str ): best_assignment_result = None step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr = use_expr , pfresult = pfresult , assignment_result = best_assignment_result , ph2energy = ph2energy , success = success , ) return step learn_combinatoric ( ch , line_names , uncalibrated_col , calibrated_col , ph_smoothing_fwhm , n_extra , use_expr = field ( default_factory = alwaysTrue )) classmethod Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test. Source code in mass2/core/rough_cal.py 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 @classmethod def learn_combinatoric ( cls , ch : Channel , line_names : list [ str ], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test.\"\"\" ( names , ee ) = line_names_and_energies ( line_names ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () assert len ( uncalibrated ) > 10 , \"not enough pulses\" pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = ph_smoothing_fwhm ) assignment_result = find_optimal_assignment2 ( pfresult . ph_sorted_by_prominence ()[: len ( ee ) + n_extra ], ee , names ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr . and_ ( pl . col ( uncalibrated_col ) < assignment_result . phzerogain ()), use_expr = use_expr , pfresult = pfresult , assignment_result = assignment_result , ph2energy = assignment_result . ph2energy , success = True , ) return step learn_combinatoric_height_info ( ch , line_names , line_heights_allowed , uncalibrated_col , calibrated_col , ph_smoothing_fwhm , n_extra , use_expr = field ( default_factory = alwaysTrue )) classmethod Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies. Source code in mass2/core/rough_cal.py 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 @classmethod def learn_combinatoric_height_info ( cls , ch : Channel , line_names : list [ str ], line_heights_allowed : list [ list [ int ]], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies.\"\"\" ( names , ee ) = line_names_and_energies ( line_names ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () assert len ( uncalibrated ) > 10 , \"not enough pulses\" pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = ph_smoothing_fwhm ) assignment_result = find_optimal_assignment2_height_info ( pfresult . ph_sorted_by_prominence ()[: len ( ee ) + n_extra ], ee , names , line_heights_allowed , ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr = use_expr , pfresult = pfresult , assignment_result = assignment_result , ph2energy = assignment_result . ph2energy , success = True , ) return step SmoothedLocalMaximaResult dataclass A set of local maxima found in a smoothed histogram of pulse heights. Source code in mass2/core/rough_cal.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 @dataclass ( frozen = True ) class SmoothedLocalMaximaResult : \"\"\"A set of local maxima found in a smoothed histogram of pulse heights.\"\"\" fwhm_pulse_height_units : float bin_centers : np . ndarray counts : np . ndarray smoothed_counts : np . ndarray local_maxima_inds : np . ndarray # inds into bin_centers local_minima_inds : np . ndarray # inds into bin_centers def inds_sorted_by_peak_height ( self ) -> NDArray : \"\"\"Indices of local maxima sorted by peak height, highest first.\"\"\" return self . local_maxima_inds [ np . argsort ( - self . peak_height ())] def inds_sorted_by_prominence ( self ) -> NDArray : \"\"\"Indices of local maxima sorted by prominence, most prominent first.\"\"\" return self . local_maxima_inds [ np . argsort ( - self . prominence ())] def ph_sorted_by_prominence ( self ) -> NDArray : \"\"\"Pulse heights of local maxima sorted by prominence, most prominent first.\"\"\" return self . bin_centers [ self . inds_sorted_by_prominence ()] def ph_sorted_by_peak_height ( self ) -> NDArray : \"\"\"Pulse heights of local maxima sorted by peak height, highest first.\"\"\" return self . bin_centers [ self . inds_sorted_by_peak_height ()] def peak_height ( self ) -> NDArray : \"\"\"Peak heights of local maxima.\"\"\" return self . smoothed_counts [ self . local_maxima_inds ] def prominence ( self ) -> NDArray : \"\"\"Prominence of local maxima, in aems order as `local_maxima_inds`.\"\"\" assert len ( self . local_minima_inds ) == len ( self . local_maxima_inds ) + 1 , ( \"peakfind_local_maxima_of_smoothed_hist must ensure this \" ) prominence = np . zeros_like ( self . local_maxima_inds , dtype = float ) for i in range ( len ( self . local_maxima_inds )): sc_max = self . smoothed_counts [ self . local_maxima_inds [ i ]] sc_min_before = self . smoothed_counts [ self . local_minima_inds [ i ]] sc_min_after = self . smoothed_counts [ self . local_minima_inds [ i + 1 ]] prominence [ i ] = ( 2 * sc_max - sc_min_before - sc_min_after ) / 2 assert np . all ( prominence >= 0 ), \"prominence should be non-negative\" return prominence def plot ( self , assignment_result : BestAssignmentPfitGainResult | None = None , n_highlight : int = 10 , plot_counts : bool = False , ax : Axes | None = None , ) -> Axes : \"\"\"Make a diagnostic plot of the smoothed histogram and local maxima.\"\"\" if ax is None : plt . figure () ax = plt . gca () inds_prominence = self . inds_sorted_by_prominence ()[: n_highlight ] inds_peak_height = self . inds_sorted_by_peak_height ()[: n_highlight ] if plot_counts : ax . plot ( self . bin_centers , self . counts , label = \"counts\" ) ax . plot ( self . bin_centers , self . smoothed_counts , label = \"smoothed_counts\" ) ax . plot ( self . bin_centers [ self . local_maxima_inds ], self . smoothed_counts [ self . local_maxima_inds ], \".\" , label = \"peaks\" , ) if assignment_result is not None : inds_assigned = np . searchsorted ( self . bin_centers , assignment_result . ph_assigned ) inds_unassigned = np . searchsorted ( self . bin_centers , assignment_result . ph_unassigned ()) bin_centers_assigned = self . bin_centers [ inds_assigned ] bin_centers_unassigned = self . bin_centers [ inds_unassigned ] smoothed_counts_assigned = self . smoothed_counts [ inds_assigned ] smoothed_counts_unassigned = self . smoothed_counts [ inds_unassigned ] ax . plot ( bin_centers_assigned , smoothed_counts_assigned , \"o\" , label = \"assigned\" ) ax . plot ( bin_centers_unassigned , smoothed_counts_unassigned , \"o\" , label = \"unassigned\" , ) for name , x , y in zip ( assignment_result . names_target , bin_centers_assigned , smoothed_counts_assigned , ): ax . annotate ( str ( name ), ( x , y ), rotation = 30 ) ax . set_title ( f \"SmoothedLocalMaximaResult rms_residual= { assignment_result . rms_residual : .2f } eV\" ) else : ax . plot ( self . bin_centers [ inds_prominence ], self . smoothed_counts [ inds_prominence ], \"o\" , label = f \" { n_highlight } most prominent\" , ) ax . plot ( self . bin_centers [ inds_peak_height ], self . smoothed_counts [ inds_peak_height ], \"v\" , label = f \" { n_highlight } highest\" , ) ax . set_title ( \"SmoothedLocalMaximaResult\" ) ax . legend () ax . set_xlabel ( \"pulse height\" ) ax . set_ylabel ( \"intensity\" ) # print(f\"{np.amax(self.smoothed_counts)=} {np.amin(self.smoothed_counts)=} \") # ax.set_ylim(1/self.fwhm_pulse_height_units, ax.get_ylim()[1]) return ax inds_sorted_by_peak_height () Indices of local maxima sorted by peak height, highest first. Source code in mass2/core/rough_cal.py 200 201 202 def inds_sorted_by_peak_height ( self ) -> NDArray : \"\"\"Indices of local maxima sorted by peak height, highest first.\"\"\" return self . local_maxima_inds [ np . argsort ( - self . peak_height ())] inds_sorted_by_prominence () Indices of local maxima sorted by prominence, most prominent first. Source code in mass2/core/rough_cal.py 204 205 206 def inds_sorted_by_prominence ( self ) -> NDArray : \"\"\"Indices of local maxima sorted by prominence, most prominent first.\"\"\" return self . local_maxima_inds [ np . argsort ( - self . prominence ())] peak_height () Peak heights of local maxima. Source code in mass2/core/rough_cal.py 216 217 218 def peak_height ( self ) -> NDArray : \"\"\"Peak heights of local maxima.\"\"\" return self . smoothed_counts [ self . local_maxima_inds ] ph_sorted_by_peak_height () Pulse heights of local maxima sorted by peak height, highest first. Source code in mass2/core/rough_cal.py 212 213 214 def ph_sorted_by_peak_height ( self ) -> NDArray : \"\"\"Pulse heights of local maxima sorted by peak height, highest first.\"\"\" return self . bin_centers [ self . inds_sorted_by_peak_height ()] ph_sorted_by_prominence () Pulse heights of local maxima sorted by prominence, most prominent first. Source code in mass2/core/rough_cal.py 208 209 210 def ph_sorted_by_prominence ( self ) -> NDArray : \"\"\"Pulse heights of local maxima sorted by prominence, most prominent first.\"\"\" return self . bin_centers [ self . inds_sorted_by_prominence ()] plot ( assignment_result = None , n_highlight = 10 , plot_counts = False , ax = None ) Make a diagnostic plot of the smoothed histogram and local maxima. Source code in mass2/core/rough_cal.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def plot ( self , assignment_result : BestAssignmentPfitGainResult | None = None , n_highlight : int = 10 , plot_counts : bool = False , ax : Axes | None = None , ) -> Axes : \"\"\"Make a diagnostic plot of the smoothed histogram and local maxima.\"\"\" if ax is None : plt . figure () ax = plt . gca () inds_prominence = self . inds_sorted_by_prominence ()[: n_highlight ] inds_peak_height = self . inds_sorted_by_peak_height ()[: n_highlight ] if plot_counts : ax . plot ( self . bin_centers , self . counts , label = \"counts\" ) ax . plot ( self . bin_centers , self . smoothed_counts , label = \"smoothed_counts\" ) ax . plot ( self . bin_centers [ self . local_maxima_inds ], self . smoothed_counts [ self . local_maxima_inds ], \".\" , label = \"peaks\" , ) if assignment_result is not None : inds_assigned = np . searchsorted ( self . bin_centers , assignment_result . ph_assigned ) inds_unassigned = np . searchsorted ( self . bin_centers , assignment_result . ph_unassigned ()) bin_centers_assigned = self . bin_centers [ inds_assigned ] bin_centers_unassigned = self . bin_centers [ inds_unassigned ] smoothed_counts_assigned = self . smoothed_counts [ inds_assigned ] smoothed_counts_unassigned = self . smoothed_counts [ inds_unassigned ] ax . plot ( bin_centers_assigned , smoothed_counts_assigned , \"o\" , label = \"assigned\" ) ax . plot ( bin_centers_unassigned , smoothed_counts_unassigned , \"o\" , label = \"unassigned\" , ) for name , x , y in zip ( assignment_result . names_target , bin_centers_assigned , smoothed_counts_assigned , ): ax . annotate ( str ( name ), ( x , y ), rotation = 30 ) ax . set_title ( f \"SmoothedLocalMaximaResult rms_residual= { assignment_result . rms_residual : .2f } eV\" ) else : ax . plot ( self . bin_centers [ inds_prominence ], self . smoothed_counts [ inds_prominence ], \"o\" , label = f \" { n_highlight } most prominent\" , ) ax . plot ( self . bin_centers [ inds_peak_height ], self . smoothed_counts [ inds_peak_height ], \"v\" , label = f \" { n_highlight } highest\" , ) ax . set_title ( \"SmoothedLocalMaximaResult\" ) ax . legend () ax . set_xlabel ( \"pulse height\" ) ax . set_ylabel ( \"intensity\" ) # print(f\"{np.amax(self.smoothed_counts)=} {np.amin(self.smoothed_counts)=} \") # ax.set_ylim(1/self.fwhm_pulse_height_units, ax.get_ylim()[1]) return ax prominence () Prominence of local maxima, in aems order as local_maxima_inds . Source code in mass2/core/rough_cal.py 220 221 222 223 224 225 226 227 228 229 230 231 232 def prominence ( self ) -> NDArray : \"\"\"Prominence of local maxima, in aems order as `local_maxima_inds`.\"\"\" assert len ( self . local_minima_inds ) == len ( self . local_maxima_inds ) + 1 , ( \"peakfind_local_maxima_of_smoothed_hist must ensure this \" ) prominence = np . zeros_like ( self . local_maxima_inds , dtype = float ) for i in range ( len ( self . local_maxima_inds )): sc_max = self . smoothed_counts [ self . local_maxima_inds [ i ]] sc_min_before = self . smoothed_counts [ self . local_minima_inds [ i ]] sc_min_after = self . smoothed_counts [ self . local_minima_inds [ i + 1 ]] prominence [ i ] = ( 2 * sc_max - sc_min_before - sc_min_after ) / 2 assert np . all ( prominence >= 0 ), \"prominence should be non-negative\" return prominence drift_correct_entropy ( slope , indicator_zero_mean , uncorrected , bin_edges , fwhm_in_bin_number_units ) Calculate the entropy of a histogram of drift-corrected pulse heights. Source code in mass2/core/rough_cal.py 634 635 636 637 638 639 640 641 642 643 644 645 def drift_correct_entropy ( slope : float , indicator_zero_mean : ndarray , uncorrected : ndarray , bin_edges : ndarray , fwhm_in_bin_number_units : int , ) -> float : \"\"\"Calculate the entropy of a histogram of drift-corrected pulse heights.\"\"\" corrected = uncorrected * ( 1 + indicator_zero_mean * slope ) smoothed_counts , bin_edges , _counts = hist_smoothed ( corrected , fwhm_in_bin_number_units , bin_edges ) w = smoothed_counts > 0 return - ( np . log ( smoothed_counts [ w ]) * smoothed_counts [ w ]) . sum () eval_3peak_assignment_pfit_gain ( ph_assigned , e_assigned , possible_phs , line_energies , line_names ) Evaluate a proposed assignment of 3 pulse heights to 3 energies by fitting a 2nd degree polynomial gain curve, and returning the RMS residual in energy after applying that gain curve to all possible pulse heights. If the proposed assignment does not lead to a well formed gain curve, return infinity and a string describing the problem. Source code in mass2/core/rough_cal.py 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 def eval_3peak_assignment_pfit_gain ( ph_assigned : NDArray , e_assigned : NDArray , possible_phs : NDArray , line_energies : NDArray , line_names : list [ str ] ) -> tuple [ float , BestAssignmentPfitGainResult | str ]: \"\"\"Evaluate a proposed assignment of 3 pulse heights to 3 energies by fitting a 2nd degree polynomial gain curve, and returning the RMS residual in energy after applying that gain curve to all possible pulse heights. If the proposed assignment does not lead to a well formed gain curve, return infinity and a string describing the problem.\"\"\" assert len ( np . unique ( ph_assigned )) == len ( ph_assigned ), \"assignments must be unique\" assert len ( np . unique ( e_assigned )) == len ( e_assigned ), \"assignments must be unique\" assert all ( np . diff ( ph_assigned ) > 0 ), \"assignments must be sorted\" assert all ( np . diff ( e_assigned ) > 0 ), \"assignments must be sorted\" gain_assigned = np . array ( ph_assigned ) / np . array ( e_assigned ) pfit_gain_3peak = np . polynomial . Polynomial . fit ( ph_assigned , gain_assigned , deg = 2 ) if pfit_gain_3peak . deriv ( 1 )( 0 ) > 0 : # well formed calibration have negative derivative at zero pulse height return np . inf , \"pfit_gain_3peak deriv at 0 should be <0\" if pfit_gain_3peak ( 1e5 ) < 0 : # well formed calibration have positive gain at 1e5 return np . inf , \"pfit_gain_3peak should be above zero at 100k ph\" if any ( np . iscomplex ( pfit_gain_3peak . roots ())): # well formed calibrations have real roots return np . inf , \"pfit_gain_3peak must have real roots\" def ph2energy ( ph : NDArray ) -> NDArray : \"Convert pulse height to energy using the fitted gain curve.\" gain = pfit_gain_3peak ( ph ) return ph / gain cba = pfit_gain_3peak . convert () . coef def energy2ph ( energy : NDArray ) -> NDArray : \"Invert the gain curve to convert energy to pulse height.\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(c + b*x + a*x^2) # so # y*c + (y*b-1)*x + a*x^2 = 0 # and given that we've selected for well formed calibrations, # we know which root we want c , bb , a = cba * energy b = bb - 1 ph = ( - b - np . sqrt ( b ** 2 - 4 * a * c )) / ( 2 * a ) return ph predicted_ph = [ energy2ph ( _e ) for _e in line_energies ] df = pl . DataFrame ({ \"line_energy\" : line_energies , \"line_name\" : line_names , \"predicted_ph\" : predicted_ph , }) . sort ( by = \"predicted_ph\" ) dfph = pl . DataFrame ({ \"possible_ph\" : possible_phs , \"ph_ind\" : np . arange ( len ( possible_phs ))}) . sort ( by = \"possible_ph\" ) # for each e find the closest possible_ph to the calculaed predicted_ph # we started with assignments for 3 energies # now we have assignments for all energies df = df . join_asof ( dfph , left_on = \"predicted_ph\" , right_on = \"possible_ph\" , strategy = \"nearest\" ) n_unique = len ( df [ \"possible_ph\" ] . unique ()) if n_unique < len ( df ): # assigned multiple energies to same pulseheight, not a good cal return np . inf , \"assignments should be unique\" # now we evaluate the assignment and create a result object residual_e , pfit_gain = find_pfit_gain_residual ( df [ \"possible_ph\" ] . to_numpy (), df [ \"line_energy\" ] . to_numpy ()) if pfit_gain ( 1e5 ) < 0 : # well formed calibration have positive gain at 1e5 return np . inf , \"pfit_gain should be above zero at 100k ph\" if any ( np . iscomplex ( pfit_gain . roots ())): # well formed calibrations have real roots return np . inf , \"pfit_gain should not have complex roots\" rms_residual_e = mass2 . misc . root_mean_squared ( residual_e ) result = BestAssignmentPfitGainResult ( rms_residual_e , ph_assigned = df [ \"possible_ph\" ] . to_numpy (), residual_e = residual_e , assignment_inds = df [ \"ph_ind\" ] . to_numpy (), pfit_gain = pfit_gain , energy_target = df [ \"line_energy\" ] . to_numpy (), names_target = df [ \"line_name\" ] . to_list (), ph_target = possible_phs , ) return rms_residual_e , result find_best_residual_among_all_possible_assignments ( ph , e ) Try all possible assignments of pulse heights to energies, and return the one with the lowest RMS residual in energy after fitting a 2nd degree polynomial gain curve. Source code in mass2/core/rough_cal.py 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 def find_best_residual_among_all_possible_assignments ( ph : ndarray , e : ndarray ) -> tuple [ float , ndarray , ndarray , ndarray , Polynomial ]: \"\"\"Try all possible assignments of pulse heights to energies, and return the one with the lowest RMS residual in energy after fitting a 2nd degree polynomial gain curve. \"\"\" assert len ( ph ) >= len ( e ) ph = np . sort ( ph ) assignments_inds = itertools . combinations ( np . arange ( len ( ph )), len ( e )) best_rms_residual = np . inf best_ph_assigned = np . array ([]) best_residual_e = np . array ([]) best_assignment_inds = np . array ([]) best_pfit = Polynomial ([ 0 ]) for i , indices in enumerate ( assignments_inds ): assignment_inds = np . array ( indices ) ph_assigned = np . array ( ph [ assignment_inds ]) residual_e , pfit_gain = find_pfit_gain_residual ( ph_assigned , e ) rms_residual = mass2 . misc . root_mean_squared ( residual_e ) if rms_residual < best_rms_residual : best_rms_residual = rms_residual best_ph_assigned = ph_assigned best_residual_e = residual_e best_assignment_inds = assignment_inds best_pfit = pfit_gain return ( best_rms_residual , best_ph_assigned , best_residual_e , best_assignment_inds , best_pfit , ) find_best_residual_among_all_possible_assignments2 ( ph , e , names ) Try all possible assignments of pulse heights to energies, and return the one with the lowest RMS residual in energy after fitting a 2nd degree polynomial gain curve. Return as a BestAssignmentPfitGainResult object. Source code in mass2/core/rough_cal.py 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 def find_best_residual_among_all_possible_assignments2 ( ph : ndarray , e : ndarray , names : list [ str ]) -> BestAssignmentPfitGainResult : \"\"\"Try all possible assignments of pulse heights to energies, and return the one with the lowest RMS residual in energy after fitting a 2nd degree polynomial gain curve. Return as a BestAssignmentPfitGainResult object. \"\"\" ( best_rms_residual , best_ph_assigned , best_residual_e , best_assignment_inds , best_pfit , ) = find_best_residual_among_all_possible_assignments ( ph , e ) return BestAssignmentPfitGainResult ( float ( best_rms_residual ), best_ph_assigned , best_residual_e , best_assignment_inds , best_pfit , e , names , ph , ) find_local_maxima ( pulse_heights , gaussian_fwhm ) Smears each pulse by a gaussian of gaussian_fhwm and finds local maxima, returns a list of their locations in pulse_height units (sorted by number of pulses in peak) AND their peak values as: (peak_locations, peak_intensities) Args: pulse_heights (np.array(dtype=float)): a list of pulse heights (eg p_filt_value) gaussian_fwhm = fwhm of a gaussian that each pulse is smeared with, in same units as pulse heights Source code in mass2/core/rough_cal.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def find_local_maxima ( pulse_heights : ArrayLike , gaussian_fwhm : float ) -> Any : \"\"\"Smears each pulse by a gaussian of gaussian_fhwm and finds local maxima, returns a list of their locations in pulse_height units (sorted by number of pulses in peak) AND their peak values as: (peak_locations, peak_intensities) Args: pulse_heights (np.array(dtype=float)): a list of pulse heights (eg p_filt_value) gaussian_fwhm = fwhm of a gaussian that each pulse is smeared with, in same units as pulse heights \"\"\" # kernel density estimation (with a gaussian kernel) n = 128 * 1024 gaussian_fwhm = float ( gaussian_fwhm ) # The above ensures that lo & hi are floats, so that (lo-hi)/n is always a float in python2 sigma = gaussian_fwhm / ( np . sqrt ( np . log ( 2 ) * 2 ) * 2 ) tbw = 1.0 / sigma / ( np . pi * 2 ) lo = np . min ( pulse_heights ) - 3 * gaussian_fwhm hi = np . max ( pulse_heights ) + 3 * gaussian_fwhm hist , bins = np . histogram ( pulse_heights , np . linspace ( lo , hi , n + 1 )) tx = np . fft . rfftfreq ( n , ( lo - hi ) / n ) ty = np . exp ( - ( tx ** 2 ) / 2 / tbw ** 2 ) x = ( bins [ 1 :] + bins [: - 1 ]) / 2 y = np . fft . irfft ( np . fft . rfft ( hist ) * ty ) flag = ( y [ 1 : - 1 ] > y [: - 2 ]) & ( y [ 1 : - 1 ] > y [ 2 :]) lm = np . arange ( 1 , n - 1 )[ flag ] lm = lm [ np . argsort ( - y [ lm ])] bin_centers , _step_size = mass2 . misc . midpoints_and_step_size ( bins ) return np . array ( x [ lm ]), np . array ( y [ lm ]), ( hist , bin_centers , y ) find_optimal_assignment ( ph , e ) Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test. Source code in mass2/core/rough_cal.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def find_optimal_assignment ( ph : ArrayLike , e : ArrayLike ) -> tuple [ float , NDArray ]: \"\"\"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test.\"\"\" # ph is a list of peak heights longer than e # e is a list of known peak energies # we want to find the set of peak heights from ph that are closest to being locally linear with the energies in e # when given 3 or less energies to match, use the largest peaks in peak order ph = np . asarray ( ph ) e = np . asarray ( e ) assert len ( e ) >= 1 if len ( e ) <= 2 : return 0 , np . array ( sorted ( ph [: len ( e )])) rms_e_residual , pha , _pha_inds = rank_assignments ( ph , e ) ind = np . argmin ( rms_e_residual ) return rms_e_residual [ ind ], pha [ ind ] find_optimal_assignment2 ( ph , e , line_names ) Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, and fit a polynomial gain curve to the result. Source code in mass2/core/rough_cal.py 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 def find_optimal_assignment2 ( ph : ArrayLike , e : ArrayLike , line_names : list [ str ]) -> BestAssignmentPfitGainResult : \"\"\"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, and fit a polynomial gain curve to the result.\"\"\" ph = np . asarray ( ph ) e = np . asarray ( e ) rms_e_residual , pha = find_optimal_assignment ( ph , e ) gain = pha / e deg = min ( len ( e ) - 1 , 2 ) if deg == 0 : pfit_gain = np . polynomial . Polynomial ( gain ) else : pfit_gain = np . polynomial . Polynomial . fit ( pha , gain , deg = min ( len ( e ) - 1 , 2 )) result = BestAssignmentPfitGainResult ( rms_e_residual , ph_assigned = pha , residual_e = None , assignment_inds = None , pfit_gain = pfit_gain , energy_target = e , names_target = line_names , ph_target = ph , ) return result find_optimal_assignment2_height_info ( ph , e , line_names , line_heights_allowed ) Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies, and fit a polynomial gain curve to the result. Source code in mass2/core/rough_cal.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 def find_optimal_assignment2_height_info ( ph : ArrayLike , e : ArrayLike , line_names : list [ str ], line_heights_allowed : ArrayLike ) -> BestAssignmentPfitGainResult : \"\"\"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies, and fit a polynomial gain curve to the result.\"\"\" rms_e_residual , pha = find_optimal_assignment_height_info ( ph , e , line_heights_allowed ) ph = np . asarray ( ph ) e = np . asarray ( e ) gain = pha / e deg = min ( len ( e ) - 1 , 2 ) if deg == 0 : pfit_gain = np . polynomial . Polynomial ( gain ) else : pfit_gain = np . polynomial . Polynomial . fit ( pha , gain , deg = min ( len ( e ) - 1 , 2 )) result = BestAssignmentPfitGainResult ( rms_e_residual , ph_assigned = pha , residual_e = None , assignment_inds = None , pfit_gain = pfit_gain , energy_target = e , names_target = line_names , ph_target = ph , ) return result find_optimal_assignment_height_info ( ph , e , line_heights_allowed ) Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies. Source code in mass2/core/rough_cal.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 def find_optimal_assignment_height_info ( ph : ArrayLike , e : ArrayLike , line_heights_allowed : ArrayLike ) -> tuple [ float , NDArray ]: \"\"\"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies.\"\"\" # ph is a list of peak heights longer than e # e is a list of known peak energies # we want to find the set of peak heights from ph that are closest to being locally linear with the energies in e # when given 3 or less energies to match, use the largest peaks in peak order ph = np . asarray ( ph ) e = np . asarray ( e ) line_heights_allowed = np . asarray ( line_heights_allowed ) assert len ( e ) >= 1 if len ( e ) <= 2 : return 0 , np . array ( sorted ( ph [: len ( e )])) rms_e_residual , pha , pha_inds = rank_assignments ( ph , e ) best_ind = None best_rms_residual = np . inf print ( f \" { e =} \" ) for i_assign in range ( len ( rms_e_residual )): rms_e_candidate = rms_e_residual [ i_assign ] # pha[i,:] is one choice of len(e) values from ph to assign to e pha_inds_candidate = pha_inds [ i_assign , :] if rms_e_candidate > best_rms_residual : continue # check if peaks mathch height info print ( f \" { pha_inds_candidate =} \" ) print ( f \" { line_heights_allowed =} \" ) failed_line_height_check = False for j in range ( len ( e )): if pha_inds_candidate [ j ] not in line_heights_allowed [ j ]: failed_line_height_check = True print ( \"not allowed\" ) a = pha_inds_candidate [ j ] b = line_heights_allowed [ j ] print ( f \" { a =} { b =} \" ) break if failed_line_height_check : continue print ( \"is new best!\" ) best_rms_residual = rms_e_candidate best_ind = i_assign if best_ind is None : raise Exception ( \"no assignment found satisfying peak height info\" ) return rms_e_residual [ best_ind ], pha [ best_ind , :] find_pfit_gain_residual ( ph , e ) Find a 2nd degree polynomial fit to the gain curve defined by ph/e, and return the residuals in energy when using that gain curve to convert ph to energy. Source code in mass2/core/rough_cal.py 562 563 564 565 566 567 568 569 570 571 572 573 574 575 def find_pfit_gain_residual ( ph : ndarray , e : ndarray ) -> tuple [ ndarray , Polynomial ]: \"\"\"Find a 2nd degree polynomial fit to the gain curve defined by ph/e, and return the residuals in energy when using that gain curve to convert ph to energy.\"\"\" assert len ( ph ) == len ( e ) gain = ph / e pfit_gain = np . polynomial . Polynomial . fit ( ph , gain , deg = 2 ) def ph2energy ( ph : NDArray ) -> NDArray : \"\"\"Convert pulse height to energy using the fitted gain curve.\"\"\" return ph / pfit_gain ( ph ) predicted_e = ph2energy ( ph ) residual_e = e - predicted_e return residual_e , pfit_gain hist_smoothed ( pulse_heights , fwhm_pulse_height_units , bin_edges = None ) Compute a histogram of pulse heights and smooth it with a Gaussian of given FWHM. Source code in mass2/core/rough_cal.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 def hist_smoothed ( pulse_heights : ndarray , fwhm_pulse_height_units : float , bin_edges : ndarray | None = None , ) -> tuple [ ndarray , ndarray , ndarray ]: \"\"\"Compute a histogram of pulse heights and smooth it with a Gaussian of given FWHM.\"\"\" pulse_heights = pulse_heights . astype ( np . float64 ) # convert to float64 to avoid warpping subtraction and platform specific behavior regarding uint16s # linux CI will throw errors, while windows does not, but maybe is just silently wrong? assert len ( pulse_heights > 10 ), \"not enough pulses\" if bin_edges is None : n = 128 * 1024 lo = ( np . min ( pulse_heights ) - 3 * fwhm_pulse_height_units ) . astype ( np . float64 ) hi = ( np . max ( pulse_heights ) + 3 * fwhm_pulse_height_units ) . astype ( np . float64 ) bin_edges = np . linspace ( lo , hi , n + 1 ) _ , step_size = mass2 . misc . midpoints_and_step_size ( bin_edges ) counts , _ = np . histogram ( pulse_heights , bin_edges ) fwhm_in_bin_number_units = fwhm_pulse_height_units / step_size smoothed_counts = smooth_hist_with_gauassian_by_fft ( counts , fwhm_in_bin_number_units ) return smoothed_counts , bin_edges , counts local_maxima ( y ) Find local maxima and minima, as 1D arrays. Source code in mass2/core/rough_cal.py 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def local_maxima ( y : ndarray ) -> tuple [ ndarray , ndarray ]: \"Find local maxima and minima, as 1D arrays.\" local_maxima_inds = [] local_minima_inds = [] increasing = False for i in range ( len ( y ) - 1 ): if increasing and ( y [ i + 1 ] < y [ i ]): local_maxima_inds . append ( i ) increasing = False if not increasing and ( y [ i + 1 ] > y [ i ]): local_minima_inds . append ( i ) increasing = True # increasing starts false, so we always start with a miniumum return np . array ( local_maxima_inds ), np . array ( local_minima_inds ) minimize_entropy_linear ( indicator , uncorrected , bin_edges , fwhm_in_bin_number_units ) Minimize the entropy of a histogram of drift-corrected pulse heights by varying the slope of a linear correction based on the given indicator. Source code in mass2/core/rough_cal.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 def minimize_entropy_linear ( indicator : ndarray , uncorrected : ndarray , bin_edges : ndarray , fwhm_in_bin_number_units : int , ) -> tuple [ OptimizeResult , float32 ]: \"\"\"Minimize the entropy of a histogram of drift-corrected pulse heights by varying the slope of a linear correction based on the given indicator.\"\"\" indicator_mean = np . mean ( indicator ) indicator_zero_mean = indicator - indicator_mean def entropy_fun ( slope : float ) -> float : \"\"\"Return the entropy of a histogram of drift-corrected pulse heights, the optimization target.\"\"\" return drift_correct_entropy ( slope , indicator_zero_mean , uncorrected , bin_edges , fwhm_in_bin_number_units ) result = sp . optimize . minimize_scalar ( entropy_fun , bracket = [ 0 , 0.1 ]) return result , indicator_mean peakfind_local_maxima_of_smoothed_hist ( pulse_heights , fwhm_pulse_height_units , bin_edges = None ) Find local maxima in a smoothed histogram of pulse heights. Source code in mass2/core/rough_cal.py 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def peakfind_local_maxima_of_smoothed_hist ( pulse_heights : ndarray , fwhm_pulse_height_units : float , bin_edges : ndarray | None = None , ) -> SmoothedLocalMaximaResult : \"\"\"Find local maxima in a smoothed histogram of pulse heights.\"\"\" pulse_heights = pulse_heights . astype ( np . float64 ) assert len ( pulse_heights > 10 ), \"not enough pulses\" smoothed_counts , bin_edges , counts = hist_smoothed ( pulse_heights , fwhm_pulse_height_units , bin_edges ) bin_centers , _step_size = mass2 . misc . midpoints_and_step_size ( bin_edges ) local_maxima_inds , local_minima_inds = local_maxima ( smoothed_counts ) # require a minimum before and after a maximum (the first check is redundant with behavior of local_maxima) if local_maxima_inds [ 0 ] < local_minima_inds [ 0 ]: local_maxima_inds = local_maxima_inds [ 1 :] if local_maxima_inds [ - 1 ] > local_minima_inds [ - 1 ]: local_maxima_inds = local_maxima_inds [: - 1 ] return SmoothedLocalMaximaResult ( fwhm_pulse_height_units , bin_centers , counts , smoothed_counts , local_maxima_inds , local_minima_inds , ) rank_3peak_assignments ( ph , e , line_names , max_fractional_energy_error_3rd_assignment = 0.1 , min_gain_fraction_at_ph_30k = 0.25 ) Explore and rank possible assignments of pulse heights to energies when there are 3 or more lines. Source code in mass2/core/rough_cal.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def rank_3peak_assignments ( ph : NDArray , e : NDArray , line_names : Iterable [ str ], max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Explore and rank possible assignments of pulse heights to energies when there are 3 or more lines.\"\"\" # we explore possible line assignments, and down select based on knowledge of gain curve shape # gain = ph/e, and we assume gain starts at zero, decreases with pulse height, and # that a 2nd order polynomial is a reasonably good approximation # with one assignment we model the gain as constant, and use that to find the most likely # 2nd assignments, then we model the gain as linear, and use that to rank 3rd assignments dfe = pl . DataFrame ({ \"e0_ind\" : np . arange ( len ( e )), \"e0\" : e , \"name\" : line_names }) dfph = pl . DataFrame ({ \"ph0_ind\" : np . arange ( len ( ph )), \"ph0\" : ph }) # dfph should know about peak_area and use it to weight choices somehow # 1st assignments #### # e0 and ph0 are the first assignment df0 = dfe . join ( dfph , how = \"cross\" ) . with_columns ( gain0 = pl . col ( \"ph0\" ) / pl . col ( \"e0\" )) # 2nd assignments #### # e1 and ph1 are the 2nd assignment df1 = ( df0 . join ( df0 , how = \"cross\" ) . rename ({ \"e0_right\" : \"e1\" , \"ph0_right\" : \"ph1\" }) . drop ( \"e0_ind_right\" , \"ph0_ind_right\" , \"gain0_right\" ) ) # 1) keep only assignments with e0<e1 and ph0<ph1 to avoid looking at the same pair in reverse df1 = df1 . filter (( pl . col ( \"e0\" ) < pl . col ( \"e1\" )) . and_ ( pl . col ( \"ph0\" ) < pl . col ( \"ph1\" ))) # 2) the gain slope must be negative df1 = ( df1 . with_columns ( gain1 = pl . col ( \"ph1\" ) / pl . col ( \"e1\" )) . with_columns ( gain_slope = ( pl . col ( \"gain1\" ) - pl . col ( \"gain0\" )) / ( pl . col ( \"ph1\" ) - pl . col ( \"ph0\" ))) . filter ( pl . col ( \"gain_slope\" ) < 0 ) ) # 3) the gain slope should not have too large a magnitude df1 = df1 . with_columns ( gain_at_0 = pl . col ( \"gain0\" ) - pl . col ( \"ph0\" ) * pl . col ( \"gain_slope\" )) df1 = df1 . with_columns ( gain_frac_at_ph30k = ( 1 + 30000 * pl . col ( \"gain_slope\" ) / pl . col ( \"gain_at_0\" ))) df1 = df1 . filter ( pl . col ( \"gain_frac_at_ph30k\" ) > min_gain_fraction_at_ph_30k ) # 3rd assignments #### # e2 and ph2 are the 3rd assignment df2 = df1 . join ( df0 . select ( e2 = \"e0\" , ph2 = \"ph0\" ), how = \"cross\" ) df2 = df2 . with_columns ( gain_at_ph2 = pl . col ( \"gain_at_0\" ) + pl . col ( \"gain_slope\" ) * pl . col ( \"ph2\" )) df2 = df2 . with_columns ( e_at_ph2 = pl . col ( \"ph2\" ) / pl . col ( \"gain_at_ph2\" )) df2 = df2 . filter (( pl . col ( \"e1\" ) < pl . col ( \"e2\" )) . and_ ( pl . col ( \"ph1\" ) < pl . col ( \"ph2\" ))) # 1) rank 3rd assignments by energy error at ph2 assuming gain = gain_slope*ph+gain_at_0 # where gain_slope and gain are calculated from assignments 1 and 2 df2 = df2 . with_columns ( e_err_at_ph2 = pl . col ( \"e_at_ph2\" ) - pl . col ( \"e2\" )) . sort ( by = np . abs ( pl . col ( \"e_err_at_ph2\" ))) # 2) return a dataframe downselected to the assignments and the ranking criteria # 3) throw away assignments with large (default 10%) energy errors df3peak = df2 . select ( \"e0\" , \"ph0\" , \"e1\" , \"ph1\" , \"e2\" , \"ph2\" , \"e_err_at_ph2\" ) . filter ( np . abs ( pl . col ( \"e_err_at_ph2\" ) / pl . col ( \"e2\" )) < max_fractional_energy_error_3rd_assignment ) return df3peak , dfe rank_assignments ( ph , e ) Rank possible assignments of pulse heights to energies by how locally linear their implied gain curves are. Source code in mass2/core/rough_cal.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def rank_assignments ( ph : ArrayLike , e : ArrayLike ) -> tuple [ NDArray , NDArray , NDArray ]: \"\"\"Rank possible assignments of pulse heights to energies by how locally linear their implied gain curves are.\"\"\" # ph is a list of peak heights longer than e # e is a list of known peak energies # we want to find the set of peak heights from ph that are closest to being locally linear with the energies in e e = np . array ( e ) ph = np . array ( ph ) e . sort () ph . sort () pha = np . array ( list ( itertools . combinations ( ph , len ( e )))) pha_inds = np . array ( list ( itertools . combinations ( np . arange ( len ( ph )), len ( e )))) # pha[i,:] is one choice of len(e) values from ph to assign to e # we use linear interpolation of the form y = y0 + (y1-y0)*(x-x0)/(x1-x0) # on each set of 3 values # with y = e and x = ph # x is pha[:,1:-1], x0 is pha[:,:-2], x1 is pha[:,2:] x = pha [:, 1 : - 1 ] x0 = pha [:, : - 2 ] x1 = pha [:, 2 :] y0 = e [: - 2 ] y1 = e [ 2 :] x_m_x0_over_x1_m_x0 = ( x - x0 ) / ( x1 - x0 ) y = y0 + ( y1 - y0 ) * x_m_x0_over_x1_m_x0 y_expected = e [ 1 : - 1 ] rms_e_residual = np . asarray ( mass2 . misc . root_mean_squared ( y - y_expected , axis = 1 )) # prefer negative slopes for gain # gain_first = ph[0]/e[0] # gain_last = ph[-1]/e[-1] return rms_e_residual , pha , pha_inds smooth_hist_with_gauassian_by_fft ( hist , fwhm_in_bin_number_units ) Smooth a histogram by convolution with a Gaussian, using FFTs. Source code in mass2/core/rough_cal.py 302 303 304 305 306 def smooth_hist_with_gauassian_by_fft ( hist : ndarray , fwhm_in_bin_number_units : float ) -> ndarray : \"\"\"Smooth a histogram by convolution with a Gaussian, using FFTs.\"\"\" kernel = smooth_hist_with_gauassian_by_fft_compute_kernel ( len ( hist ), fwhm_in_bin_number_units ) y = np . fft . irfft ( np . fft . rfft ( hist ) * kernel ) return y smooth_hist_with_gauassian_by_fft_compute_kernel ( nbins , fwhm_in_bin_number_units ) Compute the DFT of a Gaussian kernel for smoothing a histogram. Source code in mass2/core/rough_cal.py 309 310 311 312 313 314 315 def smooth_hist_with_gauassian_by_fft_compute_kernel ( nbins : int , fwhm_in_bin_number_units : float ) -> ndarray : \"\"\"Compute the DFT of a Gaussian kernel for smoothing a histogram.\"\"\" sigma = fwhm_in_bin_number_units / ( np . sqrt ( np . log ( 2 ) * 2 ) * 2 ) tbw = 1.0 / sigma / ( np . pi * 2 ) tx = np . fft . rfftfreq ( nbins ) kernel = np . exp ( - ( tx ** 2 ) / 2 / tbw ** 2 ) return kernel Tools for working with continuous data, as taken by the True Bequerel project. TriggerResult dataclass A trigger result from applying a triggering filter and threshold to a TrueBqBin data source. Source code in mass2/core/truebq_bin.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 @dataclass ( frozen = True ) class TriggerResult : \"\"\"A trigger result from applying a triggering filter and threshold to a TrueBqBin data source.\"\"\" data_source : \"TrueBqBin\" filter_in : np . ndarray threshold : float trig_inds : np . ndarray limit_samples : int def plot ( self , decimate : int = 10 , n_limit : int = 100000 , offset_raw : int = 0 , x_axis_time_s : bool = False , ax : plt . Axes | None = None ) -> None : \"\"\"Make a diagnostic plot of the trigger result.\"\"\" if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # raw (full-resolution) index ranges raw_start = offset_raw raw_stop = raw_start + n_limit * decimate data = self . data_source . data # scaling for x-axis (applied after decimation) x_scale = self . data_source . frametime_s * decimate if x_axis_time_s else 1 # raw filter output filt_raw = fast_apply_filter ( data [ raw_start : raw_stop ], self . filter_in ) # decimated data and filter data_dec = data [ raw_start : raw_stop : decimate ] filt_dec = filt_raw [:: decimate ] # truncate to the same length n = min ( len ( data_dec ), len ( filt_dec )) data_dec = data_dec [: n ] filt_dec = filt_dec [: n ] # shared x-axis x_dec = np . arange ( n ) * x_scale # plot data + filter plt . plot ( x_dec , data_dec , \".\" , label = \"data\" ) plt . plot ( x_dec , filt_dec , label = \"filter_out\" ) plt . axhline ( self . threshold , label = \"threshold\" ) # trigger indices (raw) \u2192 restrict to plotted window \u2192 convert to decimated indices trig_inds_raw = ( pl . DataFrame ({ \"trig_inds\" : self . trig_inds }) . filter ( pl . col ( \"trig_inds\" ) . is_between ( raw_start , raw_stop )) . to_series () . to_numpy () ) trig_inds_dec = ( trig_inds_raw - raw_start ) // decimate # clip to avoid indexing past n trig_inds_dec = trig_inds_dec [ trig_inds_dec < n ] plt . plot ( x_dec [ trig_inds_dec ], filt_dec [ trig_inds_dec ], \"o\" , label = \"trig_inds filt\" ) plt . plot ( x_dec [ trig_inds_dec ], data_dec [ trig_inds_dec ], \"o\" , label = \"trig_inds data\" ) # labels plt . title ( f \" { self . data_source . description } , trigger result debug plot\" ) plt . legend () plt . xlabel ( \"time with arb offset / s\" if x_axis_time_s else \"sample number (decimated)\" ) plt . ylabel ( \"signal (arb)\" ) def get_noise ( self , n_dead_samples_after_pulse_trigger : int , n_record_samples : int , max_noise_triggers : int = 200 , ) -> NoiseChannel : \"\"\"Synthesize a NoiseChannel from the data source by finding time periods without pulse triggers.\"\"\" noise_trigger_inds = get_noise_trigger_inds ( self . trig_inds , n_dead_samples_after_pulse_trigger , n_record_samples , max_noise_triggers , ) inds = noise_trigger_inds [ noise_trigger_inds > 0 ] # ensure all inds are greater than 0 inds = inds [ inds < ( len ( self . data_source . data ) - n_record_samples )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous ( self . data_source . data , npre = 0 , nsamples = n_record_samples , inds = inds , ) df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) noise = NoiseChannel ( df , header_df = self . data_source . header_df , frametime_s = self . data_source . frametime_s , ) return noise def to_channel_copy_to_memory ( self , noise_n_dead_samples_after_pulse_trigger : int , npre : int , npost : int , invert : bool = False ) -> Channel : \"\"\"Create a Channel object by copying pulse data into memory.\"\"\" noise = self . get_noise ( noise_n_dead_samples_after_pulse_trigger , npre + npost , max_noise_triggers = 1000 , ) inds = self . trig_inds [ self . trig_inds > npre ] inds = inds [ inds < ( len ( self . data_source . data ) - npre - npost )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous ( self . data_source . data , npre = npre , nsamples = npre + npost , inds = inds ) assert pulses . shape [ 0 ] == len ( inds ), \"pulses and trig_inds must have the same length\" if invert : df = pl . DataFrame ({ \"pulse\" : pulses * - 1 , \"framecount\" : inds }) else : df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) ch_header = ChannelHeader ( self . data_source . description , self . data_source . channel_number , self . data_source . frametime_s , npre , npre + npost , self . data_source . header_df , ) ch = Channel ( df , ch_header , npulses = len ( pulses ), noise = noise ) return ch def to_channel_mmap ( self , noise_n_dead_samples_after_pulse_trigger : int , npre : int , npost : int , invert : bool = False , verbose : bool = True , ) -> Channel : \"\"\"Create a Channel object by memory-mapping pulse data from disk.\"\"\" noise = self . get_noise ( noise_n_dead_samples_after_pulse_trigger , npre + npost , max_noise_triggers = 1000 , ) inds = self . trig_inds [ self . trig_inds > npre ] # ensure all inds inbounds inds = inds [ inds < ( len ( self . data_source . data ) - npre - npost )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous_mmap_with_cache ( self . data_source . data , npre = npre , nsamples = npre + npost , inds = inds , bin_path = self . data_source . bin_path , verbose = verbose , ) if invert : df = pl . DataFrame ({ \"pulse\" : pulses * - 1 , \"framecount\" : inds }) else : df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) ch_header = ChannelHeader ( self . data_source . description , self . data_source . channel_number , self . data_source . frametime_s , npre , npre + npost , self . data_source . header_df , ) ch = Channel ( df , ch_header , npulses = len ( pulses ), noise = noise ) return ch get_noise ( n_dead_samples_after_pulse_trigger , n_record_samples , max_noise_triggers = 200 ) Synthesize a NoiseChannel from the data source by finding time periods without pulse triggers. Source code in mass2/core/truebq_bin.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def get_noise ( self , n_dead_samples_after_pulse_trigger : int , n_record_samples : int , max_noise_triggers : int = 200 , ) -> NoiseChannel : \"\"\"Synthesize a NoiseChannel from the data source by finding time periods without pulse triggers.\"\"\" noise_trigger_inds = get_noise_trigger_inds ( self . trig_inds , n_dead_samples_after_pulse_trigger , n_record_samples , max_noise_triggers , ) inds = noise_trigger_inds [ noise_trigger_inds > 0 ] # ensure all inds are greater than 0 inds = inds [ inds < ( len ( self . data_source . data ) - n_record_samples )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous ( self . data_source . data , npre = 0 , nsamples = n_record_samples , inds = inds , ) df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) noise = NoiseChannel ( df , header_df = self . data_source . header_df , frametime_s = self . data_source . frametime_s , ) return noise plot ( decimate = 10 , n_limit = 100000 , offset_raw = 0 , x_axis_time_s = False , ax = None ) Make a diagnostic plot of the trigger result. Source code in mass2/core/truebq_bin.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def plot ( self , decimate : int = 10 , n_limit : int = 100000 , offset_raw : int = 0 , x_axis_time_s : bool = False , ax : plt . Axes | None = None ) -> None : \"\"\"Make a diagnostic plot of the trigger result.\"\"\" if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # raw (full-resolution) index ranges raw_start = offset_raw raw_stop = raw_start + n_limit * decimate data = self . data_source . data # scaling for x-axis (applied after decimation) x_scale = self . data_source . frametime_s * decimate if x_axis_time_s else 1 # raw filter output filt_raw = fast_apply_filter ( data [ raw_start : raw_stop ], self . filter_in ) # decimated data and filter data_dec = data [ raw_start : raw_stop : decimate ] filt_dec = filt_raw [:: decimate ] # truncate to the same length n = min ( len ( data_dec ), len ( filt_dec )) data_dec = data_dec [: n ] filt_dec = filt_dec [: n ] # shared x-axis x_dec = np . arange ( n ) * x_scale # plot data + filter plt . plot ( x_dec , data_dec , \".\" , label = \"data\" ) plt . plot ( x_dec , filt_dec , label = \"filter_out\" ) plt . axhline ( self . threshold , label = \"threshold\" ) # trigger indices (raw) \u2192 restrict to plotted window \u2192 convert to decimated indices trig_inds_raw = ( pl . DataFrame ({ \"trig_inds\" : self . trig_inds }) . filter ( pl . col ( \"trig_inds\" ) . is_between ( raw_start , raw_stop )) . to_series () . to_numpy () ) trig_inds_dec = ( trig_inds_raw - raw_start ) // decimate # clip to avoid indexing past n trig_inds_dec = trig_inds_dec [ trig_inds_dec < n ] plt . plot ( x_dec [ trig_inds_dec ], filt_dec [ trig_inds_dec ], \"o\" , label = \"trig_inds filt\" ) plt . plot ( x_dec [ trig_inds_dec ], data_dec [ trig_inds_dec ], \"o\" , label = \"trig_inds data\" ) # labels plt . title ( f \" { self . data_source . description } , trigger result debug plot\" ) plt . legend () plt . xlabel ( \"time with arb offset / s\" if x_axis_time_s else \"sample number (decimated)\" ) plt . ylabel ( \"signal (arb)\" ) to_channel_copy_to_memory ( noise_n_dead_samples_after_pulse_trigger , npre , npost , invert = False ) Create a Channel object by copying pulse data into memory. Source code in mass2/core/truebq_bin.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def to_channel_copy_to_memory ( self , noise_n_dead_samples_after_pulse_trigger : int , npre : int , npost : int , invert : bool = False ) -> Channel : \"\"\"Create a Channel object by copying pulse data into memory.\"\"\" noise = self . get_noise ( noise_n_dead_samples_after_pulse_trigger , npre + npost , max_noise_triggers = 1000 , ) inds = self . trig_inds [ self . trig_inds > npre ] inds = inds [ inds < ( len ( self . data_source . data ) - npre - npost )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous ( self . data_source . data , npre = npre , nsamples = npre + npost , inds = inds ) assert pulses . shape [ 0 ] == len ( inds ), \"pulses and trig_inds must have the same length\" if invert : df = pl . DataFrame ({ \"pulse\" : pulses * - 1 , \"framecount\" : inds }) else : df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) ch_header = ChannelHeader ( self . data_source . description , self . data_source . channel_number , self . data_source . frametime_s , npre , npre + npost , self . data_source . header_df , ) ch = Channel ( df , ch_header , npulses = len ( pulses ), noise = noise ) return ch to_channel_mmap ( noise_n_dead_samples_after_pulse_trigger , npre , npost , invert = False , verbose = True ) Create a Channel object by memory-mapping pulse data from disk. Source code in mass2/core/truebq_bin.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def to_channel_mmap ( self , noise_n_dead_samples_after_pulse_trigger : int , npre : int , npost : int , invert : bool = False , verbose : bool = True , ) -> Channel : \"\"\"Create a Channel object by memory-mapping pulse data from disk.\"\"\" noise = self . get_noise ( noise_n_dead_samples_after_pulse_trigger , npre + npost , max_noise_triggers = 1000 , ) inds = self . trig_inds [ self . trig_inds > npre ] # ensure all inds inbounds inds = inds [ inds < ( len ( self . data_source . data ) - npre - npost )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous_mmap_with_cache ( self . data_source . data , npre = npre , nsamples = npre + npost , inds = inds , bin_path = self . data_source . bin_path , verbose = verbose , ) if invert : df = pl . DataFrame ({ \"pulse\" : pulses * - 1 , \"framecount\" : inds }) else : df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) ch_header = ChannelHeader ( self . data_source . description , self . data_source . channel_number , self . data_source . frametime_s , npre , npre + npost , self . data_source . header_df , ) ch = Channel ( df , ch_header , npulses = len ( pulses ), noise = noise ) return ch TrueBqBin dataclass Represents a binary data file from the True Bequerel project. Source code in mass2/core/truebq_bin.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 @dataclass ( frozen = True ) class TrueBqBin : \"\"\"Represents a binary data file from the True Bequerel project.\"\"\" bin_path : Path description : str channel_number : int header_df : pl . DataFrame frametime_s : float voltage_scale : float data : np . ndarray # the bin file is a continuous data aqusition, untriggered @classmethod def load ( cls , bin_path : str | Path ) -> \"TrueBqBin\" : \"\"\"Create a TrueBqBin object by memory-mapping the given binary file.\"\"\" bin_path = Path ( bin_path ) try : # for when it's named like dev2_ai6 channel_number = int ( str ( bin_path . parent )[ - 1 ]) except ValueError : # for when it's named like 2A def bay2int ( bay : str ) -> int : \"\"\"Convert a bay name like '2A' to a channel number like 4.\"\"\" return ( int ( bay [ 0 ]) - 1 ) * 4 + \"ABCD\" . index ( bay [ 1 ] . upper ()) channel_number = bay2int ( str ( bin_path . parent . stem )) desc = str ( bin_path . parent . parent . stem ) + \"_\" + str ( bin_path . parent . stem ) header_np = np . memmap ( bin_path , dtype = header_dtype , mode = \"r\" , offset = 0 , shape = 1 ) sample_rate_hz = header_np [ \"sample_rate_hz\" ][ 0 ] header_df = pl . from_numpy ( header_np ) data = np . memmap ( bin_path , dtype = np . int16 , mode = \"r\" , offset = 68 ) return cls ( bin_path , desc , channel_number , header_df , 1 / sample_rate_hz , header_np [ \"voltage_scale\" ][ 0 ], data , ) def trigger ( self , filter_in : NDArray , threshold : float , limit_hours : float | None = None , verbose : bool = True ) -> TriggerResult : \"\"\"Compute trigger indices by applying the given filter and threshold to the data.\"\"\" if limit_hours is None : limit_samples = len ( self . data ) else : limit_samples = int ( limit_hours * 3600 / self . frametime_s ) trig_inds = _fasttrig_filter_trigger_with_cache ( self . data , filter_in , threshold , limit_samples , self . bin_path , verbose = verbose ) return TriggerResult ( self , filter_in , threshold , trig_inds , limit_samples ) load ( bin_path ) classmethod Create a TrueBqBin object by memory-mapping the given binary file. Source code in mass2/core/truebq_bin.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @classmethod def load ( cls , bin_path : str | Path ) -> \"TrueBqBin\" : \"\"\"Create a TrueBqBin object by memory-mapping the given binary file.\"\"\" bin_path = Path ( bin_path ) try : # for when it's named like dev2_ai6 channel_number = int ( str ( bin_path . parent )[ - 1 ]) except ValueError : # for when it's named like 2A def bay2int ( bay : str ) -> int : \"\"\"Convert a bay name like '2A' to a channel number like 4.\"\"\" return ( int ( bay [ 0 ]) - 1 ) * 4 + \"ABCD\" . index ( bay [ 1 ] . upper ()) channel_number = bay2int ( str ( bin_path . parent . stem )) desc = str ( bin_path . parent . parent . stem ) + \"_\" + str ( bin_path . parent . stem ) header_np = np . memmap ( bin_path , dtype = header_dtype , mode = \"r\" , offset = 0 , shape = 1 ) sample_rate_hz = header_np [ \"sample_rate_hz\" ][ 0 ] header_df = pl . from_numpy ( header_np ) data = np . memmap ( bin_path , dtype = np . int16 , mode = \"r\" , offset = 68 ) return cls ( bin_path , desc , channel_number , header_df , 1 / sample_rate_hz , header_np [ \"voltage_scale\" ][ 0 ], data , ) trigger ( filter_in , threshold , limit_hours = None , verbose = True ) Compute trigger indices by applying the given filter and threshold to the data. Source code in mass2/core/truebq_bin.py 302 303 304 305 306 307 308 309 def trigger ( self , filter_in : NDArray , threshold : float , limit_hours : float | None = None , verbose : bool = True ) -> TriggerResult : \"\"\"Compute trigger indices by applying the given filter and threshold to the data.\"\"\" if limit_hours is None : limit_samples = len ( self . data ) else : limit_samples = int ( limit_hours * 3600 / self . frametime_s ) trig_inds = _fasttrig_filter_trigger_with_cache ( self . data , filter_in , threshold , limit_samples , self . bin_path , verbose = verbose ) return TriggerResult ( self , filter_in , threshold , trig_inds , limit_samples ) fast_apply_filter ( data , filter_in ) Apply a filter to the data, returning the filter output. Source code in mass2/core/truebq_bin.py 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 @njit def fast_apply_filter ( data : NDArray , filter_in : NDArray ) -> NDArray : \"\"\"Apply a filter to the data, returning the filter output.\"\"\" cache = np . zeros ( len ( filter_in )) filter = np . zeros ( len ( filter_in )) filter [:] = filter_in filter_len = len ( filter ) filter_out = np . zeros ( len ( data ) - len ( filter )) j = 0 jmax = len ( data ) - filter_len - 1 while j <= jmax : cache [:] = data [ j : ( j + filter_len )] filter_out [ j ] = np . dot ( cache , filter ) j += 1 return filter_out fasttrig_filter_trigger ( data , filter_in , threshold , verbose ) Apply a filter to the data and return trigger indices where the filter output crosses the threshold. Source code in mass2/core/truebq_bin.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 @njit def fasttrig_filter_trigger ( data : NDArray , filter_in : NDArray , threshold : float , verbose : bool ) -> NDArray : \"\"\"Apply a filter to the data and return trigger indices where the filter output crosses the threshold.\"\"\" assert threshold > 0 , \"algorithm assumes we trigger with positive threshold, change sign of filter_in to accomodate\" filter_len = len ( filter_in ) inds = [] jmax = len ( data ) - filter_len - 1 # njit only likes float64s, so I'm trying to force float64 use without allocating a ton of memory cache = np . zeros ( len ( filter_in )) filter = np . zeros ( len ( filter_in )) filter [:] = filter_in # intitalize a,b,c j = 0 cache [:] = data [ j : ( j + filter_len )] b = np . dot ( cache , filter ) a = b # won't be used, just need same type j = 1 cache [:] = data [ j : ( j + filter_len )] c = np . dot ( cache , filter ) j = 2 ready = False prog_step = jmax // 100 prog_ticks = 0 while j <= jmax : if j % prog_step == 0 : prog_ticks += 1 if verbose : print ( f \"fasttrig_filter_trigger { prog_ticks } / { 100 } \" ) a , b = b , c cache [:] = data [ j : ( j + filter_len )] c = np . dot ( cache , filter ) if b > threshold and b >= c and b > a and ready : inds . append ( j ) ready = False if b < 0 : # hold off on retriggering until we see opposite sign slope ready = True j += 1 return np . array ( inds ) filter_and_residual_rms ( data , chosen_filter , avg_pulse , trig_inds , npre , nsamples , polarity ) Apply a filter to pulses extracted from data at the given trigger indices, returning filter values and residual RMS. Source code in mass2/core/truebq_bin.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 def filter_and_residual_rms ( data : NDArray , chosen_filter : NDArray , avg_pulse : NDArray , trig_inds : NDArray , npre : int , nsamples : int , polarity : int ) -> tuple [ NDArray , NDArray , NDArray ]: \"\"\"Apply a filter to pulses extracted from data at the given trigger indices, returning filter values and residual RMS.\"\"\" filt_value = np . zeros ( len ( trig_inds )) residual_rms = np . zeros ( len ( trig_inds )) filt_value_template = np . zeros ( len ( trig_inds )) template = avg_pulse - np . mean ( avg_pulse ) template /= np . sqrt ( np . dot ( template , template )) for i in range ( len ( trig_inds )): j = trig_inds [ i ] pulse = data [ j - npre : j + nsamples - npre ] * polarity pulse -= pulse . mean () filt_value [ i ] = np . dot ( chosen_filter , pulse ) filt_value_template [ i ] = np . dot ( template , pulse ) residual = pulse - template * filt_value_template [ i ] residual_rms_val = misc . root_mean_squared ( residual ) residual_rms [ i ] = residual_rms_val return filt_value , residual_rms , filt_value_template gather_pulses_from_inds_numpy_contiguous ( data , npre , nsamples , inds ) Gather pulses from data at the given trigger indices, returning a contiguous numpy array. Source code in mass2/core/truebq_bin.py 438 439 440 441 442 443 444 445 446 def gather_pulses_from_inds_numpy_contiguous ( data : NDArray , npre : int , nsamples : int , inds : NDArray ) -> NDArray : \"\"\"Gather pulses from data at the given trigger indices, returning a contiguous numpy array.\"\"\" assert all ( inds > npre ), \"all inds must be greater than npre\" assert all ( inds < ( len ( data ) - nsamples )), \"all inds must be less than len(data) - nsamples\" offsets = inds - npre # shift by npre to start at correct offset pulses = np . zeros (( len ( offsets ), nsamples ), dtype = np . int16 ) for i , offset in enumerate ( offsets ): pulses [ i , :] = data [ offset : offset + nsamples ] return pulses gather_pulses_from_inds_numpy_contiguous_mmap ( data , npre , nsamples , inds , filename = '.mmapped_pulses.npy' ) Gather pulses from data at the given trigger indices, returning a memory-mapped numpy array. Source code in mass2/core/truebq_bin.py 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def gather_pulses_from_inds_numpy_contiguous_mmap ( data : NDArray , npre : int , nsamples : int , inds : NDArray , filename : str | Path = \".mmapped_pulses.npy\" ) -> NDArray : \"\"\"Gather pulses from data at the given trigger indices, returning a memory-mapped numpy array.\"\"\" assert all ( inds > npre ), \"all inds must be greater than npre\" assert all ( inds < ( len ( data ) - nsamples )), \"all inds must be less than len(data) - nsamples\" offsets = inds - npre # shift by npre to start at correct offset pulses = np . memmap ( filename , dtype = np . int16 , mode = \"w+\" , shape = ( len ( offsets ), nsamples )) for i , offset in enumerate ( offsets ): pulses [ i , :] = data [ offset : offset + nsamples ] pulses . flush () # re-open the mmap to ensure it is read-only del pulses pulses = np . memmap ( filename , dtype = np . int16 , mode = \"r\" , shape = ( len ( offsets ), nsamples )) return pulses gather_pulses_from_inds_numpy_contiguous_mmap_with_cache ( data , npre , nsamples , inds , bin_path , verbose = True ) Gather pulses from data at the given trigger indices, returning a memory-mapped numpy array, using a cache. Source code in mass2/core/truebq_bin.py 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 def gather_pulses_from_inds_numpy_contiguous_mmap_with_cache ( data : NDArray , npre : int , nsamples : int , inds : NDArray , bin_path : Path | str , verbose : bool = True ) -> NDArray | np . memmap : \"\"\"Gather pulses from data at the given trigger indices, returning a memory-mapped numpy array, using a cache.\"\"\" bin_full_path = Path ( bin_path ) . absolute () inds = inds [ inds > npre ] # ensure all inds inbounds inds = inds [ inds < ( len ( data ) - nsamples )] # ensure all inds inbounds inds_hash = hashlib . sha256 ( inds . tobytes ()) . hexdigest () to_hash_str = str ( npre ) + str ( nsamples ) + str ( bin_full_path ) + inds_hash key = hashlib . sha256 ( to_hash_str . encode ()) . hexdigest () fname = f \". { key } .truebq_pulse_cache.npy\" cache_dir_path = bin_full_path . parent / \"_truebq_bin_cache\" cache_dir_path . mkdir ( exist_ok = True ) file_path = cache_dir_path / fname inds = np . array ( inds ) if file_path . is_file (): # check if the file is the right size Nbytes = len ( inds ) * nsamples * 2 # 2 bytes per int16 if file_path . stat () . st_size != Nbytes : # on windows the error if the file is the wrong size makes it sound like you don't have enough memory # and python doesn't seem to catch the exception, so we check the size here if verbose : print ( f \"pulse cache is corrupted, re-gathering pulses for { file_path } \" ) file_path . unlink () cache_hit = False else : cache_hit = True else : cache_hit = False if cache_hit : if verbose : print ( f \"pulse cache hit for { file_path } \" ) return np . memmap ( file_path , dtype = np . int16 , mode = \"r\" , shape = ( len ( inds ), nsamples )) if verbose : print ( f \"pulse cache miss for { file_path } \" ) return gather_pulses_from_inds_numpy_contiguous_mmap ( data , npre , nsamples , inds , filename = file_path ) get_noise_trigger_inds ( pulse_trigger_inds , n_dead_samples_after_previous_pulse , n_record_samples , max_noise_triggers ) Get trigger indices for noise periods, avoiding pulses. Source code in mass2/core/truebq_bin.py 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def get_noise_trigger_inds ( pulse_trigger_inds : ArrayLike , n_dead_samples_after_previous_pulse : int , n_record_samples : int , max_noise_triggers : int , ) -> NDArray : \"\"\"Get trigger indices for noise periods, avoiding pulses.\"\"\" pulse_trigger_inds = np . asarray ( pulse_trigger_inds ) diffs = np . diff ( pulse_trigger_inds ) inds = [] for i in range ( len ( diffs )): if diffs [ i ] > n_dead_samples_after_previous_pulse : n_make = ( diffs [ i ] - n_dead_samples_after_previous_pulse ) // n_record_samples ind0 = pulse_trigger_inds [ i ] + n_dead_samples_after_previous_pulse for j in range ( n_make ): inds . append ( ind0 + n_record_samples * j ) if len ( inds ) == max_noise_triggers : return np . array ( inds ) return np . array ( inds ) write_truebq_bin_file ( path , data , sample_rate_hz , * , voltage_scale = 1.0 , format_version = 1 , schema_version = 1 , data_reduction_factor = 1 , acquisition_flags = 0 , start_time = None , stop_time = None ) Write a binary file that can be opened by TrueBqBin.load(). This function writes data efficiently without copying by using memory mapping and direct file operations. Args: path: Output file path data: Data array to write (will be converted to int16 if not already) sample_rate_hz: Sample rate in Hz voltage_scale: Voltage scaling factor format_version: File format version (default: 1) schema_version: Schema version (default: 1) data_reduction_factor: Data reduction factor (default: 1) acquisition_flags: Acquisition flags (default: 0) start_time: Start time as uint64 array of length 2 (optional) stop_time: Stop time as uint64 array of length 2 (optional) Source code in mass2/core/truebq_bin.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 def write_truebq_bin_file ( path : str | Path , data : np . ndarray , sample_rate_hz : float , * , # force keyword only voltage_scale : float = 1.0 , format_version : int = 1 , schema_version : int = 1 , data_reduction_factor : int = 1 , acquisition_flags : int = 0 , start_time : np . ndarray | None = None , stop_time : np . ndarray | None = None , ) -> None : \"\"\" Write a binary file that can be opened by TrueBqBin.load(). This function writes data efficiently without copying by using memory mapping and direct file operations. Args: path: Output file path data: Data array to write (will be converted to int16 if not already) sample_rate_hz: Sample rate in Hz voltage_scale: Voltage scaling factor format_version: File format version (default: 1) schema_version: Schema version (default: 1) data_reduction_factor: Data reduction factor (default: 1) acquisition_flags: Acquisition flags (default: 0) start_time: Start time as uint64 array of length 2 (optional) stop_time: Stop time as uint64 array of length 2 (optional) \"\"\" path = Path ( path ) path . parent . mkdir ( parents = True , exist_ok = True ) # Ensure data is int16 (convert if necessary, but avoid unnecessary copying) if data . dtype != np . int16 : if not np . can_cast ( data . dtype , np . int16 , casting = \"safe\" ): print ( f \"Warning: Converting data from { data . dtype } to int16 may cause data loss\" ) data = data . astype ( np . int16 ) # Prepare header num_samples = len ( data ) # Default time values if not provided if start_time is None : start_time = np . array ([ 0 , 0 ], dtype = np . uint64 ) if stop_time is None : stop_time = np . array ([ 0 , 0 ], dtype = np . uint64 ) # Create header array header = np . array ( [ ( format_version , schema_version , sample_rate_hz , data_reduction_factor , voltage_scale , acquisition_flags , start_time , stop_time , num_samples , ) ], dtype = header_dtype , ) # Create the file with the correct size with open ( path , \"wb\" ) as f : # Write header f . write ( header . tobytes ()) # For large data arrays, write in chunks to avoid memory issues chunk_size = 1024 * 1024 # 1MB chunks if data . nbytes <= chunk_size : # Small data, write directly f . write ( data . tobytes ()) else : # Large data, write in chunks data_flat = data . ravel () # Flatten without copying if possible for i in range ( 0 , len ( data_flat ), chunk_size // data . itemsize ): chunk = data_flat [ i : i + chunk_size // data . itemsize ] f . write ( chunk . tobytes ()) Various utility functions and classes: MouseClickReader: a class to use as a callback for reading mouse click locations in matplotlib plots. InlineUpdater: a class that loops over a generator and prints a message to the terminal each time it yields. InlineUpdater A class to print progress updates to the terminal. Source code in mass2/core/utilities.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class InlineUpdater : \"\"\"A class to print progress updates to the terminal.\"\"\" def __init__ ( self , baseString : str ): self . fracDone = 0.0 self . minElapseTimeForCalc = 1.0 self . startTime = time . time () self . baseString = baseString self . logger = logging . getLogger ( \"mass\" ) def update ( self , fracDone : float ) -> None : \"\"\"Update the progress to the given fraction done.\"\"\" if self . logger . getEffectiveLevel () >= logging . WARNING : return self . fracDone = fracDone sys . stdout . write ( f \" \\r { self . baseString } { self . fracDone * 100.0 : .1f } % done, estimated { self . timeRemainingStr } left\" ) sys . stdout . flush () if fracDone >= 1 : sys . stdout . write ( f \" \\n { self . baseString } finished in { self . elapsedTimeStr } \\n \" ) @property def timeRemaining ( self ) -> float : \"\"\"Estimate of time remaining in seconds, or -1 if not enough information yet.\"\"\" if self . elapsedTimeSec > self . minElapseTimeForCalc and self . fracDone > 0 : fracRemaining = 1 - self . fracDone rate = self . fracDone / self . elapsedTimeSec try : return fracRemaining / rate except ZeroDivisionError : return - 1 else : return - 1 @property def timeRemainingStr ( self ) -> str : \"\"\"String version of time-remaining estimate.\"\"\" timeRemaining = self . timeRemaining if timeRemaining == - 1 : return \"?\" else : return \" %.1f min\" % ( timeRemaining / 60.0 ) @property def elapsedTimeSec ( self ) -> float : \"\"\"Elapsed time in seconds since the creation of this object.\"\"\" return time . time () - self . startTime @property def elapsedTimeStr ( self ) -> str : \"\"\"String version of elapsed time.\"\"\" return \" %.1f min\" % ( self . elapsedTimeSec / 60.0 ) elapsedTimeSec property Elapsed time in seconds since the creation of this object. elapsedTimeStr property String version of elapsed time. timeRemaining property Estimate of time remaining in seconds, or -1 if not enough information yet. timeRemainingStr property String version of time-remaining estimate. update ( fracDone ) Update the progress to the given fraction done. Source code in mass2/core/utilities.py 28 29 30 31 32 33 34 35 36 def update ( self , fracDone : float ) -> None : \"\"\"Update the progress to the given fraction done.\"\"\" if self . logger . getEffectiveLevel () >= logging . WARNING : return self . fracDone = fracDone sys . stdout . write ( f \" \\r { self . baseString } { self . fracDone * 100.0 : .1f } % done, estimated { self . timeRemainingStr } left\" ) sys . stdout . flush () if fracDone >= 1 : sys . stdout . write ( f \" \\n { self . baseString } finished in { self . elapsedTimeStr } \\n \" ) NullUpdater A do-nothing updater class with the same API as InlineUpdater. Source code in mass2/core/utilities.py 71 72 73 74 75 76 class NullUpdater : \"\"\"A do-nothing updater class with the same API as InlineUpdater.\"\"\" def update ( self , f : float ) -> None : \"\"\"Do nothing.\"\"\" pass update ( f ) Do nothing. Source code in mass2/core/utilities.py 74 75 76 def update ( self , f : float ) -> None : \"\"\"Do nothing.\"\"\" pass show_progress ( name ) A decorator to show progress updates for another function. Source code in mass2/core/utilities.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def show_progress ( name : str ) -> Callable : \"\"\"A decorator to show progress updates for another function.\"\"\" def decorator ( func : Callable ) -> Callable : \"\"\"A decorator to show progress updates for another function.\"\"\" @functools . wraps ( func ) def work ( self : Any , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update the progress of the wrapped function.\"\"\" try : if \"sphinx\" in sys . modules : # supress output during doctests print_updater = NullUpdater () else : print_updater = self . updater ( name ) except TypeError : print_updater = NullUpdater () for d in func ( self , * args , ** kwargs ): print_updater . update ( d ) return work return decorator Other docstrings See also Other docstrings for modules other than mass2.core .","title":"Core docstrings"},{"location":"docstrings/#automatic-documentation-generated-from-docstrings","text":"This page is auto-generated from the docstrings of functions, methods, classes, and modules. Each lowest-level module in Mass2 (i.e., each python file) in mass2.core that you want documented and indexed for searching should be listed in this docstrings.md file. The non-core docstrings contain the docstrings for all modules other than mass2.core .","title":"Automatic documentation generated from docstrings"},{"location":"docstrings/#core","text":"mass2.core: Core Mass2 functionality, including file I/O, microcalorimeter channel bookkeeping, recipes, fitting, filtering, and more. Data structures and methods for handling a single microcalorimeter channel's pulse data and metadata.","title":"Core"},{"location":"docstrings/#mass2.core.channel.BadChannel","text":"A wrapper around Channel that includes error information. Source code in mass2/core/channel.py 1107 1108 1109 1110 1111 1112 1113 1114 @dataclass ( frozen = True ) class BadChannel : \"\"\"A wrapper around Channel that includes error information.\"\"\" ch : Channel error_type : type | None error_msg : str backtrace : str | None","title":"BadChannel"},{"location":"docstrings/#mass2.core.channel.Channel","text":"A single microcalorimeter channel's pulse data and associated metadata. Source code in mass2/core/channel.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 @dataclass ( frozen = True ) # noqa: PLR0904 class Channel : \"\"\"A single microcalorimeter channel's pulse data and associated metadata.\"\"\" df : pl . DataFrame = field ( repr = False ) header : ChannelHeader = field ( repr = True ) npulses : int subframediv : int | None = None noise : NoiseChannel | None = field ( default = None , repr = False ) good_expr : pl . Expr = field ( default_factory = alwaysTrue ) df_history : list [ pl . DataFrame ] = field ( default_factory = list , repr = False ) steps : Recipe = field ( default_factory = Recipe . new_empty , repr = False ) steps_elapsed_s : list [ float ] = field ( default_factory = list ) transform_raw : Callable | None = None @property def shortname ( self ) -> str : \"\"\"A short name for this channel, suitable for plot titles.\"\"\" return self . header . description def mo_stepplots ( self ) -> mo . ui . dropdown : \"\"\"Marimo UI element to choose and display step plots, with a dropdown to choose channel number.\"\"\" desc_ind = { step . description : i for i , step in enumerate ( self . steps )} first_non_summarize_step = self . steps [ 0 ] for step in self . steps : if isinstance ( step , SummarizeStep ): continue first_non_summarize_step = step break mo_ui = mo . ui . dropdown ( desc_ind , value = first_non_summarize_step . description , label = f \"choose step for ch { self . header . ch_num } \" , ) def show () -> mo . Html : \"\"\"Show the selected step plot.\"\"\" return self . _mo_stepplots_explicit ( mo_ui ) def step_ind () -> Any : \"\"\"Get the selected step index from the dropdown item, if any.\"\"\" return mo_ui . value mo_ui . show = show mo_ui . step_ind = step_ind return mo_ui def _mo_stepplots_explicit ( self , mo_ui : mo . ui . dropdown ) -> mo . Html : \"\"\"Marimo UI element to choose and display step plots.\"\"\" step_ind = mo_ui . value self . step_plot ( step_ind ) fig = plt . gcf () return mo . vstack ([ mo_ui , misc . show ( fig )]) def get_step ( self , index : int ) -> tuple [ RecipeStep , int ]: \"\"\"Get the step at the given index, supporting negative indices.\"\"\" # normalize the index to a positive index if index < 0 : index = len ( self . steps ) + index step = self . steps [ index ] return step , index def step_plot ( self , step_ind : int , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a debug plot for the given step index, supporting negative indices.\"\"\" step , step_ind = self . get_step ( step_ind ) if step_ind + 1 == len ( self . df_history ): df_after = self . df else : df_after = self . df_history [ step_ind + 1 ] return step . dbg_plot ( df_after , ** kwargs ) def hist ( self , col : str , bin_edges : ArrayLike , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute a histogram of the given column, optionally filtering by good_expr and use_expr.\"\"\" if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col )) . collect () values = df_small [ col ] bin_centers , counts = misc . hist_of_series ( values , bin_edges ) return bin_centers , counts def plot_hist ( self , col : str , bin_edges : ArrayLike , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute and plot a histogram of the given column, optionally filtering by good_expr and use_expr.\"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis bin_centers , counts = self . hist ( col , bin_edges = bin_edges , use_good_expr = use_good_expr , use_expr = use_expr ) _ , step_size = misc . midpoints_and_step_size ( bin_edges ) plt . step ( bin_centers , counts , where = \"mid\" ) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } for { self . shortname } \" ) plt . tight_layout () return bin_centers , counts def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : str , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), skip_none : bool = True , ) -> tuple [ NDArray , dict [ str , NDArray ]]: \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group counts_dict : dict [ str , NDArray ] = {} for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] _ , step_size = misc . midpoints_and_step_size ( bin_edges ) bin_centers , counts = misc . hist_of_series ( values , bin_edges ) group_name_str = str ( group_name ) counts_dict [ group_name_str ] = counts plt . step ( bin_centers , counts , where = \"mid\" , label = group_name_str ) # Plot the histogram for the current group # if group_name == \"EBIT\": # ax.hist(values, bins=bin_edges, alpha=0.9, color=\"k\", label=group_name_str) # else: # ax.hist(values, bins=bin_edges, alpha=0.5, label=group_name_str) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () return bin_centers , counts_dict def plot_scatter ( self , x_col : str , y_col : str , color_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), use_good_expr : bool = True , skip_none : bool = True , ax : plt . Axes | None = None , ) -> None : \"\"\"Generate a scatter plot of `y_col` vs `x_col`, optionally colored by `color_col`.\"\"\" if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # set current axis so I can use plt api if use_good_expr : filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = self . good_expr df_small = self . df . lazy () . filter ( filter_expr ) . select ( x_col , y_col , color_col ) . collect () for ( name ,), data in df_small . group_by ( color_col , maintain_order = True ): if name is None and skip_none and color_col is not None : continue plt . plot ( data . select ( x_col ) . to_series (), data . select ( y_col ) . to_series (), \".\" , label = name , ) plt . xlabel ( str ( x_col )) plt . ylabel ( str ( y_col )) title_str = f \"\"\" { self . header . description } use_expr= { str ( use_expr ) } good_expr= { str ( self . good_expr ) } \"\"\" plt . title ( title_str ) if color_col is not None : plt . legend ( title = color_col ) plt . tight_layout () def good_series ( self , col : str , use_expr : pl . Expr = pl . lit ( True )) -> pl . Series : \"\"\"Return a Polars Series of the given column, filtered by good_expr and use_expr.\"\"\" return mass2 . misc . good_series ( self . df , col , self . good_expr , use_expr ) @property def last_avg_pulse ( self ) -> NDArray | None : \"\"\"Return the average pulse stored in the last recipe step that's an optimal filter step Returns ------- NDArray | None The last filtering step's signal model, or None if no such step \"\"\" for step in reversed ( self . steps ): if isinstance ( step , OptimalFilterStep ): return step . filter_maker . signal_model return None @property def last_filter ( self ) -> NDArray | None : \"\"\"Return the average pulse stored in the last recipe step that's an optimal filter step Returns ------- NDArray | None The last filtering step's signal model, or None if no such step \"\"\" for step in reversed ( self . steps ): if isinstance ( step , OptimalFilterStep ): return step . filter . values return None @property def last_noise_psd ( self ) -> tuple [ NDArray , NDArray ] | None : \"\"\"Return the noise PSD stored in the last recipe step that's an optimal filter step Returns ------- tuple[NDArray, NDArray] | None The last filtering step's (frequencies, noise spectrum), or None if no such step \"\"\" for step in reversed ( self . steps ): if isinstance ( step , OptimalFilterStep ) and step . spectrum is not None : return step . spectrum . frequencies , step . spectrum . psd return None @property def last_noise_autocorrelation ( self ) -> NDArray | None : \"\"\"Return the noise autocorrelation stored in the last recipe step that's an optimal filter step Returns ------- NDArray | None The last filtering step's noise autocorrelation, or None if no such step \"\"\" for step in reversed ( self . steps ): if isinstance ( step , OptimalFilterStep ) and step . spectrum is not None : return step . spectrum . autocorr_vec return None def rough_cal_combinatoric ( self , line_names : list [ str ], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying all combinatorically possible peak assignments.\"\"\" step = mass2 . core . RoughCalibrationStep . learn_combinatoric ( self , line_names , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal_combinatoric_height_info ( self , line_names : list [ str ], line_heights_allowed : list [ list [ int ]], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying all combinatorically possible peak assignments, using known relative peak heights to limit the possibilities.\"\"\" step = mass2 . core . RoughCalibrationStep . learn_combinatoric_height_info ( self , line_names , line_heights_allowed , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal ( # noqa: PLR0917 self , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying to assign the 3 brightest peaks, then fitting a line to those and looking for other peaks that fit that line. \"\"\" step = mass2 . core . RoughCalibrationStep . learn_3peak ( self , line_names , uncalibrated_col , calibrated_col , use_expr , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , fwhm_pulse_height_units , n_extra_peaks , acceptable_rms_residual_e , ) return self . with_step ( step ) def with_step ( self , step : RecipeStep ) -> \"Channel\" : \"\"\"Return a new Channel with the given step applied to generate new columns in the dataframe.\"\"\" t_start = time . time () df2 = step . calc_from_df ( self . df ) elapsed_s = time . time () - t_start ch2 = dataclasses . replace ( self , df = df2 , good_expr = step . good_expr , df_history = self . df_history + [ self . df ], steps = self . steps . with_step ( step ), steps_elapsed_s = self . steps_elapsed_s + [ elapsed_s ], ) return ch2 def with_steps ( self , steps : Recipe ) -> \"Channel\" : \"\"\"Return a new Channel with the given steps applied to generate new columns in the dataframe.\"\"\" ch2 = self for step in steps : ch2 = ch2 . with_step ( step ) return ch2 def with_good_expr ( self , good_expr : pl . Expr , replace : bool = False ) -> \"Channel\" : \"\"\"Return a new Channel with the given good_expr, combined with the existing good_expr by \"and\", of by replacing it entirely if `replace` is True.\"\"\" # the default value of self.good_expr is pl.lit(True) # and_(True) will just add visual noise when looking at good_expr and not affect behavior if not replace and good_expr is not True and not good_expr . meta . eq ( pl . lit ( True )): good_expr = good_expr . and_ ( self . good_expr ) return dataclasses . replace ( self , good_expr = good_expr ) def with_column_map_step ( self , input_col : str , output_col : str , f : Callable ) -> \"Channel\" : \"\"\"f should take a numpy array and return a numpy array with the same number of elements\"\"\" step = mass2 . core . recipe . ColumnAsNumpyMapStep ([ input_col ], [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), f = f ) return self . with_step ( step ) def with_good_expr_pretrig_rms_and_postpeak_deriv ( self , n_sigma_pretrig_rms : float = 20 , n_sigma_postpeak_deriv : float = 20 , replace : bool = False ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with pretrigger RMS or postpeak derivative above outlier-resistant thresholds.\"\"\" max_postpeak_deriv = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"postpeak_deriv\" ] . to_numpy (), nsigma = n_sigma_postpeak_deriv ) max_pretrig_rms = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"pretrig_rms\" ] . to_numpy (), nsigma = n_sigma_pretrig_rms ) good_expr = ( pl . col ( \"postpeak_deriv\" ) < max_postpeak_deriv ) . and_ ( pl . col ( \"pretrig_rms\" ) < max_pretrig_rms ) return self . with_good_expr ( good_expr , replace ) def with_range_around_median ( self , col : str , range_up : float , range_down : float ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with `col` outside the given range around its median.\"\"\" med = np . median ( self . df [ col ] . to_numpy ()) return self . with_good_expr ( pl . col ( col ) . is_between ( med - range_down , med + range_up )) def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) @functools . cache def typical_peak_ind ( self , col : str = \"pulse\" ) -> int : \"\"\"Return the typical peak index of the given column, using the median peak index for the first 100 pulses.\"\"\" raw = self . df . limit ( 100 )[ col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) return int ( np . median ( raw . argmax ( axis = 1 ))) def summarize_pulses ( self , col : str = \"pulse\" , pretrigger_ignore_samples : int = 0 , peak_index : int | None = None ) -> \"Channel\" : \"\"\"Summarize the pulses, adding columns for pulse height, pretrigger mean, etc.\"\"\" if peak_index is None : peak_index = self . typical_peak_ind ( col ) out_names = mass2 . core . pulse_algorithms . result_dtype . names # mypy (incorrectly) thinks `out_names` might be None, and `list(None)` is forbidden. Assertion makes it happy again. assert out_names is not None outputs = list ( out_names ) step = SummarizeStep ( inputs = [ col ], output = outputs , good_expr = self . good_expr , use_expr = pl . lit ( True ), frametime_s = self . header . frametime_s , peak_index = peak_index , pulse_col = col , pretrigger_ignore_samples = pretrigger_ignore_samples , n_presamples = self . header . n_presamples , transform_raw = self . transform_raw , ) return self . with_step ( step ) def correct_pretrig_mean_jumps ( self , uncorrected : str = \"pretrig_mean\" , corrected : str = \"ptm_jf\" , period : int = 4096 ) -> \"Channel\" : \"\"\"Correct pretrigger mean jumps in the raw pulse data, writing to a new column.\"\"\" step = mass2 . core . recipe . PretrigMeanJumpFixStep ( inputs = [ uncorrected ], output = [ corrected ], good_expr = self . good_expr , use_expr = pl . lit ( True ), period = period , ) return self . with_step ( step ) def with_select_step ( self , col_expr_dict : dict [ str , pl . Expr ]) -> \"Channel\" : \"\"\" This step is meant for interactive exploration; it's basically like the df.select() method, but it's saved as a step. \"\"\" extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in col_expr_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . SelectStep ( inputs = list ( inputs ), output = list ( col_expr_dict . keys ()), good_expr = self . good_expr , use_expr = pl . lit ( True ), col_expr_dict = col_expr_dict , ) return self . with_step ( step ) def with_categorize_step ( self , category_condition_dict : dict [ str , pl . Expr ], output_col : str = \"category\" ) -> \"Channel\" : \"\"\"Add a recipe step that categorizes pulses based on the given conditions.\"\"\" # ensure the first condition is True, to be used as a fallback first_expr = next ( iter ( category_condition_dict . values ())) if not first_expr . meta . eq ( pl . lit ( True )): category_condition_dict = { \"fallback\" : pl . lit ( True ), ** category_condition_dict } extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in category_condition_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . CategorizeStep ( inputs = list ( inputs ), output = [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), category_condition_dict = category_condition_dict , ) return self . with_step ( step ) def compute_average_pulse ( self , pulse_col : str = \"pulse\" , use_expr : pl . Expr = pl . lit ( True ), limit : int = 2000 ) -> NDArray : \"\"\"Compute an average pulse given a use expression. Parameters ---------- pulse_col : str, optional Name of the column in self.df containing raw pulses, by default \"pulse\" use_expr : pl.Expr, optional Selection (in addition to self.good_expr) to use, by default pl.lit(True) limit : int, optional Use no more than this many pulses, by default 2000 Returns ------- NDArray _description_ \"\"\" avg_pulse = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( pulse_col ) . limit ( limit ) . collect () . to_series () . to_numpy () . mean ( axis = 0 ) ) avg_pulse -= avg_pulse [: self . header . n_presamples ] . mean () return avg_pulse def filter5lag ( self , pulse_col : str = \"pulse\" , peak_y_col : str = \"5lagy\" , peak_x_col : str = \"5lagx\" , f_3db : float = 25e3 , use_expr : pl . Expr = pl . lit ( True ), time_constant_s_of_exp_to_be_orthogonal_to : float | None = None , ) -> \"Channel\" : \"\"\"Compute a 5-lag optimal filter and apply it. Parameters ---------- pulse_col : str, optional Which column contains raw data, by default \"pulse\" peak_y_col : str, optional Column to contain the optimal filter results, by default \"5lagy\" peak_x_col : str, optional Column to contain the 5-lag filter's estimate of arrival-time/phase, by default \"5lagx\" f_3db : float, optional A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr : pl.Expr, optional An expression to select pulses for averaging, by default pl.lit(True) time_constant_s_of_exp_to_be_orthogonal_to : float | None, optional Optionally an exponential decay time to make the filter insensitive to, by default None Returns ------- Channel This channel with a Filter5LagStep added to the recipe. \"\"\" assert self . noise noiseresult = self . noise . spectrum ( trunc_back = 2 , trunc_front = 2 ) avg_pulse = self . compute_average_pulse ( pulse_col = pulse_col , use_expr = use_expr ) filter_maker = FilterMaker ( signal_model = avg_pulse , n_pretrigger = self . header . n_presamples , noise_psd = noiseresult . psd , noise_autocorr = noiseresult . autocorr_vec , sample_time_sec = self . header . frametime_s , ) if time_constant_s_of_exp_to_be_orthogonal_to is None : filter5lag = filter_maker . compute_5lag ( f_3db = f_3db ) else : filter5lag = filter_maker . compute_5lag_noexp ( f_3db = f_3db , exp_time_seconds = time_constant_s_of_exp_to_be_orthogonal_to ) step = OptimalFilterStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter5lag , spectrum = noiseresult , filter_maker = filter_maker , transform_raw = self . transform_raw , ) return self . with_step ( step ) def compute_ats_model ( self , pulse_col : str , use_expr : pl . Expr = pl . lit ( True ), limit : int = 2000 ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute the average pulse and arrival-time model for an ATS filter. We use the first `limit` pulses that pass `good_expr` and `use_expr`. Parameters ---------- pulse_col : str _description_ use_expr : pl.Expr, optional _description_, by default pl.lit(True) limit : int, optional _description_, by default 2000 Returns ------- tuple[NDArray, NDArray] _description_ \"\"\" df = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . limit ( limit ) . select ( pulse_col , \"pulse_rms\" , \"promptness\" , \"pretrig_mean\" ) . collect () ) # Adjust promptness: subtract a linear trend with pulse_rms prms = df [ \"pulse_rms\" ] . to_numpy () promptness = df [ \"promptness\" ] . to_numpy () poly = np . poly1d ( np . polyfit ( prms , promptness , 1 )) df = df . with_columns ( promptshifted = ( promptness - poly ( prms ))) # Rescale promptness quadratically to span approximately [-0.5, +0.5], dropping any pulses with abs(t) > 0.45. x , y , z = np . percentile ( df [ \"promptshifted\" ], [ 10 , 50 , 90 ]) A = np . array ([[ x * x , x , 1 ], [ y * y , y , 1 ], [ z * z , z , 1 ]]) param = np . linalg . solve ( A , [ - 0.4 , 0 , + 0.4 ]) ATime = np . poly1d ( param )( df [ \"promptshifted\" ]) df = df . with_columns ( ATime = ATime ) . filter ( np . abs ( ATime ) < 0.45 ) . drop ( \"promptshifted\" ) # Compute mean pulse and dt model as the offset and slope of a linear fit to each pulse sample vs ATime pulse = df [ \"pulse\" ] . to_numpy () avg_pulse = np . zeros ( self . header . n_samples , dtype = float ) dt_model = np . zeros ( self . header . n_samples , dtype = float ) for i in range ( self . header . n_presamples , self . header . n_samples ): slope , offset = np . polyfit ( df [ \"ATime\" ], ( pulse [:, i ] - df [ \"pretrig_mean\" ]), 1 ) dt_model [ i ] = - slope avg_pulse [ i ] = offset return avg_pulse , dt_model def filterATS ( self , pulse_col : str = \"pulse\" , peak_y_col : str = \"ats_y\" , peak_x_col : str = \"ats_x\" , f_3db : float = 25e3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Compute an arrival-time-safe (ATS) optimal filter and apply it. Parameters ---------- pulse_col : str, optional Which column contains raw data, by default \"pulse\" peak_y_col : str, optional Column to contain the optimal filter results, by default \"ats_y\" peak_x_col : str, optional Column to contain the ATS filter's estimate of arrival-time/phase, by default \"ats_x\" f_3db : float, optional A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr : pl.Expr, optional An expression to select pulses for averaging, by default pl.lit(True) Returns ------- Channel This channel with a Filter5LagStep added to the recipe. \"\"\" assert self . noise mprms = self . good_series ( \"pulse_rms\" , use_expr ) . median () use = use_expr . and_ ( np . abs ( pl . col ( \"pulse_rms\" ) / mprms - 1.0 ) < 0.3 ) limit = 4000 avg_pulse , dt_model = self . compute_ats_model ( pulse_col , use , limit ) noiseresult = self . noise . spectrum () filter_maker = FilterMaker ( signal_model = avg_pulse , dt_model = dt_model , n_pretrigger = self . header . n_presamples , noise_psd = noiseresult . psd , noise_autocorr = noiseresult . autocorr_vec , sample_time_sec = self . header . frametime_s , ) filter_ats = filter_maker . compute_ats ( f_3db = f_3db ) step = OptimalFilterStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter_ats , spectrum = noiseresult , filter_maker = filter_maker , transform_raw = self . transform_raw , ) return self . with_step ( step ) def good_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : \"\"\"Return a Polars DataFrame of the given columns, filtered by good_expr and use_expr.\"\"\" good_df = self . df . lazy () . filter ( self . good_expr ) if use_expr is not True : good_df = good_df . filter ( use_expr ) return good_df . select ( cols ) . collect () def bad_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : \"\"\"Return a Polars DataFrame of the given columns, filtered by the inverse of good_expr, and use_expr.\"\"\" bad_df = self . df . lazy () . filter ( self . good_expr . not_ ()) if use_expr is not True : bad_df = bad_df . filter ( use_expr ) return bad_df . select ( cols ) . collect () def good_serieses ( self , cols : list [ str ], use_expr : pl . Expr = pl . lit ( True )) -> list [ pl . Series ]: \"\"\"Return a list of Polars Series of the given columns, filtered by good_expr and use_expr.\"\"\" df2 = self . good_df ( cols , use_expr ) return [ df2 [ col ] for col in cols ] def driftcorrect ( self , indicator_col : str = \"pretrig_mean\" , uncorrected_col : str = \"5lagy\" , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Correct for gain drift correlated with the given indicator column.\"\"\" # by defining a seperate learn method that takes ch as an argument, # we can move all the code for the step outside of Channel step = DriftCorrectStep . learn ( ch = self , indicator_col = indicator_col , uncorrected_col = uncorrected_col , corrected_col = corrected_col , use_expr = use_expr , ) return self . with_step ( step ) def linefit ( # noqa: PLR0917 self , line : GenericLineModel | SpectralLine | str | float , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : \"\"\"Fit a spectral line to the binned data from the given column, optionally filtering by use_expr.\"\"\" model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = self . header . description , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def step_summary ( self ) -> list [ tuple [ str , float ]]: \"\"\"Return a list of (step type name, elapsed time in seconds) for each step in the recipe.\"\"\" return [( type ( a ) . __name__ , b ) for ( a , b ) in zip ( self . steps , self . steps_elapsed_s )] def __hash__ ( self ) -> int : \"\"\"Return a hash based on the object's id.\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other : object ) -> bool : \"\"\"Return True if the other object is the same object (by id).\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results # only checks if the ids match, does not try to be equal if all contents are equal return id ( self ) == id ( other ) @classmethod def from_ljh ( cls , path : str | Path , noise_path : str | Path | None = None , keep_posix_usec : bool = False , transform_raw : Callable | None = None , ) -> \"Channel\" : \"\"\"Load a Channel from an LJH file, optionally with a NoiseChannel from a corresponding noise LJH file.\"\"\" if not noise_path : noise_channel = None else : noise_channel = NoiseChannel . from_ljh ( noise_path ) ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars ( keep_posix_usec ) header = ChannelHeader . from_ljh_header_df ( header_df ) channel = cls ( df , header = header , npulses = ljh . npulses , subframediv = ljh . subframediv , noise = noise_channel , transform_raw = transform_raw ) return channel @classmethod def from_off ( cls , off : OffFile ) -> \"Channel\" : \"\"\"Load a Channel from an OFF file.\"\"\" assert off . _mmap is not None df = pl . from_numpy ( np . asarray ( off . _mmap )) df = ( df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) . select ( pl . exclude ( \"unixnano\" )) ) df_header = pl . DataFrame ( off . header ) df_header = df_header . with_columns ( pl . Series ( \"Filename\" , [ off . filename ])) header = ChannelHeader ( f \" { os . path . split ( off . filename )[ 1 ] } \" , off . header [ \"ChannelNumberMatchingName\" ], off . framePeriodSeconds , off . _mmap [ \"recordPreSamples\" ][ 0 ], off . _mmap [ \"recordSamples\" ][ 0 ], df_header , ) channel = cls ( df , header , off . nRecords , subframediv = off . subframediv ) return channel def with_experiment_state_df ( self , df_es : pl . DataFrame , force_timestamp_monotonic : bool = False ) -> \"Channel\" : \"\"\"Add experiment states from an existing dataframe\"\"\" if not self . df [ \"timestamp\" ] . is_sorted (): df = self . df . select ( pl . col ( \"timestamp\" ) . cum_max () . alias ( \"timestamp\" )) . with_columns ( self . df . select ( pl . exclude ( \"timestamp\" ))) # print(\"WARNING: in with_experiment_state_df, timestamp is not monotonic, forcing it to be\") # print(\"This is likely a BUG in DASTARD.\") else : df = self . df df2 = df . join_asof ( df_es , on = \"timestamp\" , strategy = \"backward\" ) return self . with_replacement_df ( df2 ) def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channel\" : \"\"\"Add external trigger times from an existing dataframe\"\"\" df2 = ( self . df . with_columns ( subframecount = pl . col ( \"framecount\" ) * self . subframediv ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"backward\" , coalesce = False , suffix = \"_prev_ext_trig\" ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"forward\" , coalesce = False , suffix = \"_next_ext_trig\" ) ) return self . with_replacement_df ( df2 ) def with_replacement_df ( self , df2 : pl . DataFrame ) -> \"Channel\" : \"\"\"Replace the dataframe with a new one, keeping all other attributes the same.\"\"\" return dataclasses . replace ( self , df = df2 , ) def with_columns ( self , df2 : pl . DataFrame ) -> \"Channel\" : \"\"\"Append columns from df2 to the existing dataframe, keeping all other attributes the same.\"\"\" df3 = self . df . with_columns ( df2 ) return self . with_replacement_df ( df3 ) def multifit_quadratic_gain_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Fit multiple spectral lines, to create a quadratic gain calibration.\"\"\" step = MultiFitQuadraticGainStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def multifit_mass_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Fit multiple spectral lines, to create a Mass1-style gain calibration.\"\"\" step = MultiFitMassCalibrationStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def concat_df ( self , df : pl . DataFrame ) -> \"Channel\" : \"\"\"Concat the given dataframe to the existing dataframe, keeping all other attributes the same. If the new frame `df` has a history and/or steps, those will be lost\"\"\" ch2 = Channel ( mass2 . core . misc . concat_dfs_with_concat_state ( self . df , df ), self . header , self . npulses , subframediv = self . subframediv , noise = self . noise , good_expr = self . good_expr , ) # we won't copy over df_history and steps. I don't think you should use this when those are filled in? return ch2 def concat_ch ( self , ch : \"Channel\" ) -> \"Channel\" : \"\"\"Concat the given channel's dataframe to the existing dataframe, keeping all other attributes the same. If the new channel `ch` has a history and/or steps, those will be lost\"\"\" ch2 = self . concat_df ( ch . df ) return ch2 def phase_correct_mass_specific_lines ( self , indicator_col : str , uncorrected_col : str , line_names : Iterable [ str | float ], previous_cal_step_index : int , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Apply phase correction to the given uncorrected column, where specific lines are used to judge the correction.\"\"\" if corrected_col is None : corrected_col = uncorrected_col + \"_pc\" step = mass2 . core . phase_correct_steps . phase_correct_mass_specific_lines ( self , indicator_col , uncorrected_col , corrected_col , previous_cal_step_index , line_names , use_expr , ) return self . with_step ( step ) def as_bad ( self , error_type : type | None , error_msg : str , backtrace : str | None ) -> \"BadChannel\" : \"\"\"Return a BadChannel object, which wraps this Channel and includes error information.\"\"\" return BadChannel ( self , error_type , error_msg , backtrace ) def save_recipes ( self , filename : str ) -> dict [ int , Recipe ]: \"\"\"Save the recipe steps to a pickle file, keyed by channel number.\"\"\" steps = { self . header . ch_num : self . steps } misc . pickle_object ( steps , filename ) return steps def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr_in: pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample: int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log: bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) contents , _ , _ = plt . hist ( y , 200 , range = limits , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" ) def fit_pulse ( self , index : int = 0 , col : str = \"pulse\" , verbose : bool = True ) -> LineModelResult : \"\"\"Fit a single pulse to a 2-exponential-with-tail model, returning the fit result.\"\"\" pulse = self . df [ col ][ index ] . to_numpy () result = mass2 . core . pulse_algorithms . fit_pulse_2exp_with_tail ( pulse , npre = self . header . n_presamples , dt = self . header . frametime_s ) if verbose : print ( f \"ch= { self } \" ) print ( f \"pulse index= { index } \" ) print ( result . fit_report ()) return result","title":"Channel"},{"location":"docstrings/#mass2.core.channel.Channel.last_avg_pulse","text":"Return the average pulse stored in the last recipe step that's an optimal filter step Returns: NDArray | None \u2013 The last filtering step's signal model, or None if no such step","title":"last_avg_pulse"},{"location":"docstrings/#mass2.core.channel.Channel.last_filter","text":"Return the average pulse stored in the last recipe step that's an optimal filter step Returns: NDArray | None \u2013 The last filtering step's signal model, or None if no such step","title":"last_filter"},{"location":"docstrings/#mass2.core.channel.Channel.last_noise_autocorrelation","text":"Return the noise autocorrelation stored in the last recipe step that's an optimal filter step Returns: NDArray | None \u2013 The last filtering step's noise autocorrelation, or None if no such step","title":"last_noise_autocorrelation"},{"location":"docstrings/#mass2.core.channel.Channel.last_noise_psd","text":"Return the noise PSD stored in the last recipe step that's an optimal filter step Returns: tuple [ NDArray , NDArray ] | None \u2013 The last filtering step's (frequencies, noise spectrum), or None if no such step","title":"last_noise_psd"},{"location":"docstrings/#mass2.core.channel.Channel.shortname","text":"A short name for this channel, suitable for plot titles.","title":"shortname"},{"location":"docstrings/#mass2.core.channel.Channel.__eq__","text":"Return True if the other object is the same object (by id). Source code in mass2/core/channel.py 859 860 861 862 863 864 865 def __eq__ ( self , other : object ) -> bool : \"\"\"Return True if the other object is the same object (by id).\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results # only checks if the ids match, does not try to be equal if all contents are equal return id ( self ) == id ( other )","title":"__eq__"},{"location":"docstrings/#mass2.core.channel.Channel.__hash__","text":"Return a hash based on the object's id. Source code in mass2/core/channel.py 852 853 854 855 856 857 def __hash__ ( self ) -> int : \"\"\"Return a hash based on the object's id.\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self ))","title":"__hash__"},{"location":"docstrings/#mass2.core.channel.Channel.as_bad","text":"Return a BadChannel object, which wraps this Channel and includes error information. Source code in mass2/core/channel.py 1020 1021 1022 def as_bad ( self , error_type : type | None , error_msg : str , backtrace : str | None ) -> \"BadChannel\" : \"\"\"Return a BadChannel object, which wraps this Channel and includes error information.\"\"\" return BadChannel ( self , error_type , error_msg , backtrace )","title":"as_bad"},{"location":"docstrings/#mass2.core.channel.Channel.bad_df","text":"Return a Polars DataFrame of the given columns, filtered by the inverse of good_expr, and use_expr. Source code in mass2/core/channel.py 783 784 785 786 787 788 def bad_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : \"\"\"Return a Polars DataFrame of the given columns, filtered by the inverse of good_expr, and use_expr.\"\"\" bad_df = self . df . lazy () . filter ( self . good_expr . not_ ()) if use_expr is not True : bad_df = bad_df . filter ( use_expr ) return bad_df . select ( cols ) . collect ()","title":"bad_df"},{"location":"docstrings/#mass2.core.channel.Channel.compute_ats_model","text":"Compute the average pulse and arrival-time model for an ATS filter. We use the first limit pulses that pass good_expr and use_expr . Parameters: pulse_col ( str ) \u2013 description use_expr ( Expr , default: lit (True) ) \u2013 description , by default pl.lit(True) limit ( int , default: 2000 ) \u2013 description , by default 2000 Returns: tuple [ NDArray , NDArray ] \u2013 description Source code in mass2/core/channel.py 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 def compute_ats_model ( self , pulse_col : str , use_expr : pl . Expr = pl . lit ( True ), limit : int = 2000 ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute the average pulse and arrival-time model for an ATS filter. We use the first `limit` pulses that pass `good_expr` and `use_expr`. Parameters ---------- pulse_col : str _description_ use_expr : pl.Expr, optional _description_, by default pl.lit(True) limit : int, optional _description_, by default 2000 Returns ------- tuple[NDArray, NDArray] _description_ \"\"\" df = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . limit ( limit ) . select ( pulse_col , \"pulse_rms\" , \"promptness\" , \"pretrig_mean\" ) . collect () ) # Adjust promptness: subtract a linear trend with pulse_rms prms = df [ \"pulse_rms\" ] . to_numpy () promptness = df [ \"promptness\" ] . to_numpy () poly = np . poly1d ( np . polyfit ( prms , promptness , 1 )) df = df . with_columns ( promptshifted = ( promptness - poly ( prms ))) # Rescale promptness quadratically to span approximately [-0.5, +0.5], dropping any pulses with abs(t) > 0.45. x , y , z = np . percentile ( df [ \"promptshifted\" ], [ 10 , 50 , 90 ]) A = np . array ([[ x * x , x , 1 ], [ y * y , y , 1 ], [ z * z , z , 1 ]]) param = np . linalg . solve ( A , [ - 0.4 , 0 , + 0.4 ]) ATime = np . poly1d ( param )( df [ \"promptshifted\" ]) df = df . with_columns ( ATime = ATime ) . filter ( np . abs ( ATime ) < 0.45 ) . drop ( \"promptshifted\" ) # Compute mean pulse and dt model as the offset and slope of a linear fit to each pulse sample vs ATime pulse = df [ \"pulse\" ] . to_numpy () avg_pulse = np . zeros ( self . header . n_samples , dtype = float ) dt_model = np . zeros ( self . header . n_samples , dtype = float ) for i in range ( self . header . n_presamples , self . header . n_samples ): slope , offset = np . polyfit ( df [ \"ATime\" ], ( pulse [:, i ] - df [ \"pretrig_mean\" ]), 1 ) dt_model [ i ] = - slope avg_pulse [ i ] = offset return avg_pulse , dt_model","title":"compute_ats_model"},{"location":"docstrings/#mass2.core.channel.Channel.compute_average_pulse","text":"Compute an average pulse given a use expression. Parameters: pulse_col ( str , default: 'pulse' ) \u2013 Name of the column in self.df containing raw pulses, by default \"pulse\" use_expr ( Expr , default: lit (True) ) \u2013 Selection (in addition to self.good_expr) to use, by default pl.lit(True) limit ( int , default: 2000 ) \u2013 Use no more than this many pulses, by default 2000 Returns: NDArray \u2013 description Source code in mass2/core/channel.py 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 def compute_average_pulse ( self , pulse_col : str = \"pulse\" , use_expr : pl . Expr = pl . lit ( True ), limit : int = 2000 ) -> NDArray : \"\"\"Compute an average pulse given a use expression. Parameters ---------- pulse_col : str, optional Name of the column in self.df containing raw pulses, by default \"pulse\" use_expr : pl.Expr, optional Selection (in addition to self.good_expr) to use, by default pl.lit(True) limit : int, optional Use no more than this many pulses, by default 2000 Returns ------- NDArray _description_ \"\"\" avg_pulse = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( pulse_col ) . limit ( limit ) . collect () . to_series () . to_numpy () . mean ( axis = 0 ) ) avg_pulse -= avg_pulse [: self . header . n_presamples ] . mean () return avg_pulse","title":"compute_average_pulse"},{"location":"docstrings/#mass2.core.channel.Channel.concat_ch","text":"Concat the given channel's dataframe to the existing dataframe, keeping all other attributes the same. If the new channel ch has a history and/or steps, those will be lost Source code in mass2/core/channel.py 991 992 993 994 995 def concat_ch ( self , ch : \"Channel\" ) -> \"Channel\" : \"\"\"Concat the given channel's dataframe to the existing dataframe, keeping all other attributes the same. If the new channel `ch` has a history and/or steps, those will be lost\"\"\" ch2 = self . concat_df ( ch . df ) return ch2","title":"concat_ch"},{"location":"docstrings/#mass2.core.channel.Channel.concat_df","text":"Concat the given dataframe to the existing dataframe, keeping all other attributes the same. If the new frame df has a history and/or steps, those will be lost Source code in mass2/core/channel.py 977 978 979 980 981 982 983 984 985 986 987 988 989 def concat_df ( self , df : pl . DataFrame ) -> \"Channel\" : \"\"\"Concat the given dataframe to the existing dataframe, keeping all other attributes the same. If the new frame `df` has a history and/or steps, those will be lost\"\"\" ch2 = Channel ( mass2 . core . misc . concat_dfs_with_concat_state ( self . df , df ), self . header , self . npulses , subframediv = self . subframediv , noise = self . noise , good_expr = self . good_expr , ) # we won't copy over df_history and steps. I don't think you should use this when those are filled in? return ch2","title":"concat_df"},{"location":"docstrings/#mass2.core.channel.Channel.correct_pretrig_mean_jumps","text":"Correct pretrigger mean jumps in the raw pulse data, writing to a new column. Source code in mass2/core/channel.py 534 535 536 537 538 539 540 541 542 543 544 545 def correct_pretrig_mean_jumps ( self , uncorrected : str = \"pretrig_mean\" , corrected : str = \"ptm_jf\" , period : int = 4096 ) -> \"Channel\" : \"\"\"Correct pretrigger mean jumps in the raw pulse data, writing to a new column.\"\"\" step = mass2 . core . recipe . PretrigMeanJumpFixStep ( inputs = [ uncorrected ], output = [ corrected ], good_expr = self . good_expr , use_expr = pl . lit ( True ), period = period , ) return self . with_step ( step )","title":"correct_pretrig_mean_jumps"},{"location":"docstrings/#mass2.core.channel.Channel.driftcorrect","text":"Correct for gain drift correlated with the given indicator column. Source code in mass2/core/channel.py 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 def driftcorrect ( self , indicator_col : str = \"pretrig_mean\" , uncorrected_col : str = \"5lagy\" , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Correct for gain drift correlated with the given indicator column.\"\"\" # by defining a seperate learn method that takes ch as an argument, # we can move all the code for the step outside of Channel step = DriftCorrectStep . learn ( ch = self , indicator_col = indicator_col , uncorrected_col = uncorrected_col , corrected_col = corrected_col , use_expr = use_expr , ) return self . with_step ( step )","title":"driftcorrect"},{"location":"docstrings/#mass2.core.channel.Channel.filter5lag","text":"Compute a 5-lag optimal filter and apply it. Parameters: pulse_col ( str , default: 'pulse' ) \u2013 Which column contains raw data, by default \"pulse\" peak_y_col ( str , default: '5lagy' ) \u2013 Column to contain the optimal filter results, by default \"5lagy\" peak_x_col ( str , default: '5lagx' ) \u2013 Column to contain the 5-lag filter's estimate of arrival-time/phase, by default \"5lagx\" f_3db ( float , default: 25000.0 ) \u2013 A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr ( Expr , default: lit (True) ) \u2013 An expression to select pulses for averaging, by default pl.lit(True) time_constant_s_of_exp_to_be_orthogonal_to ( float | None , default: None ) \u2013 Optionally an exponential decay time to make the filter insensitive to, by default None Returns: Channel \u2013 This channel with a Filter5LagStep added to the recipe. Source code in mass2/core/channel.py 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 def filter5lag ( self , pulse_col : str = \"pulse\" , peak_y_col : str = \"5lagy\" , peak_x_col : str = \"5lagx\" , f_3db : float = 25e3 , use_expr : pl . Expr = pl . lit ( True ), time_constant_s_of_exp_to_be_orthogonal_to : float | None = None , ) -> \"Channel\" : \"\"\"Compute a 5-lag optimal filter and apply it. Parameters ---------- pulse_col : str, optional Which column contains raw data, by default \"pulse\" peak_y_col : str, optional Column to contain the optimal filter results, by default \"5lagy\" peak_x_col : str, optional Column to contain the 5-lag filter's estimate of arrival-time/phase, by default \"5lagx\" f_3db : float, optional A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr : pl.Expr, optional An expression to select pulses for averaging, by default pl.lit(True) time_constant_s_of_exp_to_be_orthogonal_to : float | None, optional Optionally an exponential decay time to make the filter insensitive to, by default None Returns ------- Channel This channel with a Filter5LagStep added to the recipe. \"\"\" assert self . noise noiseresult = self . noise . spectrum ( trunc_back = 2 , trunc_front = 2 ) avg_pulse = self . compute_average_pulse ( pulse_col = pulse_col , use_expr = use_expr ) filter_maker = FilterMaker ( signal_model = avg_pulse , n_pretrigger = self . header . n_presamples , noise_psd = noiseresult . psd , noise_autocorr = noiseresult . autocorr_vec , sample_time_sec = self . header . frametime_s , ) if time_constant_s_of_exp_to_be_orthogonal_to is None : filter5lag = filter_maker . compute_5lag ( f_3db = f_3db ) else : filter5lag = filter_maker . compute_5lag_noexp ( f_3db = f_3db , exp_time_seconds = time_constant_s_of_exp_to_be_orthogonal_to ) step = OptimalFilterStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter5lag , spectrum = noiseresult , filter_maker = filter_maker , transform_raw = self . transform_raw , ) return self . with_step ( step )","title":"filter5lag"},{"location":"docstrings/#mass2.core.channel.Channel.filterATS","text":"Compute an arrival-time-safe (ATS) optimal filter and apply it. Parameters: pulse_col ( str , default: 'pulse' ) \u2013 Which column contains raw data, by default \"pulse\" peak_y_col ( str , default: 'ats_y' ) \u2013 Column to contain the optimal filter results, by default \"ats_y\" peak_x_col ( str , default: 'ats_x' ) \u2013 Column to contain the ATS filter's estimate of arrival-time/phase, by default \"ats_x\" f_3db ( float , default: 25000.0 ) \u2013 A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr ( Expr , default: lit (True) ) \u2013 An expression to select pulses for averaging, by default pl.lit(True) Returns: Channel \u2013 This channel with a Filter5LagStep added to the recipe. Source code in mass2/core/channel.py 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 def filterATS ( self , pulse_col : str = \"pulse\" , peak_y_col : str = \"ats_y\" , peak_x_col : str = \"ats_x\" , f_3db : float = 25e3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Compute an arrival-time-safe (ATS) optimal filter and apply it. Parameters ---------- pulse_col : str, optional Which column contains raw data, by default \"pulse\" peak_y_col : str, optional Column to contain the optimal filter results, by default \"ats_y\" peak_x_col : str, optional Column to contain the ATS filter's estimate of arrival-time/phase, by default \"ats_x\" f_3db : float, optional A low-pass filter 3 dB point to apply to the computed filter, by default 25e3 use_expr : pl.Expr, optional An expression to select pulses for averaging, by default pl.lit(True) Returns ------- Channel This channel with a Filter5LagStep added to the recipe. \"\"\" assert self . noise mprms = self . good_series ( \"pulse_rms\" , use_expr ) . median () use = use_expr . and_ ( np . abs ( pl . col ( \"pulse_rms\" ) / mprms - 1.0 ) < 0.3 ) limit = 4000 avg_pulse , dt_model = self . compute_ats_model ( pulse_col , use , limit ) noiseresult = self . noise . spectrum () filter_maker = FilterMaker ( signal_model = avg_pulse , dt_model = dt_model , n_pretrigger = self . header . n_presamples , noise_psd = noiseresult . psd , noise_autocorr = noiseresult . autocorr_vec , sample_time_sec = self . header . frametime_s , ) filter_ats = filter_maker . compute_ats ( f_3db = f_3db ) step = OptimalFilterStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter_ats , spectrum = noiseresult , filter_maker = filter_maker , transform_raw = self . transform_raw , ) return self . with_step ( step )","title":"filterATS"},{"location":"docstrings/#mass2.core.channel.Channel.fit_pulse","text":"Fit a single pulse to a 2-exponential-with-tail model, returning the fit result. Source code in mass2/core/channel.py 1096 1097 1098 1099 1100 1101 1102 1103 1104 def fit_pulse ( self , index : int = 0 , col : str = \"pulse\" , verbose : bool = True ) -> LineModelResult : \"\"\"Fit a single pulse to a 2-exponential-with-tail model, returning the fit result.\"\"\" pulse = self . df [ col ][ index ] . to_numpy () result = mass2 . core . pulse_algorithms . fit_pulse_2exp_with_tail ( pulse , npre = self . header . n_presamples , dt = self . header . frametime_s ) if verbose : print ( f \"ch= { self } \" ) print ( f \"pulse index= { index } \" ) print ( result . fit_report ()) return result","title":"fit_pulse"},{"location":"docstrings/#mass2.core.channel.Channel.from_ljh","text":"Load a Channel from an LJH file, optionally with a NoiseChannel from a corresponding noise LJH file. Source code in mass2/core/channel.py 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 @classmethod def from_ljh ( cls , path : str | Path , noise_path : str | Path | None = None , keep_posix_usec : bool = False , transform_raw : Callable | None = None , ) -> \"Channel\" : \"\"\"Load a Channel from an LJH file, optionally with a NoiseChannel from a corresponding noise LJH file.\"\"\" if not noise_path : noise_channel = None else : noise_channel = NoiseChannel . from_ljh ( noise_path ) ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars ( keep_posix_usec ) header = ChannelHeader . from_ljh_header_df ( header_df ) channel = cls ( df , header = header , npulses = ljh . npulses , subframediv = ljh . subframediv , noise = noise_channel , transform_raw = transform_raw ) return channel","title":"from_ljh"},{"location":"docstrings/#mass2.core.channel.Channel.from_off","text":"Load a Channel from an OFF file. Source code in mass2/core/channel.py 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 @classmethod def from_off ( cls , off : OffFile ) -> \"Channel\" : \"\"\"Load a Channel from an OFF file.\"\"\" assert off . _mmap is not None df = pl . from_numpy ( np . asarray ( off . _mmap )) df = ( df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) . select ( pl . exclude ( \"unixnano\" )) ) df_header = pl . DataFrame ( off . header ) df_header = df_header . with_columns ( pl . Series ( \"Filename\" , [ off . filename ])) header = ChannelHeader ( f \" { os . path . split ( off . filename )[ 1 ] } \" , off . header [ \"ChannelNumberMatchingName\" ], off . framePeriodSeconds , off . _mmap [ \"recordPreSamples\" ][ 0 ], off . _mmap [ \"recordSamples\" ][ 0 ], df_header , ) channel = cls ( df , header , off . nRecords , subframediv = off . subframediv ) return channel","title":"from_off"},{"location":"docstrings/#mass2.core.channel.Channel.get_step","text":"Get the step at the given index, supporting negative indices. Source code in mass2/core/channel.py 112 113 114 115 116 117 118 def get_step ( self , index : int ) -> tuple [ RecipeStep , int ]: \"\"\"Get the step at the given index, supporting negative indices.\"\"\" # normalize the index to a positive index if index < 0 : index = len ( self . steps ) + index step = self . steps [ index ] return step , index","title":"get_step"},{"location":"docstrings/#mass2.core.channel.Channel.good_df","text":"Return a Polars DataFrame of the given columns, filtered by good_expr and use_expr. Source code in mass2/core/channel.py 776 777 778 779 780 781 def good_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : \"\"\"Return a Polars DataFrame of the given columns, filtered by good_expr and use_expr.\"\"\" good_df = self . df . lazy () . filter ( self . good_expr ) if use_expr is not True : good_df = good_df . filter ( use_expr ) return good_df . select ( cols ) . collect ()","title":"good_df"},{"location":"docstrings/#mass2.core.channel.Channel.good_series","text":"Return a Polars Series of the given column, filtered by good_expr and use_expr. Source code in mass2/core/channel.py 280 281 282 def good_series ( self , col : str , use_expr : pl . Expr = pl . lit ( True )) -> pl . Series : \"\"\"Return a Polars Series of the given column, filtered by good_expr and use_expr.\"\"\" return mass2 . misc . good_series ( self . df , col , self . good_expr , use_expr )","title":"good_series"},{"location":"docstrings/#mass2.core.channel.Channel.good_serieses","text":"Return a list of Polars Series of the given columns, filtered by good_expr and use_expr. Source code in mass2/core/channel.py 790 791 792 793 def good_serieses ( self , cols : list [ str ], use_expr : pl . Expr = pl . lit ( True )) -> list [ pl . Series ]: \"\"\"Return a list of Polars Series of the given columns, filtered by good_expr and use_expr.\"\"\" df2 = self . good_df ( cols , use_expr ) return [ df2 [ col ] for col in cols ]","title":"good_serieses"},{"location":"docstrings/#mass2.core.channel.Channel.hist","text":"Compute a histogram of the given column, optionally filtering by good_expr and use_expr. Source code in mass2/core/channel.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def hist ( self , col : str , bin_edges : ArrayLike , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute a histogram of the given column, optionally filtering by good_expr and use_expr.\"\"\" if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col )) . collect () values = df_small [ col ] bin_centers , counts = misc . hist_of_series ( values , bin_edges ) return bin_centers , counts","title":"hist"},{"location":"docstrings/#mass2.core.channel.Channel.linefit","text":"Fit a spectral line to the binned data from the given column, optionally filtering by use_expr. Source code in mass2/core/channel.py 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 def linefit ( # noqa: PLR0917 self , line : GenericLineModel | SpectralLine | str | float , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : \"\"\"Fit a spectral line to the binned data from the given column, optionally filtering by use_expr.\"\"\" model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = self . header . description , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result","title":"linefit"},{"location":"docstrings/#mass2.core.channel.Channel.mo_stepplots","text":"Marimo UI element to choose and display step plots, with a dropdown to choose channel number. Source code in mass2/core/channel.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def mo_stepplots ( self ) -> mo . ui . dropdown : \"\"\"Marimo UI element to choose and display step plots, with a dropdown to choose channel number.\"\"\" desc_ind = { step . description : i for i , step in enumerate ( self . steps )} first_non_summarize_step = self . steps [ 0 ] for step in self . steps : if isinstance ( step , SummarizeStep ): continue first_non_summarize_step = step break mo_ui = mo . ui . dropdown ( desc_ind , value = first_non_summarize_step . description , label = f \"choose step for ch { self . header . ch_num } \" , ) def show () -> mo . Html : \"\"\"Show the selected step plot.\"\"\" return self . _mo_stepplots_explicit ( mo_ui ) def step_ind () -> Any : \"\"\"Get the selected step index from the dropdown item, if any.\"\"\" return mo_ui . value mo_ui . show = show mo_ui . step_ind = step_ind return mo_ui","title":"mo_stepplots"},{"location":"docstrings/#mass2.core.channel.Channel.multifit_mass_cal","text":"Fit multiple spectral lines, to create a Mass1-style gain calibration. Source code in mass2/core/channel.py 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 def multifit_mass_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Fit multiple spectral lines, to create a Mass1-style gain calibration.\"\"\" step = MultiFitMassCalibrationStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step )","title":"multifit_mass_cal"},{"location":"docstrings/#mass2.core.channel.Channel.multifit_quadratic_gain_cal","text":"Fit multiple spectral lines, to create a quadratic gain calibration. Source code in mass2/core/channel.py 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 def multifit_quadratic_gain_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Fit multiple spectral lines, to create a quadratic gain calibration.\"\"\" step = MultiFitQuadraticGainStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step )","title":"multifit_quadratic_gain_cal"},{"location":"docstrings/#mass2.core.channel.Channel.phase_correct_mass_specific_lines","text":"Apply phase correction to the given uncorrected column, where specific lines are used to judge the correction. Source code in mass2/core/channel.py 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 def phase_correct_mass_specific_lines ( self , indicator_col : str , uncorrected_col : str , line_names : Iterable [ str | float ], previous_cal_step_index : int , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Apply phase correction to the given uncorrected column, where specific lines are used to judge the correction.\"\"\" if corrected_col is None : corrected_col = uncorrected_col + \"_pc\" step = mass2 . core . phase_correct_steps . phase_correct_mass_specific_lines ( self , indicator_col , uncorrected_col , corrected_col , previous_cal_step_index , line_names , use_expr , ) return self . with_step ( step )","title":"phase_correct_mass_specific_lines"},{"location":"docstrings/#mass2.core.channel.Channel.plot_hist","text":"Compute and plot a histogram of the given column, optionally filtering by good_expr and use_expr. Source code in mass2/core/channel.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def plot_hist ( self , col : str , bin_edges : ArrayLike , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: \"\"\"Compute and plot a histogram of the given column, optionally filtering by good_expr and use_expr.\"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis bin_centers , counts = self . hist ( col , bin_edges = bin_edges , use_good_expr = use_good_expr , use_expr = use_expr ) _ , step_size = misc . midpoints_and_step_size ( bin_edges ) plt . step ( bin_centers , counts , where = \"mid\" ) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } for { self . shortname } \" ) plt . tight_layout () return bin_centers , counts","title":"plot_hist"},{"location":"docstrings/#mass2.core.channel.Channel.plot_hists","text":"Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channel.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : str , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), skip_none : bool = True , ) -> tuple [ NDArray , dict [ str , NDArray ]]: \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group counts_dict : dict [ str , NDArray ] = {} for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] _ , step_size = misc . midpoints_and_step_size ( bin_edges ) bin_centers , counts = misc . hist_of_series ( values , bin_edges ) group_name_str = str ( group_name ) counts_dict [ group_name_str ] = counts plt . step ( bin_centers , counts , where = \"mid\" , label = group_name_str ) # Plot the histogram for the current group # if group_name == \"EBIT\": # ax.hist(values, bins=bin_edges, alpha=0.9, color=\"k\", label=group_name_str) # else: # ax.hist(values, bins=bin_edges, alpha=0.5, label=group_name_str) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () return bin_centers , counts_dict","title":"plot_hists"},{"location":"docstrings/#mass2.core.channel.Channel.plot_scatter","text":"Generate a scatter plot of y_col vs x_col , optionally colored by color_col . Source code in mass2/core/channel.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def plot_scatter ( self , x_col : str , y_col : str , color_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), use_good_expr : bool = True , skip_none : bool = True , ax : plt . Axes | None = None , ) -> None : \"\"\"Generate a scatter plot of `y_col` vs `x_col`, optionally colored by `color_col`.\"\"\" if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # set current axis so I can use plt api if use_good_expr : filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = self . good_expr df_small = self . df . lazy () . filter ( filter_expr ) . select ( x_col , y_col , color_col ) . collect () for ( name ,), data in df_small . group_by ( color_col , maintain_order = True ): if name is None and skip_none and color_col is not None : continue plt . plot ( data . select ( x_col ) . to_series (), data . select ( y_col ) . to_series (), \".\" , label = name , ) plt . xlabel ( str ( x_col )) plt . ylabel ( str ( y_col )) title_str = f \"\"\" { self . header . description } use_expr= { str ( use_expr ) } good_expr= { str ( self . good_expr ) } \"\"\" plt . title ( title_str ) if color_col is not None : plt . legend ( title = color_col ) plt . tight_layout ()","title":"plot_scatter"},{"location":"docstrings/#mass2.core.channel.Channel.plot_summaries","text":"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters: use_expr_in ( Expr | None , default: None ) \u2013 A polars expression to determine valid pulses, by default None. If None, use self.good_expr downsample ( int | None , default: None ) \u2013 Plot only every one of downsample pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log ( bool , default: False ) \u2013 Whether to make the histograms have a logarithmic y-scale, by default False. Source code in mass2/core/channel.py 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr_in: pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample: int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log: bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) contents , _ , _ = plt . hist ( y , 200 , range = limits , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" )","title":"plot_summaries"},{"location":"docstrings/#mass2.core.channel.Channel.rough_cal","text":"Learn a rough calibration by trying to assign the 3 brightest peaks, then fitting a line to those and looking for other peaks that fit that line. Source code in mass2/core/channel.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 def rough_cal ( # noqa: PLR0917 self , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying to assign the 3 brightest peaks, then fitting a line to those and looking for other peaks that fit that line. \"\"\" step = mass2 . core . RoughCalibrationStep . learn_3peak ( self , line_names , uncalibrated_col , calibrated_col , use_expr , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , fwhm_pulse_height_units , n_extra_peaks , acceptable_rms_residual_e , ) return self . with_step ( step )","title":"rough_cal"},{"location":"docstrings/#mass2.core.channel.Channel.rough_cal_combinatoric","text":"Learn a rough calibration by trying all combinatorically possible peak assignments. Source code in mass2/core/channel.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def rough_cal_combinatoric ( self , line_names : list [ str ], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying all combinatorically possible peak assignments.\"\"\" step = mass2 . core . RoughCalibrationStep . learn_combinatoric ( self , line_names , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step )","title":"rough_cal_combinatoric"},{"location":"docstrings/#mass2.core.channel.Channel.rough_cal_combinatoric_height_info","text":"Learn a rough calibration by trying all combinatorically possible peak assignments, using known relative peak heights to limit the possibilities. Source code in mass2/core/channel.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def rough_cal_combinatoric_height_info ( self , line_names : list [ str ], line_heights_allowed : list [ list [ int ]], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : \"\"\"Learn a rough calibration by trying all combinatorically possible peak assignments, using known relative peak heights to limit the possibilities.\"\"\" step = mass2 . core . RoughCalibrationStep . learn_combinatoric_height_info ( self , line_names , line_heights_allowed , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step )","title":"rough_cal_combinatoric_height_info"},{"location":"docstrings/#mass2.core.channel.Channel.save_recipes","text":"Save the recipe steps to a pickle file, keyed by channel number. Source code in mass2/core/channel.py 1024 1025 1026 1027 1028 def save_recipes ( self , filename : str ) -> dict [ int , Recipe ]: \"\"\"Save the recipe steps to a pickle file, keyed by channel number.\"\"\" steps = { self . header . ch_num : self . steps } misc . pickle_object ( steps , filename ) return steps","title":"save_recipes"},{"location":"docstrings/#mass2.core.channel.Channel.step_plot","text":"Make a debug plot for the given step index, supporting negative indices. Source code in mass2/core/channel.py 120 121 122 123 124 125 126 127 def step_plot ( self , step_ind : int , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a debug plot for the given step index, supporting negative indices.\"\"\" step , step_ind = self . get_step ( step_ind ) if step_ind + 1 == len ( self . df_history ): df_after = self . df else : df_after = self . df_history [ step_ind + 1 ] return step . dbg_plot ( df_after , ** kwargs )","title":"step_plot"},{"location":"docstrings/#mass2.core.channel.Channel.step_summary","text":"Return a list of (step type name, elapsed time in seconds) for each step in the recipe. Source code in mass2/core/channel.py 848 849 850 def step_summary ( self ) -> list [ tuple [ str , float ]]: \"\"\"Return a list of (step type name, elapsed time in seconds) for each step in the recipe.\"\"\" return [( type ( a ) . __name__ , b ) for ( a , b ) in zip ( self . steps , self . steps_elapsed_s )]","title":"step_summary"},{"location":"docstrings/#mass2.core.channel.Channel.summarize_pulses","text":"Summarize the pulses, adding columns for pulse height, pretrigger mean, etc. Source code in mass2/core/channel.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 def summarize_pulses ( self , col : str = \"pulse\" , pretrigger_ignore_samples : int = 0 , peak_index : int | None = None ) -> \"Channel\" : \"\"\"Summarize the pulses, adding columns for pulse height, pretrigger mean, etc.\"\"\" if peak_index is None : peak_index = self . typical_peak_ind ( col ) out_names = mass2 . core . pulse_algorithms . result_dtype . names # mypy (incorrectly) thinks `out_names` might be None, and `list(None)` is forbidden. Assertion makes it happy again. assert out_names is not None outputs = list ( out_names ) step = SummarizeStep ( inputs = [ col ], output = outputs , good_expr = self . good_expr , use_expr = pl . lit ( True ), frametime_s = self . header . frametime_s , peak_index = peak_index , pulse_col = col , pretrigger_ignore_samples = pretrigger_ignore_samples , n_presamples = self . header . n_presamples , transform_raw = self . transform_raw , ) return self . with_step ( step )","title":"summarize_pulses"},{"location":"docstrings/#mass2.core.channel.Channel.typical_peak_ind","text":"Return the typical peak index of the given column, using the median peak index for the first 100 pulses. Source code in mass2/core/channel.py 504 505 506 507 508 509 510 @functools . cache def typical_peak_ind ( self , col : str = \"pulse\" ) -> int : \"\"\"Return the typical peak index of the given column, using the median peak index for the first 100 pulses.\"\"\" raw = self . df . limit ( 100 )[ col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) return int ( np . median ( raw . argmax ( axis = 1 )))","title":"typical_peak_ind"},{"location":"docstrings/#mass2.core.channel.Channel.with_categorize_step","text":"Add a recipe step that categorizes pulses based on the given conditions. Source code in mass2/core/channel.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 def with_categorize_step ( self , category_condition_dict : dict [ str , pl . Expr ], output_col : str = \"category\" ) -> \"Channel\" : \"\"\"Add a recipe step that categorizes pulses based on the given conditions.\"\"\" # ensure the first condition is True, to be used as a fallback first_expr = next ( iter ( category_condition_dict . values ())) if not first_expr . meta . eq ( pl . lit ( True )): category_condition_dict = { \"fallback\" : pl . lit ( True ), ** category_condition_dict } extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in category_condition_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . CategorizeStep ( inputs = list ( inputs ), output = [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), category_condition_dict = category_condition_dict , ) return self . with_step ( step )","title":"with_categorize_step"},{"location":"docstrings/#mass2.core.channel.Channel.with_column_map_step","text":"f should take a numpy array and return a numpy array with the same number of elements Source code in mass2/core/channel.py 445 446 447 448 def with_column_map_step ( self , input_col : str , output_col : str , f : Callable ) -> \"Channel\" : \"\"\"f should take a numpy array and return a numpy array with the same number of elements\"\"\" step = mass2 . core . recipe . ColumnAsNumpyMapStep ([ input_col ], [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), f = f ) return self . with_step ( step )","title":"with_column_map_step"},{"location":"docstrings/#mass2.core.channel.Channel.with_columns","text":"Append columns from df2 to the existing dataframe, keeping all other attributes the same. Source code in mass2/core/channel.py 938 939 940 941 def with_columns ( self , df2 : pl . DataFrame ) -> \"Channel\" : \"\"\"Append columns from df2 to the existing dataframe, keeping all other attributes the same.\"\"\" df3 = self . df . with_columns ( df2 ) return self . with_replacement_df ( df3 )","title":"with_columns"},{"location":"docstrings/#mass2.core.channel.Channel.with_experiment_state_df","text":"Add experiment states from an existing dataframe Source code in mass2/core/channel.py 911 912 913 914 915 916 917 918 919 920 def with_experiment_state_df ( self , df_es : pl . DataFrame , force_timestamp_monotonic : bool = False ) -> \"Channel\" : \"\"\"Add experiment states from an existing dataframe\"\"\" if not self . df [ \"timestamp\" ] . is_sorted (): df = self . df . select ( pl . col ( \"timestamp\" ) . cum_max () . alias ( \"timestamp\" )) . with_columns ( self . df . select ( pl . exclude ( \"timestamp\" ))) # print(\"WARNING: in with_experiment_state_df, timestamp is not monotonic, forcing it to be\") # print(\"This is likely a BUG in DASTARD.\") else : df = self . df df2 = df . join_asof ( df_es , on = \"timestamp\" , strategy = \"backward\" ) return self . with_replacement_df ( df2 )","title":"with_experiment_state_df"},{"location":"docstrings/#mass2.core.channel.Channel.with_external_trigger_df","text":"Add external trigger times from an existing dataframe Source code in mass2/core/channel.py 922 923 924 925 926 927 928 929 def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channel\" : \"\"\"Add external trigger times from an existing dataframe\"\"\" df2 = ( self . df . with_columns ( subframecount = pl . col ( \"framecount\" ) * self . subframediv ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"backward\" , coalesce = False , suffix = \"_prev_ext_trig\" ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"forward\" , coalesce = False , suffix = \"_next_ext_trig\" ) ) return self . with_replacement_df ( df2 )","title":"with_external_trigger_df"},{"location":"docstrings/#mass2.core.channel.Channel.with_good_expr","text":"Return a new Channel with the given good_expr, combined with the existing good_expr by \"and\", of by replacing it entirely if replace is True. Source code in mass2/core/channel.py 436 437 438 439 440 441 442 443 def with_good_expr ( self , good_expr : pl . Expr , replace : bool = False ) -> \"Channel\" : \"\"\"Return a new Channel with the given good_expr, combined with the existing good_expr by \"and\", of by replacing it entirely if `replace` is True.\"\"\" # the default value of self.good_expr is pl.lit(True) # and_(True) will just add visual noise when looking at good_expr and not affect behavior if not replace and good_expr is not True and not good_expr . meta . eq ( pl . lit ( True )): good_expr = good_expr . and_ ( self . good_expr ) return dataclasses . replace ( self , good_expr = good_expr )","title":"with_good_expr"},{"location":"docstrings/#mass2.core.channel.Channel.with_good_expr_below_nsigma_outlier_resistant","text":"Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative Source code in mass2/core/channel.py 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace )","title":"with_good_expr_below_nsigma_outlier_resistant"},{"location":"docstrings/#mass2.core.channel.Channel.with_good_expr_nsigma_range_outlier_resistant","text":"Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative Source code in mass2/core/channel.py 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with any of the given columns above outlier-resistant thresholds. Always sets lower limit at 0, so don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace )","title":"with_good_expr_nsigma_range_outlier_resistant"},{"location":"docstrings/#mass2.core.channel.Channel.with_good_expr_pretrig_rms_and_postpeak_deriv","text":"Set good_expr to exclude pulses with pretrigger RMS or postpeak derivative above outlier-resistant thresholds. Source code in mass2/core/channel.py 450 451 452 453 454 455 456 457 458 459 def with_good_expr_pretrig_rms_and_postpeak_deriv ( self , n_sigma_pretrig_rms : float = 20 , n_sigma_postpeak_deriv : float = 20 , replace : bool = False ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with pretrigger RMS or postpeak derivative above outlier-resistant thresholds.\"\"\" max_postpeak_deriv = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"postpeak_deriv\" ] . to_numpy (), nsigma = n_sigma_postpeak_deriv ) max_pretrig_rms = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"pretrig_rms\" ] . to_numpy (), nsigma = n_sigma_pretrig_rms ) good_expr = ( pl . col ( \"postpeak_deriv\" ) < max_postpeak_deriv ) . and_ ( pl . col ( \"pretrig_rms\" ) < max_pretrig_rms ) return self . with_good_expr ( good_expr , replace )","title":"with_good_expr_pretrig_rms_and_postpeak_deriv"},{"location":"docstrings/#mass2.core.channel.Channel.with_range_around_median","text":"Set good_expr to exclude pulses with col outside the given range around its median. Source code in mass2/core/channel.py 461 462 463 464 def with_range_around_median ( self , col : str , range_up : float , range_down : float ) -> \"Channel\" : \"\"\"Set good_expr to exclude pulses with `col` outside the given range around its median.\"\"\" med = np . median ( self . df [ col ] . to_numpy ()) return self . with_good_expr ( pl . col ( col ) . is_between ( med - range_down , med + range_up ))","title":"with_range_around_median"},{"location":"docstrings/#mass2.core.channel.Channel.with_replacement_df","text":"Replace the dataframe with a new one, keeping all other attributes the same. Source code in mass2/core/channel.py 931 932 933 934 935 936 def with_replacement_df ( self , df2 : pl . DataFrame ) -> \"Channel\" : \"\"\"Replace the dataframe with a new one, keeping all other attributes the same.\"\"\" return dataclasses . replace ( self , df = df2 , )","title":"with_replacement_df"},{"location":"docstrings/#mass2.core.channel.Channel.with_select_step","text":"This step is meant for interactive exploration; it's basically like the df.select() method, but it's saved as a step. Source code in mass2/core/channel.py 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 def with_select_step ( self , col_expr_dict : dict [ str , pl . Expr ]) -> \"Channel\" : \"\"\" This step is meant for interactive exploration; it's basically like the df.select() method, but it's saved as a step. \"\"\" extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in col_expr_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . SelectStep ( inputs = list ( inputs ), output = list ( col_expr_dict . keys ()), good_expr = self . good_expr , use_expr = pl . lit ( True ), col_expr_dict = col_expr_dict , ) return self . with_step ( step )","title":"with_select_step"},{"location":"docstrings/#mass2.core.channel.Channel.with_step","text":"Return a new Channel with the given step applied to generate new columns in the dataframe. Source code in mass2/core/channel.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 def with_step ( self , step : RecipeStep ) -> \"Channel\" : \"\"\"Return a new Channel with the given step applied to generate new columns in the dataframe.\"\"\" t_start = time . time () df2 = step . calc_from_df ( self . df ) elapsed_s = time . time () - t_start ch2 = dataclasses . replace ( self , df = df2 , good_expr = step . good_expr , df_history = self . df_history + [ self . df ], steps = self . steps . with_step ( step ), steps_elapsed_s = self . steps_elapsed_s + [ elapsed_s ], ) return ch2","title":"with_step"},{"location":"docstrings/#mass2.core.channel.Channel.with_steps","text":"Return a new Channel with the given steps applied to generate new columns in the dataframe. Source code in mass2/core/channel.py 429 430 431 432 433 434 def with_steps ( self , steps : Recipe ) -> \"Channel\" : \"\"\"Return a new Channel with the given steps applied to generate new columns in the dataframe.\"\"\" ch2 = self for step in steps : ch2 = ch2 . with_step ( step ) return ch2","title":"with_steps"},{"location":"docstrings/#mass2.core.channel.ChannelHeader","text":"Metadata about a Channel, of the sort read from file header. Source code in mass2/core/channel.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @dataclass ( frozen = True ) class ChannelHeader : \"\"\"Metadata about a Channel, of the sort read from file header.\"\"\" description : str # filename or date/run number, etc ch_num : int frametime_s : float n_presamples : int n_samples : int df : pl . DataFrame = field ( repr = False ) @classmethod def from_ljh_header_df ( cls , df : pl . DataFrame ) -> \"ChannelHeader\" : \"\"\"Construct from the LJH header dataframe as returned by LJHFile.to_polars()\"\"\" return cls ( description = os . path . split ( df [ \"Filename\" ][ 0 ])[ - 1 ], ch_num = df [ \"Channel\" ][ 0 ], frametime_s = df [ \"Timebase\" ][ 0 ], n_presamples = df [ \"Presamples\" ][ 0 ], n_samples = df [ \"Total Samples\" ][ 0 ], df = df , )","title":"ChannelHeader"},{"location":"docstrings/#mass2.core.channel.ChannelHeader.from_ljh_header_df","text":"Construct from the LJH header dataframe as returned by LJHFile.to_polars() Source code in mass2/core/channel.py 45 46 47 48 49 50 51 52 53 54 55 @classmethod def from_ljh_header_df ( cls , df : pl . DataFrame ) -> \"ChannelHeader\" : \"\"\"Construct from the LJH header dataframe as returned by LJHFile.to_polars()\"\"\" return cls ( description = os . path . split ( df [ \"Filename\" ][ 0 ])[ - 1 ], ch_num = df [ \"Channel\" ][ 0 ], frametime_s = df [ \"Timebase\" ][ 0 ], n_presamples = df [ \"Presamples\" ][ 0 ], n_samples = df [ \"Total Samples\" ][ 0 ], df = df , ) Data structures and methods for handling a group of microcalorimeter channels.","title":"from_ljh_header_df"},{"location":"docstrings/#mass2.core.channels.Channels","text":"A collection of microcalorimeter channels, with methods to operate in parallel on all channels. Source code in mass2/core/channels.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 @dataclass ( frozen = True ) # noqa: PLR0904 class Channels : \"\"\"A collection of microcalorimeter channels, with methods to operate in parallel on all channels.\"\"\" channels : dict [ int , Channel ] description : str bad_channels : dict [ int , BadChannel ] = field ( default_factory = dict ) @property def ch0 ( self ) -> Channel : \"\"\"Return a representative Channel object for convenient exploration (the one with the lowest channel number).\"\"\" assert len ( self . channels ) > 0 , \"channels must be non-empty\" return next ( iter ( self . channels . values ())) def with_more_channels ( self , more : \"Channels\" ) -> \"Channels\" : \"\"\"Return a Channels object with additional Channels in it. New channels with the same number will overrule existing ones. Parameters ---------- more : Channels Another Channels object, to be added Returns ------- Channels The replacement \"\"\" channels = self . channels . copy () channels . update ( more . channels ) bad = self . bad_channels . copy () bad . update ( more . bad_channels ) descr = self . description + more . description + \" \\n Warning! created by with_more_channels()\" return dataclasses . replace ( self , channels = channels , bad_channels = bad , description = descr ) @functools . cache def dfg ( self , exclude : str = \"pulse\" ) -> pl . DataFrame : \"\"\"Return a DataFrame containing good pulses from each channel. Excludes the given columns (default \"pulse\").\"\"\" # return a dataframe containing good pulses from each channel, # exluding \"pulse\" by default # and including column \"ch_num\" # the more common call should be to wrap this in a convenient plotter dfs = [] for ch_num , channel in self . channels . items (): df = channel . df . select ( pl . exclude ( exclude )) . filter ( channel . good_expr ) # key_series = pl.Series(\"key\", dtype=pl.Int64).extend_constant(key, len(df)) assert ch_num == channel . header . ch_num ch_series = pl . Series ( \"ch_num\" , dtype = pl . Int64 ) . extend_constant ( channel . header . ch_num , len ( df )) dfs . append ( df . with_columns ( ch_series )) return pl . concat ( dfs ) def linefit ( # noqa: PLR0917 self , line : float | str | SpectralLine | GenericLineModel , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : \"\"\"Perform a fit to one spectral line in the coadded histogram of the given column.\"\"\" model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = mass2 . misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = f \" { len ( self . channels ) } channels, { self . description } \" , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def plot_hist ( self , col : str , bin_edges : ArrayLike , use_expr : pl . Expr = pl . lit ( True ), axis : plt . Axes | None = None ) -> None : \"\"\"Plot a histogram for the given column across all channels.\"\"\" df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () ax = mass2 . misc . plot_hist_of_series ( df_small [ col ], bin_edges , axis ) ax . set_title ( f \" { len ( self . channels ) } channels, { self . description } \" ) def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : bool , axis : plt . Axes | None = None , use_expr : pl . Expr | None = None , skip_none : bool = True , ) -> None : \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () def _limited_chan_list ( self , limit : int | None = 20 , channels : list [ int ] | None = None ) -> list [ int ]: \"\"\"A helper to get a list of channel numbers, limited to the given number if needed, and including only channel numbers from `channels` if not None.\"\"\" limited_chan = list ( self . channels . keys ()) if channels is not None : limited_chan = list ( set ( limited_chan ) . intersection ( set ( channels ))) limited_chan . sort () if limit and len ( limited_chan ) > limit : limited_chan = limited_chan [: limit ] return limited_chan def plot_filters ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the optimal filters for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] # The next line _assumes_ a 5-lag filter. Fix as needed. x = np . arange ( ch . header . n_samples - 4 ) - ch . header . n_presamples + 2 y = ch . last_filter if y is not None : plt . plot ( x , y , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . xlabel ( \"Samples after trigger\" ) plt . title ( \"Optimal filters\" ) def plot_avg_pulses ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the average pulses (the signal model) for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] x = np . arange ( ch . header . n_samples ) - ch . header . n_presamples y = ch . last_avg_pulse if y is not None : plt . plot ( x , y , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . xlabel ( \"Samples after trigger\" ) plt . title ( \"Average pulses\" ) def plot_noise_spectrum ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the noise power spectrum for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] freqpsd = ch . last_noise_psd if freqpsd is not None : freq , psd = freqpsd plt . plot ( freq , psd , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . loglog () plt . xlabel ( \"Frequency (Hz)\" ) plt . title ( \"Noise power spectral density\" ) def plot_noise_autocorr ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the noise power autocorrelation for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] ac = ch . last_noise_autocorrelation if ac is not None : color = colormap ( i / n_expected ) plt . plot ( ac , color = color , label = f \"Chan { ch_num } \" ) plt . plot ( 0 , ac [ 0 ], \"o\" , color = color ) plt . legend () plt . xlabel ( \"Lags\" ) plt . title ( \"Noise autocorrelation\" ) def map ( self , f : Callable , allow_throw : bool = False ) -> \"Channels\" : \"\"\"Map function `f` over all channels, returning a new Channels object containing the new Channel objects.\"\"\" new_channels = {} new_bad_channels = {} for key , channel in self . channels . items (): try : new_channels [ key ] = f ( channel ) except KeyboardInterrupt as kint : raise kint except Exception as ex : error_type : type = type ( ex ) error_message : str = str ( ex ) backtrace : str = traceback . format_exc () if allow_throw : raise print ( f \" { key =} { channel =} failed the step { f } \" ) print ( f \" { error_type =} \" ) print ( f \" { error_message =} \" ) new_bad_channels [ key ] = channel . as_bad ( error_type , error_message , backtrace ) new_bad_channels = mass2 . misc . merge_dicts_ordered_by_keys ( self . bad_channels , new_bad_channels ) return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def set_bad ( self , ch_num : int , msg : str , require_ch_num_exists : bool = True ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with the given channel number marked as bad.\"\"\" new_channels = {} new_bad_channels = {} if require_ch_num_exists : assert ch_num in self . channels . keys (), f \" { ch_num } can't be set bad because it does not exist\" for key , channel in self . channels . items (): if key == ch_num : new_bad_channels [ key ] = channel . as_bad ( None , msg , None ) else : new_channels [ key ] = channel return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def linefit_joblib ( self , line : str , col : str , prefer : str = \"threads\" , n_jobs : int = 4 ) -> LineModelResult : \"\"\"No one but Galen understands this function.\"\"\" def work ( key : int ) -> LineModelResult : \"\"\"A unit of parallel work: fit line to one channel.\"\"\" channel = self . channels [ key ] return channel . linefit ( line , col ) parallel = joblib . Parallel ( n_jobs = n_jobs , prefer = prefer ) # its not clear if threads are better.... what blocks the gil? results = parallel ( joblib . delayed ( work )( key ) for key in self . channels . keys ()) return results def __hash__ ( self ) -> int : \"\"\"Hash based on the object's id (identity).\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other : Any ) -> bool : \"\"\"Equality test based on object identity.\"\"\" return id ( self ) == id ( other ) @classmethod def from_ljh_path_pairs ( cls , pulse_noise_pairs : Iterable [ tuple [ str , str ]], description : str ) -> \"Channels\" : \"\"\" Create a :class:`Channels` instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of `(pulse_path, noise_path)` tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class:`Channel` per `(pulse_path, noise_path)` pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth:`Channel.from_ljh`. The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] \"\"\" channels : dict [ int , Channel ] = {} for pulse_path , noise_path in pulse_noise_pairs : channel = Channel . from_ljh ( pulse_path , noise_path ) assert channel . header . ch_num not in channels . keys () channels [ channel . header . ch_num ] = channel return cls ( channels , description ) @classmethod def from_off_paths ( cls , off_paths : Iterable [ str | Path ], description : str ) -> \"Channels\" : \"\"\"Create an instance from a sequence of OFF-file paths\"\"\" channels = {} for path in off_paths : ch = Channel . from_off ( mass2 . core . OffFile ( str ( path ))) channels [ ch . header . ch_num ] = ch return cls ( channels , description ) @classmethod def from_ljh_folder ( cls , pulse_folder : str | Path , noise_folder : str | Path | None = None , limit : int | None = None , exclude_ch_nums : list [ int ] | None = None , ) -> \"Channels\" : \"\"\"Create an instance from a directory of LJH files.\"\"\" assert os . path . isdir ( pulse_folder ), f \" { pulse_folder =} { noise_folder =} \" pulse_folder = str ( pulse_folder ) if exclude_ch_nums is None : exclude_ch_nums = [] if noise_folder is None : paths = ljhutil . find_ljh_files ( pulse_folder , exclude_ch_nums = exclude_ch_nums ) if limit is not None : paths = paths [: limit ] pairs = [( path , \"\" ) for path in paths ] else : assert os . path . isdir ( noise_folder ), f \" { pulse_folder =} { noise_folder =} \" noise_folder = str ( noise_folder ) pairs = ljhutil . match_files_by_channel ( pulse_folder , noise_folder , limit = limit , exclude_ch_nums = exclude_ch_nums ) description = f \"from_ljh_folder { pulse_folder =} { noise_folder =} \" print ( f \" { description } \" ) print ( f \" from_ljh_folder has { len ( pairs ) } pairs\" ) data = cls . from_ljh_path_pairs ( pairs , description ) print ( f \" and the Channels obj has { len ( data . channels ) } pairs\" ) return data def get_an_ljh_path ( self ) -> Path : \"\"\"Return the path to a representative one of the LJH files used to create this Channels object.\"\"\" return pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) def get_path_in_output_folder ( self , filename : str | Path ) -> Path : \"\"\"Return a path in an output folder named like the run number, sibling to the LJH folder.\"\"\" ljh_path = self . get_an_ljh_path () base_name , _ = ljh_path . name . split ( \"_chan\" ) date , run_num = base_name . split ( \"_run\" ) # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") output_dir = ljh_path . parent . parent / f \" { run_num } mass2_output\" output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir / filename def get_experiment_state_df ( self , experiment_state_path : str | Path | None = None ) -> pl . DataFrame : \"\"\"Return a DataFrame containing experiment state information, loading from the given path or (if None) inferring it from an LJH file.\"\"\" if experiment_state_path is None : ljh_path = self . get_an_ljh_path () experiment_state_path = ljhutil . experiment_state_path_from_ljh_path ( ljh_path ) df = pl . read_csv ( experiment_state_path , new_columns = [ \"unixnano\" , \"state_label\" ]) # _col0, _col1 = df.columns df_es = df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) # strip whitespace from state_label column sl_series = df . select ( pl . col ( \"state_label\" ) . str . strip_chars ()) . to_series () df_es = df_es . with_columns ( state_label = pl . Series ( values = sl_series , dtype = pl . Categorical )) return df_es def with_experiment_state_by_path ( self , experiment_state_path : str | None = None ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with experiment state information added, loaded from the given path.\"\"\" df_es = self . get_experiment_state_df ( experiment_state_path ) return self . with_experiment_state_df ( df_es ) def with_external_trigger_by_path ( self , path : str | None = None ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with external trigger information added, loaded from the given path or EVENTUALLY (if None) inferring it from an LJH file (not yet implemented).\"\"\" if path is None : raise NotImplementedError ( \"cannot infer external trigger path yet\" ) with open ( path , \"rb\" ) as _f : _header_line = _f . readline () # read the one header line before opening the binary data external_trigger_subframe_count = np . fromfile ( _f , \"int64\" ) df_ext = pl . DataFrame ({ \"subframecount\" : external_trigger_subframe_count , }) return self . with_external_trigger_df ( df_ext ) def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with external trigger information added to each Channel, found from the given DataFrame.\"\"\" def with_etrig_df ( channel : Channel ) -> Channel : \"\"\"Return a copy of one Channel object with external trigger information added to it\"\"\" return channel . with_external_trigger_df ( df_ext ) return self . map ( with_etrig_df ) def with_experiment_state_df ( self , df_es : pl . DataFrame ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with experiment state information added to each Channel, found from the given DataFrame.\"\"\" # this is not as performant as making use_exprs for states # and using .set_sorted on the timestamp column ch2s = {} for ch_num , ch in self . channels . items (): ch2s [ ch_num ] = ch . with_experiment_state_df ( df_es ) return Channels ( ch2s , self . description ) def with_steps_dict ( self , steps_dict : dict [ int , Recipe ]) -> \"Channels\" : \"\"\"Return a copy of this Channels object with the given Recipe objects added to each Channel.\"\"\" def load_recipes ( channel : Channel ) -> Channel : \"\"\"Return a copy of one Channel object with Recipe steps added to it\"\"\" try : steps = steps_dict [ channel . header . ch_num ] except KeyError : raise Exception ( \"steps dict did not contain steps for this ch_num\" ) return channel . with_steps ( steps ) return self . map ( load_recipes ) def save_recipes ( self , filename : str , required_fields : str | Iterable [ str ] | None = None , drop_debug : bool = True ) -> dict [ int , Recipe ]: \"\"\"Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set `required_fields` to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters ---------- filename : str Filename to store recipe in, typically of the form \"*.pkl\" required_fields : str | Iterable[str] | None The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug: bool Whether to remove debugging-related data from each `RecipeStep`, if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns ------- dict Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. \"\"\" steps = {} for channum , ch in self . channels . items (): steps [ channum ] = ch . steps . trim_dead_ends ( required_fields = required_fields , drop_debug = drop_debug ) mass2 . misc . pickle_object ( steps , filename ) return steps def load_recipes ( self , filename : str ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with Recipe objects loaded from the given pickle file and applied to each Channel.\"\"\" steps = mass2 . misc . unpickle_object ( filename ) return self . with_steps_dict ( steps ) def parent_folder_path ( self ) -> pathlib . Path : \"\"\"Return the parent folder of the LJH files used to create this Channels object. Specifically, the `self.ch0` channel's directory is used (normally the answer would be the same for all channels).\"\"\" parent_folder_path = pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) . parent . parent print ( f \" { parent_folder_path =} \" ) return parent_folder_path def concat_data ( self , other_data : \"Channels\" ) -> \"Channels\" : \"\"\"Return a new Channels object with data from this and the other Channels object concatenated together. Only channels that exist in both objects are included in the result.\"\"\" # sorting here to show intention, but I think set is sorted by insertion order as # an implementation detail so this may not do anything ch_nums = sorted ( list ( set ( self . channels . keys ()) . intersection ( other_data . channels . keys ()))) new_channels = {} for ch_num in ch_nums : ch = self . channels [ ch_num ] other_ch = other_data . channels [ ch_num ] combined_df = mass2 . core . misc . concat_dfs_with_concat_state ( ch . df , other_ch . df ) new_ch = ch . with_replacement_df ( combined_df ) new_channels [ ch_num ] = new_ch return mass2 . Channels ( new_channels , self . description + other_data . description ) @classmethod def from_df ( cls , df_in : pl . DataFrame , frametime_s : float , n_presamples : int , n_samples : int , description : str = \"from Channels.channels_from_df\" , ) -> \"Channels\" : \"\"\"Create a Channels object from a single DataFrame that holds data from multiple channels.\"\"\" # requres a column named \"ch_num\" containing the channel number keys_df : dict [ tuple , pl . DataFrame ] = df_in . partition_by ( by = [ \"ch_num\" ], as_dict = True ) dfs : dict [ int , pl . DataFrame ] = { keys [ 0 ]: df for ( keys , df ) in keys_df . items ()} channels : dict [ int , Channel ] = {} for ch_num , df in dfs . items (): channels [ ch_num ] = Channel ( df , header = ChannelHeader ( description = \"from df\" , ch_num = ch_num , frametime_s = frametime_s , n_presamples = n_presamples , n_samples = n_samples , df = df , ), npulses = len ( df ), ) return Channels ( channels , description )","title":"Channels"},{"location":"docstrings/#mass2.core.channels.Channels.ch0","text":"Return a representative Channel object for convenient exploration (the one with the lowest channel number).","title":"ch0"},{"location":"docstrings/#mass2.core.channels.Channels.__eq__","text":"Equality test based on object identity. Source code in mass2/core/channels.py 406 407 408 def __eq__ ( self , other : Any ) -> bool : \"\"\"Equality test based on object identity.\"\"\" return id ( self ) == id ( other )","title":"__eq__"},{"location":"docstrings/#mass2.core.channels.Channels.__hash__","text":"Hash based on the object's id (identity). Source code in mass2/core/channels.py 399 400 401 402 403 404 def __hash__ ( self ) -> int : \"\"\"Hash based on the object's id (identity).\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self ))","title":"__hash__"},{"location":"docstrings/#mass2.core.channels.Channels.concat_data","text":"Return a new Channels object with data from this and the other Channels object concatenated together. Only channels that exist in both objects are included in the result. Source code in mass2/core/channels.py 613 614 615 616 617 618 619 620 621 622 623 624 625 626 def concat_data ( self , other_data : \"Channels\" ) -> \"Channels\" : \"\"\"Return a new Channels object with data from this and the other Channels object concatenated together. Only channels that exist in both objects are included in the result.\"\"\" # sorting here to show intention, but I think set is sorted by insertion order as # an implementation detail so this may not do anything ch_nums = sorted ( list ( set ( self . channels . keys ()) . intersection ( other_data . channels . keys ()))) new_channels = {} for ch_num in ch_nums : ch = self . channels [ ch_num ] other_ch = other_data . channels [ ch_num ] combined_df = mass2 . core . misc . concat_dfs_with_concat_state ( ch . df , other_ch . df ) new_ch = ch . with_replacement_df ( combined_df ) new_channels [ ch_num ] = new_ch return mass2 . Channels ( new_channels , self . description + other_data . description )","title":"concat_data"},{"location":"docstrings/#mass2.core.channels.Channels.dfg","text":"Return a DataFrame containing good pulses from each channel. Excludes the given columns (default \"pulse\"). Source code in mass2/core/channels.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @functools . cache def dfg ( self , exclude : str = \"pulse\" ) -> pl . DataFrame : \"\"\"Return a DataFrame containing good pulses from each channel. Excludes the given columns (default \"pulse\").\"\"\" # return a dataframe containing good pulses from each channel, # exluding \"pulse\" by default # and including column \"ch_num\" # the more common call should be to wrap this in a convenient plotter dfs = [] for ch_num , channel in self . channels . items (): df = channel . df . select ( pl . exclude ( exclude )) . filter ( channel . good_expr ) # key_series = pl.Series(\"key\", dtype=pl.Int64).extend_constant(key, len(df)) assert ch_num == channel . header . ch_num ch_series = pl . Series ( \"ch_num\" , dtype = pl . Int64 ) . extend_constant ( channel . header . ch_num , len ( df )) dfs . append ( df . with_columns ( ch_series )) return pl . concat ( dfs )","title":"dfg"},{"location":"docstrings/#mass2.core.channels.Channels.from_df","text":"Create a Channels object from a single DataFrame that holds data from multiple channels. Source code in mass2/core/channels.py 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 @classmethod def from_df ( cls , df_in : pl . DataFrame , frametime_s : float , n_presamples : int , n_samples : int , description : str = \"from Channels.channels_from_df\" , ) -> \"Channels\" : \"\"\"Create a Channels object from a single DataFrame that holds data from multiple channels.\"\"\" # requres a column named \"ch_num\" containing the channel number keys_df : dict [ tuple , pl . DataFrame ] = df_in . partition_by ( by = [ \"ch_num\" ], as_dict = True ) dfs : dict [ int , pl . DataFrame ] = { keys [ 0 ]: df for ( keys , df ) in keys_df . items ()} channels : dict [ int , Channel ] = {} for ch_num , df in dfs . items (): channels [ ch_num ] = Channel ( df , header = ChannelHeader ( description = \"from df\" , ch_num = ch_num , frametime_s = frametime_s , n_presamples = n_presamples , n_samples = n_samples , df = df , ), npulses = len ( df ), ) return Channels ( channels , description )","title":"from_df"},{"location":"docstrings/#mass2.core.channels.Channels.from_ljh_folder","text":"Create an instance from a directory of LJH files. Source code in mass2/core/channels.py 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 @classmethod def from_ljh_folder ( cls , pulse_folder : str | Path , noise_folder : str | Path | None = None , limit : int | None = None , exclude_ch_nums : list [ int ] | None = None , ) -> \"Channels\" : \"\"\"Create an instance from a directory of LJH files.\"\"\" assert os . path . isdir ( pulse_folder ), f \" { pulse_folder =} { noise_folder =} \" pulse_folder = str ( pulse_folder ) if exclude_ch_nums is None : exclude_ch_nums = [] if noise_folder is None : paths = ljhutil . find_ljh_files ( pulse_folder , exclude_ch_nums = exclude_ch_nums ) if limit is not None : paths = paths [: limit ] pairs = [( path , \"\" ) for path in paths ] else : assert os . path . isdir ( noise_folder ), f \" { pulse_folder =} { noise_folder =} \" noise_folder = str ( noise_folder ) pairs = ljhutil . match_files_by_channel ( pulse_folder , noise_folder , limit = limit , exclude_ch_nums = exclude_ch_nums ) description = f \"from_ljh_folder { pulse_folder =} { noise_folder =} \" print ( f \" { description } \" ) print ( f \" from_ljh_folder has { len ( pairs ) } pairs\" ) data = cls . from_ljh_path_pairs ( pairs , description ) print ( f \" and the Channels obj has { len ( data . channels ) } pairs\" ) return data","title":"from_ljh_folder"},{"location":"docstrings/#mass2.core.channels.Channels.from_ljh_path_pairs","text":"Create a :class: Channels instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of (pulse_path, noise_path) tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class: Channel per (pulse_path, noise_path) pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth: Channel.from_ljh . The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] Source code in mass2/core/channels.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 @classmethod def from_ljh_path_pairs ( cls , pulse_noise_pairs : Iterable [ tuple [ str , str ]], description : str ) -> \"Channels\" : \"\"\" Create a :class:`Channels` instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of `(pulse_path, noise_path)` tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class:`Channel` per `(pulse_path, noise_path)` pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth:`Channel.from_ljh`. The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] \"\"\" channels : dict [ int , Channel ] = {} for pulse_path , noise_path in pulse_noise_pairs : channel = Channel . from_ljh ( pulse_path , noise_path ) assert channel . header . ch_num not in channels . keys () channels [ channel . header . ch_num ] = channel return cls ( channels , description )","title":"from_ljh_path_pairs"},{"location":"docstrings/#mass2.core.channels.Channels.from_off_paths","text":"Create an instance from a sequence of OFF-file paths Source code in mass2/core/channels.py 451 452 453 454 455 456 457 458 @classmethod def from_off_paths ( cls , off_paths : Iterable [ str | Path ], description : str ) -> \"Channels\" : \"\"\"Create an instance from a sequence of OFF-file paths\"\"\" channels = {} for path in off_paths : ch = Channel . from_off ( mass2 . core . OffFile ( str ( path ))) channels [ ch . header . ch_num ] = ch return cls ( channels , description )","title":"from_off_paths"},{"location":"docstrings/#mass2.core.channels.Channels.get_an_ljh_path","text":"Return the path to a representative one of the LJH files used to create this Channels object. Source code in mass2/core/channels.py 489 490 491 def get_an_ljh_path ( self ) -> Path : \"\"\"Return the path to a representative one of the LJH files used to create this Channels object.\"\"\" return pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ])","title":"get_an_ljh_path"},{"location":"docstrings/#mass2.core.channels.Channels.get_experiment_state_df","text":"Return a DataFrame containing experiment state information, loading from the given path or (if None) inferring it from an LJH file. Source code in mass2/core/channels.py 502 503 504 505 506 507 508 509 510 511 512 513 514 def get_experiment_state_df ( self , experiment_state_path : str | Path | None = None ) -> pl . DataFrame : \"\"\"Return a DataFrame containing experiment state information, loading from the given path or (if None) inferring it from an LJH file.\"\"\" if experiment_state_path is None : ljh_path = self . get_an_ljh_path () experiment_state_path = ljhutil . experiment_state_path_from_ljh_path ( ljh_path ) df = pl . read_csv ( experiment_state_path , new_columns = [ \"unixnano\" , \"state_label\" ]) # _col0, _col1 = df.columns df_es = df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) # strip whitespace from state_label column sl_series = df . select ( pl . col ( \"state_label\" ) . str . strip_chars ()) . to_series () df_es = df_es . with_columns ( state_label = pl . Series ( values = sl_series , dtype = pl . Categorical )) return df_es","title":"get_experiment_state_df"},{"location":"docstrings/#mass2.core.channels.Channels.get_path_in_output_folder","text":"Return a path in an output folder named like the run number, sibling to the LJH folder. Source code in mass2/core/channels.py 493 494 495 496 497 498 499 500 def get_path_in_output_folder ( self , filename : str | Path ) -> Path : \"\"\"Return a path in an output folder named like the run number, sibling to the LJH folder.\"\"\" ljh_path = self . get_an_ljh_path () base_name , _ = ljh_path . name . split ( \"_chan\" ) date , run_num = base_name . split ( \"_run\" ) # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") output_dir = ljh_path . parent . parent / f \" { run_num } mass2_output\" output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir / filename","title":"get_path_in_output_folder"},{"location":"docstrings/#mass2.core.channels.Channels.linefit","text":"Perform a fit to one spectral line in the coadded histogram of the given column. Source code in mass2/core/channels.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def linefit ( # noqa: PLR0917 self , line : float | str | SpectralLine | GenericLineModel , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : \"\"\"Perform a fit to one spectral line in the coadded histogram of the given column.\"\"\" model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = mass2 . misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = f \" { len ( self . channels ) } channels, { self . description } \" , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result","title":"linefit"},{"location":"docstrings/#mass2.core.channels.Channels.linefit_joblib","text":"No one but Galen understands this function. Source code in mass2/core/channels.py 387 388 389 390 391 392 393 394 395 396 397 def linefit_joblib ( self , line : str , col : str , prefer : str = \"threads\" , n_jobs : int = 4 ) -> LineModelResult : \"\"\"No one but Galen understands this function.\"\"\" def work ( key : int ) -> LineModelResult : \"\"\"A unit of parallel work: fit line to one channel.\"\"\" channel = self . channels [ key ] return channel . linefit ( line , col ) parallel = joblib . Parallel ( n_jobs = n_jobs , prefer = prefer ) # its not clear if threads are better.... what blocks the gil? results = parallel ( joblib . delayed ( work )( key ) for key in self . channels . keys ()) return results","title":"linefit_joblib"},{"location":"docstrings/#mass2.core.channels.Channels.load_recipes","text":"Return a copy of this Channels object with Recipe objects loaded from the given pickle file and applied to each Channel. Source code in mass2/core/channels.py 600 601 602 603 604 def load_recipes ( self , filename : str ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with Recipe objects loaded from the given pickle file and applied to each Channel.\"\"\" steps = mass2 . misc . unpickle_object ( filename ) return self . with_steps_dict ( steps )","title":"load_recipes"},{"location":"docstrings/#mass2.core.channels.Channels.map","text":"Map function f over all channels, returning a new Channels object containing the new Channel objects. Source code in mass2/core/channels.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def map ( self , f : Callable , allow_throw : bool = False ) -> \"Channels\" : \"\"\"Map function `f` over all channels, returning a new Channels object containing the new Channel objects.\"\"\" new_channels = {} new_bad_channels = {} for key , channel in self . channels . items (): try : new_channels [ key ] = f ( channel ) except KeyboardInterrupt as kint : raise kint except Exception as ex : error_type : type = type ( ex ) error_message : str = str ( ex ) backtrace : str = traceback . format_exc () if allow_throw : raise print ( f \" { key =} { channel =} failed the step { f } \" ) print ( f \" { error_type =} \" ) print ( f \" { error_message =} \" ) new_bad_channels [ key ] = channel . as_bad ( error_type , error_message , backtrace ) new_bad_channels = mass2 . misc . merge_dicts_ordered_by_keys ( self . bad_channels , new_bad_channels ) return Channels ( new_channels , self . description , bad_channels = new_bad_channels )","title":"map"},{"location":"docstrings/#mass2.core.channels.Channels.parent_folder_path","text":"Return the parent folder of the LJH files used to create this Channels object. Specifically, the self.ch0 channel's directory is used (normally the answer would be the same for all channels). Source code in mass2/core/channels.py 606 607 608 609 610 611 def parent_folder_path ( self ) -> pathlib . Path : \"\"\"Return the parent folder of the LJH files used to create this Channels object. Specifically, the `self.ch0` channel's directory is used (normally the answer would be the same for all channels).\"\"\" parent_folder_path = pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) . parent . parent print ( f \" { parent_folder_path =} \" ) return parent_folder_path","title":"parent_folder_path"},{"location":"docstrings/#mass2.core.channels.Channels.plot_avg_pulses","text":"Plot the average pulses (the signal model) for the channels in this Channels object. Parameters: limit ( int | None , default: 20 ) \u2013 Plot at most this many filters if not None, by default 20 channels ( list [ int ] | None , default: None ) \u2013 Plot only channels with numbers in this list if not None, by default None colormap ( Colormap , default: viridis ) \u2013 The color scale to use, by default plt.cm.viridis axis ( Axes | None , default: None ) \u2013 A plt.Axes to plot on, or if None a new one, by default None Returns: Axes \u2013 The plt.Axes containing the plot. Source code in mass2/core/channels.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def plot_avg_pulses ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the average pulses (the signal model) for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] x = np . arange ( ch . header . n_samples ) - ch . header . n_presamples y = ch . last_avg_pulse if y is not None : plt . plot ( x , y , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . xlabel ( \"Samples after trigger\" ) plt . title ( \"Average pulses\" )","title":"plot_avg_pulses"},{"location":"docstrings/#mass2.core.channels.Channels.plot_filters","text":"Plot the optimal filters for the channels in this Channels object. Parameters: limit ( int | None , default: 20 ) \u2013 Plot at most this many filters if not None, by default 20 channels ( list [ int ] | None , default: None ) \u2013 Plot only channels with numbers in this list if not None, by default None colormap ( Colormap , default: viridis ) \u2013 The color scale to use, by default plt.cm.viridis axis ( Axes | None , default: None ) \u2013 A plt.Axes to plot on, or if None a new one, by default None Returns: Axes \u2013 The plt.Axes containing the plot. Source code in mass2/core/channels.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def plot_filters ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the optimal filters for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] # The next line _assumes_ a 5-lag filter. Fix as needed. x = np . arange ( ch . header . n_samples - 4 ) - ch . header . n_presamples + 2 y = ch . last_filter if y is not None : plt . plot ( x , y , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . xlabel ( \"Samples after trigger\" ) plt . title ( \"Optimal filters\" )","title":"plot_filters"},{"location":"docstrings/#mass2.core.channels.Channels.plot_hist","text":"Plot a histogram for the given column across all channels. Source code in mass2/core/channels.py 115 116 117 118 119 def plot_hist ( self , col : str , bin_edges : ArrayLike , use_expr : pl . Expr = pl . lit ( True ), axis : plt . Axes | None = None ) -> None : \"\"\"Plot a histogram for the given column across all channels.\"\"\" df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () ax = mass2 . misc . plot_hist_of_series ( df_small [ col ], bin_edges , axis ) ax . set_title ( f \" { len ( self . channels ) } channels, { self . description } \" )","title":"plot_hist"},{"location":"docstrings/#mass2.core.channels.Channels.plot_hists","text":"Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channels.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : bool , axis : plt . Axes | None = None , use_expr : pl . Expr | None = None , skip_none : bool = True , ) -> None : \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout ()","title":"plot_hists"},{"location":"docstrings/#mass2.core.channels.Channels.plot_noise_autocorr","text":"Plot the noise power autocorrelation for the channels in this Channels object. Parameters: limit ( int | None , default: 20 ) \u2013 Plot at most this many filters if not None, by default 20 channels ( list [ int ] | None , default: None ) \u2013 Plot only channels with numbers in this list if not None, by default None colormap ( Colormap , default: viridis ) \u2013 The color scale to use, by default plt.cm.viridis axis ( Axes | None , default: None ) \u2013 A plt.Axes to plot on, or if None a new one, by default None Returns: Axes \u2013 The plt.Axes containing the plot. Source code in mass2/core/channels.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def plot_noise_autocorr ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the noise power autocorrelation for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] ac = ch . last_noise_autocorrelation if ac is not None : color = colormap ( i / n_expected ) plt . plot ( ac , color = color , label = f \"Chan { ch_num } \" ) plt . plot ( 0 , ac [ 0 ], \"o\" , color = color ) plt . legend () plt . xlabel ( \"Lags\" ) plt . title ( \"Noise autocorrelation\" )","title":"plot_noise_autocorr"},{"location":"docstrings/#mass2.core.channels.Channels.plot_noise_spectrum","text":"Plot the noise power spectrum for the channels in this Channels object. Parameters: limit ( int | None , default: 20 ) \u2013 Plot at most this many filters if not None, by default 20 channels ( list [ int ] | None , default: None ) \u2013 Plot only channels with numbers in this list if not None, by default None colormap ( Colormap , default: viridis ) \u2013 The color scale to use, by default plt.cm.viridis axis ( Axes | None , default: None ) \u2013 A plt.Axes to plot on, or if None a new one, by default None Returns: Axes \u2013 The plt.Axes containing the plot. Source code in mass2/core/channels.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def plot_noise_spectrum ( self , limit : int | None = 20 , channels : list [ int ] | None = None , colormap : matplotlib . colors . Colormap = plt . cm . viridis , axis : plt . Axes | None = None , ) -> plt . Axes : \"\"\"Plot the noise power spectrum for the channels in this Channels object. Parameters ---------- limit : int | None, optional Plot at most this many filters if not None, by default 20 channels : list[int] | None, optional Plot only channels with numbers in this list if not None, by default None colormap : matplotlib.colors.Colormap, optional The color scale to use, by default plt.cm.viridis axis : plt.Axes | None, optional A `plt.Axes` to plot on, or if None a new one, by default None Returns ------- plt.Axes The `plt.Axes` containing the plot. \"\"\" if axis is None : fig = plt . figure () axis = fig . subplots () plot_these_chan = self . _limited_chan_list ( limit , channels ) n_expected = len ( plot_these_chan ) for i , ch_num in enumerate ( plot_these_chan ): ch = self . channels [ ch_num ] freqpsd = ch . last_noise_psd if freqpsd is not None : freq , psd = freqpsd plt . plot ( freq , psd , color = colormap ( i / n_expected ), label = f \"Chan { ch_num } \" ) plt . legend () plt . loglog () plt . xlabel ( \"Frequency (Hz)\" ) plt . title ( \"Noise power spectral density\" )","title":"plot_noise_spectrum"},{"location":"docstrings/#mass2.core.channels.Channels.save_recipes","text":"Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set required_fields to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters: filename ( str ) \u2013 Filename to store recipe in, typically of the form \"*.pkl\" required_fields ( str | Iterable [ str ] | None , default: None ) \u2013 The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug ( bool , default: True ) \u2013 Whether to remove debugging-related data from each RecipeStep , if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns: dict \u2013 Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. Source code in mass2/core/channels.py 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 def save_recipes ( self , filename : str , required_fields : str | Iterable [ str ] | None = None , drop_debug : bool = True ) -> dict [ int , Recipe ]: \"\"\"Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set `required_fields` to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters ---------- filename : str Filename to store recipe in, typically of the form \"*.pkl\" required_fields : str | Iterable[str] | None The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug: bool Whether to remove debugging-related data from each `RecipeStep`, if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns ------- dict Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. \"\"\" steps = {} for channum , ch in self . channels . items (): steps [ channum ] = ch . steps . trim_dead_ends ( required_fields = required_fields , drop_debug = drop_debug ) mass2 . misc . pickle_object ( steps , filename ) return steps","title":"save_recipes"},{"location":"docstrings/#mass2.core.channels.Channels.set_bad","text":"Return a copy of this Channels object with the given channel number marked as bad. Source code in mass2/core/channels.py 374 375 376 377 378 379 380 381 382 383 384 385 def set_bad ( self , ch_num : int , msg : str , require_ch_num_exists : bool = True ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with the given channel number marked as bad.\"\"\" new_channels = {} new_bad_channels = {} if require_ch_num_exists : assert ch_num in self . channels . keys (), f \" { ch_num } can't be set bad because it does not exist\" for key , channel in self . channels . items (): if key == ch_num : new_bad_channels [ key ] = channel . as_bad ( None , msg , None ) else : new_channels [ key ] = channel return Channels ( new_channels , self . description , bad_channels = new_bad_channels )","title":"set_bad"},{"location":"docstrings/#mass2.core.channels.Channels.with_experiment_state_by_path","text":"Return a copy of this Channels object with experiment state information added, loaded from the given path. Source code in mass2/core/channels.py 516 517 518 519 520 def with_experiment_state_by_path ( self , experiment_state_path : str | None = None ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with experiment state information added, loaded from the given path.\"\"\" df_es = self . get_experiment_state_df ( experiment_state_path ) return self . with_experiment_state_df ( df_es )","title":"with_experiment_state_by_path"},{"location":"docstrings/#mass2.core.channels.Channels.with_experiment_state_df","text":"Return a copy of this Channels object with experiment state information added to each Channel, found from the given DataFrame. Source code in mass2/core/channels.py 545 546 547 548 549 550 551 552 553 def with_experiment_state_df ( self , df_es : pl . DataFrame ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with experiment state information added to each Channel, found from the given DataFrame.\"\"\" # this is not as performant as making use_exprs for states # and using .set_sorted on the timestamp column ch2s = {} for ch_num , ch in self . channels . items (): ch2s [ ch_num ] = ch . with_experiment_state_df ( df_es ) return Channels ( ch2s , self . description )","title":"with_experiment_state_df"},{"location":"docstrings/#mass2.core.channels.Channels.with_external_trigger_by_path","text":"Return a copy of this Channels object with external trigger information added, loaded from the given path or EVENTUALLY (if None) inferring it from an LJH file (not yet implemented). Source code in mass2/core/channels.py 522 523 524 525 526 527 528 529 530 531 532 533 def with_external_trigger_by_path ( self , path : str | None = None ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with external trigger information added, loaded from the given path or EVENTUALLY (if None) inferring it from an LJH file (not yet implemented).\"\"\" if path is None : raise NotImplementedError ( \"cannot infer external trigger path yet\" ) with open ( path , \"rb\" ) as _f : _header_line = _f . readline () # read the one header line before opening the binary data external_trigger_subframe_count = np . fromfile ( _f , \"int64\" ) df_ext = pl . DataFrame ({ \"subframecount\" : external_trigger_subframe_count , }) return self . with_external_trigger_df ( df_ext )","title":"with_external_trigger_by_path"},{"location":"docstrings/#mass2.core.channels.Channels.with_external_trigger_df","text":"Return a copy of this Channels object with external trigger information added to each Channel, found from the given DataFrame. Source code in mass2/core/channels.py 535 536 537 538 539 540 541 542 543 def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channels\" : \"\"\"Return a copy of this Channels object with external trigger information added to each Channel, found from the given DataFrame.\"\"\" def with_etrig_df ( channel : Channel ) -> Channel : \"\"\"Return a copy of one Channel object with external trigger information added to it\"\"\" return channel . with_external_trigger_df ( df_ext ) return self . map ( with_etrig_df )","title":"with_external_trigger_df"},{"location":"docstrings/#mass2.core.channels.Channels.with_more_channels","text":"Return a Channels object with additional Channels in it. New channels with the same number will overrule existing ones. Parameters: more ( Channels ) \u2013 Another Channels object, to be added Returns: Channels \u2013 The replacement Source code in mass2/core/channels.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def with_more_channels ( self , more : \"Channels\" ) -> \"Channels\" : \"\"\"Return a Channels object with additional Channels in it. New channels with the same number will overrule existing ones. Parameters ---------- more : Channels Another Channels object, to be added Returns ------- Channels The replacement \"\"\" channels = self . channels . copy () channels . update ( more . channels ) bad = self . bad_channels . copy () bad . update ( more . bad_channels ) descr = self . description + more . description + \" \\n Warning! created by with_more_channels()\" return dataclasses . replace ( self , channels = channels , bad_channels = bad , description = descr )","title":"with_more_channels"},{"location":"docstrings/#mass2.core.channels.Channels.with_steps_dict","text":"Return a copy of this Channels object with the given Recipe objects added to each Channel. Source code in mass2/core/channels.py 555 556 557 558 559 560 561 562 563 564 565 566 def with_steps_dict ( self , steps_dict : dict [ int , Recipe ]) -> \"Channels\" : \"\"\"Return a copy of this Channels object with the given Recipe objects added to each Channel.\"\"\" def load_recipes ( channel : Channel ) -> Channel : \"\"\"Return a copy of one Channel object with Recipe steps added to it\"\"\" try : steps = steps_dict [ channel . header . ch_num ] except KeyError : raise Exception ( \"steps dict did not contain steps for this ch_num\" ) return channel . with_steps ( steps ) return self . map ( load_recipes ) mass2.core.analysis_algorithms - main algorithms used in data analysis Designed to abstract certain key algorithms out of the class MicrocalDataSet and be able to run them fast. Created on Jun 9, 2014 @author: fowlerj","title":"with_steps_dict"},{"location":"docstrings/#mass2.core.analysis_algorithms.HistogramSmoother","text":"Object that can repeatedly smooth histograms with the same bin count and width to the same Gaussian width. By pre-computing the smoothing kernel for that histogram, we can smooth multiple histograms with the same geometry. Source code in mass2/core/analysis_algorithms.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 class HistogramSmoother : \"\"\"Object that can repeatedly smooth histograms with the same bin count and width to the same Gaussian width. By pre-computing the smoothing kernel for that histogram, we can smooth multiple histograms with the same geometry. \"\"\" def __init__ ( self , smooth_sigma : float , limits : ArrayLike ): \"\"\"Give the smoothing Gaussian's width as <smooth_sigma> and the [lower,upper] histogram limits as <limits>.\"\"\" self . limits = tuple ( np . asarray ( limits , dtype = float )) self . smooth_sigma = smooth_sigma # Choose a reasonable # of bins, at least 1024 and a power of 2 stepsize = 0.4 * smooth_sigma dlimits = self . limits [ 1 ] - self . limits [ 0 ] nbins_guess = int ( dlimits / stepsize + 0.5 ) min_nbins = 1024 max_nbins = 32768 # 32k bins, 2**15 # Clamp nbins_guess to at least min_nbins clamped_nbins = np . clip ( nbins_guess , min_nbins , max_nbins ) nbins_forced_to_power_of_2 = int ( 2 ** np . ceil ( np . log2 ( clamped_nbins ))) # if nbins_forced_to_power_of_2 == max_nbins: # print(f\"Warning: HistogramSmoother (for drift correct) Limiting histogram bins to {max_nbins} (requested {nbins_guess})\") self . nbins = nbins_forced_to_power_of_2 self . stepsize = dlimits / self . nbins # Compute the Fourier-space smoothing kernel kernel = np . exp ( - 0.5 * ( np . arange ( self . nbins ) * self . stepsize / self . smooth_sigma ) ** 2 ) kernel [ 1 :] += kernel [ - 1 : 0 : - 1 ] # Handle the negative frequencies kernel /= kernel . sum () self . kernel_ft = np . fft . rfft ( kernel ) def __call__ ( self , values : ArrayLike ) -> NDArray : \"\"\"Return a smoothed histogram of the data vector <values>\"\"\" contents , _ = np . histogram ( values , self . nbins , self . limits ) ftc = np . fft . rfft ( contents ) csmooth = np . fft . irfft ( self . kernel_ft * ftc ) csmooth [ csmooth < 0 ] = 0 return csmooth","title":"HistogramSmoother"},{"location":"docstrings/#mass2.core.analysis_algorithms.HistogramSmoother.__call__","text":"Return a smoothed histogram of the data vector Source code in mass2/core/analysis_algorithms.py 215 216 217 218 219 220 221 def __call__ ( self , values : ArrayLike ) -> NDArray : \"\"\"Return a smoothed histogram of the data vector <values>\"\"\" contents , _ = np . histogram ( values , self . nbins , self . limits ) ftc = np . fft . rfft ( contents ) csmooth = np . fft . irfft ( self . kernel_ft * ftc ) csmooth [ csmooth < 0 ] = 0 return csmooth","title":"__call__"},{"location":"docstrings/#mass2.core.analysis_algorithms.HistogramSmoother.__init__","text":"Give the smoothing Gaussian's width as and the [lower,upper] histogram limits as . Source code in mass2/core/analysis_algorithms.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def __init__ ( self , smooth_sigma : float , limits : ArrayLike ): \"\"\"Give the smoothing Gaussian's width as <smooth_sigma> and the [lower,upper] histogram limits as <limits>.\"\"\" self . limits = tuple ( np . asarray ( limits , dtype = float )) self . smooth_sigma = smooth_sigma # Choose a reasonable # of bins, at least 1024 and a power of 2 stepsize = 0.4 * smooth_sigma dlimits = self . limits [ 1 ] - self . limits [ 0 ] nbins_guess = int ( dlimits / stepsize + 0.5 ) min_nbins = 1024 max_nbins = 32768 # 32k bins, 2**15 # Clamp nbins_guess to at least min_nbins clamped_nbins = np . clip ( nbins_guess , min_nbins , max_nbins ) nbins_forced_to_power_of_2 = int ( 2 ** np . ceil ( np . log2 ( clamped_nbins ))) # if nbins_forced_to_power_of_2 == max_nbins: # print(f\"Warning: HistogramSmoother (for drift correct) Limiting histogram bins to {max_nbins} (requested {nbins_guess})\") self . nbins = nbins_forced_to_power_of_2 self . stepsize = dlimits / self . nbins # Compute the Fourier-space smoothing kernel kernel = np . exp ( - 0.5 * ( np . arange ( self . nbins ) * self . stepsize / self . smooth_sigma ) ** 2 ) kernel [ 1 :] += kernel [ - 1 : 0 : - 1 ] # Handle the negative frequencies kernel /= kernel . sum () self . kernel_ft = np . fft . rfft ( kernel )","title":"__init__"},{"location":"docstrings/#mass2.core.analysis_algorithms.compute_max_deriv","text":"Computes the maximum derivative in timeseries . can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in . Args: pulse_data: ignore_leading: spike_reject: (default True) kernel: the linear filter against which the signals will be convolved (CONVOLED, not correlated, so reverse the filter as needed). If None, then the default kernel of [+.2 +.1 0 -.1 -.2] will be used. If \"SG\", then the cubic 5-point Savitzky-Golay filter will be used (see below). Otherwise, kernel needs to be a (short) array which will be converted to a 1xN 2-dimensional np.ndarray. (default None) Returns: An np.ndarray, dimension 1: the value of the maximum derivative (units of per sample). When kernel==\"SG\", then we estimate the derivative by Savitzky-Golay filtering (with 1 point before/3 points after the point in question and fitting polynomial of order 3). Find the right general area by first doing a simple difference. Source code in mass2/core/analysis_algorithms.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def compute_max_deriv ( pulse_data : ArrayLike , ignore_leading : int , spike_reject : bool = True , kernel : ArrayLike | str | None = None ) -> NDArray : \"\"\"Computes the maximum derivative in timeseries <pulse_data>. <pulse_data> can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in <pulse_data>. Args: pulse_data: ignore_leading: spike_reject: (default True) kernel: the linear filter against which the signals will be convolved (CONVOLED, not correlated, so reverse the filter as needed). If None, then the default kernel of [+.2 +.1 0 -.1 -.2] will be used. If \"SG\", then the cubic 5-point Savitzky-Golay filter will be used (see below). Otherwise, kernel needs to be a (short) array which will be converted to a 1xN 2-dimensional np.ndarray. (default None) Returns: An np.ndarray, dimension 1: the value of the maximum derivative (units of <pulse_data units> per sample). When kernel==\"SG\", then we estimate the derivative by Savitzky-Golay filtering (with 1 point before/3 points after the point in question and fitting polynomial of order 3). Find the right general area by first doing a simple difference. \"\"\" # If pulse_data is a 1D array, turn it into 2 pulse_data = np . asarray ( pulse_data ) ndim = len ( pulse_data . shape ) if ndim > 2 or ndim < 1 : raise ValueError ( \"input pulse_data should be a 1d or 2d array.\" ) if ndim == 1 : pulse_data . shape = ( 1 , pulse_data . shape [ 0 ]) pulse_view = pulse_data [:, ignore_leading :] NPulse = pulse_view . shape [ 0 ] NSamp = pulse_view . shape [ 1 ] # The default filter: filter_coef = np . array ([ + 0.2 , + 0.1 , 0 , - 0.1 , - 0.2 ]) if kernel == \"SG\" : # This filter is the Savitzky-Golay filter of n_L=1, n_R=3 and M=3, to use the # language of Numerical Recipes 3rd edition. It amounts to least-squares fitting # of an M=3rd order polynomial to the five points [-1,+3] and # finding the slope of the polynomial at 0. # Note that we reverse the order of coefficients because convolution will re-reverse filter_coef = np . array ([ - 0.45238 , - 0.02381 , 0.28571 , 0.30952 , - 0.11905 ])[:: - 1 ] elif kernel is not None : filter_coef = np . array ( kernel ) . ravel () f0 , f1 , f2 , f3 , f4 = filter_coef max_deriv = np . zeros ( NPulse , dtype = np . float64 ) if spike_reject : for i in range ( NPulse ): pulses = pulse_view [ i ] t0 = f4 * pulses [ 0 ] + f3 * pulses [ 1 ] + f2 * pulses [ 2 ] + f1 * pulses [ 3 ] + f0 * pulses [ 4 ] t1 = f4 * pulses [ 1 ] + f3 * pulses [ 2 ] + f2 * pulses [ 3 ] + f1 * pulses [ 4 ] + f0 * pulses [ 5 ] t2 = f4 * pulses [ 2 ] + f3 * pulses [ 3 ] + f2 * pulses [ 4 ] + f1 * pulses [ 5 ] + f0 * pulses [ 6 ] t_max_deriv = t2 if t2 < t0 else t0 for j in range ( 7 , NSamp ): t3 = f4 * pulses [ j - 4 ] + f3 * pulses [ j - 3 ] + f2 * pulses [ j - 2 ] + f1 * pulses [ j - 1 ] + f0 * pulses [ j ] t4 = t3 if t3 < t1 else t1 t_max_deriv = max ( t4 , t_max_deriv ) t0 , t1 , t2 = t1 , t2 , t3 max_deriv [ i ] = t_max_deriv else : for i in range ( NPulse ): pulses = pulse_view [ i ] t0 = f4 * pulses [ 0 ] + f3 * pulses [ 1 ] + f2 * pulses [ 2 ] + f1 * pulses [ 3 ] + f0 * pulses [ 4 ] t_max_deriv = t0 for j in range ( 5 , NSamp ): t0 = f4 * pulses [ j - 4 ] + f3 * pulses [ j - 3 ] + f2 * pulses [ j - 2 ] + f1 * pulses [ j - 1 ] + f0 * pulses [ j ] t_max_deriv = max ( t0 , t_max_deriv ) max_deriv [ i ] = t_max_deriv return np . asarray ( max_deriv , dtype = np . float32 )","title":"compute_max_deriv"},{"location":"docstrings/#mass2.core.analysis_algorithms.correct_flux_jumps","text":"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected Source code in mass2/core/analysis_algorithms.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 def correct_flux_jumps ( vals : ArrayLike , mask : ArrayLike , flux_quant : float ) -> NDArray : \"\"\"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected \"\"\" return unwrap_n ( vals , flux_quant , mask )","title":"correct_flux_jumps"},{"location":"docstrings/#mass2.core.analysis_algorithms.correct_flux_jumps_original","text":"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected Source code in mass2/core/analysis_algorithms.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def correct_flux_jumps_original ( vals : ArrayLike , mask : ArrayLike , flux_quant : float ) -> NDArray : \"\"\"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected \"\"\" # The naive thing is to simply replace each value with its value mod # the flux quantum. But of the baseline value turns out to fluctuate # about an integer number of flux quanta, this will introduce new # jumps. I don't know the best way to handle this in general. For now, # if there are still jumps after the mod, I add 1/4 of a flux quanta # before modding, then mod, then subtract the 1/4 flux quantum and then # *add* a single flux quantum so that the values never go negative. # # To determine whether there are \"still jumps after the mod\" I look at the # difference between the largest and smallest values for \"good\" pulses. If # you don't exclude \"bad\" pulses, this check can be tricked in cases where # the pretrigger section contains a (sufficiently large) tail. vals = np . asarray ( vals ) mask = np . asarray ( mask ) if ( np . amax ( vals ) - np . amin ( vals )) >= flux_quant : corrected = vals % flux_quant if ( np . amax ( corrected [ mask ]) - np . amin ( corrected [ mask ])) > 0.75 * flux_quant : corrected = ( vals + flux_quant / 4 ) % ( flux_quant ) corrected = corrected - flux_quant / 4 + flux_quant corrected -= corrected [ 0 ] - vals [ 0 ] return corrected else : return vals","title":"correct_flux_jumps_original"},{"location":"docstrings/#mass2.core.analysis_algorithms.drift_correct","text":"Compute a drift correction that minimizes the spectral entropy. Args: indicator: The \"x-axis\", which indicates the size of the correction. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. limit: The upper limit of uncorrected values over which entropy is computed (default None). Generally indicator will be the pretrigger mean of the pulses, but you can experiment with other choices. The entropy will be computed on corrected values only in the range [0, limit], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then in will be compute as 25% larger than the 99%ile point of uncorrected. The model is that the filtered pulse height PH should be scaled by (1 + a*PTM) where a is an arbitrary parameter computed here, and PTM is the difference between each record's pretrigger mean and the median value of all pretrigger means. (Or replace \"pretrigger mean\" with whatever quantity you passed in as .) Source code in mass2/core/analysis_algorithms.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def drift_correct ( indicator : ArrayLike , uncorrected : ArrayLike , limit : float | None = None ) -> tuple [ float , dict ]: \"\"\"Compute a drift correction that minimizes the spectral entropy. Args: indicator: The \"x-axis\", which indicates the size of the correction. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. limit: The upper limit of uncorrected values over which entropy is computed (default None). Generally indicator will be the pretrigger mean of the pulses, but you can experiment with other choices. The entropy will be computed on corrected values only in the range [0, limit], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then in will be compute as 25% larger than the 99%ile point of uncorrected. The model is that the filtered pulse height PH should be scaled by (1 + a*PTM) where a is an arbitrary parameter computed here, and PTM is the difference between each record's pretrigger mean and the median value of all pretrigger means. (Or replace \"pretrigger mean\" with whatever quantity you passed in as <indicator>.) \"\"\" uncorrected = np . asarray ( uncorrected ) indicator = np . array ( indicator ) # make a copy ptm_offset = np . median ( indicator ) indicator -= ptm_offset if limit is None : pct99 = np . percentile ( uncorrected , 99 ) limit = 1.25 * pct99 smoother = HistogramSmoother ( 0.5 , [ 0 , limit ]) assert smoother . nbins < 1e6 , \"will be crazy slow, should not be possible\" def entropy ( param : NDArray , indicator : NDArray , uncorrected : NDArray , smoother : HistogramSmoother ) -> float : \"\"\"Return the entropy of the drift-corrected values\"\"\" corrected = uncorrected * ( 1 + indicator * param ) hsmooth = smoother ( corrected ) w = hsmooth > 0 return - ( np . log ( hsmooth [ w ]) * hsmooth [ w ]) . sum () drift_corr_param = sp . optimize . brent ( entropy , ( indicator , uncorrected , smoother ), brack = [ 0 , 0.001 ]) drift_correct_info = { \"type\" : \"ptmean_gain\" , \"slope\" : drift_corr_param , \"median_pretrig_mean\" : ptm_offset } return drift_corr_param , drift_correct_info","title":"drift_correct"},{"location":"docstrings/#mass2.core.analysis_algorithms.estimateRiseTime","text":"Computes the rise time of timeseries , where the time steps are . can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in . If nPretrig >= 4, then the samples pulse_data[:nPretrig] are averaged to estimate the baseline. Otherwise, the minimum of pulse_data is assumed to be the baseline. Specifically, take the first and last of the rising points in the range of 10% to 90% of the peak value, interpolate a line between the two, and use its slope to find the time to rise from 0 to the peak. Args: pulse_data: An np.ndarray of dimension 1 (a single pulse record) or 2 (an array with each row being a pulse record). timebase: The sampling time. nPretrig: The number of samples that are recorded before the trigger. Returns: An ndarray of dimension 1, giving the rise times. Source code in mass2/core/analysis_algorithms.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @njit def estimateRiseTime ( pulse_data : ArrayLike , timebase : float , nPretrig : int ) -> NDArray : \"\"\"Computes the rise time of timeseries <pulse_data>, where the time steps are <timebase>. <pulse_data> can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in <pulse_data>. If nPretrig >= 4, then the samples pulse_data[:nPretrig] are averaged to estimate the baseline. Otherwise, the minimum of pulse_data is assumed to be the baseline. Specifically, take the first and last of the rising points in the range of 10% to 90% of the peak value, interpolate a line between the two, and use its slope to find the time to rise from 0 to the peak. Args: pulse_data: An np.ndarray of dimension 1 (a single pulse record) or 2 (an array with each row being a pulse record). timebase: The sampling time. nPretrig: The number of samples that are recorded before the trigger. Returns: An ndarray of dimension 1, giving the rise times. \"\"\" MINTHRESH , MAXTHRESH = 0.1 , 0.9 # If pulse_data is a 1D array, turn it into 2 pulse_data = np . asarray ( pulse_data ) ndim = len ( pulse_data . shape ) if ndim > 2 or ndim < 1 : raise ValueError ( \"input pulse_data should be a 1d or 2d array.\" ) if ndim == 1 : pulse_data . shape = ( 1 , pulse_data . shape [ 0 ]) # The following requires a lot of numpy foo to read. Sorry! if nPretrig >= 4 : baseline_value = pulse_data [:, 0 : nPretrig ] . mean ( axis = 1 ) else : baseline_value = pulse_data . min ( axis = 1 ) nPretrig = 0 value_at_peak = pulse_data . max ( axis = 1 ) - baseline_value idx_last_pk = pulse_data . argmax ( axis = 1 ) . max () npulses = pulse_data . shape [ 0 ] try : rising_data = ( pulse_data [:, nPretrig : idx_last_pk + 1 ] - baseline_value [:, np . newaxis ]) / value_at_peak [:, np . newaxis ] # Find the last and first indices at which the data are in (0.1, 0.9] times the # peak value. Then make sure last is at least 1 past first. last_idx = ( rising_data > MAXTHRESH ) . argmax ( axis = 1 ) - 1 first_idx = ( rising_data > MINTHRESH ) . argmax ( axis = 1 ) last_idx [ last_idx < first_idx ] = first_idx [ last_idx < first_idx ] + 1 last_idx [ last_idx == rising_data . shape [ 1 ]] = rising_data . shape [ 1 ] - 1 pulsenum = np . arange ( npulses ) y_diff = np . asarray ( rising_data [ pulsenum , last_idx ] - rising_data [ pulsenum , first_idx ], dtype = float ) y_diff [ y_diff < timebase ] = timebase time_diff = timebase * ( last_idx - first_idx ) rise_time = time_diff / y_diff rise_time [ y_diff <= 0 ] = - 9.9e-6 return rise_time except ValueError : return - 9.9e-6 + np . zeros ( npulses , dtype = float )","title":"estimateRiseTime"},{"location":"docstrings/#mass2.core.analysis_algorithms.filter_signal_lowpass","text":"Tophat lowpass filter using an FFT Args: sig - the signal to be filtered fs - the sampling frequency of the signal fcut - the frequency at which to cutoff the signal Returns: the filtered signal Source code in mass2/core/analysis_algorithms.py 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 @njit def filter_signal_lowpass ( sig : NDArray , fs : float , fcut : float ) -> NDArray : \"\"\"Tophat lowpass filter using an FFT Args: sig - the signal to be filtered fs - the sampling frequency of the signal fcut - the frequency at which to cutoff the signal Returns: the filtered signal \"\"\" N = sig . shape [ 0 ] SIG = np . fft . fft ( sig ) freqs = ( fs / N ) * np . concatenate (( np . arange ( 0 , N / 2 + 1 ), np . arange ( N / 2 - 1 , 0 , - 1 ))) filt = np . zeros_like ( SIG ) filt [ freqs < fcut ] = 1.0 sig_filt = np . fft . ifft ( SIG * filt ) return sig_filt","title":"filter_signal_lowpass"},{"location":"docstrings/#mass2.core.analysis_algorithms.make_smooth_histogram","text":"Convert a vector of arbitrary info a smoothed histogram by histogramming it and smoothing. This is a convenience function using the HistogramSmoother class. Args: values: The vector of data to be histogrammed. smooth_sigma: The smoothing Gaussian's width (FWHM) limit, upper_limit: The histogram limits are [limit,upper_limit] or [0,limit] if upper_limit is None. Returns: The smoothed histogram as an array. Source code in mass2/core/analysis_algorithms.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 @njit def make_smooth_histogram ( values : ArrayLike , smooth_sigma : float , limit : float , upper_limit : float | None = None ) -> NDArray : \"\"\"Convert a vector of arbitrary <values> info a smoothed histogram by histogramming it and smoothing. This is a convenience function using the HistogramSmoother class. Args: values: The vector of data to be histogrammed. smooth_sigma: The smoothing Gaussian's width (FWHM) limit, upper_limit: The histogram limits are [limit,upper_limit] or [0,limit] if upper_limit is None. Returns: The smoothed histogram as an array. \"\"\" if upper_limit is None : limit , upper_limit = 0 , limit return HistogramSmoother ( smooth_sigma , [ limit , upper_limit ])( values )","title":"make_smooth_histogram"},{"location":"docstrings/#mass2.core.analysis_algorithms.nearest_arrivals","text":"Find the external trigger time immediately before and after each pulse timestamp Args: pulse_timestamps - 1d array of pulse timestamps whose nearest neighbors need to be found. external_trigger_timestamps - 1d array of possible nearest neighbors. Returns: (before_times, after_times) before_times is an ndarray of the same size as pulse_timestamps. before_times[i] contains the difference between the closest lesser time contained in external_trigger_timestamps and pulse_timestamps[i] or inf if there was no earlier time in other_times Note that before_times is always a positive number even though the time difference it represents is negative. after_times is an ndarray of the same size as pulse_timestamps. after_times[i] contains the difference between pulse_timestamps[i] and the closest greater time contained in other_times or a inf number if there was no later time in external_trigger_timestamps. Source code in mass2/core/analysis_algorithms.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 @njit def nearest_arrivals ( reference_times : ArrayLike , other_times : ArrayLike ) -> tuple [ NDArray , NDArray ]: \"\"\"Find the external trigger time immediately before and after each pulse timestamp Args: pulse_timestamps - 1d array of pulse timestamps whose nearest neighbors need to be found. external_trigger_timestamps - 1d array of possible nearest neighbors. Returns: (before_times, after_times) before_times is an ndarray of the same size as pulse_timestamps. before_times[i] contains the difference between the closest lesser time contained in external_trigger_timestamps and pulse_timestamps[i] or inf if there was no earlier time in other_times Note that before_times is always a positive number even though the time difference it represents is negative. after_times is an ndarray of the same size as pulse_timestamps. after_times[i] contains the difference between pulse_timestamps[i] and the closest greater time contained in other_times or a inf number if there was no later time in external_trigger_timestamps. \"\"\" other_times = np . asarray ( other_times ) nearest_after_index = np . searchsorted ( other_times , reference_times ) # because both sets of arrival times should be sorted, there are faster algorithms than searchsorted # for example: https://github.com/kwgoodman/bottleneck/issues/47 # we could use one if performance becomes an issue last_index = np . searchsorted ( nearest_after_index , other_times . size , side = \"left\" ) first_index = np . searchsorted ( nearest_after_index , 1 ) nearest_before_index = np . copy ( nearest_after_index ) nearest_before_index [: first_index ] = 1 nearest_before_index -= 1 before_times = reference_times - other_times [ nearest_before_index ] before_times [: first_index ] = np . inf nearest_after_index [ last_index :] = other_times . size - 1 after_times = other_times [ nearest_after_index ] - reference_times after_times [ last_index :] = np . inf return before_times , after_times","title":"nearest_arrivals"},{"location":"docstrings/#mass2.core.analysis_algorithms.time_drift_correct","text":"Compute a time-based drift correction that minimizes the spectral entropy. Args: time: The \"time-axis\". Correction will be a low-order polynomial in this. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. w: the kernel width for the Laplace KDE density estimator sec_per_degree: assign as many as one polynomial degree per this many seconds pulses_per_degree: assign as many as one polynomial degree per this many pulses max_degrees: never use more than this many degrees of Legendre polynomial. n_deg: If not None, use this many degrees, regardless of the values of sec_per_degree, pulses_per_degree, and max_degress. In this case, never downsample. limit: The [lower,upper] limit of uncorrected values over which entropy is computed (default None). The entropy will be computed on corrected values only in the range [limit[0], limit[1]], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then it will be computed as 25%% larger than the 99%%ile point of uncorrected. Possible improvements in the future: * Use Numba to speed up. * Allow the parameters to be function arguments with defaults: photons per degree of freedom, seconds per degree of freedom, and max degrees of freedom. * Figure out how to span the available time with more than one set of legendre polynomials, so that we can have more than 20 d.o.f. eventually, for long runs. Source code in mass2/core/analysis_algorithms.py 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 def time_drift_correct ( # noqa: PLR0914 time : ArrayLike , uncorrected : ArrayLike , w : float , sec_per_degree : float = 2000 , pulses_per_degree : int = 2000 , max_degrees : int = 20 , ndeg : int | None = None , limit : tuple [ float , float ] | None = None , ) -> dict [ str , Any ]: \"\"\"Compute a time-based drift correction that minimizes the spectral entropy. Args: time: The \"time-axis\". Correction will be a low-order polynomial in this. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. w: the kernel width for the Laplace KDE density estimator sec_per_degree: assign as many as one polynomial degree per this many seconds pulses_per_degree: assign as many as one polynomial degree per this many pulses max_degrees: never use more than this many degrees of Legendre polynomial. n_deg: If not None, use this many degrees, regardless of the values of sec_per_degree, pulses_per_degree, and max_degress. In this case, never downsample. limit: The [lower,upper] limit of uncorrected values over which entropy is computed (default None). The entropy will be computed on corrected values only in the range [limit[0], limit[1]], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then it will be computed as 25%% larger than the 99%%ile point of uncorrected. Possible improvements in the future: * Use Numba to speed up. * Allow the parameters to be function arguments with defaults: photons per degree of freedom, seconds per degree of freedom, and max degrees of freedom. * Figure out how to span the available time with more than one set of legendre polynomials, so that we can have more than 20 d.o.f. eventually, for long runs. \"\"\" time = np . asarray ( time ) uncorrected = np . asarray ( uncorrected ) if limit is None : pct99 = np . percentile ( uncorrected , 99 ) limit = ( 0 , 1.25 * pct99 ) use = np . logical_and ( uncorrected > limit [ 0 ], uncorrected < limit [ 1 ]) time = np . asarray ( time [ use ]) uncorrected = np . asarray ( uncorrected [ use ]) tmin , tmax = np . min ( time ), np . max ( time ) def normalize ( t : NDArray ) -> NDArray : \"\"\"Rescale time to the range [-1,1]\"\"\" return ( t - tmin ) / ( tmax - tmin ) * 2 - 1 info = { \"tmin\" : tmin , \"tmax\" : tmax , \"normalize\" : normalize , } dtime = tmax - tmin N = len ( time ) if ndeg is None : ndeg = int ( np . minimum ( dtime / sec_per_degree , N / pulses_per_degree )) ndeg = min ( ndeg , max_degrees ) ndeg = max ( ndeg , 1 ) phot_per_degree = N / float ( ndeg ) if phot_per_degree >= 2 * pulses_per_degree : downsample = int ( phot_per_degree / pulses_per_degree ) time = time [:: downsample ] uncorrected = uncorrected [:: downsample ] N = len ( time ) else : downsample = 1 else : downsample = 1 LOG . info ( \"Using %2d degrees for %6d photons (after %d downsample)\" , ndeg , N , downsample ) LOG . info ( \"That's %6.1f photons per degree, and %6.1f seconds per degree.\" , N / float ( ndeg ), dtime / ndeg ) def model1 ( pi : NDArray , i : int , param : NDArray , basis : NDArray ) -> NDArray : \"The model function, with one parameter pi varied, others fixed.\" pcopy = np . array ( param ) pcopy [ i ] = pi return 1 + np . dot ( basis . T , pcopy ) def cost1 ( pi : NDArray , i : int , param : NDArray , y : NDArray , w : float , basis : NDArray ) -> float : \"The cost function (spectral entropy), with one parameter pi varied, others fixed.\" return laplace_entropy ( y * model1 ( pi , i , param , basis ), w = w ) param = np . zeros ( ndeg , dtype = float ) xnorm = np . asarray ( normalize ( time ), dtype = float ) basis = np . vstack ([ sp . special . legendre ( i + 1 )( xnorm ) for i in range ( ndeg )]) fc = 0 model : Callable = np . poly1d ([ 0 ]) info [ \"coefficients\" ] = np . zeros ( ndeg , dtype = float ) for i in range ( ndeg ): result , _fval , _iter , funcalls = sp . optimize . brent ( cost1 , ( i , param , uncorrected , w , basis ), [ - 0.001 , 0.001 ], tol = 1e-5 , full_output = True ) param [ i ] = result fc += funcalls model += sp . special . legendre ( i + 1 ) * result info [ \"coefficients\" ][ i ] = result info [ \"funccalls\" ] = fc xk = np . linspace ( - 1 , 1 , 1 + 2 * ndeg ) model2 = CubicSpline ( xk , model ( xk )) H1 = laplace_entropy ( uncorrected , w = w ) H2 = laplace_entropy ( uncorrected * ( 1 + model ( xnorm )), w = w ) H3 = laplace_entropy ( uncorrected * ( 1 + model2 ( xnorm )), w = w ) if H2 <= 0 or H2 - H1 > 0.0 : model = np . poly1d ([ 0 ]) elif H3 <= 0 or H3 - H2 > 0.00001 : model = model2 info [ \"entropies\" ] = ( H1 , H2 , H3 ) info [ \"model\" ] = model return info","title":"time_drift_correct"},{"location":"docstrings/#mass2.core.analysis_algorithms.unwrap_n","text":"Unwrap data that has been restricted to a given period. The algorithm iterates through each data point and compares it to the average of the previous n data points. It then offsets the data point by the multiple of the period that will minimize the difference from that n-point running average. For the first n data points, there are not enough preceding points to average n of them, so the algorithm will average fewer points. This code was written by Thomas Baker; integrated into MASS by Dan Becker. Sped up 300x by @njit. Parameters: data ( array of data values ) \u2013 period ( the range over which the data loops ) \u2013 n ( how many preceding points to average , default: 3 ) \u2013 mask ( ArrayLike ) \u2013 Source code in mass2/core/analysis_algorithms.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 @njit def unwrap_n ( data : NDArray [ np . uint16 ], period : float , mask : ArrayLike , n : int = 3 ) -> NDArray : \"\"\"Unwrap data that has been restricted to a given period. The algorithm iterates through each data point and compares it to the average of the previous n data points. It then offsets the data point by the multiple of the period that will minimize the difference from that n-point running average. For the first n data points, there are not enough preceding points to average n of them, so the algorithm will average fewer points. This code was written by Thomas Baker; integrated into MASS by Dan Becker. Sped up 300x by @njit. Parameters ---------- data : array of data values period : the range over which the data loops n : how many preceding points to average mask: mask indentifying \"good\" pulses \"\"\" mask = np . asarray ( mask ) udata = np . copy ( data ) # make a copy for output if n <= 0 : return udata # Iterate through each data point and offset it by # an amount that will minimize the difference from the # rolling average nprior = 0 firstgoodidx = np . argmax ( mask ) priorvalues = np . full ( n , udata [ firstgoodidx ]) for i in range ( len ( data )): # Take the average of the previous n data points (only those with mask[i]==True). # Offset the data point by the most reasonable multiple of period (make this point closest to the running average). if mask [ i ]: avg = np . mean ( priorvalues ) if nprior == 0 : avg = float ( priorvalues [ 0 ]) elif nprior < n : avg = np . mean ( priorvalues [: nprior ]) udata [ i ] -= np . round (( udata [ i ] - avg ) / period ) * period if mask [ i ]: priorvalues [ nprior % n ] = udata [ i ] nprior += 1 return udata Provide DriftCorrectStep and DriftCorrection for correcting gain drifts that correlate with pretrigger mean.","title":"unwrap_n"},{"location":"docstrings/#mass2.core.drift_correction.DriftCorrectStep","text":"Bases: RecipeStep A RecipeStep to apply a linear drift correction to pulse data in a DataFrame. Source code in mass2/core/drift_correction.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @dataclass ( frozen = True ) class DriftCorrectStep ( RecipeStep ): \"\"\"A RecipeStep to apply a linear drift correction to pulse data in a DataFrame.\"\"\" dc : typing . Any def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Apply the drift correction to the input DataFrame and return a new DataFrame with results.\"\"\" indicator_col , uncorrected_col = self . inputs slope , offset = self . dc . slope , self . dc . offset df2 = df . select (( pl . col ( uncorrected_col ) * ( 1 + slope * ( pl . col ( indicator_col ) - offset ))) . alias ( self . output [ 0 ])) . with_columns ( df ) return df2 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the uncorrected and corrected values against the indicator for debugging purposes.\"\"\" indicator_col , uncorrected_col = self . inputs # breakpoint() df_small = df_after . lazy () . filter ( self . good_expr ) . filter ( self . use_expr ) . select ( self . inputs + self . output ) . collect () mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ uncorrected_col ]) mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ self . output [ 0 ]], plt . gca (), ) plt . legend () plt . tight_layout () return plt . gca () @classmethod def learn ( cls , ch : \"Channel\" , indicator_col : str , uncorrected_col : str , corrected_col : str | None , use_expr : pl . Expr ) -> \"DriftCorrectStep\" : \"\"\"Create a DriftCorrectStep by learning the correction from data in the given Channel.\"\"\" if corrected_col is None : corrected_col = uncorrected_col + \"_dc\" indicator_s , uncorrected_s = ch . good_serieses ([ indicator_col , uncorrected_col ], use_expr ) dc = mass2 . core . drift_correct ( indicator = indicator_s . to_numpy (), uncorrected = uncorrected_s . to_numpy (), ) step = cls ( inputs = [ indicator_col , uncorrected_col ], output = [ corrected_col ], good_expr = ch . good_expr , use_expr = use_expr , dc = dc , ) return step","title":"DriftCorrectStep"},{"location":"docstrings/#mass2.core.drift_correction.DriftCorrectStep.calc_from_df","text":"Apply the drift correction to the input DataFrame and return a new DataFrame with results. Source code in mass2/core/drift_correction.py 47 48 49 50 51 52 53 54 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Apply the drift correction to the input DataFrame and return a new DataFrame with results.\"\"\" indicator_col , uncorrected_col = self . inputs slope , offset = self . dc . slope , self . dc . offset df2 = df . select (( pl . col ( uncorrected_col ) * ( 1 + slope * ( pl . col ( indicator_col ) - offset ))) . alias ( self . output [ 0 ])) . with_columns ( df ) return df2","title":"calc_from_df"},{"location":"docstrings/#mass2.core.drift_correction.DriftCorrectStep.dbg_plot","text":"Plot the uncorrected and corrected values against the indicator for debugging purposes. Source code in mass2/core/drift_correction.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the uncorrected and corrected values against the indicator for debugging purposes.\"\"\" indicator_col , uncorrected_col = self . inputs # breakpoint() df_small = df_after . lazy () . filter ( self . good_expr ) . filter ( self . use_expr ) . select ( self . inputs + self . output ) . collect () mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ uncorrected_col ]) mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ self . output [ 0 ]], plt . gca (), ) plt . legend () plt . tight_layout () return plt . gca ()","title":"dbg_plot"},{"location":"docstrings/#mass2.core.drift_correction.DriftCorrectStep.learn","text":"Create a DriftCorrectStep by learning the correction from data in the given Channel. Source code in mass2/core/drift_correction.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @classmethod def learn ( cls , ch : \"Channel\" , indicator_col : str , uncorrected_col : str , corrected_col : str | None , use_expr : pl . Expr ) -> \"DriftCorrectStep\" : \"\"\"Create a DriftCorrectStep by learning the correction from data in the given Channel.\"\"\" if corrected_col is None : corrected_col = uncorrected_col + \"_dc\" indicator_s , uncorrected_s = ch . good_serieses ([ indicator_col , uncorrected_col ], use_expr ) dc = mass2 . core . drift_correct ( indicator = indicator_s . to_numpy (), uncorrected = uncorrected_s . to_numpy (), ) step = cls ( inputs = [ indicator_col , uncorrected_col ], output = [ corrected_col ], good_expr = ch . good_expr , use_expr = use_expr , dc = dc , ) return step","title":"learn"},{"location":"docstrings/#mass2.core.drift_correction.DriftCorrection","text":"A linear correction used to attempt remove any correlation between pretrigger mean and pulse height; will work with other quantities instead. Source code in mass2/core/drift_correction.py 93 94 95 96 97 98 99 100 101 102 103 104 105 @dataclass class DriftCorrection : \"\"\"A linear correction used to attempt remove any correlation between pretrigger mean and pulse height; will work with other quantities instead.\"\"\" offset : float slope : float def __call__ ( self , indicator : ArrayLike , uncorrected : ArrayLike ) -> NDArray : \"\"\"Apply the drift correction to the given uncorrected values using the given indicator values.\"\"\" indicator = np . asarray ( indicator ) uncorrected = np . asarray ( uncorrected ) return uncorrected * ( 1 + ( indicator - self . offset ) * self . slope )","title":"DriftCorrection"},{"location":"docstrings/#mass2.core.drift_correction.DriftCorrection.__call__","text":"Apply the drift correction to the given uncorrected values using the given indicator values. Source code in mass2/core/drift_correction.py 101 102 103 104 105 def __call__ ( self , indicator : ArrayLike , uncorrected : ArrayLike ) -> NDArray : \"\"\"Apply the drift correction to the given uncorrected values using the given indicator values.\"\"\" indicator = np . asarray ( indicator ) uncorrected = np . asarray ( uncorrected ) return uncorrected * ( 1 + ( indicator - self . offset ) * self . slope )","title":"__call__"},{"location":"docstrings/#mass2.core.drift_correction.drift_correct_mass","text":"Determine drift correction parameters using mass2.core.analysis_algorithms.drift_correct. Source code in mass2/core/drift_correction.py 20 21 22 23 24 def drift_correct_mass ( indicator : ArrayLike , uncorrected : ArrayLike ) -> \"DriftCorrection\" : \"\"\"Determine drift correction parameters using mass2.core.analysis_algorithms.drift_correct.\"\"\" slope , dc_info = mass2 . core . analysis_algorithms . drift_correct ( indicator , uncorrected ) offset = dc_info [ \"median_pretrig_mean\" ] return DriftCorrection ( slope = slope , offset = offset )","title":"drift_correct_mass"},{"location":"docstrings/#mass2.core.drift_correction.drift_correct_wip","text":"Work in progress to Determine drift correction parameters directly (??). Source code in mass2/core/drift_correction.py 27 28 29 30 31 32 33 34 35 def drift_correct_wip ( indicator : ArrayLike , uncorrected : ArrayLike ) -> \"DriftCorrection\" : \"\"\"Work in progress to Determine drift correction parameters directly (??).\"\"\" opt_result , offset = mass2 . core . rough_cal . minimize_entropy_linear ( np . asarray ( indicator ), np . asarray ( uncorrected ), bin_edges = np . arange ( 0 , 60000 , 1 ), fwhm_in_bin_number_units = 5 , ) return DriftCorrection ( offset = float ( offset ), slope = opt_result . x . astype ( np . float64 )) Provide OptimalFilterStep , a step to apply an optimal filter to pulse data in a DataFrame.","title":"drift_correct_wip"},{"location":"docstrings/#mass2.core.filter_steps.OptimalFilterStep","text":"Bases: RecipeStep A step to apply an optimal filter to pulse data in a DataFrame. Source code in mass2/core/filter_steps.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @dataclass ( frozen = True ) class OptimalFilterStep ( RecipeStep ): \"\"\"A step to apply an optimal filter to pulse data in a DataFrame.\"\"\" filter : Filter spectrum : NoiseResult | None filter_maker : \"FilterMaker\" transform_raw : Callable | None = None def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Apply the optimal filter to the input DataFrame and return a new DataFrame with results.\"\"\" dfs = [] for df_iter in df . iter_slices ( 10000 ): raw = df_iter [ self . inputs [ 0 ]] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) peak_y , peak_x = self . filter . filter_records ( raw ) dfs . append ( pl . DataFrame ({ \"peak_x\" : peak_x , \"peak_y\" : peak_y })) df2 = pl . concat ( dfs ) . with_columns ( df ) df2 = df2 . rename ({ \"peak_x\" : self . output [ 0 ], \"peak_y\" : self . output [ 1 ]}) return df2 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the filter shape for debugging purposes.\"\"\" self . filter . plot () return plt . gca () def drop_debug ( self ) -> \"OptimalFilterStep\" : \"\"\"Return a copy of this step with debugging information (the NoiseResult) removed.\"\"\" return dataclasses . replace ( self , spectrum = None )","title":"OptimalFilterStep"},{"location":"docstrings/#mass2.core.filter_steps.OptimalFilterStep.calc_from_df","text":"Apply the optimal filter to the input DataFrame and return a new DataFrame with results. Source code in mass2/core/filter_steps.py 25 26 27 28 29 30 31 32 33 34 35 36 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Apply the optimal filter to the input DataFrame and return a new DataFrame with results.\"\"\" dfs = [] for df_iter in df . iter_slices ( 10000 ): raw = df_iter [ self . inputs [ 0 ]] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) peak_y , peak_x = self . filter . filter_records ( raw ) dfs . append ( pl . DataFrame ({ \"peak_x\" : peak_x , \"peak_y\" : peak_y })) df2 = pl . concat ( dfs ) . with_columns ( df ) df2 = df2 . rename ({ \"peak_x\" : self . output [ 0 ], \"peak_y\" : self . output [ 1 ]}) return df2","title":"calc_from_df"},{"location":"docstrings/#mass2.core.filter_steps.OptimalFilterStep.dbg_plot","text":"Plot the filter shape for debugging purposes. Source code in mass2/core/filter_steps.py 38 39 40 41 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the filter shape for debugging purposes.\"\"\" self . filter . plot () return plt . gca ()","title":"dbg_plot"},{"location":"docstrings/#mass2.core.filter_steps.OptimalFilterStep.drop_debug","text":"Return a copy of this step with debugging information (the NoiseResult) removed. Source code in mass2/core/filter_steps.py 43 44 45 def drop_debug ( self ) -> \"OptimalFilterStep\" : \"\"\"Return a copy of this step with debugging information (the NoiseResult) removed.\"\"\" return dataclasses . replace ( self , spectrum = None ) Classes and functions for reading and handling LJH files.","title":"drop_debug"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile","text":"Bases: ABC Represents the header and binary information of a single LJH file. Includes the complete ASCII header stored both as a dictionary and a string, and key attributes including the number of pulses, number of samples (and presamples) in each pulse record, client information stored by the LJH writer, and the filename. Also includes a np.memmap to the raw binary data. This memmap always starts with pulse zero and extends to the last full pulse given the file size at the time of object creation. To extend the memmap for files that are growing, use LJHFile.reopen_binary() to return a new object with a possibly longer memmap. Source code in mass2/core/ljhfiles.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @dataclass ( frozen = True ) class LJHFile ( ABC ): \"\"\"Represents the header and binary information of a single LJH file. Includes the complete ASCII header stored both as a dictionary and a string, and key attributes including the number of pulses, number of samples (and presamples) in each pulse record, client information stored by the LJH writer, and the filename. Also includes a `np.memmap` to the raw binary data. This memmap always starts with pulse zero and extends to the last full pulse given the file size at the time of object creation. To extend the memmap for files that are growing, use `LJHFile.reopen_binary()` to return a new object with a possibly longer memmap. \"\"\" filename : str channum : int dtype : np . dtype npulses : int timebase : float nsamples : int npresamples : int subframediv : int | None client : str header : dict header_string : str header_size : int binary_size : int _mmap : np . memmap ljh_version : Version max_pulses : int | None = None OVERLONG_HEADER : ClassVar [ int ] = 100 def __repr__ ( self ) -> str : \"\"\"A string that can be evaluated to re-open this LJH file.\"\"\" return f \"\"\"mass2.core.ljhfiles.LJHFile.open(\" { self . filename } \")\"\"\" @classmethod def open ( cls , filename : str | Path , max_pulses : int | None = None ) -> \"LJHFile\" : \"\"\"Open an LJH file, parsing its header information and returning an LJHFile object.\"\"\" filename = str ( filename ) header_dict , header_string , header_size = cls . read_header ( filename ) channum = header_dict [ \"Channel\" ] timebase = header_dict [ \"Timebase\" ] nsamples = header_dict [ \"Total Samples\" ] npresamples = header_dict [ \"Presamples\" ] client = header_dict . get ( \"Software Version\" , \"UNKNOWN\" ) subframediv : int | None = None if \"Subframe divisions\" in header_dict : subframediv = header_dict [ \"Subframe divisions\" ] elif \"Number of rows\" in header_dict : subframediv = header_dict [ \"Number of rows\" ] ljh_version = Version ( header_dict [ \"Save File Format Version\" ]) if ljh_version < Version ( \"2.0.0\" ): raise NotImplementedError ( \"LJH files version 1 are not supported\" ) if ljh_version < Version ( \"2.1.0\" ): dtype = np . dtype ([ ( \"internal_unused\" , np . uint16 ), ( \"internal_ms\" , np . uint32 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type : type [ LJHFile ] = LJHFile_2_0 elif ljh_version < Version ( \"2.2.0\" ): dtype = np . dtype ([ ( \"internal_us\" , np . uint8 ), ( \"internal_unused\" , np . uint8 ), ( \"internal_ms\" , np . uint32 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type = LJHFile_2_1 else : dtype = np . dtype ([ ( \"subframecount\" , np . int64 ), ( \"posix_usec\" , np . int64 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type = LJHFile_2_2 pulse_size_bytes = dtype . itemsize binary_size = os . path . getsize ( filename ) - header_size # Fix long-standing bug in LJH files made by MATTER or XCALDAQ_client: # It adds 3 to the \"true value\" of nPresamples. Assume only DASTARD clients have value correct. if \"DASTARD\" not in client : npresamples += 3 npulses = binary_size // pulse_size_bytes if max_pulses is not None : npulses = min ( max_pulses , npulses ) mmap = np . memmap ( filename , dtype , mode = \"r\" , offset = header_size , shape = ( npulses ,)) return concrete_LJHFile_type ( filename , channum , dtype , npulses , timebase , nsamples , npresamples , subframediv , client , header_dict , header_string , header_size , binary_size , mmap , ljh_version , max_pulses , ) @classmethod def read_header ( cls , filename : str ) -> tuple [ dict , str , int ]: \"\"\"Read in the text header of an LJH file. Return the header parsed into a dictionary, the complete header string (in case you want to generate a new LJH file from this one), and the size of the header in bytes. The file does not remain open after this method. Returns: (header_dict, header_string, header_size) Args: filename: path to the file to be opened. \"\"\" # parse header into a dictionary header_dict : dict [ str , Any ] = {} with open ( filename , \"rb\" ) as fp : i = 0 lines = [] while True : line = fp . readline () . decode () lines . append ( line ) i += 1 if line . startswith ( \"#End of Header\" ): break elif not line : raise Exception ( \"reached EOF before #End of Header\" ) elif i > cls . OVERLONG_HEADER : raise Exception ( f \"header is too long--seems not to contain '#End of Header' \\n in file { filename } \" ) # ignore lines without \":\" elif \":\" in line : a , b = line . split ( \":\" , maxsplit = 1 ) a = a . strip () b = b . strip () header_dict [ a ] = b header_size = fp . tell () header_string = \"\" . join ( lines ) # Convert values from header_dict into numeric types, when appropriate header_dict [ \"Filename\" ] = filename for name , datatype in ( ( \"Channel\" , int ), ( \"Timebase\" , float ), ( \"Total Samples\" , int ), ( \"Presamples\" , int ), ( \"Number of columns\" , int ), ( \"Number of rows\" , int ), ( \"Subframe divisions\" , int ), ( \"Timestamp offset (s)\" , float ), ): # Have to convert to float first, as some early LJH have \"Channel: 1.0\" header_dict [ name ] = datatype ( float ( header_dict . get ( name , - 1 ))) return header_dict , header_string , header_size @property def pulse_size_bytes ( self ) -> int : \"\"\"The size in bytes of each binary pulse record (including the timestamps)\"\"\" return self . dtype . itemsize def reopen_binary ( self , max_pulses : int | None = None ) -> \"LJHFile\" : \"\"\"Reopen the underlying binary section of the LJH file, in case its size has changed, without re-reading the LJH header section. Parameters ---------- max_pulses : Optional[int], optional A limit to the number of pulses to memory map or None for no limit, by default None Returns ------- Self A new `LJHFile` object with the same header but a new memmap and number of pulses. \"\"\" current_binary_size = os . path . getsize ( self . filename ) - self . header_size npulses = current_binary_size // self . pulse_size_bytes if max_pulses is not None : npulses = min ( max_pulses , npulses ) mmap = np . memmap ( self . filename , self . dtype , mode = \"r\" , offset = self . header_size , shape = ( npulses ,), ) return dataclasses . replace ( self , npulses = npulses , _mmap = mmap , max_pulses = max_pulses , binary_size = current_binary_size , ) @property def subframecount ( self ) -> NDArray : \"\"\"Return a copy of the subframecount memory map. Old LJH versions don't have this: return zeros, unless overridden by derived class (LJHFile_2_2 will be the only one). Returns ------- np.ndarray An array of subframecount values for each pulse record. \"\"\" return np . zeros ( self . npulses , dtype = np . int64 ) @property @abstractmethod def datatimes_raw ( self ) -> NDArray : \"\"\"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of timestamp values for each pulse record, in microseconds since the epoh (1970). \"\"\" raise NotImplementedError ( \"illegal: this is an abstract base class\" ) @property def datatimes_float ( self ) -> NDArray : \"\"\"Compute pulse record times in floating-point (seconds since the 1970 epoch). In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, compute on chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of pulse record times in floating-point (seconds since the 1970 epoch). \"\"\" return self . datatimes_raw / 1e6 def read_trace ( self , i : int ) -> NDArray : \"\"\"Return a single pulse record from an LJH file. Parameters ---------- i : int Pulse record number (0-indexed) Returns ------- ArrayLike A view into the pulse record. \"\"\" return self . _mmap [ \"data\" ][ i ] def read_trace_with_timing ( self , i : int ) -> tuple [ int , int , NDArray ]: \"\"\"Return a single data trace as (subframecount, posix_usec, pulse_record).\"\"\" pulse_record = self . read_trace ( i ) return ( self . subframecount [ i ], self . datatimes_raw [ i ], pulse_record ) def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Convert this LJH file to two Polars dataframes: one for the binary data, one for the header. Parameters ---------- first_pulse : int, optional The pulse dataframe starts with this pulse record number, by default 0 keep_posix_usec : bool, optional Whether to keep the raw `posix_usec` field in the pulse dataframe, by default False force_continuous: bool Whether to claim that the data stream is actually continuous (because it cannot be learned from data for LJH files before version 2.2.0). Only relevant for noise data files. Returns ------- tuple[pl.DataFrame, pl.DataFrame] (df, header_df) df: the dataframe containing raw pulse information, one row per pulse header_df: a one-row dataframe containing the information from the LJH file header \"\"\" data = { \"pulse\" : self . _mmap [ \"data\" ][ first_pulse :], \"posix_usec\" : self . datatimes_raw [ first_pulse :], \"subframecount\" : self . subframecount [ first_pulse :], } schema : pl . _typing . SchemaDict = { \"pulse\" : pl . Array ( pl . UInt16 , self . nsamples ), \"posix_usec\" : pl . UInt64 , \"subframecount\" : pl . UInt64 , } df = pl . DataFrame ( data , schema = schema ) df = df . select ( pl . from_epoch ( \"posix_usec\" , time_unit = \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) if not keep_posix_usec : df = df . select ( pl . exclude ( \"posix_usec\" )) continuous = self . is_continuous or force_continuous header_df = pl . DataFrame ( self . header ) . with_columns ( continuous = continuous ) return df , header_df def write_truncated_ljh ( self , filename : str , npulses : int ) -> None : \"\"\"Write an LJH copy of this file, with a limited number of pulses. Parameters ---------- filename : str The path where a new LJH file will be created (or replaced). npulses : int Number of pulse records to write \"\"\" npulses = max ( npulses , self . npulses ) with open ( filename , \"wb\" ) as f : f . write ( self . header_string . encode ( \"utf-8\" )) f . write ( self . _mmap [: npulses ] . tobytes ()) @property def is_continuous ( self ) -> bool : \"\"\"Is this LJH file made of a perfectly continuous data stream? For pre-version 2.2 LJH files, we cannot discern from the data. So just claim False. (LJH_2_2 subtype will override by actually computing.) Returns ------- bool Whether every record is strictly continuous with the ones before and after \"\"\" return False","title":"LJHFile"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.datatimes_float","text":"Compute pulse record times in floating-point (seconds since the 1970 epoch). In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, compute on chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of pulse record times in floating-point (seconds since the 1970 epoch).","title":"datatimes_float"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.datatimes_raw","text":"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of timestamp values for each pulse record, in microseconds since the epoh (1970).","title":"datatimes_raw"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.is_continuous","text":"Is this LJH file made of a perfectly continuous data stream? For pre-version 2.2 LJH files, we cannot discern from the data. So just claim False. (LJH_2_2 subtype will override by actually computing.) Returns: bool \u2013 Whether every record is strictly continuous with the ones before and after","title":"is_continuous"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.pulse_size_bytes","text":"The size in bytes of each binary pulse record (including the timestamps)","title":"pulse_size_bytes"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.subframecount","text":"Return a copy of the subframecount memory map. Old LJH versions don't have this: return zeros, unless overridden by derived class (LJHFile_2_2 will be the only one). Returns: ndarray \u2013 An array of subframecount values for each pulse record.","title":"subframecount"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.__repr__","text":"A string that can be evaluated to re-open this LJH file. Source code in mass2/core/ljhfiles.py 50 51 52 def __repr__ ( self ) -> str : \"\"\"A string that can be evaluated to re-open this LJH file.\"\"\" return f \"\"\"mass2.core.ljhfiles.LJHFile.open(\" { self . filename } \")\"\"\"","title":"__repr__"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.open","text":"Open an LJH file, parsing its header information and returning an LJHFile object. Source code in mass2/core/ljhfiles.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @classmethod def open ( cls , filename : str | Path , max_pulses : int | None = None ) -> \"LJHFile\" : \"\"\"Open an LJH file, parsing its header information and returning an LJHFile object.\"\"\" filename = str ( filename ) header_dict , header_string , header_size = cls . read_header ( filename ) channum = header_dict [ \"Channel\" ] timebase = header_dict [ \"Timebase\" ] nsamples = header_dict [ \"Total Samples\" ] npresamples = header_dict [ \"Presamples\" ] client = header_dict . get ( \"Software Version\" , \"UNKNOWN\" ) subframediv : int | None = None if \"Subframe divisions\" in header_dict : subframediv = header_dict [ \"Subframe divisions\" ] elif \"Number of rows\" in header_dict : subframediv = header_dict [ \"Number of rows\" ] ljh_version = Version ( header_dict [ \"Save File Format Version\" ]) if ljh_version < Version ( \"2.0.0\" ): raise NotImplementedError ( \"LJH files version 1 are not supported\" ) if ljh_version < Version ( \"2.1.0\" ): dtype = np . dtype ([ ( \"internal_unused\" , np . uint16 ), ( \"internal_ms\" , np . uint32 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type : type [ LJHFile ] = LJHFile_2_0 elif ljh_version < Version ( \"2.2.0\" ): dtype = np . dtype ([ ( \"internal_us\" , np . uint8 ), ( \"internal_unused\" , np . uint8 ), ( \"internal_ms\" , np . uint32 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type = LJHFile_2_1 else : dtype = np . dtype ([ ( \"subframecount\" , np . int64 ), ( \"posix_usec\" , np . int64 ), ( \"data\" , np . uint16 , nsamples ), ]) concrete_LJHFile_type = LJHFile_2_2 pulse_size_bytes = dtype . itemsize binary_size = os . path . getsize ( filename ) - header_size # Fix long-standing bug in LJH files made by MATTER or XCALDAQ_client: # It adds 3 to the \"true value\" of nPresamples. Assume only DASTARD clients have value correct. if \"DASTARD\" not in client : npresamples += 3 npulses = binary_size // pulse_size_bytes if max_pulses is not None : npulses = min ( max_pulses , npulses ) mmap = np . memmap ( filename , dtype , mode = \"r\" , offset = header_size , shape = ( npulses ,)) return concrete_LJHFile_type ( filename , channum , dtype , npulses , timebase , nsamples , npresamples , subframediv , client , header_dict , header_string , header_size , binary_size , mmap , ljh_version , max_pulses , )","title":"open"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.read_header","text":"Read in the text header of an LJH file. Return the header parsed into a dictionary, the complete header string (in case you want to generate a new LJH file from this one), and the size of the header in bytes. The file does not remain open after this method. Returns: (header_dict, header_string, header_size) Args: filename: path to the file to be opened. Source code in mass2/core/ljhfiles.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 @classmethod def read_header ( cls , filename : str ) -> tuple [ dict , str , int ]: \"\"\"Read in the text header of an LJH file. Return the header parsed into a dictionary, the complete header string (in case you want to generate a new LJH file from this one), and the size of the header in bytes. The file does not remain open after this method. Returns: (header_dict, header_string, header_size) Args: filename: path to the file to be opened. \"\"\" # parse header into a dictionary header_dict : dict [ str , Any ] = {} with open ( filename , \"rb\" ) as fp : i = 0 lines = [] while True : line = fp . readline () . decode () lines . append ( line ) i += 1 if line . startswith ( \"#End of Header\" ): break elif not line : raise Exception ( \"reached EOF before #End of Header\" ) elif i > cls . OVERLONG_HEADER : raise Exception ( f \"header is too long--seems not to contain '#End of Header' \\n in file { filename } \" ) # ignore lines without \":\" elif \":\" in line : a , b = line . split ( \":\" , maxsplit = 1 ) a = a . strip () b = b . strip () header_dict [ a ] = b header_size = fp . tell () header_string = \"\" . join ( lines ) # Convert values from header_dict into numeric types, when appropriate header_dict [ \"Filename\" ] = filename for name , datatype in ( ( \"Channel\" , int ), ( \"Timebase\" , float ), ( \"Total Samples\" , int ), ( \"Presamples\" , int ), ( \"Number of columns\" , int ), ( \"Number of rows\" , int ), ( \"Subframe divisions\" , int ), ( \"Timestamp offset (s)\" , float ), ): # Have to convert to float first, as some early LJH have \"Channel: 1.0\" header_dict [ name ] = datatype ( float ( header_dict . get ( name , - 1 ))) return header_dict , header_string , header_size","title":"read_header"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.read_trace","text":"Return a single pulse record from an LJH file. Parameters: i ( int ) \u2013 Pulse record number (0-indexed) Returns: ArrayLike \u2013 A view into the pulse record. Source code in mass2/core/ljhfiles.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def read_trace ( self , i : int ) -> NDArray : \"\"\"Return a single pulse record from an LJH file. Parameters ---------- i : int Pulse record number (0-indexed) Returns ------- ArrayLike A view into the pulse record. \"\"\" return self . _mmap [ \"data\" ][ i ]","title":"read_trace"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.read_trace_with_timing","text":"Return a single data trace as (subframecount, posix_usec, pulse_record). Source code in mass2/core/ljhfiles.py 276 277 278 279 def read_trace_with_timing ( self , i : int ) -> tuple [ int , int , NDArray ]: \"\"\"Return a single data trace as (subframecount, posix_usec, pulse_record).\"\"\" pulse_record = self . read_trace ( i ) return ( self . subframecount [ i ], self . datatimes_raw [ i ], pulse_record )","title":"read_trace_with_timing"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.reopen_binary","text":"Reopen the underlying binary section of the LJH file, in case its size has changed, without re-reading the LJH header section. Parameters: max_pulses ( Optional [ int ] , default: None ) \u2013 A limit to the number of pulses to memory map or None for no limit, by default None Returns: Self \u2013 A new LJHFile object with the same header but a new memmap and number of pulses. Source code in mass2/core/ljhfiles.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def reopen_binary ( self , max_pulses : int | None = None ) -> \"LJHFile\" : \"\"\"Reopen the underlying binary section of the LJH file, in case its size has changed, without re-reading the LJH header section. Parameters ---------- max_pulses : Optional[int], optional A limit to the number of pulses to memory map or None for no limit, by default None Returns ------- Self A new `LJHFile` object with the same header but a new memmap and number of pulses. \"\"\" current_binary_size = os . path . getsize ( self . filename ) - self . header_size npulses = current_binary_size // self . pulse_size_bytes if max_pulses is not None : npulses = min ( max_pulses , npulses ) mmap = np . memmap ( self . filename , self . dtype , mode = \"r\" , offset = self . header_size , shape = ( npulses ,), ) return dataclasses . replace ( self , npulses = npulses , _mmap = mmap , max_pulses = max_pulses , binary_size = current_binary_size , )","title":"reopen_binary"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.to_polars","text":"Convert this LJH file to two Polars dataframes: one for the binary data, one for the header. Parameters: first_pulse ( int , default: 0 ) \u2013 The pulse dataframe starts with this pulse record number, by default 0 keep_posix_usec ( bool , default: False ) \u2013 Whether to keep the raw posix_usec field in the pulse dataframe, by default False force_continuous ( bool , default: False ) \u2013 Whether to claim that the data stream is actually continuous (because it cannot be learned from data for LJH files before version 2.2.0). Only relevant for noise data files. Returns: tuple [ DataFrame , DataFrame ] \u2013 (df, header_df) df: the dataframe containing raw pulse information, one row per pulse header_df: a one-row dataframe containing the information from the LJH file header Source code in mass2/core/ljhfiles.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Convert this LJH file to two Polars dataframes: one for the binary data, one for the header. Parameters ---------- first_pulse : int, optional The pulse dataframe starts with this pulse record number, by default 0 keep_posix_usec : bool, optional Whether to keep the raw `posix_usec` field in the pulse dataframe, by default False force_continuous: bool Whether to claim that the data stream is actually continuous (because it cannot be learned from data for LJH files before version 2.2.0). Only relevant for noise data files. Returns ------- tuple[pl.DataFrame, pl.DataFrame] (df, header_df) df: the dataframe containing raw pulse information, one row per pulse header_df: a one-row dataframe containing the information from the LJH file header \"\"\" data = { \"pulse\" : self . _mmap [ \"data\" ][ first_pulse :], \"posix_usec\" : self . datatimes_raw [ first_pulse :], \"subframecount\" : self . subframecount [ first_pulse :], } schema : pl . _typing . SchemaDict = { \"pulse\" : pl . Array ( pl . UInt16 , self . nsamples ), \"posix_usec\" : pl . UInt64 , \"subframecount\" : pl . UInt64 , } df = pl . DataFrame ( data , schema = schema ) df = df . select ( pl . from_epoch ( \"posix_usec\" , time_unit = \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) if not keep_posix_usec : df = df . select ( pl . exclude ( \"posix_usec\" )) continuous = self . is_continuous or force_continuous header_df = pl . DataFrame ( self . header ) . with_columns ( continuous = continuous ) return df , header_df","title":"to_polars"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile.write_truncated_ljh","text":"Write an LJH copy of this file, with a limited number of pulses. Parameters: filename ( str ) \u2013 The path where a new LJH file will be created (or replaced). npulses ( int ) \u2013 Number of pulse records to write Source code in mass2/core/ljhfiles.py 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def write_truncated_ljh ( self , filename : str , npulses : int ) -> None : \"\"\"Write an LJH copy of this file, with a limited number of pulses. Parameters ---------- filename : str The path where a new LJH file will be created (or replaced). npulses : int Number of pulse records to write \"\"\" npulses = max ( npulses , self . npulses ) with open ( filename , \"wb\" ) as f : f . write ( self . header_string . encode ( \"utf-8\" )) f . write ( self . _mmap [: npulses ] . tobytes ())","title":"write_truncated_ljh"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile_2_0","text":"Bases: LJHFile LJH files version 2.0, which have internal_ms fields only, but no subframecount and no \u00b5s information. Source code in mass2/core/ljhfiles.py 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 class LJHFile_2_0 ( LJHFile ): \"\"\"LJH files version 2.0, which have internal_ms fields only, but no subframecount and no \u00b5s information.\"\"\" @property def datatimes_raw ( self ) -> NDArray : \"\"\"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of timestamp values for each pulse record, in microseconds since the epoh (1970). \"\"\" usec = np . zeros ( self . npulses , dtype = np . int64 ) mmap = self . _mmap [ \"internal_ms\" ] scale = 1000 offset = round ( self . header [ \"Timestamp offset (s)\" ] * 1e6 ) MAXSEGMENT = 4096 first = 0 while first < self . npulses : last = min ( first + MAXSEGMENT , self . npulses ) usec [ first : last ] = mmap [ first : last ] first = last usec = usec * scale + offset return usec def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Generate two Polars dataframes from this LJH file: one for the binary data, one for the header.\"\"\" df , df_header = super () . to_polars ( first_pulse , keep_posix_usec , force_continuous = force_continuous ) return df . select ( pl . exclude ( \"subframecount\" )), df_header","title":"LJHFile_2_0"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile_2_0.datatimes_raw","text":"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of timestamp values for each pulse record, in microseconds since the epoh (1970).","title":"datatimes_raw"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile_2_0.to_polars","text":"Generate two Polars dataframes from this LJH file: one for the binary data, one for the header. Source code in mass2/core/ljhfiles.py 500 501 502 503 504 505 def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Generate two Polars dataframes from this LJH file: one for the binary data, one for the header.\"\"\" df , df_header = super () . to_polars ( first_pulse , keep_posix_usec , force_continuous = force_continuous ) return df . select ( pl . exclude ( \"subframecount\" )), df_header","title":"to_polars"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile_2_1","text":"Bases: LJHFile LJH files version 2.1, which have internal_us and internal_ms fields, but no subframecount. Source code in mass2/core/ljhfiles.py 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 class LJHFile_2_1 ( LJHFile ): \"\"\"LJH files version 2.1, which have internal_us and internal_ms fields, but no subframecount.\"\"\" @property def datatimes_raw ( self ) -> NDArray : \"\"\"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of timestamp values for each pulse record, in microseconds since the epoh (1970). \"\"\" usec = np . zeros ( self . npulses , dtype = np . int64 ) mmap = self . _mmap [ \"internal_ms\" ] scale = 1000 offset = round ( self . header [ \"Timestamp offset (s)\" ] * 1e6 ) MAXSEGMENT = 4096 first = 0 while first < self . npulses : last = min ( first + MAXSEGMENT , self . npulses ) usec [ first : last ] = mmap [ first : last ] first = last usec = usec * scale + offset # Add the 4 \u00b5s units found in LJH version 2.1 assert self . dtype . names is not None and \"internal_us\" in self . dtype . names first = 0 mmap = self . _mmap [ \"internal_us\" ] while first < self . npulses : last = min ( first + MAXSEGMENT , self . npulses ) usec [ first : last ] += mmap [ first : last ] * 4 first = last return usec def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Generate two Polars dataframes from this LJH file: one for the binary data, one for the header.\"\"\" df , df_header = super () . to_polars ( first_pulse , keep_posix_usec , force_continuous = force_continuous ) return df . select ( pl . exclude ( \"subframecount\" )), df_header","title":"LJHFile_2_1"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile_2_1.datatimes_raw","text":"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of timestamp values for each pulse record, in microseconds since the epoh (1970).","title":"datatimes_raw"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile_2_1.to_polars","text":"Generate two Polars dataframes from this LJH file: one for the binary data, one for the header. Source code in mass2/core/ljhfiles.py 461 462 463 464 465 466 def to_polars ( self , first_pulse : int = 0 , keep_posix_usec : bool = False , force_continuous : bool = False ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Generate two Polars dataframes from this LJH file: one for the binary data, one for the header.\"\"\" df , df_header = super () . to_polars ( first_pulse , keep_posix_usec , force_continuous = force_continuous ) return df . select ( pl . exclude ( \"subframecount\" )), df_header","title":"to_polars"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile_2_2","text":"Bases: LJHFile LJH files version 2.2 and later, which have subframecount and posix_usec fields. Source code in mass2/core/ljhfiles.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 class LJHFile_2_2 ( LJHFile ): \"\"\"LJH files version 2.2 and later, which have subframecount and posix_usec fields.\"\"\" @property def subframecount ( self ) -> NDArray : \"\"\"Return a copy of the subframecount memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of subframecount values for each pulse record. \"\"\" subframecount = np . zeros ( self . npulses , dtype = np . int64 ) mmap = self . _mmap [ \"subframecount\" ] MAXSEGMENT = 4096 first = 0 while first < self . npulses : last = min ( first + MAXSEGMENT , self . npulses ) subframecount [ first : last ] = mmap [ first : last ] first = last return subframecount @property def datatimes_raw ( self ) -> NDArray : \"\"\"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than `MAXSEGMENT` records at once. Returns ------- np.ndarray An array of timestamp values for each pulse record, in microseconds since the epoh (1970). \"\"\" usec = np . zeros ( self . npulses , dtype = np . int64 ) assert self . dtype . names is not None and \"posix_usec\" in self . dtype . names mmap = self . _mmap [ \"posix_usec\" ] MAXSEGMENT = 4096 first = 0 while first < self . npulses : last = min ( first + MAXSEGMENT , self . npulses ) usec [ first : last ] = mmap [ first : last ] first = last return usec @property def is_continuous ( self ) -> bool : \"\"\"Is this LJH file made of a perfectly continuous data stream? We generally do take noise data in this mode, and it's useful to analyze the noise data by gluing many records together. This property says whether such gluing is valid. Returns ------- bool Whether every record is strictly continuous with the ones before and after \"\"\" if self . subframediv is None or self . npulses <= 1 : return False expected_subframe_diff = self . nsamples * self . subframediv subframe = self . _mmap [ \"subframecount\" ] return np . max ( np . diff ( subframe )) <= expected_subframe_diff","title":"LJHFile_2_2"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile_2_2.datatimes_raw","text":"Return a copy of the raw timestamp (posix usec) memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of timestamp values for each pulse record, in microseconds since the epoh (1970).","title":"datatimes_raw"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile_2_2.is_continuous","text":"Is this LJH file made of a perfectly continuous data stream? We generally do take noise data in this mode, and it's useful to analyze the noise data by gluing many records together. This property says whether such gluing is valid. Returns: bool \u2013 Whether every record is strictly continuous with the ones before and after","title":"is_continuous"},{"location":"docstrings/#mass2.core.ljhfiles.LJHFile_2_2.subframecount","text":"Return a copy of the subframecount memory map. In mass issue #337, we found that computing on the entire memory map at once was prohibitively expensive for large files. To prevent problems, copy chunks of no more than MAXSEGMENT records at once. Returns: ndarray \u2013 An array of subframecount values for each pulse record. Utility functions for handling and finding LJH files, and opening them as Channel or Channels objects.","title":"subframecount"},{"location":"docstrings/#mass2.core.ljhutil.experiment_state_path_from_ljh_path","text":"Find the experiment_state.txt file in the directory of the given ljh file. Source code in mass2/core/ljhutil.py 131 132 133 134 135 136 137 138 def experiment_state_path_from_ljh_path ( ljh_path : str | pathlib . Path , ) -> pathlib . Path : \"\"\"Find the experiment_state.txt file in the directory of the given ljh file.\"\"\" ljh_path = pathlib . Path ( ljh_path ) # Convert to Path if it's a string base_name = ljh_path . name . split ( \"_chan\" )[ 0 ] new_file_name = f \" { base_name } _experiment_state.txt\" return ljh_path . parent / new_file_name","title":"experiment_state_path_from_ljh_path"},{"location":"docstrings/#mass2.core.ljhutil.external_trigger_bin_path_from_ljh_path","text":"Find the external_trigger.bin file in the directory of the given ljh file. Source code in mass2/core/ljhutil.py 141 142 143 144 145 146 147 148 def external_trigger_bin_path_from_ljh_path ( ljh_path : str | pathlib . Path , ) -> pathlib . Path : \"\"\"Find the external_trigger.bin file in the directory of the given ljh file.\"\"\" ljh_path = pathlib . Path ( ljh_path ) # Convert to Path if it's a string base_name = ljh_path . name . split ( \"_chan\" )[ 0 ] new_file_name = f \" { base_name } _external_trigger.bin\" return ljh_path . parent / new_file_name","title":"external_trigger_bin_path_from_ljh_path"},{"location":"docstrings/#mass2.core.ljhutil.extract_channel_number","text":"Extracts the channel number from the .ljh file name. Args: - file_path (str): The path to the .ljh file. Returns: - int: The channel number. Source code in mass2/core/ljhutil.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def extract_channel_number ( file_path : str ) -> int : \"\"\" Extracts the channel number from the .ljh file name. Args: - file_path (str): The path to the .ljh file. Returns: - int: The channel number. \"\"\" match = re . search ( r \"_chan(\\d+)\\..*$\" , file_path ) if match : return int ( match . group ( 1 )) else : raise ValueError ( f \"File path does not match expected pattern: { file_path } \" )","title":"extract_channel_number"},{"location":"docstrings/#mass2.core.ljhutil.filename_glob_expand","text":"Return the result of glob-expansion on the input pattern. :param pattern: Aglob pattern and return the glob-result as a list. :type pattern: str :return: filenames; the result is sorted first by str.sort, then by ljh_sort_filenames_numerically() :rtype: list Source code in mass2/core/ljhutil.py 179 180 181 182 183 184 185 186 187 188 def filename_glob_expand ( pattern : str ) -> list [ str ]: \"\"\"Return the result of glob-expansion on the input pattern. :param pattern: Aglob pattern and return the glob-result as a list. :type pattern: str :return: filenames; the result is sorted first by str.sort, then by ljh_sort_filenames_numerically() :rtype: list \"\"\" result = glob . glob ( pattern ) return ljh_sort_filenames_numerically ( result )","title":"filename_glob_expand"},{"location":"docstrings/#mass2.core.ljhutil.find_folders_with_extension","text":"Finds all folders within the root_path that contain at least one file with the given extension. Args: - root_path (str): The root directory to start the search from. - extension (str): The file extension to search for (e.g., '.txt'). Returns: - list[str]: A list of paths to directories containing at least one file with the given extension. Source code in mass2/core/ljhutil.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def find_folders_with_extension ( root_path : str , extensions : list [ str ]) -> list [ str ]: \"\"\" Finds all folders within the root_path that contain at least one file with the given extension. Args: - root_path (str): The root directory to start the search from. - extension (str): The file extension to search for (e.g., '.txt'). Returns: - list[str]: A list of paths to directories containing at least one file with the given extension. \"\"\" matching_folders = set () # Walk through the directory tree for dirpath , _ , filenames in os . walk ( root_path ): # Check if any file in the current directory has the given extension for filename in filenames : for extension in extensions : if filename . endswith ( extension ): matching_folders . add ( dirpath ) break # No need to check further, move to the next directory return list ( matching_folders )","title":"find_folders_with_extension"},{"location":"docstrings/#mass2.core.ljhutil.find_ljh_files","text":"Finds all .ljh files in the given folder and its subfolders. Args: - folder (str): The root directory to start the search from. Returns: - list[str]: A list of paths to .ljh files. Source code in mass2/core/ljhutil.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def find_ljh_files ( folder : str , ext : str = \".ljh\" , search_subdirectories : bool = False , exclude_ch_nums : list [ int ] = []) -> list [ str ]: \"\"\" Finds all .ljh files in the given folder and its subfolders. Args: - folder (str): The root directory to start the search from. Returns: - list[str]: A list of paths to .ljh files. \"\"\" ljh_files = [] if search_subdirectories : pathgen = os . walk ( folder ) else : pathgen = zip ([ folder ], [[ \"\" ]], [ os . listdir ( folder )]) for dirpath , _ , filenames in pathgen : for filename in filenames : if filename . endswith ( ext ): if extract_channel_number ( filename ) in exclude_ch_nums : continue ljh_files . append ( os . path . join ( dirpath , filename )) return ljh_files","title":"find_ljh_files"},{"location":"docstrings/#mass2.core.ljhutil.helper_write_pulse","text":"Write a single pulse from one LJHFile to another open file. Source code in mass2/core/ljhutil.py 191 192 193 194 195 196 197 198 def helper_write_pulse ( dest : BinaryIO , src : LJHFile , i : int ) -> None : \"\"\"Write a single pulse from one LJHFile to another open file.\"\"\" subframecount , timestamp_usec , trace = src . read_trace_with_timing ( i ) prefix = struct . pack ( \"<Q\" , int ( subframecount )) dest . write ( prefix ) prefix = struct . pack ( \"<Q\" , int ( timestamp_usec )) dest . write ( prefix ) trace . tofile ( dest , sep = \"\" )","title":"helper_write_pulse"},{"location":"docstrings/#mass2.core.ljhutil.ljh_append_traces","text":"Append traces from one LJH file onto another. The destination file is assumed to be version 2.2.0. Can be used to grab specific traces from some other ljh file, and append them onto an existing ljh file. Args: src_name: the name of the source file dest_name: the name of the destination file pulses: indices of the pulses to copy (default: None, meaning copy all) Source code in mass2/core/ljhutil.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def ljh_append_traces ( src_name : str , dest_name : str , pulses : range | None = None ) -> None : \"\"\"Append traces from one LJH file onto another. The destination file is assumed to be version 2.2.0. Can be used to grab specific traces from some other ljh file, and append them onto an existing ljh file. Args: src_name: the name of the source file dest_name: the name of the destination file pulses: indices of the pulses to copy (default: None, meaning copy all) \"\"\" src = LJHFile . open ( src_name ) if pulses is None : pulses = range ( src . npulses ) with open ( dest_name , \"ab\" ) as dest_fp : for i in pulses : helper_write_pulse ( dest_fp , src , i )","title":"ljh_append_traces"},{"location":"docstrings/#mass2.core.ljhutil.ljh_merge","text":"Merge a set of LJH files to a single output file. Source code in mass2/core/ljhutil.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def ljh_merge ( out_path : str , filenames : list [ str ], overwrite : bool ) -> None : \"\"\"Merge a set of LJH files to a single output file.\"\"\" if not overwrite and os . path . isfile ( out_path ): raise OSError ( f \"To overwrite destination { out_path } , use the --force flag\" ) shutil . copy ( filenames [ 0 ], out_path ) f = LJHFile . open ( out_path ) channum = f . channum print ( f \"Combining { len ( filenames ) } LJH files from channel { channum } \" ) print ( f \"<-- { filenames [ 0 ] } \" ) for in_fname in filenames [ 1 :]: f = LJHFile . open ( in_fname ) if f . channum != channum : raise RuntimeError ( f \"file ' { in_fname } ' channel= { f . channum } , but want { channum } \" ) print ( f \"<-- { in_fname } \" ) ljh_append_traces ( in_fname , out_path ) size = os . stat ( out_path ) . st_size print ( f \"--> { out_path } size: { size } bytes. \\n \" )","title":"ljh_merge"},{"location":"docstrings/#mass2.core.ljhutil.ljh_sort_filenames_numerically","text":"Sort filenames of the form ' _chanXXX. ', according to the numerical value of channel number XXX. Filenames are first sorted by the usual string comparisons, then by channel number. In this way, the standard sort is applied to all files with the same channel number. :param fnames: A sequence of filenames of the form ' _chan .*' :type fnames: list of str :param inclusion_list: If not None, a container with channel numbers. All files whose channel numbers are not on this list will be omitted from the output, defaults to None :type inclusion_list: sequence of int, optional :return: A list containg the same filenames, sorted according to the numerical value of channel number. :rtype: list Source code in mass2/core/ljhutil.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def ljh_sort_filenames_numerically ( fnames : list [ str ], inclusion_list : list [ int ] | None = None ) -> list [ str ]: \"\"\"Sort filenames of the form '*_chanXXX.*', according to the numerical value of channel number XXX. Filenames are first sorted by the usual string comparisons, then by channel number. In this way, the standard sort is applied to all files with the same channel number. :param fnames: A sequence of filenames of the form '*_chan*.*' :type fnames: list of str :param inclusion_list: If not None, a container with channel numbers. All files whose channel numbers are not on this list will be omitted from the output, defaults to None :type inclusion_list: sequence of int, optional :return: A list containg the same filenames, sorted according to the numerical value of channel number. :rtype: list \"\"\" if fnames is None or len ( fnames ) == 0 : return [] if inclusion_list is not None : fnames = list ( filter ( lambda n : extract_channel_number ( n ) in inclusion_list , fnames )) # Sort the results first by raw filename, then sort numerically by LJH channel number. # Because string sort and the builtin `sorted` are both stable, we ensure that the first # sort is used to break ties in channel number. fnames . sort () return sorted ( fnames , key = extract_channel_number )","title":"ljh_sort_filenames_numerically"},{"location":"docstrings/#mass2.core.ljhutil.ljh_truncate","text":"Truncate an LJH file. Writes a new copy of an LJH file, with the same header but fewer raw data pulses. Arguments: input_filename -- name of file to truncate output_filename -- filename for truncated file n_pulses -- truncate to include only this many pulses (default None) timestamp -- truncate to include only pulses with timestamp earlier than this number (default None) Exactly one of n_pulses and timestamp must be specified. Source code in mass2/core/ljhutil.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def ljh_truncate ( input_filename : str , output_filename : str , n_pulses : int | None = None , timestamp : float | None = None ) -> None : \"\"\"Truncate an LJH file. Writes a new copy of an LJH file, with the same header but fewer raw data pulses. Arguments: input_filename -- name of file to truncate output_filename -- filename for truncated file n_pulses -- truncate to include only this many pulses (default None) timestamp -- truncate to include only pulses with timestamp earlier than this number (default None) Exactly one of n_pulses and timestamp must be specified. \"\"\" if ( n_pulses is None and timestamp is None ) or ( n_pulses is not None and timestamp is not None ): msg = \"Must specify exactly one of n_pulses, timestamp.\" msg += f \" Values were { str ( n_pulses ) } , { str ( timestamp ) } \" raise Exception ( msg ) # Check for file problems, then open the input and output LJH files. if os . path . exists ( output_filename ): if os . path . samefile ( input_filename , output_filename ): msg = f \"Input ' { input_filename } ' and output ' { output_filename } ' are the same file, which is not allowed\" raise ValueError ( msg ) infile = LJHFile . open ( input_filename ) if infile . ljh_version < Version ( \"2.2.0\" ): raise Exception ( f \"Don't know how to truncate this LJH version [ { infile . ljh_version } ]\" ) with open ( output_filename , \"wb\" ) as outfile : # write the header as a single string. for k , v in infile . header . items (): outfile . write ( bytes ( f \" { k } : { v } \\n \" , encoding = \"utf-8\" )) outfile . write ( b \"#End of Header \\n \" ) # Write pulses. if n_pulses is None : n_pulses = infile . npulses for i in range ( n_pulses ): if timestamp is not None and infile . datatimes_float [ i ] > timestamp : break prefix = struct . pack ( \"<Q\" , np . uint64 ( infile . subframecount [ i ])) outfile . write ( prefix ) prefix = struct . pack ( \"<Q\" , np . uint64 ( infile . datatimes_raw [ i ])) outfile . write ( prefix ) trace = infile . read_trace ( i ) trace . tofile ( outfile , sep = \"\" )","title":"ljh_truncate"},{"location":"docstrings/#mass2.core.ljhutil.main_ljh_merge","text":"Merge all LJH files that match a pattern to a single output file. The idea is that all such files come from a single TES and could have been (but were not) written as a single continuous file. The pattern should be of the form \"blah_blah_*_chan1.ljh\" or something. The output will then be \"merged_chan1.ljh\" in the directory of the first file found (or alter the directory with the --outdir argument). It is not (currently) possible to merge data from LJH files that represent channels with different numbers. Source code in mass2/core/ljhutil.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def main_ljh_merge () -> None : \"\"\" Merge all LJH files that match a pattern to a single output file. The idea is that all such files come from a single TES and could have been (but were not) written as a single continuous file. The pattern should be of the form \"blah_blah_*_chan1.ljh\" or something. The output will then be \"merged_chan1.ljh\" in the directory of the first file found (or alter the directory with the --outdir argument). It is not (currently) possible to merge data from LJH files that represent channels with different numbers. \"\"\" parser = argparse . ArgumentParser ( description = \"Merge a set of LJH files\" , epilog = \"Beware! Python glob does not perform brace-expansion, so braces must be expanded by the shell.\" , ) parser . add_argument ( \"patterns\" , type = str , nargs = \"+\" , help = 'glob pattern of files to process, e.g. \"20171116_*_chan1.ljh\" (suggest double quotes)' ) parser . add_argument ( \"-d\" , \"--outdir\" , type = str , default = \"\" , help = \"directory to place output file (default: same as directory of first file to be merged\" , ) # TODO: add way to control the output _filename_ parser . add_argument ( \"-F\" , \"--force\" , action = \"store_true\" , help = \"force overwrite of existing target? (default: False)\" ) parser . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" , help = \"list files found before merging (default: False)\" ) parser . add_argument ( \"-n\" , \"--dry-run\" , action = \"store_true\" , help = \"list files found, then quit without merging (default: False)\" ) args = parser . parse_args () filenames : list [ str ] = [] for pattern in args . patterns : filenames . extend ( filename_glob_expand ( pattern )) assert len ( filenames ) > 0 if args . verbose or args . dry_run : print ( f \"Will expand the following { len ( filenames ) } files:\" ) for f in filenames : print ( \" - \" , f ) if args . dry_run : return ljh = LJHFile . open ( filenames [ 0 ]) channum = ljh . channum out_dir = args . outdir if not out_dir : out_dir = os . path . split ( filenames [ 0 ])[ 0 ] out_path = os . path . join ( out_dir , f \"merged_chan { channum } .ljh\" ) overwrite : bool = args . force ljh_merge ( out_path , filenames , overwrite = overwrite )","title":"main_ljh_merge"},{"location":"docstrings/#mass2.core.ljhutil.main_ljh_truncate","text":"A convenience script to truncate all LJH files that match a pattern, writing a new LJH file for each that contains only the first N pulse records. Source code in mass2/core/ljhutil.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def main_ljh_truncate () -> None : \"\"\" A convenience script to truncate all LJH files that match a pattern, writing a new LJH file for each that contains only the first N pulse records. \"\"\" parser = argparse . ArgumentParser ( description = \"Truncate a set of LJH files\" ) parser . add_argument ( \"pattern\" , type = str , help = \"basename of files to process, e.g. 20171116_152922\" ) parser . add_argument ( \"out\" , type = str , help = \"string to append to basename when creating output filename\" ) group = parser . add_mutually_exclusive_group ( required = True ) group . add_argument ( \"--npulses\" , type = int , help = \"Number of pulses to keep\" ) group . add_argument ( \"--timestamp\" , type = float , help = \"Keep only pulses before this timestamp\" ) args = parser . parse_args () pattern = f \" { args . pattern } _chan*.ljh\" filenames = filename_glob_expand ( pattern ) for in_fname in filenames : matches = re . search ( r \"chan(\\d+)\\.ljh\" , in_fname ) if matches : ch = matches . groups ()[ 0 ] out_fname = f \" { args . pattern } _ { args . out } _chan { ch } .ljh\" ljh_truncate ( in_fname , out_fname , n_pulses = args . npulses , timestamp = args . timestamp )","title":"main_ljh_truncate"},{"location":"docstrings/#mass2.core.ljhutil.match_files_by_channel","text":"Matches .ljh files from two folders by channel number. Args: - folder1 (str): The first root directory. - folder2 (str): The second root directory. Returns: - list[Iterator[tuple[str, str]]]: A list of iterators, each containing pairs of paths with matching channel numbers. Source code in mass2/core/ljhutil.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def match_files_by_channel ( folder1 : str , folder2 : str , limit : int | None = None , exclude_ch_nums : list [ int ] = [] ) -> list [ tuple [ str , str ]]: \"\"\" Matches .ljh files from two folders by channel number. Args: - folder1 (str): The first root directory. - folder2 (str): The second root directory. Returns: - list[Iterator[tuple[str, str]]]: A list of iterators, each containing pairs of paths with matching channel numbers. \"\"\" files1 = find_ljh_files ( folder1 ) files2 = find_ljh_files ( folder2 ) # print(f\"in folder {folder1} found {len(files1)} files\") # print(f\"in folder {folder2} found {len(files2)} files\") def collect_to_dict_error_on_repeat_channel ( files : list [ str ]) -> dict : \"\"\" Collects files into a dictionary by channel number, raising an error if a channel number is repeated. \"\"\" files_by_channel : dict [ int , str ] = {} for file in files : channel = extract_channel_number ( file ) if channel in files_by_channel . keys (): existing_file = files_by_channel [ channel ] raise ValueError ( f \"Duplicate channel number found: { channel } in file { file } and already in { existing_file } \" ) files_by_channel [ channel ] = file return files_by_channel # we could have repeat channels even in the same folder, so we should error on that files1_by_channel = collect_to_dict_error_on_repeat_channel ( files1 ) files2_by_channel = collect_to_dict_error_on_repeat_channel ( files2 ) matching_pairs = [] for channel in sorted ( files1_by_channel . keys ()): if channel in files2_by_channel . keys () and channel not in exclude_ch_nums : matching_pairs . append (( files1_by_channel [ channel ], files2_by_channel [ channel ])) if limit is not None : matching_pairs = matching_pairs [: limit ] return matching_pairs Miscellaneous utility functions used in mass2 for plotting, pickling, statistics, and DataFrame manipulation.","title":"match_files_by_channel"},{"location":"docstrings/#mass2.core.misc.alwaysTrue","text":"alwaysTrue: a factory function to generate a new copy of polars literal True for class construction Returns: Expr \u2013 Literal True Source code in mass2/core/misc.py 209 210 211 212 213 214 215 216 217 def alwaysTrue () -> pl . Expr : \"\"\"alwaysTrue: a factory function to generate a new copy of polars literal True for class construction Returns ------- pl.Expr Literal True \"\"\" return pl . lit ( True )","title":"alwaysTrue"},{"location":"docstrings/#mass2.core.misc.concat_dfs_with_concat_state","text":"Concatenate two DataFrames vertically, adding a column concat_state (or named according to concat_state_col ) to indicate which DataFrame each row came from. Source code in mass2/core/misc.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def concat_dfs_with_concat_state ( df1 : pl . DataFrame , df2 : pl . DataFrame , concat_state_col : str = \"concat_state\" ) -> pl . DataFrame : \"\"\"Concatenate two DataFrames vertically, adding a column `concat_state` (or named according to `concat_state_col`) to indicate which DataFrame each row came from.\"\"\" if concat_state_col in df1 . columns : # Continue incrementing from the last known concat_state max_state = df1 [ concat_state_col ][ - 1 ] df2 = df2 . with_columns ( pl . lit ( max_state + 1 ) . alias ( concat_state_col )) else : # Fresh concat: label first as 0, second as 1 df1 = df1 . with_columns ( pl . lit ( 0 ) . alias ( concat_state_col )) df2 = df2 . with_columns ( pl . lit ( 1 ) . alias ( concat_state_col )) df_out = pl . concat ([ df1 , df2 ], how = \"vertical\" ) return df_out","title":"concat_dfs_with_concat_state"},{"location":"docstrings/#mass2.core.misc.extract_column_names_from_polars_expr","text":"Recursively extract all column names from a Polars expression. Source code in mass2/core/misc.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def extract_column_names_from_polars_expr ( expr : pl . Expr ) -> list [ str ]: \"\"\"Recursively extract all column names from a Polars expression.\"\"\" names = set () if hasattr ( expr , \"meta\" ): meta = expr . meta if hasattr ( meta , \"root_names\" ): # For polars >=0.19.0 names . update ( meta . root_names ()) elif hasattr ( meta , \"output_name\" ): # For older polars names . add ( meta . output_name ()) if hasattr ( meta , \"inputs\" ): for subexpr in meta . inputs (): names . update ( extract_column_names_from_polars_expr ( subexpr )) return list ( names )","title":"extract_column_names_from_polars_expr"},{"location":"docstrings/#mass2.core.misc.good_series","text":"Return a Series from the given DataFrame column, filtered by the given good_expr and use_expr. Source code in mass2/core/misc.py 49 50 51 52 53 54 55 56 def good_series ( df : pl . DataFrame , col : str , good_expr : pl . Expr , use_expr : bool | pl . Expr ) -> pl . Series : \"\"\"Return a Series from the given DataFrame column, filtered by the given good_expr and use_expr.\"\"\" # This uses lazy before filtering. We hope this will allow polars to only access the data needed to filter # and the data needed to output what we want. good_df = df . lazy () . filter ( good_expr ) if use_expr is not True : good_df = good_df . filter ( use_expr ) return good_df . select ( pl . col ( col )) . collect () . to_series ()","title":"good_series"},{"location":"docstrings/#mass2.core.misc.hist_of_series","text":"Return the bin centers and counts of a histogram of the given Series using the given bin edges. Source code in mass2/core/misc.py 95 96 97 98 99 100 def hist_of_series ( series : pl . Series , bin_edges : ArrayLike ) -> tuple [ NDArray , NDArray ]: \"\"\"Return the bin centers and counts of a histogram of the given Series using the given bin edges.\"\"\" bin_edges = np . asarray ( bin_edges ) bin_centers , _ = midpoints_and_step_size ( bin_edges ) counts = series . rename ( \"count\" ) . hist ( list ( bin_edges ), include_category = False , include_breakpoint = False ) return bin_centers , counts . to_numpy () . T [ 0 ]","title":"hist_of_series"},{"location":"docstrings/#mass2.core.misc.launch_examples","text":"Launch marimo edit in the examples folder. Source code in mass2/core/misc.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def launch_examples () -> None : \"\"\"Launch marimo edit in the examples folder.\"\"\" examples_folder = pathlib . Path ( __file__ ) . parent . parent . parent / \"examples\" # use relative path to avoid this bug: https://github.com/marimo-team/marimo/issues/1895 examples_folder_relative = str ( examples_folder . relative_to ( pathlib . Path . cwd ())) # Prepare the command command = [ \"marimo\" , \"edit\" , examples_folder_relative ] + sys . argv [ 1 :] # Execute the command print ( f \"launching marimo edit in { examples_folder_relative } \" ) try : # Execute the command and directly forward stdout and stderr process = subprocess . Popen ( command , stdout = sys . stdout , stderr = sys . stderr ) process . communicate () except KeyboardInterrupt : # Handle cleanup on Ctrl-C try : process . terminate () except OSError : pass process . wait () sys . exit ( 1 ) # Check if the command was successful if process . returncode != 0 : sys . exit ( process . returncode )","title":"launch_examples"},{"location":"docstrings/#mass2.core.misc.median_absolute_deviation","text":"Return the median absolute deviation of the input, unnormalized. Source code in mass2/core/misc.py 59 60 61 62 def median_absolute_deviation ( x : ArrayLike ) -> float : \"\"\"Return the median absolute deviation of the input, unnormalized.\"\"\" x = np . asarray ( x ) return float ( np . median ( np . abs ( x - np . median ( x ))))","title":"median_absolute_deviation"},{"location":"docstrings/#mass2.core.misc.merge_dicts_ordered_by_keys","text":"Merge two dictionaries and return a new dictionary with items ordered by key. Source code in mass2/core/misc.py 162 163 164 165 166 167 168 169 170 171 172 173 def merge_dicts_ordered_by_keys ( dict1 : dict [ int , Any ], dict2 : dict [ int , Any ]) -> dict [ int , Any ]: \"\"\"Merge two dictionaries and return a new dictionary with items ordered by key.\"\"\" # Combine both dictionaries' items (key, value) into a list of tuples combined_items = list ( dict1 . items ()) + list ( dict2 . items ()) # Sort the combined list of tuples by key combined_items . sort ( key = lambda item : item [ 0 ]) # Convert the sorted list of tuples back into a dictionary merged_dict : dict [ int , Any ] = { key : value for key , value in combined_items } return merged_dict","title":"merge_dicts_ordered_by_keys"},{"location":"docstrings/#mass2.core.misc.midpoints_and_step_size","text":"return midpoints, step_size for bin edges x Source code in mass2/core/misc.py 86 87 88 89 90 91 92 def midpoints_and_step_size ( x : ArrayLike ) -> tuple [ NDArray , float ]: \"\"\"return midpoints, step_size for bin edges x\"\"\" x = np . asarray ( x ) d = np . diff ( x ) step_size = float ( d [ 0 ]) assert np . allclose ( d , step_size , atol = 1e-9 ), f \" { d =} \" return x [: - 1 ] + step_size , step_size","title":"midpoints_and_step_size"},{"location":"docstrings/#mass2.core.misc.outlier_resistant_nsigma_above_mid","text":"RReturn the value that is nsigma median absolute deviations (MADs) above the median of the input. Source code in mass2/core/misc.py 71 72 73 74 75 def outlier_resistant_nsigma_above_mid ( x : ArrayLike , nsigma : float = 5 ) -> float : \"\"\"RReturn the value that is `nsigma` median absolute deviations (MADs) above the median of the input.\"\"\" x = np . asarray ( x ) mid = np . median ( x ) return mid + nsigma * sigma_mad ( x )","title":"outlier_resistant_nsigma_above_mid"},{"location":"docstrings/#mass2.core.misc.outlier_resistant_nsigma_range_from_mid","text":"Return the values that are nsigma median absolute deviations (MADs) below and above the median of the input Source code in mass2/core/misc.py 78 79 80 81 82 83 def outlier_resistant_nsigma_range_from_mid ( x : ArrayLike , nsigma : float = 5 ) -> tuple [ float , float ]: \"\"\"Return the values that are `nsigma` median absolute deviations (MADs) below and above the median of the input\"\"\" x = np . asarray ( x ) mid = np . median ( x ) smad = sigma_mad ( x ) return mid - nsigma * smad , mid + nsigma * smad","title":"outlier_resistant_nsigma_range_from_mid"},{"location":"docstrings/#mass2.core.misc.pickle_object","text":"Pickle the given object to the given filename using dill. Mass2 Recipe objects are compatible with dill but not with the standard pickle module. Source code in mass2/core/misc.py 24 25 26 27 28 def pickle_object ( obj : Any , filename : str ) -> None : \"\"\"Pickle the given object to the given filename using dill. Mass2 Recipe objects are compatible with `dill` but _not_ with the standard `pickle` module.\"\"\" with open ( filename , \"wb\" ) as file : dill . dump ( obj , file )","title":"pickle_object"},{"location":"docstrings/#mass2.core.misc.plot_a_vs_b_series","text":"Plot the two given Series as a scatterplot on the given axis (or a new one if None). Source code in mass2/core/misc.py 117 118 119 120 121 122 123 124 def plot_a_vs_b_series ( a : pl . Series , b : pl . Series , axis : plt . Axes | None = None , ** plotkwarg : dict ) -> None : \"\"\"Plot the two given Series as a scatterplot on the given axis (or a new one if None).\"\"\" if axis is None : plt . figure () axis = plt . gca () axis . plot ( a , b , \".\" , label = b . name , ** plotkwarg ) axis . set_xlabel ( a . name ) axis . set_ylabel ( b . name )","title":"plot_a_vs_b_series"},{"location":"docstrings/#mass2.core.misc.plot_hist_of_series","text":"Plot a histogram of the given Series using the given bin edges on the given axis (or a new one if None). Source code in mass2/core/misc.py 103 104 105 106 107 108 109 110 111 112 113 114 def plot_hist_of_series ( series : pl . Series , bin_edges : ArrayLike , axis : plt . Axes | None = None , ** plotkwarg : dict ) -> plt . Axes : \"\"\"Plot a histogram of the given Series using the given bin edges on the given axis (or a new one if None).\"\"\" if axis is None : plt . figure () axis = plt . gca () bin_edges = np . asarray ( bin_edges ) bin_centers , step_size = midpoints_and_step_size ( bin_edges ) hist = series . rename ( \"count\" ) . hist ( list ( bin_edges ), include_category = False , include_breakpoint = False ) axis . plot ( bin_centers , hist , label = series . name , ** plotkwarg ) axis . set_xlabel ( series . name ) axis . set_ylabel ( f \"counts per { step_size : .2f } unit bin\" ) return axis","title":"plot_hist_of_series"},{"location":"docstrings/#mass2.core.misc.root_mean_squared","text":"Return the root mean square of the input along the given axis or axes. Does not subtract the mean first. Source code in mass2/core/misc.py 156 157 158 159 def root_mean_squared ( x : ArrayLike , axis : int | tuple [ int ] | None = None ) -> float : \"\"\"Return the root mean square of the input along the given axis or axes. Does _not_ subtract the mean first.\"\"\" return np . sqrt ( np . mean ( np . asarray ( x ) ** 2 , axis ))","title":"root_mean_squared"},{"location":"docstrings/#mass2.core.misc.show","text":"Create a Marimo interactive view of the given Matplotlib figure (or the current figure if None). Source code in mass2/core/misc.py 17 18 19 20 21 def show ( fig : plt . Figure | None = None ) -> mo . Html : \"\"\"Create a Marimo interactive view of the given Matplotlib figure (or the current figure if None).\"\"\" if fig is None : fig = plt . gcf () return mo . mpl . interactive ( fig )","title":"show"},{"location":"docstrings/#mass2.core.misc.sigma_mad","text":"Return the nomrlized median absolute deviation of the input, rescaled to give the standard deviation if distribution is Gaussian. This method is more robust to outliers than calculating the standard deviation directly. Source code in mass2/core/misc.py 65 66 67 68 def sigma_mad ( x : ArrayLike ) -> float : \"\"\"Return the nomrlized median absolute deviation of the input, rescaled to give the standard deviation if distribution is Gaussian. This method is more robust to outliers than calculating the standard deviation directly.\"\"\" return median_absolute_deviation ( x ) * 1.4826","title":"sigma_mad"},{"location":"docstrings/#mass2.core.misc.smallest_positive_real","text":"Return the smallest positive real number in the given array-like object. Source code in mass2/core/misc.py 38 39 40 41 42 43 44 45 46 def smallest_positive_real ( arr : ArrayLike ) -> float : \"\"\"Return the smallest positive real number in the given array-like object.\"\"\" def is_positive_real ( x : Any ) -> bool : \"Is `x` a positive real number?\" return x > 0 and np . isreal ( x ) positive_real_numbers = np . array ( list ( filter ( is_positive_real , np . asarray ( arr )))) return np . min ( positive_real_numbers )","title":"smallest_positive_real"},{"location":"docstrings/#mass2.core.misc.unpickle_object","text":"Unpickle an object from the given filename using dill. Source code in mass2/core/misc.py 31 32 33 34 35 def unpickle_object ( filename : str ) -> Any : \"\"\"Unpickle an object from the given filename using dill.\"\"\" with open ( filename , \"rb\" ) as file : obj = dill . load ( file ) return obj Tools for fitting multiple spectral lines in a single pass.","title":"unpickle_object"},{"location":"docstrings/#mass2.core.multifit.FitSpec","text":"Specification of a single line fit within a MultiFit. Source code in mass2/core/multifit.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @dataclass ( frozen = True ) class FitSpec : \"\"\"Specification of a single line fit within a MultiFit.\"\"\" model : GenericLineModel bin_edges : np . ndarray use_expr : pl . Expr params_update : lmfit . parameter . Parameters def params ( self , bin_centers : NDArray , counts : NDArray ) -> lmfit . Parameters : \"\"\"Return a reasonable guess at the parameters given the spectrum to be fit.\"\"\" params = self . model . make_params () params = self . model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) params = params . update ( self . params_update ) return params def fit_series_without_use_expr ( self , series : pl . Series ) -> LineModelResult : \"\"\"Fit the given Series without applying a use_expr filter.\"\"\" bin_centers , counts = mass2 . misc . hist_of_series ( series , self . bin_edges ) params = self . params ( bin_centers , counts ) bin_centers , bin_size = mass2 . misc . midpoints_and_step_size ( self . bin_edges ) result = self . model . fit ( counts , params , bin_centers = bin_centers ) result . set_label_hints ( binsize = bin_size , ds_shortname = \"??\" , unit_str = \"eV\" , attr_str = series . name , states_hint = f \" { self . use_expr } \" , cut_hint = \"\" , ) return result def fit_df ( self , df : pl . DataFrame , col : str , good_expr : pl . Expr ) -> LineModelResult : \"\"\"Fit the given DataFrame column `col` after applying good_expr and use_expr filters.\"\"\" series = mass2 . misc . good_series ( df , col , good_expr , use_expr = self . use_expr ) return self . fit_series_without_use_expr ( series ) def fit_ch ( self , ch : \"Channel\" , col : str ) -> LineModelResult : \"\"\"Fit the given Channel's DataFrame column `col` after applying the Channel's good_expr and this FitSpec's use_expr filters.\"\"\" return self . fit_df ( ch . df , col , ch . good_expr )","title":"FitSpec"},{"location":"docstrings/#mass2.core.multifit.FitSpec.fit_ch","text":"Fit the given Channel's DataFrame column col after applying the Channel's good_expr and this FitSpec's use_expr filters. Source code in mass2/core/multifit.py 76 77 78 79 def fit_ch ( self , ch : \"Channel\" , col : str ) -> LineModelResult : \"\"\"Fit the given Channel's DataFrame column `col` after applying the Channel's good_expr and this FitSpec's use_expr filters.\"\"\" return self . fit_df ( ch . df , col , ch . good_expr )","title":"fit_ch"},{"location":"docstrings/#mass2.core.multifit.FitSpec.fit_df","text":"Fit the given DataFrame column col after applying good_expr and use_expr filters. Source code in mass2/core/multifit.py 71 72 73 74 def fit_df ( self , df : pl . DataFrame , col : str , good_expr : pl . Expr ) -> LineModelResult : \"\"\"Fit the given DataFrame column `col` after applying good_expr and use_expr filters.\"\"\" series = mass2 . misc . good_series ( df , col , good_expr , use_expr = self . use_expr ) return self . fit_series_without_use_expr ( series )","title":"fit_df"},{"location":"docstrings/#mass2.core.multifit.FitSpec.fit_series_without_use_expr","text":"Fit the given Series without applying a use_expr filter. Source code in mass2/core/multifit.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def fit_series_without_use_expr ( self , series : pl . Series ) -> LineModelResult : \"\"\"Fit the given Series without applying a use_expr filter.\"\"\" bin_centers , counts = mass2 . misc . hist_of_series ( series , self . bin_edges ) params = self . params ( bin_centers , counts ) bin_centers , bin_size = mass2 . misc . midpoints_and_step_size ( self . bin_edges ) result = self . model . fit ( counts , params , bin_centers = bin_centers ) result . set_label_hints ( binsize = bin_size , ds_shortname = \"??\" , unit_str = \"eV\" , attr_str = series . name , states_hint = f \" { self . use_expr } \" , cut_hint = \"\" , ) return result","title":"fit_series_without_use_expr"},{"location":"docstrings/#mass2.core.multifit.FitSpec.params","text":"Return a reasonable guess at the parameters given the spectrum to be fit. Source code in mass2/core/multifit.py 47 48 49 50 51 52 53 def params ( self , bin_centers : NDArray , counts : NDArray ) -> lmfit . Parameters : \"\"\"Return a reasonable guess at the parameters given the spectrum to be fit.\"\"\" params = self . model . make_params () params = self . model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) params = params . update ( self . params_update ) return params","title":"params"},{"location":"docstrings/#mass2.core.multifit.MultiFit","text":"Specification of multiple emission-line fits to be done in one pass. Source code in mass2/core/multifit.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 @dataclass ( frozen = True ) class MultiFit : \"\"\"Specification of multiple emission-line fits to be done in one pass.\"\"\" default_fit_width : float = 50 default_bin_size : float = 0.5 default_use_expr : pl . Expr = field ( default_factory = alwaysTrue ) default_params_update : dict = field ( default_factory = lmfit . Parameters ) fitspecs : list [ FitSpec ] = field ( default_factory = list ) results : list | None = None def with_line ( self , line : GenericLineModel | SpectralLine | str | float , dlo : float | None = None , dhi : float | None = None , bin_size : float | None = None , use_expr : pl . Expr | None = None , params_update : lmfit . Parameters | None = None , ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with an additional FitSpec for the given line.\"\"\" model = get_model ( line ) peak_energy = model . spect . peak_energy dlo = handle_none ( dlo , self . default_fit_width / 2 ) dhi = handle_none ( dhi , self . default_fit_width / 2 ) bin_size = handle_none ( bin_size , self . default_bin_size ) params_update = handle_none ( params_update , self . default_params_update ) use_expr = handle_none ( use_expr , self . default_use_expr ) bin_edges = np . arange ( - dlo , dhi + bin_size , bin_size ) + peak_energy fitspec = FitSpec ( model , bin_edges , use_expr , params_update ) return self . with_fitspec ( fitspec ) def with_fitspec ( self , fitspec : FitSpec ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with a new FitSpec added.\"\"\" # make sure they're always sorted by energy newfitspecs = sorted ( self . fitspecs + [ fitspec ], key = lambda x : x . model . spect . peak_energy ) return dataclasses . replace ( self , fitspecs = newfitspecs ) def with_results ( self , results : list ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with the given results added.\"\"\" return dataclasses . replace ( self , results = results ) def results_params_as_df ( self ) -> pl . DataFrame : \"\"\"Return a DataFrame made from the fit parameters from the results.\"\"\" assert self . results is not None result = self . results [ 0 ] param_names = result . params . keys () d : dict [ str , list ] = {} d [ \"line\" ] = [ fitspec . model . spect . shortname for fitspec in self . fitspecs ] d [ \"peak_energy_ref\" ] = [ fitspec . model . spect . peak_energy for fitspec in self . fitspecs ] d [ \"peak_energy_ref_err\" ] = [] # for quickline, position_uncertainty is a string # translate that into a large value for uncertainty so we can proceed without crashing for fitspec in self . fitspecs : if isinstance ( fitspec . model . spect . position_uncertainty , str ): v = 0.1 * fitspec . model . spect . peak_energy # 10% error is large! else : v = fitspec . model . spect . position_uncertainty d [ \"peak_energy_ref_err\" ] . append ( v ) for param_name in param_names : d [ param_name ] = [ result . params [ param_name ] . value for result in self . results ] d [ param_name + \"_stderr\" ] = [ result . params [ param_name ] . stderr for result in self . results ] return pl . DataFrame ( d ) def fit_series_without_use_expr ( self , series : pl . Series ) -> \"MultiFit\" : \"Fit all the FitSpecs in this MultiFit to the given Series without applying any use_expr filter.\" results = [ fitspec . fit_series_without_use_expr ( series ) for fitspec in self . fitspecs ] return self . with_results ( results ) def fit_df ( self , df : pl . DataFrame , col : str , good_expr : pl . Expr ) -> \"MultiFit\" : \"\"\"Fit all the FitSpecs in this MultiFit to the given DataFrame column `col` after applying good_expr filter.\"\"\" results = [] for fitspec in self . fitspecs : result = fitspec . fit_df ( df , col , good_expr ) results . append ( result ) return self . with_results ( results ) def fit_ch ( self , ch : \"Channel\" , col : str ) -> \"MultiFit\" : \"\"\"Fit all the FitSpecs in this MultiFit to the given Channel's DataFrame column `col` after applying the Channel's good_expr filter.\"\"\" return self . fit_df ( ch . df , col , ch . good_expr ) def plot_results ( self , n_extra_axes : int = 0 ) -> tuple [ plt . Figure , plt . Axes ]: \"\"\"Plot all the fit results in subplots, with n_extra_axes empty subplots included at the end.\"\"\" assert self . results is not None n = len ( self . results ) + n_extra_axes cols = min ( 3 , n ) rows = math . ceil ( n / cols ) fig , axes = plt . subplots ( rows , cols , figsize = ( cols * 4 , rows * 4 )) # Adjust figure size as needed # If there's only one subplot, axes is not a list but a single Axes object. if rows == 1 and cols == 1 : axes = [ axes ] elif rows == 1 or cols == 1 : axes = axes . flatten () else : axes = axes . ravel () for result , ax in zip ( self . results , axes ): result . plotm ( ax = ax ) # Hide any remaining empty subplots for ax in axes [ n :]: ax . axis ( \"off\" ) plt . tight_layout () return fig , axes def plot_results_and_pfit ( self , uncalibrated_name : str , previous_energy2ph : Callable , n_extra_axes : int = 0 ) -> plt . Axes : \"\"\"Plot all the fit results in subplots, and also plot the gain curve on an extra axis.\"\"\" assert self . results is not None _fig , axes = self . plot_results ( n_extra_axes = 1 + n_extra_axes ) ax = axes [ len ( self . results )] multifit_df = self . results_params_as_df () peaks_in_energy_rough_cal = multifit_df [ \"peak_ph\" ] . to_numpy () peaks_uncalibrated = previous_energy2ph ( peaks_in_energy_rough_cal ) peaks_in_energy_reference = multifit_df [ \"peak_energy_ref\" ] . to_numpy () pfit_gain , rms_residual_energy = self . to_pfit_gain ( previous_energy2ph ) plt . sca ( ax ) x = np . linspace ( 0 , np . amax ( peaks_uncalibrated ), 100 ) plt . plot ( x , pfit_gain ( x ), \"k\" , label = \"fit\" ) gain = peaks_uncalibrated / peaks_in_energy_reference plt . plot ( peaks_uncalibrated , gain , \"o\" ) plt . xlabel ( uncalibrated_name ) plt . ylabel ( \"gain\" ) plt . title ( f \" { rms_residual_energy =: .3f } \" ) for name , x , y in zip ( multifit_df [ \"line\" ], peaks_uncalibrated , gain ): ax . annotate ( str ( name ), ( x , y )) return axes def to_pfit_gain ( self , previous_energy2ph : Callable ) -> tuple [ np . polynomial . Polynomial , float ]: \"\"\"Return a best-fit 2nd degree polynomial for gain (ph/energy) vs uncalibrated ph, and the rms residual in energy after applying that gain correction.\"\"\" multifit_df = self . results_params_as_df () peaks_in_energy_rough_cal = multifit_df [ \"peak_ph\" ] . to_numpy () peaks_uncalibrated = np . array ([ previous_energy2ph ( e ) for e in peaks_in_energy_rough_cal ]) . ravel () peaks_in_energy_reference = multifit_df [ \"peak_energy_ref\" ] . to_numpy () gain = peaks_uncalibrated / peaks_in_energy_reference pfit_gain = np . polynomial . Polynomial . fit ( peaks_uncalibrated , gain , deg = 2 ) def ph2energy ( ph : NDArray ) -> NDArray : \"Given an array `ph` of pulse heights, return the corresponding energies as an array.\" gain = pfit_gain ( ph ) return ph / gain e_predicted = ph2energy ( peaks_uncalibrated ) rms_residual_energy = mass2 . misc . root_mean_squared ( e_predicted - peaks_in_energy_reference ) return pfit_gain , rms_residual_energy def to_mass_cal ( self , previous_energy2ph : Callable , curvetype : Curvetypes = Curvetypes . GAIN , approximate : bool = False ) -> EnergyCalibration : \"\"\"Return a calibration object made from the fit results in this MultiFit.\"\"\" df = self . results_params_as_df () maker = EnergyCalibrationMaker ( ph = np . array ([ previous_energy2ph ( x ) for x in df [ \"peak_ph\" ] . to_numpy ()]), energy = df [ \"peak_energy_ref\" ] . to_numpy (), dph = df [ \"peak_ph_stderr\" ] . to_numpy (), de = df [ \"peak_energy_ref_err\" ] . to_numpy (), names = [ name for name in df [ \"line\" ]], ) cal = maker . make_calibration ( curvename = curvetype , approximate = approximate ) return cal","title":"MultiFit"},{"location":"docstrings/#mass2.core.multifit.MultiFit.fit_ch","text":"Fit all the FitSpecs in this MultiFit to the given Channel's DataFrame column col after applying the Channel's good_expr filter. Source code in mass2/core/multifit.py 159 160 161 162 def fit_ch ( self , ch : \"Channel\" , col : str ) -> \"MultiFit\" : \"\"\"Fit all the FitSpecs in this MultiFit to the given Channel's DataFrame column `col` after applying the Channel's good_expr filter.\"\"\" return self . fit_df ( ch . df , col , ch . good_expr )","title":"fit_ch"},{"location":"docstrings/#mass2.core.multifit.MultiFit.fit_df","text":"Fit all the FitSpecs in this MultiFit to the given DataFrame column col after applying good_expr filter. Source code in mass2/core/multifit.py 151 152 153 154 155 156 157 def fit_df ( self , df : pl . DataFrame , col : str , good_expr : pl . Expr ) -> \"MultiFit\" : \"\"\"Fit all the FitSpecs in this MultiFit to the given DataFrame column `col` after applying good_expr filter.\"\"\" results = [] for fitspec in self . fitspecs : result = fitspec . fit_df ( df , col , good_expr ) results . append ( result ) return self . with_results ( results )","title":"fit_df"},{"location":"docstrings/#mass2.core.multifit.MultiFit.fit_series_without_use_expr","text":"Fit all the FitSpecs in this MultiFit to the given Series without applying any use_expr filter. Source code in mass2/core/multifit.py 146 147 148 149 def fit_series_without_use_expr ( self , series : pl . Series ) -> \"MultiFit\" : \"Fit all the FitSpecs in this MultiFit to the given Series without applying any use_expr filter.\" results = [ fitspec . fit_series_without_use_expr ( series ) for fitspec in self . fitspecs ] return self . with_results ( results )","title":"fit_series_without_use_expr"},{"location":"docstrings/#mass2.core.multifit.MultiFit.plot_results","text":"Plot all the fit results in subplots, with n_extra_axes empty subplots included at the end. Source code in mass2/core/multifit.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def plot_results ( self , n_extra_axes : int = 0 ) -> tuple [ plt . Figure , plt . Axes ]: \"\"\"Plot all the fit results in subplots, with n_extra_axes empty subplots included at the end.\"\"\" assert self . results is not None n = len ( self . results ) + n_extra_axes cols = min ( 3 , n ) rows = math . ceil ( n / cols ) fig , axes = plt . subplots ( rows , cols , figsize = ( cols * 4 , rows * 4 )) # Adjust figure size as needed # If there's only one subplot, axes is not a list but a single Axes object. if rows == 1 and cols == 1 : axes = [ axes ] elif rows == 1 or cols == 1 : axes = axes . flatten () else : axes = axes . ravel () for result , ax in zip ( self . results , axes ): result . plotm ( ax = ax ) # Hide any remaining empty subplots for ax in axes [ n :]: ax . axis ( \"off\" ) plt . tight_layout () return fig , axes","title":"plot_results"},{"location":"docstrings/#mass2.core.multifit.MultiFit.plot_results_and_pfit","text":"Plot all the fit results in subplots, and also plot the gain curve on an extra axis. Source code in mass2/core/multifit.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def plot_results_and_pfit ( self , uncalibrated_name : str , previous_energy2ph : Callable , n_extra_axes : int = 0 ) -> plt . Axes : \"\"\"Plot all the fit results in subplots, and also plot the gain curve on an extra axis.\"\"\" assert self . results is not None _fig , axes = self . plot_results ( n_extra_axes = 1 + n_extra_axes ) ax = axes [ len ( self . results )] multifit_df = self . results_params_as_df () peaks_in_energy_rough_cal = multifit_df [ \"peak_ph\" ] . to_numpy () peaks_uncalibrated = previous_energy2ph ( peaks_in_energy_rough_cal ) peaks_in_energy_reference = multifit_df [ \"peak_energy_ref\" ] . to_numpy () pfit_gain , rms_residual_energy = self . to_pfit_gain ( previous_energy2ph ) plt . sca ( ax ) x = np . linspace ( 0 , np . amax ( peaks_uncalibrated ), 100 ) plt . plot ( x , pfit_gain ( x ), \"k\" , label = \"fit\" ) gain = peaks_uncalibrated / peaks_in_energy_reference plt . plot ( peaks_uncalibrated , gain , \"o\" ) plt . xlabel ( uncalibrated_name ) plt . ylabel ( \"gain\" ) plt . title ( f \" { rms_residual_energy =: .3f } \" ) for name , x , y in zip ( multifit_df [ \"line\" ], peaks_uncalibrated , gain ): ax . annotate ( str ( name ), ( x , y )) return axes","title":"plot_results_and_pfit"},{"location":"docstrings/#mass2.core.multifit.MultiFit.results_params_as_df","text":"Return a DataFrame made from the fit parameters from the results. Source code in mass2/core/multifit.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def results_params_as_df ( self ) -> pl . DataFrame : \"\"\"Return a DataFrame made from the fit parameters from the results.\"\"\" assert self . results is not None result = self . results [ 0 ] param_names = result . params . keys () d : dict [ str , list ] = {} d [ \"line\" ] = [ fitspec . model . spect . shortname for fitspec in self . fitspecs ] d [ \"peak_energy_ref\" ] = [ fitspec . model . spect . peak_energy for fitspec in self . fitspecs ] d [ \"peak_energy_ref_err\" ] = [] # for quickline, position_uncertainty is a string # translate that into a large value for uncertainty so we can proceed without crashing for fitspec in self . fitspecs : if isinstance ( fitspec . model . spect . position_uncertainty , str ): v = 0.1 * fitspec . model . spect . peak_energy # 10% error is large! else : v = fitspec . model . spect . position_uncertainty d [ \"peak_energy_ref_err\" ] . append ( v ) for param_name in param_names : d [ param_name ] = [ result . params [ param_name ] . value for result in self . results ] d [ param_name + \"_stderr\" ] = [ result . params [ param_name ] . stderr for result in self . results ] return pl . DataFrame ( d )","title":"results_params_as_df"},{"location":"docstrings/#mass2.core.multifit.MultiFit.to_mass_cal","text":"Return a calibration object made from the fit results in this MultiFit. Source code in mass2/core/multifit.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def to_mass_cal ( self , previous_energy2ph : Callable , curvetype : Curvetypes = Curvetypes . GAIN , approximate : bool = False ) -> EnergyCalibration : \"\"\"Return a calibration object made from the fit results in this MultiFit.\"\"\" df = self . results_params_as_df () maker = EnergyCalibrationMaker ( ph = np . array ([ previous_energy2ph ( x ) for x in df [ \"peak_ph\" ] . to_numpy ()]), energy = df [ \"peak_energy_ref\" ] . to_numpy (), dph = df [ \"peak_ph_stderr\" ] . to_numpy (), de = df [ \"peak_energy_ref_err\" ] . to_numpy (), names = [ name for name in df [ \"line\" ]], ) cal = maker . make_calibration ( curvename = curvetype , approximate = approximate ) return cal","title":"to_mass_cal"},{"location":"docstrings/#mass2.core.multifit.MultiFit.to_pfit_gain","text":"Return a best-fit 2nd degree polynomial for gain (ph/energy) vs uncalibrated ph, and the rms residual in energy after applying that gain correction. Source code in mass2/core/multifit.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def to_pfit_gain ( self , previous_energy2ph : Callable ) -> tuple [ np . polynomial . Polynomial , float ]: \"\"\"Return a best-fit 2nd degree polynomial for gain (ph/energy) vs uncalibrated ph, and the rms residual in energy after applying that gain correction.\"\"\" multifit_df = self . results_params_as_df () peaks_in_energy_rough_cal = multifit_df [ \"peak_ph\" ] . to_numpy () peaks_uncalibrated = np . array ([ previous_energy2ph ( e ) for e in peaks_in_energy_rough_cal ]) . ravel () peaks_in_energy_reference = multifit_df [ \"peak_energy_ref\" ] . to_numpy () gain = peaks_uncalibrated / peaks_in_energy_reference pfit_gain = np . polynomial . Polynomial . fit ( peaks_uncalibrated , gain , deg = 2 ) def ph2energy ( ph : NDArray ) -> NDArray : \"Given an array `ph` of pulse heights, return the corresponding energies as an array.\" gain = pfit_gain ( ph ) return ph / gain e_predicted = ph2energy ( peaks_uncalibrated ) rms_residual_energy = mass2 . misc . root_mean_squared ( e_predicted - peaks_in_energy_reference ) return pfit_gain , rms_residual_energy","title":"to_pfit_gain"},{"location":"docstrings/#mass2.core.multifit.MultiFit.with_fitspec","text":"Return a copy of this MultiFit with a new FitSpec added. Source code in mass2/core/multifit.py 114 115 116 117 118 def with_fitspec ( self , fitspec : FitSpec ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with a new FitSpec added.\"\"\" # make sure they're always sorted by energy newfitspecs = sorted ( self . fitspecs + [ fitspec ], key = lambda x : x . model . spect . peak_energy ) return dataclasses . replace ( self , fitspecs = newfitspecs )","title":"with_fitspec"},{"location":"docstrings/#mass2.core.multifit.MultiFit.with_line","text":"Return a copy of this MultiFit with an additional FitSpec for the given line. Source code in mass2/core/multifit.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def with_line ( self , line : GenericLineModel | SpectralLine | str | float , dlo : float | None = None , dhi : float | None = None , bin_size : float | None = None , use_expr : pl . Expr | None = None , params_update : lmfit . Parameters | None = None , ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with an additional FitSpec for the given line.\"\"\" model = get_model ( line ) peak_energy = model . spect . peak_energy dlo = handle_none ( dlo , self . default_fit_width / 2 ) dhi = handle_none ( dhi , self . default_fit_width / 2 ) bin_size = handle_none ( bin_size , self . default_bin_size ) params_update = handle_none ( params_update , self . default_params_update ) use_expr = handle_none ( use_expr , self . default_use_expr ) bin_edges = np . arange ( - dlo , dhi + bin_size , bin_size ) + peak_energy fitspec = FitSpec ( model , bin_edges , use_expr , params_update ) return self . with_fitspec ( fitspec )","title":"with_line"},{"location":"docstrings/#mass2.core.multifit.MultiFit.with_results","text":"Return a copy of this MultiFit with the given results added. Source code in mass2/core/multifit.py 120 121 122 def with_results ( self , results : list ) -> \"MultiFit\" : \"\"\"Return a copy of this MultiFit with the given results added.\"\"\" return dataclasses . replace ( self , results = results )","title":"with_results"},{"location":"docstrings/#mass2.core.multifit.MultiFitMassCalibrationStep","text":"Bases: RecipeStep A RecipeStep to apply a mass-style calibration derived from a MultiFit. Source code in mass2/core/multifit.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 @dataclass ( frozen = True ) class MultiFitMassCalibrationStep ( RecipeStep ): \"\"\"A RecipeStep to apply a mass-style calibration derived from a MultiFit.\"\"\" cal : EnergyCalibration multifit : MultiFit | None def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calibrate energy and return a new DataFrame with results.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2 def drop_debug ( self ) -> \"MultiFitMassCalibrationStep\" : \"For slimmer object pickling, return a copy of self with the fat debugging info removed\" return dataclasses . replace ( self , multifit = None ) def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"Plot the fit results and gain curve for debugging purposes.\" assert self . multifit is not None axes = self . multifit . plot_results_and_pfit ( uncalibrated_name = self . inputs [ 0 ], previous_energy2ph = self . energy2ph , ) return axes def ph2energy ( self , ph : ArrayLike ) -> NDArray : \"The quadratic gain calibration curve: ph -> energy\" ph = np . asarray ( ph ) return self . cal . ph2energy ( ph ) def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"The inverse of the quadratic gain calibration curve: energy -> ph\"\"\" energy = np . asarray ( energy ) return self . cal . energy2ph ( energy ) @classmethod def learn ( cls , ch : \"Channel\" , multifit_spec : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"MultiFitMassCalibrationStep\" : \"\"\"multifit then make a mass calibration object with curve_type=Curvetypes.GAIN and approx=False TODO: support more options\"\"\" previous_cal_step = ch . steps [ previous_cal_step_index ] assert hasattr ( previous_cal_step , \"energy2ph\" ) rough_energy_col = previous_cal_step . output [ 0 ] uncalibrated_col = previous_cal_step . inputs [ 0 ] multifit_with_results = multifit_spec . fit_ch ( ch , col = rough_energy_col ) cal = multifit_with_results . to_mass_cal ( previous_cal_step . energy2ph ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr , cal , multifit_with_results , ) return step","title":"MultiFitMassCalibrationStep"},{"location":"docstrings/#mass2.core.multifit.MultiFitMassCalibrationStep.calc_from_df","text":"Calibrate energy and return a new DataFrame with results. Source code in mass2/core/multifit.py 333 334 335 336 337 338 339 340 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calibrate energy and return a new DataFrame with results.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2","title":"calc_from_df"},{"location":"docstrings/#mass2.core.multifit.MultiFitMassCalibrationStep.dbg_plot","text":"Plot the fit results and gain curve for debugging purposes. Source code in mass2/core/multifit.py 346 347 348 349 350 351 352 353 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"Plot the fit results and gain curve for debugging purposes.\" assert self . multifit is not None axes = self . multifit . plot_results_and_pfit ( uncalibrated_name = self . inputs [ 0 ], previous_energy2ph = self . energy2ph , ) return axes","title":"dbg_plot"},{"location":"docstrings/#mass2.core.multifit.MultiFitMassCalibrationStep.drop_debug","text":"For slimmer object pickling, return a copy of self with the fat debugging info removed Source code in mass2/core/multifit.py 342 343 344 def drop_debug ( self ) -> \"MultiFitMassCalibrationStep\" : \"For slimmer object pickling, return a copy of self with the fat debugging info removed\" return dataclasses . replace ( self , multifit = None )","title":"drop_debug"},{"location":"docstrings/#mass2.core.multifit.MultiFitMassCalibrationStep.energy2ph","text":"The inverse of the quadratic gain calibration curve: energy -> ph Source code in mass2/core/multifit.py 360 361 362 363 def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"The inverse of the quadratic gain calibration curve: energy -> ph\"\"\" energy = np . asarray ( energy ) return self . cal . energy2ph ( energy )","title":"energy2ph"},{"location":"docstrings/#mass2.core.multifit.MultiFitMassCalibrationStep.learn","text":"multifit then make a mass calibration object with curve_type=Curvetypes.GAIN and approx=False TODO: support more options Source code in mass2/core/multifit.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 @classmethod def learn ( cls , ch : \"Channel\" , multifit_spec : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"MultiFitMassCalibrationStep\" : \"\"\"multifit then make a mass calibration object with curve_type=Curvetypes.GAIN and approx=False TODO: support more options\"\"\" previous_cal_step = ch . steps [ previous_cal_step_index ] assert hasattr ( previous_cal_step , \"energy2ph\" ) rough_energy_col = previous_cal_step . output [ 0 ] uncalibrated_col = previous_cal_step . inputs [ 0 ] multifit_with_results = multifit_spec . fit_ch ( ch , col = rough_energy_col ) cal = multifit_with_results . to_mass_cal ( previous_cal_step . energy2ph ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr , cal , multifit_with_results , ) return step","title":"learn"},{"location":"docstrings/#mass2.core.multifit.MultiFitMassCalibrationStep.ph2energy","text":"The quadratic gain calibration curve: ph -> energy Source code in mass2/core/multifit.py 355 356 357 358 def ph2energy ( self , ph : ArrayLike ) -> NDArray : \"The quadratic gain calibration curve: ph -> energy\" ph = np . asarray ( ph ) return self . cal . ph2energy ( ph )","title":"ph2energy"},{"location":"docstrings/#mass2.core.multifit.MultiFitQuadraticGainStep","text":"Bases: RecipeStep A RecipeStep to apply a quadratic gain curve, after fitting multiple emission lines. Source code in mass2/core/multifit.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 @dataclass ( frozen = True ) class MultiFitQuadraticGainStep ( RecipeStep ): \"\"\"A RecipeStep to apply a quadratic gain curve, after fitting multiple emission lines.\"\"\" pfit_gain : np . polynomial . Polynomial multifit : MultiFit | None rms_residual_energy : float def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calibrate energy and return a new DataFrame with results.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the fit results and gain curve for debugging purposes.\"\"\" if self . multifit is not None : self . multifit . plot_results_and_pfit ( uncalibrated_name = self . inputs [ 0 ], previous_energy2ph = self . energy2ph ) return plt . gca () def drop_debug ( self ) -> \"MultiFitQuadraticGainStep\" : \"For slimmer object pickling, return a copy of self with the fat debugging info removed\" return dataclasses . replace ( self , multifit = None ) def ph2energy ( self , ph : ArrayLike ) -> NDArray : \"The quadratic gain calibration curve: ph -> energy\" ph = np . asarray ( ph ) gain = self . pfit_gain ( ph ) return ph / gain def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"The inverse of the quadratic gain calibration curve: energy -> ph\"\"\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(c + b*x + a*x^2) # so # y*c + (y*b-1)*x + a*x^2 = 0 # and given that we've selected for well formed calibrations, # we know which root we want energy = np . asarray ( energy ) cba = self . pfit_gain . convert () . coef c , bb , a = cba * energy b = bb - 1 ph = ( - b - np . sqrt ( b ** 2 - 4 * a * c )) / ( 2 * a ) assert math . isclose ( self . ph2energy ( ph ), energy , rel_tol = 1e-6 , abs_tol = 1e-3 ) return ph @classmethod def learn ( cls , ch : \"Channel\" , multifit_spec : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"MultiFitQuadraticGainStep\" : \"\"\"Perform a multifit then make a quadratic gain calibration object.\"\"\" previous_cal_step = ch . steps [ previous_cal_step_index ] assert hasattr ( previous_cal_step , \"energy2ph\" ) rough_energy_col = previous_cal_step . output [ 0 ] uncalibrated_col = previous_cal_step . inputs [ 0 ] multifit_with_results = multifit_spec . fit_ch ( ch , col = rough_energy_col ) # multifit_df = multifit_with_results.results_params_as_df() pfit_gain , rms_residual_energy = multifit_with_results . to_pfit_gain ( previous_cal_step . energy2ph ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr , pfit_gain , multifit_with_results , rms_residual_energy , ) return step","title":"MultiFitQuadraticGainStep"},{"location":"docstrings/#mass2.core.multifit.MultiFitQuadraticGainStep.calc_from_df","text":"Calibrate energy and return a new DataFrame with results. Source code in mass2/core/multifit.py 255 256 257 258 259 260 261 262 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calibrate energy and return a new DataFrame with results.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2","title":"calc_from_df"},{"location":"docstrings/#mass2.core.multifit.MultiFitQuadraticGainStep.dbg_plot","text":"Plot the fit results and gain curve for debugging purposes. Source code in mass2/core/multifit.py 264 265 266 267 268 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Plot the fit results and gain curve for debugging purposes.\"\"\" if self . multifit is not None : self . multifit . plot_results_and_pfit ( uncalibrated_name = self . inputs [ 0 ], previous_energy2ph = self . energy2ph ) return plt . gca ()","title":"dbg_plot"},{"location":"docstrings/#mass2.core.multifit.MultiFitQuadraticGainStep.drop_debug","text":"For slimmer object pickling, return a copy of self with the fat debugging info removed Source code in mass2/core/multifit.py 270 271 272 def drop_debug ( self ) -> \"MultiFitQuadraticGainStep\" : \"For slimmer object pickling, return a copy of self with the fat debugging info removed\" return dataclasses . replace ( self , multifit = None )","title":"drop_debug"},{"location":"docstrings/#mass2.core.multifit.MultiFitQuadraticGainStep.energy2ph","text":"The inverse of the quadratic gain calibration curve: energy -> ph Source code in mass2/core/multifit.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"The inverse of the quadratic gain calibration curve: energy -> ph\"\"\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(c + b*x + a*x^2) # so # y*c + (y*b-1)*x + a*x^2 = 0 # and given that we've selected for well formed calibrations, # we know which root we want energy = np . asarray ( energy ) cba = self . pfit_gain . convert () . coef c , bb , a = cba * energy b = bb - 1 ph = ( - b - np . sqrt ( b ** 2 - 4 * a * c )) / ( 2 * a ) assert math . isclose ( self . ph2energy ( ph ), energy , rel_tol = 1e-6 , abs_tol = 1e-3 ) return ph","title":"energy2ph"},{"location":"docstrings/#mass2.core.multifit.MultiFitQuadraticGainStep.learn","text":"Perform a multifit then make a quadratic gain calibration object. Source code in mass2/core/multifit.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 @classmethod def learn ( cls , ch : \"Channel\" , multifit_spec : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"MultiFitQuadraticGainStep\" : \"\"\"Perform a multifit then make a quadratic gain calibration object.\"\"\" previous_cal_step = ch . steps [ previous_cal_step_index ] assert hasattr ( previous_cal_step , \"energy2ph\" ) rough_energy_col = previous_cal_step . output [ 0 ] uncalibrated_col = previous_cal_step . inputs [ 0 ] multifit_with_results = multifit_spec . fit_ch ( ch , col = rough_energy_col ) # multifit_df = multifit_with_results.results_params_as_df() pfit_gain , rms_residual_energy = multifit_with_results . to_pfit_gain ( previous_cal_step . energy2ph ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr , pfit_gain , multifit_with_results , rms_residual_energy , ) return step","title":"learn"},{"location":"docstrings/#mass2.core.multifit.MultiFitQuadraticGainStep.ph2energy","text":"The quadratic gain calibration curve: ph -> energy Source code in mass2/core/multifit.py 274 275 276 277 278 def ph2energy ( self , ph : ArrayLike ) -> NDArray : \"The quadratic gain calibration curve: ph -> energy\" ph = np . asarray ( ph ) gain = self . pfit_gain ( ph ) return ph / gain","title":"ph2energy"},{"location":"docstrings/#mass2.core.multifit.handle_none","text":"If val is None, return a copy of default, else return val. Source code in mass2/core/multifit.py 31 32 33 34 35 def handle_none ( val : T | None , default : T ) -> T : \"If val is None, return a copy of default, else return val.\" if val is None : return copy . copy ( default ) return val Algorithms to analyze noise data.","title":"handle_none"},{"location":"docstrings/#mass2.core.noise_algorithms.NoiseResult","text":"A dataclass to hold the results of noise analysis, both power-spectral density and (optionally) autocorrelation. Source code in mass2/core/noise_algorithms.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 @dataclass class NoiseResult : \"\"\"A dataclass to hold the results of noise analysis, both power-spectral density and (optionally) autocorrelation.\"\"\" psd : np . ndarray autocorr_vec : np . ndarray | None frequencies : np . ndarray def plot ( self , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , loglog : bool = True , ** plotkwarg : dict , ) -> None : \"\"\"Plot the power spectral density.\"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . psd [ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies [ 1 :] if sqrt_psd : axis . plot ( freq , np . sqrt ( psd ), ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( freq , psd , ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) if loglog : plt . loglog () axis . grid () axis . set_xlabel ( \"Frequency (Hz)\" ) plt . title ( f \"noise from records of length { len ( self . frequencies ) * 2 - 2 } \" ) axis . figure . tight_layout () def plot_log_rebinned ( self , bins_per_decade : int = 10 , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , ** plotkwarg : dict , ) -> None : \"\"\"Plot PSD rebinned into logarithmically spaced frequency bins.\"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . psd [ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies [ 1 :] # define logarithmically spaced bin edges fmin , fmax = freq [ 0 ], freq [ - 1 ] n_decades = np . log10 ( fmax / fmin ) n_bins = int ( bins_per_decade * n_decades ) bin_edges = np . logspace ( np . log10 ( fmin ), np . log10 ( fmax ), n_bins + 1 ) # digitize frequencies into bins inds = np . digitize ( freq , bin_edges ) # average PSD per bin binned_freqs = np . zeros ( n_bins , dtype = float ) binned_psd = np . zeros ( n_bins , dtype = float ) for i in range ( 1 , len ( bin_edges )): mask = inds == i if np . any ( mask ): binned_freqs [ i - 1 ] = np . exp ( np . mean ( np . log ( freq [ mask ]))) # geometric mean binned_psd [ i - 1 ] = np . mean ( psd [ mask ]) if sqrt_psd : axis . plot ( binned_freqs , np . sqrt ( binned_psd ), ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( binned_freqs , binned_psd , ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) axis . set_xscale ( \"log\" ) axis . set_yscale ( \"log\" ) axis . grid ( True , which = \"both\" ) axis . set_xlabel ( \"Frequency (Hz)\" ) axis . set_title ( f \"Log-rebinned noise from { len ( self . frequencies ) * 2 - 2 } samples\" ) axis . figure . tight_layout ()","title":"NoiseResult"},{"location":"docstrings/#mass2.core.noise_algorithms.NoiseResult.plot","text":"Plot the power spectral density. Source code in mass2/core/noise_algorithms.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def plot ( self , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , loglog : bool = True , ** plotkwarg : dict , ) -> None : \"\"\"Plot the power spectral density.\"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . psd [ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies [ 1 :] if sqrt_psd : axis . plot ( freq , np . sqrt ( psd ), ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( freq , psd , ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) if loglog : plt . loglog () axis . grid () axis . set_xlabel ( \"Frequency (Hz)\" ) plt . title ( f \"noise from records of length { len ( self . frequencies ) * 2 - 2 } \" ) axis . figure . tight_layout ()","title":"plot"},{"location":"docstrings/#mass2.core.noise_algorithms.NoiseResult.plot_log_rebinned","text":"Plot PSD rebinned into logarithmically spaced frequency bins. Source code in mass2/core/noise_algorithms.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def plot_log_rebinned ( self , bins_per_decade : int = 10 , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , ** plotkwarg : dict , ) -> None : \"\"\"Plot PSD rebinned into logarithmically spaced frequency bins.\"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . psd [ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies [ 1 :] # define logarithmically spaced bin edges fmin , fmax = freq [ 0 ], freq [ - 1 ] n_decades = np . log10 ( fmax / fmin ) n_bins = int ( bins_per_decade * n_decades ) bin_edges = np . logspace ( np . log10 ( fmin ), np . log10 ( fmax ), n_bins + 1 ) # digitize frequencies into bins inds = np . digitize ( freq , bin_edges ) # average PSD per bin binned_freqs = np . zeros ( n_bins , dtype = float ) binned_psd = np . zeros ( n_bins , dtype = float ) for i in range ( 1 , len ( bin_edges )): mask = inds == i if np . any ( mask ): binned_freqs [ i - 1 ] = np . exp ( np . mean ( np . log ( freq [ mask ]))) # geometric mean binned_psd [ i - 1 ] = np . mean ( psd [ mask ]) if sqrt_psd : axis . plot ( binned_freqs , np . sqrt ( binned_psd ), ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( binned_freqs , binned_psd , ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) axis . set_xscale ( \"log\" ) axis . set_yscale ( \"log\" ) axis . grid ( True , which = \"both\" ) axis . set_xlabel ( \"Frequency (Hz)\" ) axis . set_title ( f \"Log-rebinned noise from { len ( self . frequencies ) * 2 - 2 } samples\" ) axis . figure . tight_layout ()","title":"plot_log_rebinned"},{"location":"docstrings/#mass2.core.noise_algorithms.calc_autocorrelation_times","text":"Compute the timesteps for an autocorrelation function Parameters: n ( int ) \u2013 Number of lags dt ( float ) \u2013 Sample time Returns: NDArray \u2013 The time delays for each lag Source code in mass2/core/noise_algorithms.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def calc_autocorrelation_times ( n : int , dt : float ) -> NDArray : \"\"\"Compute the timesteps for an autocorrelation function Parameters ---------- n : int Number of lags dt : float Sample time Returns ------- NDArray The time delays for each lag \"\"\" return np . arange ( n ) * dt","title":"calc_autocorrelation_times"},{"location":"docstrings/#mass2.core.noise_algorithms.calc_continuous_autocorrelation","text":"Calculate the autocorrelation of the input data, assuming the entire array is continuous. Parameters: data ( ArrayLike ) \u2013 Data to be autocorrelated. Arrays of 2+ dimensions will be converted to a 1D array via .ravel() . n_lags ( int ) \u2013 Compute the autocorrelation for lags in the range [0, n_lags-1] . max_excursion ( int , default: 1000 ) \u2013 Chunks of data with max absolute excursion from the mean this large will be excluded from the calculation, by default 1000 Returns: NDArray \u2013 The autocorrelation array Raises: ValueError \u2013 If the data are too short to provide the requested number of lags, or the data contain apparent pulses. Source code in mass2/core/noise_algorithms.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def calc_continuous_autocorrelation ( data : ArrayLike , n_lags : int , max_excursion : int = 1000 ) -> NDArray : \"\"\"Calculate the autocorrelation of the input data, assuming the entire array is continuous. Parameters ---------- data : ArrayLike Data to be autocorrelated. Arrays of 2+ dimensions will be converted to a 1D array via `.ravel()`. n_lags : int Compute the autocorrelation for lags in the range `[0, n_lags-1]`. max_excursion : int, optional Chunks of data with max absolute excursion from the mean this large will be excluded from the calculation, by default 1000 Returns ------- NDArray The autocorrelation array Raises ------ ValueError If the data are too short to provide the requested number of lags, or the data contain apparent pulses. \"\"\" data = np . asarray ( data ) . ravel () n_data = len ( data ) assert n_lags < n_data def padded_length ( n : int ) -> int : \"\"\"Return a sensible number in the range [n, 2n] which is not too much larger than n, yet is good for FFTs. Returns: A number: (1, 3, or 5)*(a power of two), whichever is smallest. \"\"\" pow2 = np . round ( 2 ** np . ceil ( np . log2 ( n ))) if n == pow2 : return int ( n ) elif n > 0.75 * pow2 : return int ( pow2 ) elif n > 0.625 * pow2 : return int ( np . round ( 0.75 * pow2 )) else : return int ( np . round ( 0.625 * pow2 )) # When there are 10 million data points and only 10,000 lags wanted, # it's hugely inefficient to compute the full autocorrelation, especially # in memory. Instead, compute it on chunks several times the length of the desired # correlation, and average. CHUNK_MULTIPLE = 15 if n_data < CHUNK_MULTIPLE * n_lags : msg = f \"There are too few data values ( { n_data =} ) to compute at least { n_lags } lags.\" raise ValueError ( msg ) # Be sure to pad chunksize samples by AT LEAST n_lags zeros, to prevent # unwanted wraparound in the autocorrelation. # padded_data is what we do DFT/InvDFT on; ac is the unnormalized output. chunksize = CHUNK_MULTIPLE * n_lags padsize = n_lags padded_data = np . zeros ( padded_length ( padsize + chunksize ), dtype = float ) ac = np . zeros ( n_lags , dtype = float ) entries = 0 Nchunks = n_data // chunksize datachunks = data [: Nchunks * chunksize ] . reshape ( Nchunks , chunksize ) for data in datachunks : padded_data [: chunksize ] = data - np . asarray ( data ) . mean () if np . abs ( padded_data ) . max () > max_excursion : continue ft = np . fft . rfft ( padded_data ) ft [ 0 ] = 0 # this redundantly removes the mean of the data set power = ( ft * ft . conj ()) . real acsum = np . fft . irfft ( power ) ac += acsum [: n_lags ] entries += 1 if entries == 0 : raise ValueError ( \"Apparently all 'noise' chunks had large excursions from baseline, so no autocorrelation was computed\" ) ac /= entries ac /= np . arange ( chunksize , chunksize - n_lags + 0.5 , - 1.0 , dtype = float ) return ac","title":"calc_continuous_autocorrelation"},{"location":"docstrings/#mass2.core.noise_algorithms.calc_discontinuous_autocorrelation","text":"Calculate the autocorrelation of the input data, assuming the rows of the array are NOT continuous in time. Parameters: data ( ArrayLike ) \u2013 A 2D array of noise data. Shape is (ntraces, nsamples) . max_excursion ( int , default: 1000 ) \u2013 description , by default 1000 Returns: NDArray \u2013 The mean autocorrelation of the rows (\"traces\") in the input data , from lags [0, nsamples-1] . Source code in mass2/core/noise_algorithms.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def calc_discontinuous_autocorrelation ( data : ArrayLike , max_excursion : int = 1000 ) -> NDArray : \"\"\"Calculate the autocorrelation of the input data, assuming the rows of the array are NOT continuous in time. Parameters ---------- data : ArrayLike A 2D array of noise data. Shape is `(ntraces, nsamples)`. max_excursion : int, optional _description_, by default 1000 Returns ------- NDArray The mean autocorrelation of the rows (\"traces\") in the input `data`, from lags `[0, nsamples-1]`. \"\"\" data = np . asarray ( data ) ntraces , nsamples = data . shape ac = np . zeros ( nsamples , dtype = float ) traces_used = 0 for i in range ( ntraces ): pulse = data [ i , :] - data [ i , :] . mean () if np . abs ( pulse ) . max () > max_excursion : continue ac += np . correlate ( pulse , pulse , \"full\" )[ nsamples - 1 :] traces_used += 1 ac /= traces_used ac /= nsamples - np . arange ( nsamples , dtype = float ) return ac","title":"calc_discontinuous_autocorrelation"},{"location":"docstrings/#mass2.core.noise_algorithms.calc_noise_result","text":"Analyze the noise as Mass has always done. Compute autocorrelation with a lower noise at longer lags when data are known to be continuous Subtract the mean before computing the power spectrum Parameters: data ( ArrayLike ) \u2013 A 2d array of noise data, of shape (npulses, nsamples) dt ( float ) \u2013 Periodic sampling time, in seconds continuous ( bool ) \u2013 Whether the \"pulses\" in the data array are continuous in time window ( callable , default: None ) \u2013 A function to compute a data window (or if None, no windowing), by default None Returns: NoiseResult \u2013 The derived noise spectrum and autocorrelation Source code in mass2/core/noise_algorithms.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def calc_noise_result ( data : ArrayLike , dt : float , continuous : bool , window : Callable | None = None , skip_autocorr_if_length_over : int = 10000 ) -> \"NoiseResult\" : \"\"\"Analyze the noise as Mass has always done. * Compute autocorrelation with a lower noise at longer lags when data are known to be continuous * Subtract the mean before computing the power spectrum Parameters ---------- data : ArrayLike A 2d array of noise data, of shape `(npulses, nsamples)` dt : float Periodic sampling time, in seconds continuous : bool Whether the \"pulses\" in the `data` array are continuous in time window : callable, optional A function to compute a data window (or if None, no windowing), by default None Returns ------- NoiseResult The derived noise spectrum and autocorrelation \"\"\" data = np . asarray ( data ) data_zeromean = data - np . mean ( data ) ( n_pulses , nsamples ) = data_zeromean . shape # see test_ravel_behavior to be sure this is written correctly f_mass , psd_mass = mass2 . mathstat . power_spectrum . computeSpectrum ( data_zeromean . ravel (), segfactor = n_pulses , dt = dt , window = window ) if nsamples <= skip_autocorr_if_length_over : if continuous : autocorr_vec = calc_continuous_autocorrelation ( data_zeromean . ravel (), n_lags = nsamples ) else : autocorr_vec = calc_discontinuous_autocorrelation ( data_zeromean ) else : print ( \"\"\"warning: noise_psd_mass skipping autocorrelation calculation for long traces, use skip_autocorr_if_length_over argument to override this\"\"\" ) autocorr_vec = None return NoiseResult ( psd = psd_mass , autocorr_vec = autocorr_vec , frequencies = f_mass )","title":"calc_noise_result"},{"location":"docstrings/#mass2.core.noise_algorithms.noise_psd_periodogram","text":"Compute the noise power spectral density using scipy's periodogram function and the autocorrelation. Source code in mass2/core/noise_algorithms.py 151 152 153 154 155 156 157 158 159 def noise_psd_periodogram ( data : ndarray , dt : float , window : ArrayLike | str = \"boxcar\" , detrend : bool = False ) -> \"NoiseResult\" : \"\"\"Compute the noise power spectral density using scipy's periodogram function and the autocorrelation.\"\"\" f , Pxx = sp . signal . periodogram ( data , fs = 1 / dt , window = window , axis =- 1 , detrend = detrend ) # len(f) = data.shape[1]//2+1 # Pxx[i, j] is the PSD at frequency f[j] for the i\u2011th trace data[i, :] Pxx_mean = np . mean ( Pxx , axis = 0 ) # Pxx_mean[j] is the averaged PSD at frequency f[j] over all traces autocorr_vec = calc_discontinuous_autocorrelation ( data ) return NoiseResult ( psd = Pxx_mean , autocorr_vec = autocorr_vec , frequencies = f ) Hold a class to represent a channel with noise data only, and to analyze its noise characteristics.","title":"noise_psd_periodogram"},{"location":"docstrings/#mass2.core.noise_channel.NoiseChannel","text":"A class to represent a channel with noise data only, and to analyze its noise characteristics. Source code in mass2/core/noise_channel.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @dataclass ( frozen = True ) class NoiseChannel : \"\"\"A class to represent a channel with noise data only, and to analyze its noise characteristics.\"\"\" df : pl . DataFrame # DO NOT MUTATE THIS!!! header_df : pl . DataFrame # DO NOT MUTATE THIS!! frametime_s : float # @functools.cache def calc_max_excursion ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 ) -> tuple [ pl . DataFrame , float ]: \"\"\"Compute the maximum excursion from the median for each noise record, and store in dataframe.\"\"\" def excursion2d ( noise_trace : NDArray ) -> float : \"\"\"Return the excursion (max - min) for each trace in a 2D array of traces.\"\"\" return np . amax ( noise_trace , axis = 1 ) - np . amin ( noise_trace , axis = 1 ) noise_traces = self . df . limit ( n_limit )[ trace_col_name ] . to_numpy () excursion = excursion2d ( noise_traces ) max_excursion = mass2 . misc . outlier_resistant_nsigma_above_mid ( excursion , nsigma = excursion_nsigma ) df_noise2 = self . df . limit ( n_limit ) . with_columns ( excursion = excursion ) return df_noise2 , max_excursion def get_records_2d ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 , trunc_front : int = 0 , trunc_back : int = 0 , ) -> NDArray : \"\"\" Return a 2D NumPy array of cleaned noise traces from the specified column. This method identifies noise traces with excursions below a threshold and optionally truncates the beginning and/or end of each trace. Parameters: ---------- trace_col_name : str, optional Name of the column containing trace data. Default is \"pulse\". n_limit : int, optional Maximum number of traces to analyze. Default is 10000. excursion_nsigma : float, optional Threshold for maximum excursion in units of noise sigma. Default is 5. trunc_front : int, optional Number of samples to truncate from the front of each trace. Default is 0. trunc_back : int, optional Number of samples to truncate from the back of each trace. Must be >= 0. Default is 0. Returns: ------- np.ndarray A 2D array of cleaned and optionally truncated noise traces. Shape: (n_pulses, len(pulse)) \"\"\" df_noise2 , max_excursion = self . calc_max_excursion ( trace_col_name , n_limit , excursion_nsigma ) noise_traces_clean = df_noise2 . filter ( pl . col ( \"excursion\" ) <= max_excursion )[ \"pulse\" ] . to_numpy () if trunc_back == 0 : noise_traces_clean2 = noise_traces_clean [:, trunc_front :] elif trunc_back > 0 : noise_traces_clean2 = noise_traces_clean [:, trunc_front : - trunc_back ] else : raise ValueError ( \"trunc_back must be >= 0\" ) assert noise_traces_clean2 . shape [ 0 ] > 0 return noise_traces_clean2 # @functools.cache def spectrum ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 , trunc_front : int = 0 , trunc_back : int = 0 , skip_autocorr_if_length_over : int = 10000 , ) -> NoiseResult : \"\"\"Compute and return the noise result from the noise traces.\"\"\" records = self . get_records_2d ( trace_col_name , n_limit , excursion_nsigma , trunc_front , trunc_back ) spectrum = mass2 . core . noise_algorithms . calc_noise_result ( records , continuous = self . is_continuous , dt = self . frametime_s , skip_autocorr_if_length_over = skip_autocorr_if_length_over ) return spectrum def __hash__ ( self ) -> int : \"\"\"A hash function based on the object's id.\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other : Any ) -> bool : \"\"\"Equality based on object identity.\"\"\" return id ( self ) == id ( other ) @property def is_continuous ( self ) -> bool : \"Whether this channel is continuous data (True) or triggered records with arbitrary gaps (False).\" if \"continuous\" in self . header_df : return self . header_df [ \"continuous\" ][ 0 ] return False @classmethod def from_ljh ( cls , path : str | Path ) -> \"NoiseChannel\" : \"\"\"Create a NoiseChannel by loading data from the given LJH file path.\"\"\" ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars () noise_channel = cls ( df , header_df , header_df [ \"Timebase\" ][ 0 ]) return noise_channel","title":"NoiseChannel"},{"location":"docstrings/#mass2.core.noise_channel.NoiseChannel.is_continuous","text":"Whether this channel is continuous data (True) or triggered records with arbitrary gaps (False).","title":"is_continuous"},{"location":"docstrings/#mass2.core.noise_channel.NoiseChannel.__eq__","text":"Equality based on object identity. Source code in mass2/core/noise_channel.py 108 109 110 def __eq__ ( self , other : Any ) -> bool : \"\"\"Equality based on object identity.\"\"\" return id ( self ) == id ( other )","title":"__eq__"},{"location":"docstrings/#mass2.core.noise_channel.NoiseChannel.__hash__","text":"A hash function based on the object's id. Source code in mass2/core/noise_channel.py 101 102 103 104 105 106 def __hash__ ( self ) -> int : \"\"\"A hash function based on the object's id.\"\"\" # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self ))","title":"__hash__"},{"location":"docstrings/#mass2.core.noise_channel.NoiseChannel.calc_max_excursion","text":"Compute the maximum excursion from the median for each noise record, and store in dataframe. Source code in mass2/core/noise_channel.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def calc_max_excursion ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 ) -> tuple [ pl . DataFrame , float ]: \"\"\"Compute the maximum excursion from the median for each noise record, and store in dataframe.\"\"\" def excursion2d ( noise_trace : NDArray ) -> float : \"\"\"Return the excursion (max - min) for each trace in a 2D array of traces.\"\"\" return np . amax ( noise_trace , axis = 1 ) - np . amin ( noise_trace , axis = 1 ) noise_traces = self . df . limit ( n_limit )[ trace_col_name ] . to_numpy () excursion = excursion2d ( noise_traces ) max_excursion = mass2 . misc . outlier_resistant_nsigma_above_mid ( excursion , nsigma = excursion_nsigma ) df_noise2 = self . df . limit ( n_limit ) . with_columns ( excursion = excursion ) return df_noise2 , max_excursion","title":"calc_max_excursion"},{"location":"docstrings/#mass2.core.noise_channel.NoiseChannel.from_ljh","text":"Create a NoiseChannel by loading data from the given LJH file path. Source code in mass2/core/noise_channel.py 119 120 121 122 123 124 125 @classmethod def from_ljh ( cls , path : str | Path ) -> \"NoiseChannel\" : \"\"\"Create a NoiseChannel by loading data from the given LJH file path.\"\"\" ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars () noise_channel = cls ( df , header_df , header_df [ \"Timebase\" ][ 0 ]) return noise_channel","title":"from_ljh"},{"location":"docstrings/#mass2.core.noise_channel.NoiseChannel.get_records_2d","text":"Return a 2D NumPy array of cleaned noise traces from the specified column. This method identifies noise traces with excursions below a threshold and optionally truncates the beginning and/or end of each trace. Parameters: trace_col_name : str, optional Name of the column containing trace data. Default is \"pulse\". n_limit : int, optional Maximum number of traces to analyze. Default is 10000. excursion_nsigma : float, optional Threshold for maximum excursion in units of noise sigma. Default is 5. trunc_front : int, optional Number of samples to truncate from the front of each trace. Default is 0. trunc_back : int, optional Number of samples to truncate from the back of each trace. Must be >= 0. Default is 0. Returns: np.ndarray A 2D array of cleaned and optionally truncated noise traces. Shape: (n_pulses, len(pulse)) Source code in mass2/core/noise_channel.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def get_records_2d ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 , trunc_front : int = 0 , trunc_back : int = 0 , ) -> NDArray : \"\"\" Return a 2D NumPy array of cleaned noise traces from the specified column. This method identifies noise traces with excursions below a threshold and optionally truncates the beginning and/or end of each trace. Parameters: ---------- trace_col_name : str, optional Name of the column containing trace data. Default is \"pulse\". n_limit : int, optional Maximum number of traces to analyze. Default is 10000. excursion_nsigma : float, optional Threshold for maximum excursion in units of noise sigma. Default is 5. trunc_front : int, optional Number of samples to truncate from the front of each trace. Default is 0. trunc_back : int, optional Number of samples to truncate from the back of each trace. Must be >= 0. Default is 0. Returns: ------- np.ndarray A 2D array of cleaned and optionally truncated noise traces. Shape: (n_pulses, len(pulse)) \"\"\" df_noise2 , max_excursion = self . calc_max_excursion ( trace_col_name , n_limit , excursion_nsigma ) noise_traces_clean = df_noise2 . filter ( pl . col ( \"excursion\" ) <= max_excursion )[ \"pulse\" ] . to_numpy () if trunc_back == 0 : noise_traces_clean2 = noise_traces_clean [:, trunc_front :] elif trunc_back > 0 : noise_traces_clean2 = noise_traces_clean [:, trunc_front : - trunc_back ] else : raise ValueError ( \"trunc_back must be >= 0\" ) assert noise_traces_clean2 . shape [ 0 ] > 0 return noise_traces_clean2","title":"get_records_2d"},{"location":"docstrings/#mass2.core.noise_channel.NoiseChannel.spectrum","text":"Compute and return the noise result from the noise traces. Source code in mass2/core/noise_channel.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def spectrum ( self , trace_col_name : str = \"pulse\" , n_limit : int = 10000 , excursion_nsigma : float = 5 , trunc_front : int = 0 , trunc_back : int = 0 , skip_autocorr_if_length_over : int = 10000 , ) -> NoiseResult : \"\"\"Compute and return the noise result from the noise traces.\"\"\" records = self . get_records_2d ( trace_col_name , n_limit , excursion_nsigma , trunc_front , trunc_back ) spectrum = mass2 . core . noise_algorithms . calc_noise_result ( records , continuous = self . is_continuous , dt = self . frametime_s , skip_autocorr_if_length_over = skip_autocorr_if_length_over ) return spectrum Code copied from Mass version 1. Not up to date with latest style. Sorry. Supported off versions: 0.1.0 has projectors and basis in json with base64 encoding 0.2.0 has projectors and basis after json as binary 0.3.0 adds pretrigDelta field","title":"spectrum"},{"location":"docstrings/#mass2.core.offfiles.OffFile","text":"Working with an OFF file: off = OffFile(\"filename\") print off.dtype # show the fields available off[0] # get record 0 off[0][\"coefs\"] # get the model coefs for record 0 x,y = off.recordXY(0) plot(x,y) # plot record 0 Source code in mass2/core/offfiles.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 class OffFile : \"\"\" Working with an OFF file: off = OffFile(\"filename\") print off.dtype # show the fields available off[0] # get record 0 off[0][\"coefs\"] # get the model coefs for record 0 x,y = off.recordXY(0) plot(x,y) # plot record 0 \"\"\" def __init__ ( self , filename : str ): self . filename = filename with open ( self . filename , \"rb\" ) as f : self . headerString = readJsonString ( f ) # self.headerStringLength = f.tell() # doesn't work on windows because readline uses a readahead buffer self . headerStringLength = len ( self . headerString ) self . header = json . loads ( self . headerString ) self . dtype = recordDtype ( self . header [ \"FileFormatVersion\" ], self . header [ \"NumberOfBases\" ]) self . _dtype_non_descriptive = recordDtype ( self . header [ \"FileFormatVersion\" ], self . header [ \"NumberOfBases\" ], descriptive_coefs_names = False ) self . framePeriodSeconds = float ( self . header [ \"FramePeriodSeconds\" ]) # Estimate subframe division rate. If explicitly given in ReadoutInfo, use that. # Otherwise, if it's a Lancero-TDM system, then we know it's equal to the # of rows. # Otherwise, we don't know any way to estimate subframe divisions. (But add it if you think of any!) self . subframediv : int | None = None try : self . subframediv = self . header [ \"ReadoutInfo\" ][ \"Subframedivisions\" ] except KeyError : try : if self . header [ \"CreationInfo\" ][ \"SourceName\" ] == \"Lancero\" : self . subframediv = self . header [ \"ReadoutInfo\" ][ \"NumberOfRows\" ] except KeyError : self . subframediv = None self . validateHeader () self . _mmap : np . memmap | None = None self . projectors : NDArray | None = None self . basis : NDArray | None = None self . _decodeModelInfo () # calculates afterHeaderPos used by _updateMmap self . _updateMmap () def close ( self ) -> None : \"\"\"Close the memory map and projectors and basis memmaps\"\"\" del self . _mmap del self . projectors del self . basis def validateHeader ( self ) -> None : \"Check that the header looks like we expect, with a valid version code\" with open ( self . filename , \"rb\" ) as f : f . seek ( self . headerStringLength - 2 ) if not f . readline () . decode ( \"utf-8\" ) == \"} \\n \" : raise Exception ( \"failed to find end of header\" ) if self . header [ \"FileFormat\" ] != \"OFF\" : raise Exception ( \"FileFormatVersion is {} , want OFF\" . format ( self . header [ \"FileFormatVersion\" ])) def _updateMmap ( self , _nRecords : int | None = None ) -> None : \"\"\"Memory map an OFF file's data. `_nRecords` maps only a subset--designed for testing only \"\"\" fileSize = os . path . getsize ( self . filename ) recordSize = fileSize - self . afterHeaderPos if _nRecords is None : self . nRecords = recordSize // self . dtype . itemsize else : # for testing only self . nRecords = _nRecords self . _mmap = np . memmap ( self . filename , self . dtype , mode = \"r\" , offset = self . afterHeaderPos , shape = ( self . nRecords ,)) self . shape = self . _mmap . shape def __getitem__ ( self , * args : Any , ** kwargs : Any ) -> Any : \"Make indexing into the off the same as indexing into the memory mapped array\" assert self . _mmap is not None return self . _mmap . __getitem__ ( * args , ** kwargs ) def __len__ ( self ) -> int : \"\"\"Number of records in the OFF file\"\"\" assert self . _mmap is not None return len ( self . _mmap ) def __sizeof__ ( self ) -> int : \"\"\"Size of the memory mapped array in bytes\"\"\" assert self . _mmap is not None return self . _mmap . __sizeof__ () def _decodeModelInfo ( self ) -> None : \"\"\"Decode the model info (projectors and basis) from the OFF file, either from base64 in json or a later proprietary, binary format\"\"\" if ( \"RowMajorFloat64ValuesBase64\" in self . header [ \"ModelInfo\" ][ \"Projectors\" ] and \"RowMajorFloat64ValuesBase64\" in self . header [ \"ModelInfo\" ][ \"Basis\" ] ): # should only be in version 0.1.0 files self . _decodeModelInfoBase64 () else : self . _decodeModelInfoMmap () def _decodeModelInfoBase64 ( self ) -> None : \"\"\"Decode the model info (projectors and basis) from the OFF file, from base64 in json.\"\"\" projectorsData = decodebytes ( self . header [ \"ModelInfo\" ][ \"Projectors\" ][ \"RowMajorFloat64ValuesBase64\" ] . encode ()) projectorsRows = int ( self . header [ \"ModelInfo\" ][ \"Projectors\" ][ \"Rows\" ]) projectorsCols = int ( self . header [ \"ModelInfo\" ][ \"Projectors\" ][ \"Cols\" ]) self . projectors = np . frombuffer ( projectorsData , np . float64 ) self . projectors = self . projectors . reshape (( projectorsRows , projectorsCols )) basisData = decodebytes ( self . header [ \"ModelInfo\" ][ \"Basis\" ][ \"RowMajorFloat64ValuesBase64\" ] . encode ()) basisRows = int ( self . header [ \"ModelInfo\" ][ \"Basis\" ][ \"Rows\" ]) basisCols = int ( self . header [ \"ModelInfo\" ][ \"Basis\" ][ \"Cols\" ]) self . basis = np . frombuffer ( basisData , np . float64 ) self . basis = self . basis . reshape (( basisRows , basisCols )) if basisRows != projectorsCols or basisCols != projectorsRows or self . header [ \"NumberOfBases\" ] != projectorsRows : raise Exception ( \"basis shape should be transpose of projectors shape. have basis \" f \"( { basisCols } , { basisRows } ), projectors ( { projectorsCols } , { projectorsRows } ), \" f \"NumberOfBases { self . header [ 'NumberOfBases' ] } \" ) self . afterHeaderPos = self . headerStringLength def _decodeModelInfoMmap ( self ) -> None : \"\"\"Decode the model info (projectors and basis) from the OFF file, from binary.\"\"\" projectorsRows = int ( self . header [ \"ModelInfo\" ][ \"Projectors\" ][ \"Rows\" ]) projectorsCols = int ( self . header [ \"ModelInfo\" ][ \"Projectors\" ][ \"Cols\" ]) basisRows = int ( self . header [ \"ModelInfo\" ][ \"Basis\" ][ \"Rows\" ]) basisCols = int ( self . header [ \"ModelInfo\" ][ \"Basis\" ][ \"Cols\" ]) # 8 for float64, basis and projectors have the same number of elements and therefore of bytes nBytes = basisCols * basisRows * 8 projectorsPos = self . headerStringLength basisPos = projectorsPos + nBytes self . afterHeaderPos = basisPos + nBytes self . projectors = np . memmap ( self . filename , np . float64 , mode = \"r\" , offset = projectorsPos , shape = ( projectorsRows , projectorsCols )) self . basis = np . memmap ( self . filename , np . float64 , mode = \"r\" , offset = basisPos , shape = ( basisRows , basisCols )) if basisRows != projectorsCols or basisCols != projectorsRows or self . header [ \"NumberOfBases\" ] != projectorsRows : raise Exception ( \"basis shape should be transpose of projectors shape. have basis \" f \"( { basisCols } , { basisRows } ), projectors ( { projectorsCols } , { projectorsRows } ), \" f \"NumberOfBases { self . header [ 'NumberOfBases' ] } \" ) def __repr__ ( self ) -> str : \"\"\"Return a string representation of the OffFile object.\"\"\" return \"<OFF file> {} , {} records, {} length basis \\n \" . format ( self . filename , self . nRecords , self . header [ \"NumberOfBases\" ]) def sampleTimes ( self , i : int ) -> NDArray : \"\"\"return a vector of sample times for record i, approriate for plotting\"\"\" recordSamples = self [ i ][ \"recordSamples\" ] recordPreSamples = self [ i ][ \"recordPreSamples\" ] return np . arange ( - recordPreSamples , recordSamples - recordPreSamples ) * self . framePeriodSeconds def modeledPulse ( self , i : int ) -> NDArray : \"\"\"return a vector of the modeled pulse samples, the best available value of the actual raw samples\"\"\" # projectors has size (n,z) where it is (rows,cols) # basis has size (z,n) # coefs has size (n,1) # coefs (n,1) = projectors (n,z) * data (z,1) # modelData (z,1) = basis (z,n) * coefs (n,1) # n = number of basis (eg 3) # z = record length (eg 4) # .view(self._dtype_non_descriptive) should be a copy-free way of changing # the dtype so we can access the coefs all together assert self . basis is not None allVals = np . matmul ( self . basis , self . _mmap_with_coefs [ i ][ \"coefs\" ]) return np . asarray ( allVals ) def recordXY ( self , i : int ) -> tuple [ NDArray , NDArray ]: \"\"\"return (x,y) for record i, where x is time and y is modeled pulse\"\"\" return self . sampleTimes ( i ), self . modeledPulse ( i ) @property def _mmap_with_coefs ( self ) -> NDArray : \"\"\"Return a view of the memmap with the coefs all together in one field\"\"\" assert self . _mmap is not None return self . _mmap . view ( self . _dtype_non_descriptive ) def view ( self , * args : Any ) -> NDArray : \"\"\"Return a view of the memmap with the given args\"\"\" assert self . _mmap is not None return self . _mmap . view ( * args )","title":"OffFile"},{"location":"docstrings/#mass2.core.offfiles.OffFile.__getitem__","text":"Make indexing into the off the same as indexing into the memory mapped array Source code in mass2/core/offfiles.py 143 144 145 146 def __getitem__ ( self , * args : Any , ** kwargs : Any ) -> Any : \"Make indexing into the off the same as indexing into the memory mapped array\" assert self . _mmap is not None return self . _mmap . __getitem__ ( * args , ** kwargs )","title":"__getitem__"},{"location":"docstrings/#mass2.core.offfiles.OffFile.__len__","text":"Number of records in the OFF file Source code in mass2/core/offfiles.py 148 149 150 151 def __len__ ( self ) -> int : \"\"\"Number of records in the OFF file\"\"\" assert self . _mmap is not None return len ( self . _mmap )","title":"__len__"},{"location":"docstrings/#mass2.core.offfiles.OffFile.__repr__","text":"Return a string representation of the OffFile object. Source code in mass2/core/offfiles.py 210 211 212 def __repr__ ( self ) -> str : \"\"\"Return a string representation of the OffFile object.\"\"\" return \"<OFF file> {} , {} records, {} length basis \\n \" . format ( self . filename , self . nRecords , self . header [ \"NumberOfBases\" ])","title":"__repr__"},{"location":"docstrings/#mass2.core.offfiles.OffFile.__sizeof__","text":"Size of the memory mapped array in bytes Source code in mass2/core/offfiles.py 153 154 155 156 def __sizeof__ ( self ) -> int : \"\"\"Size of the memory mapped array in bytes\"\"\" assert self . _mmap is not None return self . _mmap . __sizeof__ ()","title":"__sizeof__"},{"location":"docstrings/#mass2.core.offfiles.OffFile.close","text":"Close the memory map and projectors and basis memmaps Source code in mass2/core/offfiles.py 115 116 117 118 119 def close ( self ) -> None : \"\"\"Close the memory map and projectors and basis memmaps\"\"\" del self . _mmap del self . projectors del self . basis","title":"close"},{"location":"docstrings/#mass2.core.offfiles.OffFile.modeledPulse","text":"return a vector of the modeled pulse samples, the best available value of the actual raw samples Source code in mass2/core/offfiles.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def modeledPulse ( self , i : int ) -> NDArray : \"\"\"return a vector of the modeled pulse samples, the best available value of the actual raw samples\"\"\" # projectors has size (n,z) where it is (rows,cols) # basis has size (z,n) # coefs has size (n,1) # coefs (n,1) = projectors (n,z) * data (z,1) # modelData (z,1) = basis (z,n) * coefs (n,1) # n = number of basis (eg 3) # z = record length (eg 4) # .view(self._dtype_non_descriptive) should be a copy-free way of changing # the dtype so we can access the coefs all together assert self . basis is not None allVals = np . matmul ( self . basis , self . _mmap_with_coefs [ i ][ \"coefs\" ]) return np . asarray ( allVals )","title":"modeledPulse"},{"location":"docstrings/#mass2.core.offfiles.OffFile.recordXY","text":"return (x,y) for record i, where x is time and y is modeled pulse Source code in mass2/core/offfiles.py 236 237 238 def recordXY ( self , i : int ) -> tuple [ NDArray , NDArray ]: \"\"\"return (x,y) for record i, where x is time and y is modeled pulse\"\"\" return self . sampleTimes ( i ), self . modeledPulse ( i )","title":"recordXY"},{"location":"docstrings/#mass2.core.offfiles.OffFile.sampleTimes","text":"return a vector of sample times for record i, approriate for plotting Source code in mass2/core/offfiles.py 214 215 216 217 218 def sampleTimes ( self , i : int ) -> NDArray : \"\"\"return a vector of sample times for record i, approriate for plotting\"\"\" recordSamples = self [ i ][ \"recordSamples\" ] recordPreSamples = self [ i ][ \"recordPreSamples\" ] return np . arange ( - recordPreSamples , recordSamples - recordPreSamples ) * self . framePeriodSeconds","title":"sampleTimes"},{"location":"docstrings/#mass2.core.offfiles.OffFile.validateHeader","text":"Check that the header looks like we expect, with a valid version code Source code in mass2/core/offfiles.py 121 122 123 124 125 126 127 128 def validateHeader ( self ) -> None : \"Check that the header looks like we expect, with a valid version code\" with open ( self . filename , \"rb\" ) as f : f . seek ( self . headerStringLength - 2 ) if not f . readline () . decode ( \"utf-8\" ) == \"} \\n \" : raise Exception ( \"failed to find end of header\" ) if self . header [ \"FileFormat\" ] != \"OFF\" : raise Exception ( \"FileFormatVersion is {} , want OFF\" . format ( self . header [ \"FileFormatVersion\" ]))","title":"validateHeader"},{"location":"docstrings/#mass2.core.offfiles.OffFile.view","text":"Return a view of the memmap with the given args Source code in mass2/core/offfiles.py 246 247 248 249 def view ( self , * args : Any ) -> NDArray : \"\"\"Return a view of the memmap with the given args\"\"\" assert self . _mmap is not None return self . _mmap . view ( * args )","title":"view"},{"location":"docstrings/#mass2.core.offfiles.readJsonString","text":"look in file f for a line \"}\\n\" and return all contents up to that point for an OFF file this can be parsed by json.dumps and all remaining data is records Source code in mass2/core/offfiles.py 57 58 59 60 61 62 63 64 65 66 67 68 def readJsonString ( f : io . BufferedReader ) -> str : \"\"\"look in file f for a line \"}\\\\n\" and return all contents up to that point for an OFF file this can be parsed by json.dumps and all remaining data is records\"\"\" lines : list [ str ] = [] while True : line = f . readline () . decode ( \"utf-8\" ) lines += line if line == \"} \\n \" : return \"\" . join ( lines ) if len ( line ) == 0 : raise Exception ( \"\"\"reached end of file without finding an end-of-JSON line \"} \\\\ n\" \"\"\" )","title":"readJsonString"},{"location":"docstrings/#mass2.core.offfiles.recordDtype","text":"return a np.dtype matching the record datatype for the given offVersion and nBasis descriptive_coefs_names - determines how the modeled pulse coefficients are name, you usually want True For True, the names will be derivLike , pulseLike , and if nBasis>3, also extraCoefs For False, they will all have the single name coefs . False is to make implementing recordXY and other methods that want access to all coefs simultaneously easier Source code in mass2/core/offfiles.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def recordDtype ( offVersion : str , nBasis : int , descriptive_coefs_names : bool = True ) -> np . dtype : \"\"\"return a np.dtype matching the record datatype for the given offVersion and nBasis descriptive_coefs_names - determines how the modeled pulse coefficients are name, you usually want True For True, the names will be `derivLike`, `pulseLike`, and if nBasis>3, also `extraCoefs` For False, they will all have the single name `coefs`. False is to make implementing recordXY and other methods that want access to all coefs simultaneously easier\"\"\" if offVersion in { \"0.1.0\" , \"0.2.0\" }: # start of the dtype is identical for all cases dt_list : list [ tuple [ str , type ] | tuple [ str , type , int ]] = [ ( \"recordSamples\" , np . int32 ), ( \"recordPreSamples\" , np . int32 ), ( \"framecount\" , np . int64 ), ( \"unixnano\" , np . int64 ), ( \"pretriggerMean\" , np . float32 ), ( \"residualStdDev\" , np . float32 ), ] elif offVersion == \"0.3.0\" : dt_list = [ ( \"recordSamples\" , np . int32 ), ( \"recordPreSamples\" , np . int32 ), ( \"framecount\" , np . int64 ), ( \"unixnano\" , np . int64 ), ( \"pretriggerMean\" , np . float32 ), ( \"pretriggerDelta\" , np . float32 ), ( \"residualStdDev\" , np . float32 ), ] else : raise Exception ( f \"dtype for OFF version { offVersion } not implemented\" ) if descriptive_coefs_names : dt_list += [( \"pulseMean\" , np . float32 ), ( \"derivativeLike\" , np . float32 ), ( \"filtValue\" , np . float32 )] if nBasis > 3 : dt_list += [( \"extraCoefs\" , np . float32 , ( nBasis - 3 ))] else : dt_list += [( \"coefs\" , np . float32 , ( nBasis ))] return np . dtype ( dt_list ) Classes to create time-domain and Fourier-domain optimal filters.","title":"recordDtype"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter","text":"Bases: ABC A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level. Returns: Filter \u2013 A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted variance due to noise and the resulting predicted_v_over_dv , the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: nominal_peak . Source code in mass2/core/optimal_filtering.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 @dataclass ( frozen = True ) class Filter ( ABC ): \"\"\"A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level. Returns ------- Filter A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted `variance` due to noise and the resulting `predicted_v_over_dv`, the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: `nominal_peak`. \"\"\" values : np . ndarray nominal_peak : float variance : float predicted_v_over_dv : float dt_values : np . ndarray | None const_values : np . ndarray | None signal_model : np . ndarray | None dt_model : np . ndarray | None convolution_lags : int = 1 fmax : float | None = None f_3db : float | None = None cut_pre : int = 0 cut_post : int = 0 @property @abstractmethod def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property @abstractmethod def _filter_type ( self ) -> str : \"\"\"The name for this filter type\"\"\" return \"illegal: this is supposed to be an abstract base class\" def plot ( self , axis : plt . Axes | None = None , ** kwargs : Any ) -> None : \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout () def report ( self , std_energy : float = 5898.8 ) -> None : \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" ) @abstractmethod def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records.\"\"\" pass","title":"Filter"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.is_arrival_time_safe","text":"Is this an arrival-time-safe filter?","title":"is_arrival_time_safe"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.filter_records","text":"Filter one microcalorimeter record or an array of records. Source code in mass2/core/optimal_filtering.py 316 317 318 319 @abstractmethod def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records.\"\"\" pass","title":"filter_records"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.plot","text":"Make a plot of the filter Parameters: axis ( Axes , default: None ) \u2013 A pre-existing axis to plot on, by default None Source code in mass2/core/optimal_filtering.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def plot ( self , axis : plt . Axes | None = None , ** kwargs : Any ) -> None : \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout ()","title":"plot"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.report","text":"Report on estimated V/dV for the filter. Parameters: std_energy ( float , default: 5898.8 ) \u2013 Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 Source code in mass2/core/optimal_filtering.py 302 303 304 305 306 307 308 309 310 311 312 313 314 def report ( self , std_energy : float = 5898.8 ) -> None : \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" )","title":"report"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag","text":"Bases: Filter Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015. Returns: Filter5Lag \u2013 An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 @dataclass ( frozen = True ) class Filter5Lag ( Filter ): \"\"\"Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015. Returns ------- Filter5Lag An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ) -> None : \"\"\"Post-init checks that this filter, indeed, is a 5-lag one\"\"\" assert self . convolution_lags == 5 @property def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property def _filter_type ( self ) -> str : \"\"\"Name for this filter type\"\"\" return \"5lag\" # These parameters fit a parabola to any 5 evenly-spaced points FIVELAG_FITTER = ( np . array ( ( ( - 6 , 24 , 34 , 24 , - 6 ), ( - 14 , - 7 , 0 , 7 , 14 ), ( 10 , - 5 , - 10 , - 5 , 10 ), ), dtype = float , ) / 70.0 ) def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x","title":"Filter5Lag"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag.is_arrival_time_safe","text":"Is this an arrival-time-safe filter?","title":"is_arrival_time_safe"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag.__post_init__","text":"Post-init checks that this filter, indeed, is a 5-lag one Source code in mass2/core/optimal_filtering.py 334 335 336 def __post_init__ ( self ) -> None : \"\"\"Post-init checks that this filter, indeed, is a 5-lag one\"\"\" assert self . convolution_lags == 5","title":"__post_init__"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag.filter_records","text":"Filter one microcalorimeter record or an array of records. Parameters: x ( ArrayLike ) \u2013 A 1-d array, a single pulse record, or a 2-d array, where x[i, :] is pulse record number i . Returns: tuple [ ndarray , ndarray ] \u2013 The optimally filtered value, or an array (one per row) if the input is a 2-d array. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises: AssertionError \u2013 If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x","title":"filter_records"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS","text":"Bases: Filter Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015. Returns: FilterATS \u2013 An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 @dataclass ( frozen = True ) class FilterATS ( Filter ): \"\"\"Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015. Returns ------- FilterATS An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ) -> None : \"\"\"Post-init checks that this filter aexpects one lag, and has a dt_values array\"\"\" assert self . convolution_lags == 1 assert self . dt_values is not None @property def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return True @property def _filter_type ( self ) -> str : \"\"\"Return the name for this filter type\"\"\" return \"ats\" def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values )","title":"FilterATS"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS.is_arrival_time_safe","text":"Is this an arrival-time-safe filter?","title":"is_arrival_time_safe"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS.__post_init__","text":"Post-init checks that this filter aexpects one lag, and has a dt_values array Source code in mass2/core/optimal_filtering.py 412 413 414 415 def __post_init__ ( self ) -> None : \"\"\"Post-init checks that this filter aexpects one lag, and has a dt_values array\"\"\" assert self . convolution_lags == 1 assert self . dt_values is not None","title":"__post_init__"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS.filter_records","text":"Filter one microcalorimeter record or an array of records. Parameters: x ( ArrayLike ) \u2013 A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns: tuple [ ndarray , ndarray ] \u2013 The optimally filtered value, or an array (one per row) if the input is a 2-d array. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises: AssertionError \u2013 If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values )","title":"filter_records"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker","text":"An object capable of creating optimal filter based on a single signal and noise set. Parameters: signal_model ( ArrayLike ) \u2013 The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the peak value of this filter (that is, peak value relative to the baseline level). n_pretrigger ( int ) \u2013 The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only n_pretrigger samples at the start of a record. noise_autocorr ( Optional [ ArrayLike ] , default: None ) \u2013 The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of avg_signal . noise_psd ( Optional [ ArrayLike ] , default: None ) \u2013 The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of avg_signal , and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method compute_fourier() will not work. whitener ( Optional [ ToeplitzWhitener ] , default: None ) \u2013 An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes noise_autocorr if both are given. sample_time_sec ( float , default: 0.0 ) \u2013 The time step between samples in avg_signal and noise_autocorr (in seconds). This must be given if fmax or f_3db are ever to be used. peak ( float , default: 0.0 ) \u2013 The peak amplitude of the standard signal Notes If both noise_autocorr and whitener are None, then methods compute_5lag and compute_ats will both fail, as they require a time-domain characterization of the noise. The units of noise_autocorr are the square of the units used in signal_model and/or peak . The units of whitener are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. The units of noise_psd are square signal units, per Hertz. Returns: FilterMaker \u2013 An object that can make a variety of optimal filters, assuming a single signal and noise analysis. Source code in mass2/core/optimal_filtering.py 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 @dataclass ( frozen = True ) class FilterMaker : \"\"\"An object capable of creating optimal filter based on a single signal and noise set. Parameters --------- signal_model : ArrayLike The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the *peak value* of this filter (that is, peak value relative to the baseline level). n_pretrigger : int The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only `n_pretrigger` samples at the start of a record. noise_autocorr : Optional[ArrayLike] The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of `avg_signal`. noise_psd : Optional[ArrayLike] The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of `avg_signal`, and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method `compute_fourier()` will not work. whitener : Optional[ToeplitzWhitener] An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes `noise_autocorr` if both are given. sample_time_sec : float The time step between samples in `avg_signal` and `noise_autocorr` (in seconds). This must be given if `fmax` or `f_3db` are ever to be used. peak : float The peak amplitude of the standard signal Notes ----- * If both `noise_autocorr` and `whitener` are None, then methods `compute_5lag` and `compute_ats` will both fail, as they require a time-domain characterization of the noise. * The units of `noise_autocorr` are the square of the units used in `signal_model` and/or `peak`. The units of `whitener` are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. * The units of `noise_psd` are square signal units, per Hertz. Returns ------- FilterMaker An object that can make a variety of optimal filters, assuming a single signal and noise analysis. \"\"\" signal_model : NDArray n_pretrigger : int noise_autocorr : NDArray | None = None noise_psd : NDArray | None = None dt_model : NDArray | None = None whitener : ToeplitzWhitener | None = None sample_time_sec : float = 0.0 peak : float = 0.0 def compute_constrained_5lag ( self , constraints : ArrayLike | None = None , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 , ) -> Filter : \"\"\"Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- constraints: ndarray, optional The vector or vectors to which the filter should be orthogonal. If a 2d array, each _row_ is a constraint, and the number of columns should be equal to the len(self.signal_model) minus `(cut_pre+cut_post)`. fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) shorten = 2 # for 5-lag convolution truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" pulse_model = np . vstack (( truncated_signal , np . ones_like ( truncated_signal ))) if constraints is not None : pulse_model = np . vstack (( pulse_model , constraints )) assert pulse_model . shape [ 1 ] == n noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_model = np . vstack ([ TS ( r ) for r in pulse_model ]) A = pulse_model . dot ( Rinv_model . T ) all_filters = np . linalg . solve ( A , Rinv_model ) filt_noconst = all_filters [ 0 ] band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) if variance <= 0 : vdv = np . inf else : vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post ) def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" return self . compute_constrained_5lag ( None , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) def compute_5lag_noexp ( self , exp_time_seconds : float , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- exp_time_seconds: float Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" assert exp_time_seconds > 0 n = len ( self . signal_model ) - 4 - ( cut_pre + cut_post ) log_per_sample = self . sample_time_sec / exp_time_seconds constraint = np . exp ( - np . arange ( n ) * log_per_sample ) return self . compute_constrained_5lag ( constraint , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , ) def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post ) def _compute_autocorr ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> np . ndarray : \"\"\"Return the noise autocorrelation, if any, cut down by the requested number of values at the start and end. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- np.ndarray The noise autocorrelation of the appropriate length. Or a length-0 array if not known. \"\"\" # If there's an autocorrelation, cut it down to length. if self . noise_autocorr is None : return np . array ([], dtype = float ) N = len ( np . asarray ( self . signal_model )) return np . asarray ( self . noise_autocorr )[: N - ( cut_pre + cut_post )] def _normalize_signal ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> tuple [ np . ndarray , float , np . ndarray ]: \"\"\"Compute the normalized signal, peak value, and first-order arrival-time model. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- tuple[np.ndarray, float, np.ndarray] (sig, pk, dsig), where `sig` is the nominal signal model (normalized to have unit amplitude), `pk` is the peak values of the nominal signal, and `dsig` is the delta between `sig` that differ by one sample in arrival time. The `dsig` will be an empty array if no arrival-time model is known. Raises ------ ValueError If negative numbers of samples are to be cut, or the entire record is to be cut. \"\"\" avg_signal = np . array ( self . signal_model ) ns = len ( avg_signal ) pre_avg = avg_signal [ cut_pre : self . n_pretrigger - 1 ] . mean () if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) # Unless passed in, find the signal's peak value. This is normally peak=(max-pretrigger). # If signal is negative-going, however, then peak=(pretrigger-min). if self . peak > 0.0 : peak_signal = self . peak else : a = avg_signal [ cut_pre : ns - cut_post ] . min () b = avg_signal [ cut_pre : ns - cut_post ] . max () is_negative = pre_avg - a > b - pre_avg if is_negative : peak_signal = a - pre_avg else : peak_signal = b - pre_avg # avg_signal: normalize to have unit peak avg_signal -= pre_avg rescale = 1 / np . max ( avg_signal ) avg_signal *= rescale avg_signal [: self . n_pretrigger ] = 0.0 avg_signal = avg_signal [ cut_pre : ns - cut_post ] if self . dt_model is None : dt_model = np . array ([], dtype = float ) else : dt_model = self . dt_model * rescale dt_model = dt_model [ cut_pre : ns - cut_post ] return avg_signal , peak_signal , dt_model @staticmethod def _normalize_5lag_filter ( f : np . ndarray , avg_signal : np . ndarray ) -> None : \"\"\"Rescale 5-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) <= len ( avg_signal ) - 4 conv = np . zeros ( 5 , dtype = float ) for i in range ( 5 ): conv [ i ] = np . dot ( f , avg_signal [ i : i + len ( f )]) x = np . linspace ( - 2 , 2 , 5 ) fit = np . polyfit ( x , conv , 2 ) fit_ctr = - 0.5 * fit [ 1 ] / fit [ 0 ] fit_peak = np . polyval ( fit , fit_ctr ) f *= 1.0 / fit_peak @staticmethod def _normalize_filter ( f : np . ndarray , avg_signal : np . ndarray ) -> None : \"\"\"Rescale single-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) == len ( avg_signal ) f *= 1 / np . dot ( f , avg_signal )","title":"FilterMaker"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_5lag","text":"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" return self . compute_constrained_5lag ( None , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post )","title":"compute_5lag"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_5lag_noexp","text":"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: exp_time_seconds ( float ) \u2013 Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 def compute_5lag_noexp ( self , exp_time_seconds : float , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- exp_time_seconds: float Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" assert exp_time_seconds > 0 n = len ( self . signal_model ) - 4 - ( cut_pre + cut_post ) log_per_sample = self . sample_time_sec / exp_time_seconds constraint = np . exp ( - np . arange ( n ) * log_per_sample ) return self . compute_constrained_5lag ( constraint , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post )","title":"compute_5lag_noexp"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_ats","text":"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 An arrival-time-safe optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post )","title":"compute_ats"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_constrained_5lag","text":"Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: constraints ( ArrayLike | None , default: None ) \u2013 The vector or vectors to which the filter should be orthogonal. If a 2d array, each row is a constraint, and the number of columns should be equal to the len(self.signal_model) minus (cut_pre+cut_post) . fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 def compute_constrained_5lag ( self , constraints : ArrayLike | None = None , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 , ) -> Filter : \"\"\"Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- constraints: ndarray, optional The vector or vectors to which the filter should be orthogonal. If a 2d array, each _row_ is a constraint, and the number of columns should be equal to the len(self.signal_model) minus `(cut_pre+cut_post)`. fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) shorten = 2 # for 5-lag convolution truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" pulse_model = np . vstack (( truncated_signal , np . ones_like ( truncated_signal ))) if constraints is not None : pulse_model = np . vstack (( pulse_model , constraints )) assert pulse_model . shape [ 1 ] == n noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_model = np . vstack ([ TS ( r ) for r in pulse_model ]) A = pulse_model . dot ( Rinv_model . T ) all_filters = np . linalg . solve ( A , Rinv_model ) filt_noconst = all_filters [ 0 ] band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) if variance <= 0 : vdv = np . inf else : vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post )","title":"compute_constrained_5lag"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_fourier","text":"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter, computed in the Fourier domain. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , )","title":"compute_fourier"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener","text":"An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: tw.whiten(v) returns Wv; it is equivalent to tw(v) tw.solveWT(v) returns inv(W')*v tw.applyWT(v) returns W'v tw.solveW(v) returns inv(W)*v Arguments theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients Returns: ToeplitzWhitener \u2013 Object that can perform approximate, time-invariant noise whitening. Raises: ValueError \u2013 If the operative methods are passed an array of dimension higher than 2. Source code in mass2/core/optimal_filtering.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @dataclass ( frozen = True ) class ToeplitzWhitener : \"\"\"An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: * `tw.whiten(v)` returns Wv; it is equivalent to `tw(v)` * `tw.solveWT(v)` returns inv(W')*v * `tw.applyWT(v)` returns W'v * `tw.solveW(v)` returns inv(W)*v Arguments --------- theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients Returns ------- ToeplitzWhitener Object that can perform approximate, time-invariant noise whitening. Raises ------ ValueError If the operative methods are passed an array of dimension higher than 2. \"\"\" theta : np . ndarray phi : np . ndarray @property def p ( self ) -> int : \"\"\"Return the autoregressive order\"\"\" return len ( self . phi ) - 1 @property def q ( self ) -> int : \"\"\"Return the moving-average order\"\"\" return len ( self . theta ) - 1 def whiten ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v ) def __call__ ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y def solveW ( self , v : ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y def solveWT ( self , v : ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :] def applyWT ( self , v : ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :] def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR )","title":"ToeplitzWhitener"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.p","text":"Return the autoregressive order","title":"p"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.q","text":"Return the moving-average order","title":"q"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.W","text":"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. Source code in mass2/core/optimal_filtering.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR )","title":"W"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.__call__","text":"Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __call__ ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y","title":"__call__"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.applyWT","text":"Return vector (or matrix of column vectors) W'v Source code in mass2/core/optimal_filtering.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def applyWT ( self , v : ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :]","title":"applyWT"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.solveW","text":"Return unwhitened vector (or matrix of column vectors) inv(W)*v Source code in mass2/core/optimal_filtering.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def solveW ( self , v : ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y","title":"solveW"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.solveWT","text":"Return vector (or matrix of column vectors) inv(W')*v Source code in mass2/core/optimal_filtering.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def solveWT ( self , v : ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :]","title":"solveWT"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.whiten","text":"Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 68 69 70 def whiten ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v )","title":"whiten"},{"location":"docstrings/#mass2.core.optimal_filtering.band_limit","text":"Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input modelmatrix in-place. No effect if both fmax and f_3db are None . Parameters: modelmatrix ( ndarray ) \u2013 The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec ( float ) \u2013 The sampling period, normally in seconds. fmax ( Optional [ float ] ) \u2013 The hard maximum frequency (units are inverse of sample_time_sec units, or Hz) f_3db ( Optional [ float ] ) \u2013 The 1-pole low-pass filter's 3 dB point (same units as fmax ) Source code in mass2/core/optimal_filtering.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def band_limit ( modelmatrix : np . ndarray , sample_time_sec : float , fmax : float | None , f_3db : float | None ) -> None : \"\"\"Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input `modelmatrix` in-place. No effect if both `fmax` and `f_3db` are `None`. Parameters ---------- modelmatrix : np.ndarray The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec : float The sampling period, normally in seconds. fmax : Optional[float] The hard maximum frequency (units are inverse of `sample_time_sec` units, or Hz) f_3db : Optional[float] The 1-pole low-pass filter's 3 dB point (same units as `fmax`) \"\"\" if fmax is None and f_3db is None : return # Handle the 2D case by calling this function once per column. assert len ( modelmatrix . shape ) <= 2 if len ( modelmatrix . shape ) == 2 : for i in range ( modelmatrix . shape [ 1 ]): band_limit ( modelmatrix [:, i ], sample_time_sec , fmax , f_3db ) return vector = modelmatrix filt_length = len ( vector ) sig_ft = np . fft . rfft ( vector ) freq = np . fft . fftfreq ( filt_length , d = sample_time_sec ) freq = np . abs ( freq [: len ( sig_ft )]) if fmax is not None : sig_ft [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft /= 1.0 + ( 1.0 * freq / f_3db ) ** 2 # n=filt_length is needed when filt_length is ODD vector [:] = np . fft . irfft ( sig_ft , n = filt_length )","title":"band_limit"},{"location":"docstrings/#mass2.core.optimal_filtering.bracketR","text":"Return the dot product (q^T R q) for vector and matrix R constructed from the vector by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). Source code in mass2/core/optimal_filtering.py 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 def bracketR ( q : NDArray , noise : NDArray ) -> float : \"\"\"Return the dot product (q^T R q) for vector <q> and matrix R constructed from the vector <noise> by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). \"\"\" if len ( noise ) < len ( q ): raise ValueError ( f \"Vector q (length { len ( q ) } ) cannot be longer than the noise (length { len ( noise ) } )\" ) n = len ( q ) r = np . zeros ( 2 * n - 1 , dtype = float ) r [ n - 1 :] = noise [: n ] r [ n - 1 :: - 1 ] = noise [: n ] dot = 0.0 for i in range ( n ): dot += q [ i ] * r [ n - i - 1 : 2 * n - i - 1 ] . dot ( q ) return dot Classes and functions to correct for arrival-time bias in optimal filtering.","title":"bracketR"},{"location":"docstrings/#mass2.core.phase_correct.PhaseCorrector","text":"A class to correct for arrival-time bias in optimal filtering. Source code in mass2/core/phase_correct.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class PhaseCorrector : \"\"\"A class to correct for arrival-time bias in optimal filtering.\"\"\" version = 1 def __init__ ( self , phase_uniformifier_x : ArrayLike , phase_uniformifier_y : ArrayLike , corrections : list [ CubicSpline ], indicatorName : str , uncorrectedName : str , ): self . corrections = corrections self . phase_uniformifier_x = np . array ( phase_uniformifier_x ) self . phase_uniformifier_y = np . array ( phase_uniformifier_y ) self . indicatorName = tostr ( indicatorName ) self . uncorrectedName = tostr ( uncorrectedName ) self . phase_uniformifier = CubicSpline ( self . phase_uniformifier_x , self . phase_uniformifier_y ) def toHDF5 ( self , hdf5_group : h5py . Group , name : str = \"phase_correction\" , overwrite : bool = False ) -> None : \"\"\"Write to the given HDF5 group for later recovery from disk (by fromHDF5 class method).\"\"\" group = hdf5_group . require_group ( name ) def h5group_update ( name : str , vector : ArrayLike ) -> None : \"Overwrite or create a dataset in the given group.\" if name in group : if overwrite : del group [ name ] else : raise AttributeError ( f \"Cannot overwrite phase correction dataset ' { name } '\" ) group [ name ] = vector h5group_update ( \"phase_uniformifier_x\" , self . phase_uniformifier_x ) h5group_update ( \"phase_uniformifier_y\" , self . phase_uniformifier_y ) h5group_update ( \"uncorrected_name\" , self . uncorrectedName ) h5group_update ( \"indicator_name\" , self . indicatorName ) h5group_update ( \"version\" , self . version ) for i , correction in enumerate ( self . corrections ): h5group_update ( f \"correction_ { i } _x\" , correction . _x ) h5group_update ( f \"correction_ { i } _y\" , correction . _y ) def correct ( self , phase : ArrayLike , ph : ArrayLike ) -> NDArray : \"\"\"Apply the phase correction to the given data `ph`.\"\"\" ph = np . asarray ( ph ) # attempt to force phases to fall between X and X phase_uniformified = np . asarray ( phase ) - self . phase_uniformifier ( ph ) # Compute a correction for each pulse for each correction-line energy # For the actual correction, don't let |ph| > 0.6 sample phase_clipped = np . clip ( phase_uniformified , - 0.6 , 0.6 ) pheight_corrected = _phase_corrected_filtvals ( phase_clipped , ph , self . corrections ) return pheight_corrected def __call__ ( self , phase_indicator : ArrayLike , ph : ArrayLike ) -> NDArray : \"Equivalent to self.correct()\" return self . correct ( phase_indicator , ph ) @classmethod def fromHDF5 ( cls , hdf5_group : h5py . Group , name : str = \"phase_correction\" ) -> \"PhaseCorrector\" : \"\"\"Recover a PhaseCorrector object from the given HDF5 group.\"\"\" x = hdf5_group [ f \" { name } /phase_uniformifier_x\" ][()] y = hdf5_group [ f \" { name } /phase_uniformifier_y\" ][()] uncorrectedName = tostr ( hdf5_group [ f \" { name } /uncorrected_name\" ][()]) indicatorName = tostr ( hdf5_group [ f \" { name } /indicator_name\" ][()]) version = hdf5_group [ f \" { name } /version\" ][()] i = 0 corrections = [] while f \" { name } /correction_ { i } _x\" in hdf5_group : _x = hdf5_group [ f \" { name } /correction_ { i } _x\" ][()] _y = hdf5_group [ f \" { name } /correction_ { i } _y\" ][()] corrections . append ( CubicSpline ( _x , _y )) i += 1 assert version == cls . version return cls ( x , y , corrections , indicatorName , uncorrectedName ) def __repr__ ( self ) -> str : \"\"\"String representation of this object.\"\"\" s = f \"\"\"PhaseCorrector with splines at this many levels: { len ( self . corrections ) } phase_uniformifier_x: { self . phase_uniformifier_x } phase_uniformifier_y: { self . phase_uniformifier_y } uncorrectedName: { self . uncorrectedName } \"\"\" return s","title":"PhaseCorrector"},{"location":"docstrings/#mass2.core.phase_correct.PhaseCorrector.__call__","text":"Equivalent to self.correct() Source code in mass2/core/phase_correct.py 71 72 73 def __call__ ( self , phase_indicator : ArrayLike , ph : ArrayLike ) -> NDArray : \"Equivalent to self.correct()\" return self . correct ( phase_indicator , ph )","title":"__call__"},{"location":"docstrings/#mass2.core.phase_correct.PhaseCorrector.__repr__","text":"String representation of this object. Source code in mass2/core/phase_correct.py 93 94 95 96 97 98 99 100 101 def __repr__ ( self ) -> str : \"\"\"String representation of this object.\"\"\" s = f \"\"\"PhaseCorrector with splines at this many levels: { len ( self . corrections ) } phase_uniformifier_x: { self . phase_uniformifier_x } phase_uniformifier_y: { self . phase_uniformifier_y } uncorrectedName: { self . uncorrectedName } \"\"\" return s","title":"__repr__"},{"location":"docstrings/#mass2.core.phase_correct.PhaseCorrector.correct","text":"Apply the phase correction to the given data ph . Source code in mass2/core/phase_correct.py 60 61 62 63 64 65 66 67 68 69 def correct ( self , phase : ArrayLike , ph : ArrayLike ) -> NDArray : \"\"\"Apply the phase correction to the given data `ph`.\"\"\" ph = np . asarray ( ph ) # attempt to force phases to fall between X and X phase_uniformified = np . asarray ( phase ) - self . phase_uniformifier ( ph ) # Compute a correction for each pulse for each correction-line energy # For the actual correction, don't let |ph| > 0.6 sample phase_clipped = np . clip ( phase_uniformified , - 0.6 , 0.6 ) pheight_corrected = _phase_corrected_filtvals ( phase_clipped , ph , self . corrections ) return pheight_corrected","title":"correct"},{"location":"docstrings/#mass2.core.phase_correct.PhaseCorrector.fromHDF5","text":"Recover a PhaseCorrector object from the given HDF5 group. Source code in mass2/core/phase_correct.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 @classmethod def fromHDF5 ( cls , hdf5_group : h5py . Group , name : str = \"phase_correction\" ) -> \"PhaseCorrector\" : \"\"\"Recover a PhaseCorrector object from the given HDF5 group.\"\"\" x = hdf5_group [ f \" { name } /phase_uniformifier_x\" ][()] y = hdf5_group [ f \" { name } /phase_uniformifier_y\" ][()] uncorrectedName = tostr ( hdf5_group [ f \" { name } /uncorrected_name\" ][()]) indicatorName = tostr ( hdf5_group [ f \" { name } /indicator_name\" ][()]) version = hdf5_group [ f \" { name } /version\" ][()] i = 0 corrections = [] while f \" { name } /correction_ { i } _x\" in hdf5_group : _x = hdf5_group [ f \" { name } /correction_ { i } _x\" ][()] _y = hdf5_group [ f \" { name } /correction_ { i } _y\" ][()] corrections . append ( CubicSpline ( _x , _y )) i += 1 assert version == cls . version return cls ( x , y , corrections , indicatorName , uncorrectedName )","title":"fromHDF5"},{"location":"docstrings/#mass2.core.phase_correct.PhaseCorrector.toHDF5","text":"Write to the given HDF5 group for later recovery from disk (by fromHDF5 class method). Source code in mass2/core/phase_correct.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def toHDF5 ( self , hdf5_group : h5py . Group , name : str = \"phase_correction\" , overwrite : bool = False ) -> None : \"\"\"Write to the given HDF5 group for later recovery from disk (by fromHDF5 class method).\"\"\" group = hdf5_group . require_group ( name ) def h5group_update ( name : str , vector : ArrayLike ) -> None : \"Overwrite or create a dataset in the given group.\" if name in group : if overwrite : del group [ name ] else : raise AttributeError ( f \"Cannot overwrite phase correction dataset ' { name } '\" ) group [ name ] = vector h5group_update ( \"phase_uniformifier_x\" , self . phase_uniformifier_x ) h5group_update ( \"phase_uniformifier_y\" , self . phase_uniformifier_y ) h5group_update ( \"uncorrected_name\" , self . uncorrectedName ) h5group_update ( \"indicator_name\" , self . indicatorName ) h5group_update ( \"version\" , self . version ) for i , correction in enumerate ( self . corrections ): h5group_update ( f \"correction_ { i } _x\" , correction . _x ) h5group_update ( f \"correction_ { i } _y\" , correction . _y )","title":"toHDF5"},{"location":"docstrings/#mass2.core.phase_correct.phase_correct","text":"Create a PhaseCorrector object to correct for arrival-time bias in optimal filtering. Source code in mass2/core/phase_correct.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def phase_correct ( phase : ArrayLike , pheight : ArrayLike , ph_peaks : ArrayLike | None = None , method2017 : bool = True , kernel_width : float | None = None , indicatorName : str = \"\" , uncorrectedName : str = \"\" , ) -> PhaseCorrector : \"\"\"Create a PhaseCorrector object to correct for arrival-time bias in optimal filtering.\"\"\" phase = np . asarray ( phase ) pheight = np . asarray ( pheight ) if ph_peaks is None : ph_peaks = _find_peaks_heuristic ( pheight ) ph_peaks = np . asarray ( ph_peaks ) if len ( ph_peaks ) <= 0 : raise ValueError ( \"Could not phase_correct because no peaks found\" ) ph_peaks . sort () # Compute a correction function at each line in ph_peaks corrections = [] median_phase = [] if kernel_width is None : kernel_width = np . max ( ph_peaks ) / 1000.0 for pk in ph_peaks : nextcorr , mphase = _phasecorr_find_alignment ( phase , pheight , pk , 0.012 * np . mean ( ph_peaks ), method2017 = method2017 , kernel_width = kernel_width ) corrections . append ( nextcorr ) median_phase . append ( mphase ) NC = len ( corrections ) if NC > 3 : phase_uniformifier_x = ph_peaks phase_uniformifier_y = np . array ( median_phase ) else : # Too few peaks to spline, so just bin and take the median per bin, then # interpolated (approximating) spline through/near these points. NBINS = 10 top = min ( pheight . max (), 1.2 * np . percentile ( pheight , 98 )) bin = np . digitize ( pheight , np . linspace ( 0 , top , 1 + NBINS )) - 1 x = np . zeros ( NBINS , dtype = float ) y = np . zeros ( NBINS , dtype = float ) w = np . zeros ( NBINS , dtype = float ) for i in range ( NBINS ): w [ i ] = ( bin == i ) . sum () if w [ i ] == 0 : continue x [ i ] = np . median ( pheight [ bin == i ]) y [ i ] = np . median ( phase [ bin == i ]) nonempty = w > 0 # Use sp.interpolate.UnivariateSpline because it can make an approximating # spline. But then use its x/y data and knots to create a Mass CubicSpline, # because that one can have natural boundary conditions instead of insane # cubic functions in the extrapolation. if nonempty . sum () > 1 : spline_order = min ( 3 , nonempty . sum () - 1 ) crazy_spline = sp . interpolate . UnivariateSpline ( x [ nonempty ], y [ nonempty ], w = w [ nonempty ] * ( 12 **- 0.5 ), k = spline_order ) phase_uniformifier_x = crazy_spline . _data [ 0 ] phase_uniformifier_y = crazy_spline . _data [ 1 ] else : phase_uniformifier_x = np . array ([ 0 , 0 , 0 , 0 ]) phase_uniformifier_y = np . array ([ 0 , 0 , 0 , 0 ]) return PhaseCorrector ( phase_uniformifier_x , phase_uniformifier_y , corrections , indicatorName , uncorrectedName ) Phase correction step, Mass-style.","title":"phase_correct"},{"location":"docstrings/#mass2.core.phase_correct_steps.PhaseCorrectMassStep","text":"Bases: RecipeStep Perform a Mass-style phase correction step. Source code in mass2/core/phase_correct_steps.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @dataclass ( frozen = True ) class PhaseCorrectMassStep ( RecipeStep ): \"\"\"Perform a Mass-style phase correction step.\"\"\" line_names : list [ str ] line_energies : list [ float ] previous_step_index : int phase_corrector : PhaseCorrector def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the phase-corrected pulse height and return a new DataFrame.\"\"\" # since we only need to load two columns I'm assuming we can fit them in memory and just # loading them whole # if it becomes an issues, use iter_slices or # add a user defined funciton in rust indicator_col , uncorrected_col = self . inputs corrected_col = self . output [ 0 ] indicator = df [ indicator_col ] . to_numpy () uncorrected = df [ uncorrected_col ] . to_numpy () corrected = self . phase_corrector ( indicator , uncorrected ) series = pl . Series ( corrected_col , corrected ) df2 = df . with_columns ( series ) return df2 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a diagnostic plot of the phase correction.\"\"\" indicator_col , uncorrected_col = self . inputs df_small = df_after . lazy () . filter ( self . good_expr ) . filter ( self . use_expr ) . select ( self . inputs + self . output ) . collect () mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ uncorrected_col ]) mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ self . output [ 0 ]], plt . gca (), ) plt . legend () plt . tight_layout () return plt . gca ()","title":"PhaseCorrectMassStep"},{"location":"docstrings/#mass2.core.phase_correct_steps.PhaseCorrectMassStep.calc_from_df","text":"Calculate the phase-corrected pulse height and return a new DataFrame. Source code in mass2/core/phase_correct_steps.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the phase-corrected pulse height and return a new DataFrame.\"\"\" # since we only need to load two columns I'm assuming we can fit them in memory and just # loading them whole # if it becomes an issues, use iter_slices or # add a user defined funciton in rust indicator_col , uncorrected_col = self . inputs corrected_col = self . output [ 0 ] indicator = df [ indicator_col ] . to_numpy () uncorrected = df [ uncorrected_col ] . to_numpy () corrected = self . phase_corrector ( indicator , uncorrected ) series = pl . Series ( corrected_col , corrected ) df2 = df . with_columns ( series ) return df2","title":"calc_from_df"},{"location":"docstrings/#mass2.core.phase_correct_steps.PhaseCorrectMassStep.dbg_plot","text":"Make a diagnostic plot of the phase correction. Source code in mass2/core/phase_correct_steps.py 40 41 42 43 44 45 46 47 48 49 50 51 52 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a diagnostic plot of the phase correction.\"\"\" indicator_col , uncorrected_col = self . inputs df_small = df_after . lazy () . filter ( self . good_expr ) . filter ( self . use_expr ) . select ( self . inputs + self . output ) . collect () mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ uncorrected_col ]) mass2 . misc . plot_a_vs_b_series ( df_small [ indicator_col ], df_small [ self . output [ 0 ]], plt . gca (), ) plt . legend () plt . tight_layout () return plt . gca ()","title":"dbg_plot"},{"location":"docstrings/#mass2.core.phase_correct_steps.phase_correct_mass_specific_lines","text":"Perform a Mass-style phase correction step using specific lines. Source code in mass2/core/phase_correct_steps.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def phase_correct_mass_specific_lines ( ch : Channel , indicator_col : str , uncorrected_col : str , corrected_col : str , previous_step_index : int , line_names : Iterable [ str | float ], use_expr : pl . Expr , ) -> PhaseCorrectMassStep : \"\"\"Perform a Mass-style phase correction step using specific lines.\"\"\" previous_step , previous_step_index = ch . get_step ( previous_step_index ) assert hasattr ( previous_step , \"energy2ph\" ) ( line_names , line_energies ) = mass2 . calibration . algorithms . line_names_and_energies ( line_names ) line_positions = [ previous_step . energy2ph ( line_energy ) for line_energy in line_energies ] [ indicator , uncorrected ] = ch . good_serieses ([ indicator_col , uncorrected_col ], use_expr = use_expr ) phase_corrector = mass2 . core . phase_correct . phase_correct ( indicator . to_numpy (), uncorrected . to_numpy (), line_positions , indicatorName = indicator_col , uncorrectedName = uncorrected_col , ) return PhaseCorrectMassStep ( inputs = [ indicator_col , uncorrected_col ], output = [ corrected_col ], good_expr = ch . good_expr , use_expr = use_expr , line_names = line_names , line_energies = line_energies , previous_step_index = previous_step_index , phase_corrector = phase_corrector , ) Pulse summarizing algorithms.","title":"phase_correct_mass_specific_lines"},{"location":"docstrings/#mass2.core.pulse_algorithms.fit_pulse_2exp_with_tail","text":"Fit a pulse shape to data using two exponentials plus an exponential tail. Source code in mass2/core/pulse_algorithms.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def fit_pulse_2exp_with_tail ( data : ArrayLike , npre : int , dt : float = 1 , guess_tau : float | None = None ) -> LineModelResult : \"\"\"Fit a pulse shape to data using two exponentials plus an exponential tail.\"\"\" data = np . asarray ( data ) if guess_tau is None : guess_tau = dt * len ( data ) / 5 model = lmfit . Model ( pulse_2exp_with_tail ) baseline = np . amin ( data ) params = model . make_params ( t0 = npre * dt , a_tail = data [ 0 ] - baseline , baseline = baseline , a = np . amax ( data ) - baseline , tau_tail = guess_tau , tau_rise = guess_tau , tau_fall_factor = 2.0 , ) params [ \"a_tail\" ] . set ( min = 0 ) params [ \"a\" ] . set ( min = 0 ) params [ \"tau_tail\" ] . set ( min = dt / 5 ) params [ \"tau_rise\" ] . set ( min = dt / 5 ) params [ \"tau_fall_factor\" ] . set ( min = 1 ) params . add ( \"tau_fall\" , expr = \"tau_rise*tau_fall_factor\" ) result = model . fit ( data , params , t = np . arange ( len ( data )) * dt ) return result","title":"fit_pulse_2exp_with_tail"},{"location":"docstrings/#mass2.core.pulse_algorithms.pulse_2exp_with_tail","text":"Create a pulse shape from two exponentials plus an exponential tail. Source code in mass2/core/pulse_algorithms.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def pulse_2exp_with_tail ( t : ArrayLike , t0 : float , a_tail : float , tau_tail : float , a : float , tau_rise : float , tau_fall_factor : float , baseline : float ) -> NDArray : \"\"\"Create a pulse shape from two exponentials plus an exponential tail.\"\"\" tt = np . asarray ( t ) - t0 tau_fall = tau_rise * tau_fall_factor assert tau_fall_factor >= 1 if tau_fall_factor > 1 : # location of peak t_peak = ( tau_rise * tau_fall ) / ( tau_fall - tau_rise ) * np . log ( tau_fall / tau_rise ) # value at peak max_val = np . exp ( - t_peak / tau_fall ) - np . exp ( - t_peak / tau_rise ) else : # tau_fall == tau_rise max_val = 1 / np . e return ( a_tail * np . exp ( - tt / tau_tail ) / np . exp ( - tt [ 0 ] / tau_tail ) # normalized tail + a * ( np . exp ( - tt / tau_fall ) - np . exp ( - tt / tau_rise )) * np . greater ( tt , 0 ) / max_val + baseline )","title":"pulse_2exp_with_tail"},{"location":"docstrings/#mass2.core.pulse_algorithms.summarize_data_numba","text":"Summarize one segment of the data file, loading it into cache. Source code in mass2/core/pulse_algorithms.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 @njit def summarize_data_numba ( # noqa: PLR0914 rawdata : NDArray [ np . uint16 ], timebase : float , peak_samplenumber : int , pretrigger_ignore_samples : int , nPresamples : int , ) -> ResultArrayType : \"\"\"Summarize one segment of the data file, loading it into cache.\"\"\" nPulses = rawdata . shape [ 0 ] nSamples = rawdata . shape [ 1 ] e_nPresamples = nPresamples - pretrigger_ignore_samples # Create the structured array for results results = np . zeros ( nPulses , dtype = result_dtype ) for j in range ( nPulses ): pulse = rawdata [ j , :] pretrig_sum = 0.0 pretrig_rms_sum = 0.0 pulse_sum = 0.0 pulse_rms_sum = 0.0 promptness_sum = 0.0 peak_value = 0 peak_index = 0 min_value = np . iinfo ( np . uint16 ) . max s_prompt = nPresamples + 2 e_prompt = nPresamples + 8 for k in range ( nSamples ): signal = pulse [ k ] if signal > peak_value : peak_value = signal peak_index = k min_value = min ( min_value , signal ) if k < e_nPresamples : pretrig_sum += signal pretrig_rms_sum += signal ** 2 if s_prompt <= k < e_prompt : promptness_sum += signal if k == nPresamples - 1 : ptm = pretrig_sum / e_nPresamples ptrms = np . sqrt ( pretrig_rms_sum / e_nPresamples - ptm ** 2 ) if signal - ptm > 4.3 * ptrms : e_prompt -= 1 s_prompt -= 1 results [ \"shift1\" ][ j ] = 1 else : results [ \"shift1\" ][ j ] = 0 if k >= nPresamples - 1 : pulse_sum += signal pulse_rms_sum += signal ** 2 results [ \"pretrig_mean\" ][ j ] = ptm results [ \"pretrig_rms\" ][ j ] = ptrms if ptm < peak_value : peak_value -= int ( ptm + 0.5 ) results [ \"promptness\" ][ j ] = ( promptness_sum / 6.0 - ptm ) / peak_value results [ \"peak_value\" ][ j ] = peak_value results [ \"peak_index\" ][ j ] = peak_index else : results [ \"promptness\" ][ j ] = 0.0 results [ \"peak_value\" ][ j ] = 0 results [ \"peak_index\" ][ j ] = 0 results [ \"min_value\" ][ j ] = min_value pulse_avg = pulse_sum / ( nSamples - nPresamples + 1 ) - ptm results [ \"pulse_average\" ][ j ] = pulse_avg results [ \"pulse_rms\" ][ j ] = np . sqrt ( pulse_rms_sum / ( nSamples - nPresamples + 1 ) - ptm * pulse_avg * 2 - ptm ** 2 ) low_th = int ( 0.1 * peak_value + ptm ) high_th = int ( 0.9 * peak_value + ptm ) k = nPresamples low_value = high_value = pulse [ k ] while k < nSamples : signal = pulse [ k ] if signal > low_th : low_idx = k low_value = signal break k += 1 high_value = low_value high_idx = low_idx while k < nSamples : signal = pulse [ k ] if signal > high_th : high_idx = k - 1 high_value = pulse [ high_idx ] break k += 1 if high_value > low_value : results [ \"rise_time\" ][ j ] = timebase * ( high_idx - low_idx ) * peak_value / ( high_value - low_value ) else : results [ \"rise_time\" ][ j ] = timebase # The following is quite confusing, but it appears to be equivalent to # slope = -2 * pulse[peak_samplenumber:-4] # slope -= pulse[peak_samplenumber+1:-3] # slope += pulse[peak_samplenumber+3:-1] # slope += 2*pulse[peak_samplenumber+4:] # slope = np.minimum(slope[2:], slope[:-2]) # results[\"postpeak_deriv\"][j] = 0.1 * np.max(slope) # TODO: consider replacing, if the above is not slower? f0 , f1 , f3 , f4 = 2 , 1 , - 1 , - 2 s0 , s1 , s2 , s3 = ( pulse [ peak_samplenumber ], pulse [ peak_samplenumber + 1 ], pulse [ peak_samplenumber + 2 ], pulse [ peak_samplenumber + 3 ], ) s4 = pulse [ peak_samplenumber + 4 ] t0 = f4 * s0 + f3 * s1 + f1 * s3 + f0 * s4 s0 , s1 , s2 , s3 = s1 , s2 , s3 , s4 s4 = pulse [ peak_samplenumber + 5 ] t1 = f4 * s0 + f3 * s1 + f1 * s3 + f0 * s4 t_max_deriv = np . iinfo ( np . int32 ) . min for k in range ( peak_samplenumber + 6 , nSamples ): s0 , s1 , s2 , s3 = s1 , s2 , s3 , s4 s4 = pulse [ k ] t2 = f4 * s0 + f3 * s1 + f1 * s3 + f0 * s4 t3 = min ( t2 , t0 ) t_max_deriv = max ( t_max_deriv , t3 ) t0 , t1 = t1 , t2 results [ \"postpeak_deriv\" ][ j ] = 0.1 * t_max_deriv return results Pulse model object, to hold a low-dimensional linear basis able to express all normal pulses,","title":"summarize_data_numba"},{"location":"docstrings/#mass2.core.pulse_model.PulseModel","text":"Object to hold a \"pulse model\", meaning a low-dimensional linear basis to express \"all\" pulses, along with a projector such that projector.dot(basis) is the identity matrix. Also has the capacity to store to and restore from HDF5, and the ability to compute additional basis elements and corresponding projectors with method _additional_projectors_tsvd Source code in mass2/core/pulse_model.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class PulseModel : \"\"\"Object to hold a \"pulse model\", meaning a low-dimensional linear basis to express \"all\" pulses, along with a projector such that projector.dot(basis) is the identity matrix. Also has the capacity to store to and restore from HDF5, and the ability to compute additional basis elements and corresponding projectors with method _additional_projectors_tsvd\"\"\" version = 2 def __init__ ( # noqa: PLR0917 self , projectors_so_far : NDArray , basis_so_far : NDArray , n_basis : int , pulses_for_svd : NDArray , v_dv : float , pretrig_rms_median : float , pretrig_rms_sigma : float , file_name : str , extra_n_basis_5lag : int , f_5lag : NDArray , average_pulse_for_5lag : NDArray , noise_psd : NDArray , noise_psd_delta_f : NDArray , noise_autocorr : NDArray , _from_hdf5 : bool = False , ): self . pulses_for_svd = pulses_for_svd self . n_basis = n_basis dn = n_basis - extra_n_basis_5lag if projectors_so_far . shape [ 0 ] < dn : self . projectors , self . basis = self . _additional_projectors_tsvd ( projectors_so_far , basis_so_far , dn , pulses_for_svd ) elif ( projectors_so_far . shape [ 0 ] == dn ) or _from_hdf5 : self . projectors , self . basis = projectors_so_far , basis_so_far else : # don't throw error on s = f \"n_basis-extra_n_basis_5lag= { dn } < projectors_so_far.shape[0] = { projectors_so_far . shape [ 0 ] } \" s += f \", extra_n_basis_5lag= { extra_n_basis_5lag } \" raise Exception ( s ) if ( not _from_hdf5 ) and ( extra_n_basis_5lag > 0 ): filters_5lag = np . zeros (( len ( f_5lag ) + 4 , 5 )) for i in range ( 5 ): if i < 4 : filters_5lag [ i : - 4 + i , i ] = projectors_so_far [ 2 , 2 : - 2 ] else : filters_5lag [ i :, i ] = projectors_so_far [ 2 , 2 : - 2 ] self . projectors , self . basis = self . _additional_projectors_tsvd ( self . projectors , self . basis , n_basis , filters_5lag ) self . v_dv = v_dv self . pretrig_rms_median = pretrig_rms_median self . pretrig_rms_sigma = pretrig_rms_sigma self . file_name = str ( file_name ) self . extra_n_basis_5lag = extra_n_basis_5lag self . f_5lag = f_5lag self . average_pulse_for_5lag = average_pulse_for_5lag self . noise_psd = noise_psd self . noise_psd_delta_f = noise_psd_delta_f self . noise_autocorr = noise_autocorr def toHDF5 ( self , hdf5_group : h5py . Group , save_inverted : bool ) -> None : \"\"\"Save the pulse model to an HDF5 group.\"\"\" projectors , basis = self . projectors [()], self . basis [()] if save_inverted : # flip every component except the mean component if data is being inverted basis [:, 1 :] *= - 1 projectors [ 1 :, :] *= - 1 # projectors is MxN, where N is samples/record and M the number of basis elements # basis is NxM hdf5_group [ \"svdbasis/projectors\" ] = projectors hdf5_group [ \"svdbasis/basis\" ] = basis hdf5_group [ \"svdbasis/v_dv\" ] = self . v_dv hdf5_group [ \"svdbasis/training_pulses_for_plots\" ] = self . pulses_for_svd hdf5_group [ \"svdbasis/was_saved_inverted\" ] = save_inverted hdf5_group [ \"svdbasis/pretrig_rms_median\" ] = self . pretrig_rms_median hdf5_group [ \"svdbasis/pretrig_rms_sigma\" ] = self . pretrig_rms_sigma hdf5_group [ \"svdbasis/version\" ] = self . version hdf5_group [ \"svdbasis/file_name\" ] = self . file_name hdf5_group [ \"svdbasis/extra_n_basis_5lag\" ] = self . extra_n_basis_5lag hdf5_group [ \"svdbasis/5lag_filter\" ] = self . f_5lag hdf5_group [ \"svdbasis/average_pulse_for_5lag\" ] = self . average_pulse_for_5lag hdf5_group [ \"svdbasis/noise_psd\" ] = self . noise_psd hdf5_group [ \"svdbasis/noise_psd_delta_f\" ] = self . noise_psd_delta_f hdf5_group [ \"svdbasis/noise_autocorr\" ] = self . noise_autocorr @classmethod def fromHDF5 ( cls , hdf5_group : h5py . Group ) -> \"PulseModel\" : \"\"\"Restore a pulse model from an HDF5 group.\"\"\" projectors = hdf5_group [ \"svdbasis/projectors\" ][()] n_basis = projectors . shape [ 0 ] basis = hdf5_group [ \"svdbasis/basis\" ][()] v_dv = hdf5_group [ \"svdbasis/v_dv\" ][()] pulses_for_svd = hdf5_group [ \"svdbasis/training_pulses_for_plots\" ][()] pretrig_rms_median = hdf5_group [ \"svdbasis/pretrig_rms_median\" ][()] pretrig_rms_sigma = hdf5_group [ \"svdbasis/pretrig_rms_sigma\" ][()] version = hdf5_group [ \"svdbasis/version\" ][()] file_name = tostr ( hdf5_group [ \"svdbasis/file_name\" ][()]) extra_n_basis_5lag = hdf5_group [ \"svdbasis/extra_n_basis_5lag\" ][()] f_5lag = hdf5_group [ \"svdbasis/5lag_filter\" ][()] average_pulse_for_5lag = hdf5_group [ \"svdbasis/average_pulse_for_5lag\" ][()] noise_psd = hdf5_group [ \"svdbasis/noise_psd\" ][()] noise_psd_delta_f = hdf5_group [ \"svdbasis/noise_psd_delta_f\" ][()] noise_autocorr = hdf5_group [ \"svdbasis/noise_autocorr\" ][()] if version != cls . version : raise Exception ( f \"loading not implemented for other versions, version= { version } \" ) return cls ( projectors , basis , n_basis , pulses_for_svd , v_dv , pretrig_rms_median , pretrig_rms_sigma , file_name , extra_n_basis_5lag , f_5lag , average_pulse_for_5lag , noise_psd , noise_psd_delta_f , noise_autocorr , _from_hdf5 = True , ) @staticmethod def _additional_projectors_tsvd ( projectors : NDArray , basis : NDArray , n_basis : int , pulses_for_svd : NDArray ) -> tuple [ NDArray , NDArray ]: \"\"\" Given an existing basis with projectors, compute a basis with n_basis elements by randomized SVD of the residual elements of the training data in pulses_for_svd. It should be the case that projectors.dot(basis) is approximately the identity matrix. It is assumed that the projectors will have been computed from the basis in some noise-optimal way, say, from optimal filtering. However, the additional basis elements will be computed from a standard (non-noise-weighted) SVD, and the additional projectors will be computed without noise optimization. The projectors and basis will be ordered as: mean, deriv_ike, pulse_like, any svd components... \"\"\" # Check sanity of inputs n_samples , n_existing = basis . shape assert ( n_existing , n_samples ) == projectors . shape assert n_basis >= n_existing if n_basis == n_existing : return projectors , basis mpc = np . matmul ( projectors , pulses_for_svd ) # modeled pulse coefs mp = np . matmul ( basis , mpc ) # modeled pulse residuals = pulses_for_svd - mp Q = mass2 . mathstat . utilities . find_range_randomly ( residuals , n_basis - n_existing ) projectors2 = np . linalg . pinv ( Q ) # = Q.T, perhaps?? projectors2 -= projectors2 . dot ( basis ) . dot ( projectors ) basis = np . hstack ([ basis , Q ]) projectors = np . vstack ([ projectors , projectors2 ]) return projectors , basis def labels ( self ) -> list [ str ]: \"\"\"Return a list of labels for the basis elements.\"\"\" labels = [ \"const\" , \"deriv\" , \"pulse\" ] for i in range ( self . n_basis - 3 ): if i > self . n_basis - 3 - self . extra_n_basis_5lag : labels += [ f \"5lag { i + 2 - self . extra_n_basis_5lag } \" ] else : labels += [ f \"svd { i } \" ] return labels def plot ( self , fig1 : plt . Axes | None = None , fig2 : plt . Axes | None = None ) -> None : \"\"\"Plot a pulse model\"\"\" # plots information about a pulse model # fig1 and fig2 are optional matplotlib.pyplot (plt) figures if you need to embed the plots. # you can pass in the reference like fig=plt.figure() call or the figure's number, e.g. fig.number # fig1 has modeled pulse vs true pulse # fig2 has projectors, basis, \"from ljh\", residuals, and a measure of \"wrongness\" labels = self . labels () mpc = np . matmul ( self . projectors , self . pulses_for_svd ) mp = np . matmul ( self . basis , mpc ) residuals = self . pulses_for_svd - mp if fig1 is None : fig = plt . figure ( figsize = ( 10 , 14 )) else : fig = plt . figure ( fig1 ) plt . subplot ( 511 ) plt . plot ( self . projectors [:: - 1 , :] . T ) plt . title ( \"projectors\" ) # projector_scale = np.amax(np.abs(self.projectors[2, :])) # plt.ylim(-2*projector_scale, 2*projector_scale) plt . legend ( labels [:: - 1 ]) plt . grid ( True ) plt . subplot ( 512 ) plt . plot ( self . basis [:, :: - 1 ]) plt . title ( \"basis\" ) plt . legend ( labels [:: - 1 ]) plt . grid ( True ) plt . subplot ( 513 ) plt . plot ( self . pulses_for_svd [:, : 10 ]) plt . title ( \"from ljh\" ) plt . legend ([ f \" { i } \" for i in range ( 10 )]) plt . grid ( True ) plt . subplot ( 514 ) plt . plot ( residuals [:, : 10 ]) plt . title ( \"residuals\" ) plt . legend ([ f \" { i } \" for i in range ( 10 )]) plt . grid ( True ) should_be_identity = np . matmul ( self . projectors , self . basis ) identity = np . identity ( self . n_basis ) wrongness = np . abs ( should_be_identity - identity ) wrongness [ wrongness < 1e-20 ] = 1e-20 # avoid warnings plt . subplot ( 515 ) plt . imshow ( np . log10 ( wrongness )) plt . title ( \"log10(abs(projectors*basis-identity))\" ) plt . colorbar () fig . suptitle ( self . file_name ) if fig2 is None : plt . figure ( figsize = ( 10 , 14 )) else : plt . figure ( fig2 ) plt . plot ( self . pulses_for_svd [:, 0 ], label = \"from ljh index 0\" ) plt . plot ( mp [:, 0 ], label = \"modeled pulse index 0\" ) plt . legend () plt . title ( \"modeled pulse vs true pulse\" )","title":"PulseModel"},{"location":"docstrings/#mass2.core.pulse_model.PulseModel.fromHDF5","text":"Restore a pulse model from an HDF5 group. Source code in mass2/core/pulse_model.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @classmethod def fromHDF5 ( cls , hdf5_group : h5py . Group ) -> \"PulseModel\" : \"\"\"Restore a pulse model from an HDF5 group.\"\"\" projectors = hdf5_group [ \"svdbasis/projectors\" ][()] n_basis = projectors . shape [ 0 ] basis = hdf5_group [ \"svdbasis/basis\" ][()] v_dv = hdf5_group [ \"svdbasis/v_dv\" ][()] pulses_for_svd = hdf5_group [ \"svdbasis/training_pulses_for_plots\" ][()] pretrig_rms_median = hdf5_group [ \"svdbasis/pretrig_rms_median\" ][()] pretrig_rms_sigma = hdf5_group [ \"svdbasis/pretrig_rms_sigma\" ][()] version = hdf5_group [ \"svdbasis/version\" ][()] file_name = tostr ( hdf5_group [ \"svdbasis/file_name\" ][()]) extra_n_basis_5lag = hdf5_group [ \"svdbasis/extra_n_basis_5lag\" ][()] f_5lag = hdf5_group [ \"svdbasis/5lag_filter\" ][()] average_pulse_for_5lag = hdf5_group [ \"svdbasis/average_pulse_for_5lag\" ][()] noise_psd = hdf5_group [ \"svdbasis/noise_psd\" ][()] noise_psd_delta_f = hdf5_group [ \"svdbasis/noise_psd_delta_f\" ][()] noise_autocorr = hdf5_group [ \"svdbasis/noise_autocorr\" ][()] if version != cls . version : raise Exception ( f \"loading not implemented for other versions, version= { version } \" ) return cls ( projectors , basis , n_basis , pulses_for_svd , v_dv , pretrig_rms_median , pretrig_rms_sigma , file_name , extra_n_basis_5lag , f_5lag , average_pulse_for_5lag , noise_psd , noise_psd_delta_f , noise_autocorr , _from_hdf5 = True , )","title":"fromHDF5"},{"location":"docstrings/#mass2.core.pulse_model.PulseModel.labels","text":"Return a list of labels for the basis elements. Source code in mass2/core/pulse_model.py 174 175 176 177 178 179 180 181 182 def labels ( self ) -> list [ str ]: \"\"\"Return a list of labels for the basis elements.\"\"\" labels = [ \"const\" , \"deriv\" , \"pulse\" ] for i in range ( self . n_basis - 3 ): if i > self . n_basis - 3 - self . extra_n_basis_5lag : labels += [ f \"5lag { i + 2 - self . extra_n_basis_5lag } \" ] else : labels += [ f \"svd { i } \" ] return labels","title":"labels"},{"location":"docstrings/#mass2.core.pulse_model.PulseModel.plot","text":"Plot a pulse model Source code in mass2/core/pulse_model.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def plot ( self , fig1 : plt . Axes | None = None , fig2 : plt . Axes | None = None ) -> None : \"\"\"Plot a pulse model\"\"\" # plots information about a pulse model # fig1 and fig2 are optional matplotlib.pyplot (plt) figures if you need to embed the plots. # you can pass in the reference like fig=plt.figure() call or the figure's number, e.g. fig.number # fig1 has modeled pulse vs true pulse # fig2 has projectors, basis, \"from ljh\", residuals, and a measure of \"wrongness\" labels = self . labels () mpc = np . matmul ( self . projectors , self . pulses_for_svd ) mp = np . matmul ( self . basis , mpc ) residuals = self . pulses_for_svd - mp if fig1 is None : fig = plt . figure ( figsize = ( 10 , 14 )) else : fig = plt . figure ( fig1 ) plt . subplot ( 511 ) plt . plot ( self . projectors [:: - 1 , :] . T ) plt . title ( \"projectors\" ) # projector_scale = np.amax(np.abs(self.projectors[2, :])) # plt.ylim(-2*projector_scale, 2*projector_scale) plt . legend ( labels [:: - 1 ]) plt . grid ( True ) plt . subplot ( 512 ) plt . plot ( self . basis [:, :: - 1 ]) plt . title ( \"basis\" ) plt . legend ( labels [:: - 1 ]) plt . grid ( True ) plt . subplot ( 513 ) plt . plot ( self . pulses_for_svd [:, : 10 ]) plt . title ( \"from ljh\" ) plt . legend ([ f \" { i } \" for i in range ( 10 )]) plt . grid ( True ) plt . subplot ( 514 ) plt . plot ( residuals [:, : 10 ]) plt . title ( \"residuals\" ) plt . legend ([ f \" { i } \" for i in range ( 10 )]) plt . grid ( True ) should_be_identity = np . matmul ( self . projectors , self . basis ) identity = np . identity ( self . n_basis ) wrongness = np . abs ( should_be_identity - identity ) wrongness [ wrongness < 1e-20 ] = 1e-20 # avoid warnings plt . subplot ( 515 ) plt . imshow ( np . log10 ( wrongness )) plt . title ( \"log10(abs(projectors*basis-identity))\" ) plt . colorbar () fig . suptitle ( self . file_name ) if fig2 is None : plt . figure ( figsize = ( 10 , 14 )) else : plt . figure ( fig2 ) plt . plot ( self . pulses_for_svd [:, 0 ], label = \"from ljh index 0\" ) plt . plot ( mp [:, 0 ], label = \"modeled pulse index 0\" ) plt . legend () plt . title ( \"modeled pulse vs true pulse\" )","title":"plot"},{"location":"docstrings/#mass2.core.pulse_model.PulseModel.toHDF5","text":"Save the pulse model to an HDF5 group. Source code in mass2/core/pulse_model.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def toHDF5 ( self , hdf5_group : h5py . Group , save_inverted : bool ) -> None : \"\"\"Save the pulse model to an HDF5 group.\"\"\" projectors , basis = self . projectors [()], self . basis [()] if save_inverted : # flip every component except the mean component if data is being inverted basis [:, 1 :] *= - 1 projectors [ 1 :, :] *= - 1 # projectors is MxN, where N is samples/record and M the number of basis elements # basis is NxM hdf5_group [ \"svdbasis/projectors\" ] = projectors hdf5_group [ \"svdbasis/basis\" ] = basis hdf5_group [ \"svdbasis/v_dv\" ] = self . v_dv hdf5_group [ \"svdbasis/training_pulses_for_plots\" ] = self . pulses_for_svd hdf5_group [ \"svdbasis/was_saved_inverted\" ] = save_inverted hdf5_group [ \"svdbasis/pretrig_rms_median\" ] = self . pretrig_rms_median hdf5_group [ \"svdbasis/pretrig_rms_sigma\" ] = self . pretrig_rms_sigma hdf5_group [ \"svdbasis/version\" ] = self . version hdf5_group [ \"svdbasis/file_name\" ] = self . file_name hdf5_group [ \"svdbasis/extra_n_basis_5lag\" ] = self . extra_n_basis_5lag hdf5_group [ \"svdbasis/5lag_filter\" ] = self . f_5lag hdf5_group [ \"svdbasis/average_pulse_for_5lag\" ] = self . average_pulse_for_5lag hdf5_group [ \"svdbasis/noise_psd\" ] = self . noise_psd hdf5_group [ \"svdbasis/noise_psd_delta_f\" ] = self . noise_psd_delta_f hdf5_group [ \"svdbasis/noise_autocorr\" ] = self . noise_autocorr Define RecipeStep and Recipe classes for processing pulse data in a sequence of steps.","title":"toHDF5"},{"location":"docstrings/#mass2.core.recipe.CategorizeStep","text":"Bases: RecipeStep A step to categorize pulses into discrete categories based on conditions given in a dictionary mapping category names to polars expressions. The first condition must be True, to be used as a fallback. Source code in mass2/core/recipe.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @dataclass ( frozen = True ) class CategorizeStep ( RecipeStep ): \"\"\"A step to categorize pulses into discrete categories based on conditions given in a dictionary mapping category names to polars expressions. The first condition must be True, to be used as a fallback.\"\"\" category_condition_dict : dict [ str , pl . Expr ] def __post_init__ ( self ) -> None : \"\"\"Verify that the first condition is always True.\"\"\" err_msg = \"The first condition must be True, to be used as a fallback\" first_condition = next ( iter ( self . category_condition_dict . values ())) assert first_condition is True or first_condition . meta . eq ( pl . lit ( True )), err_msg def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the category for each pulse and return a new DataFrame with a column for the category names.\"\"\" output_col = self . output [ 0 ] def categorize_df ( df : pl . DataFrame , category_condition_dict : dict [ str , pl . Expr ], output_col : str ) -> pl . DataFrame : \"\"\"returns a series showing which category each pulse is in pulses will be assigned to the last category for which the condition evaluates to True\"\"\" dtype = pl . Enum ( category_condition_dict . keys ()) physical = np . zeros ( len ( df ), dtype = int ) for category_int , ( category_str , condition_expr ) in enumerate ( category_condition_dict . items ()): if condition_expr is True or condition_expr . meta . eq ( pl . lit ( True )): in_category = np . ones ( len ( df ), dtype = bool ) else : in_category = df . select ( condition_expr ) . fill_null ( False ) . to_numpy () . flatten () assert in_category . dtype == bool physical [ in_category ] = category_int series = pl . Series ( name = output_col , values = physical ) . cast ( dtype ) df = pl . DataFrame ({ output_col : series }) return df df2 = categorize_df ( df , self . category_condition_dict , output_col ) . with_columns ( df ) return df2","title":"CategorizeStep"},{"location":"docstrings/#mass2.core.recipe.CategorizeStep.__post_init__","text":"Verify that the first condition is always True. Source code in mass2/core/recipe.py 167 168 169 170 171 def __post_init__ ( self ) -> None : \"\"\"Verify that the first condition is always True.\"\"\" err_msg = \"The first condition must be True, to be used as a fallback\" first_condition = next ( iter ( self . category_condition_dict . values ())) assert first_condition is True or first_condition . meta . eq ( pl . lit ( True )), err_msg","title":"__post_init__"},{"location":"docstrings/#mass2.core.recipe.CategorizeStep.calc_from_df","text":"Calculate the category for each pulse and return a new DataFrame with a column for the category names. Source code in mass2/core/recipe.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the category for each pulse and return a new DataFrame with a column for the category names.\"\"\" output_col = self . output [ 0 ] def categorize_df ( df : pl . DataFrame , category_condition_dict : dict [ str , pl . Expr ], output_col : str ) -> pl . DataFrame : \"\"\"returns a series showing which category each pulse is in pulses will be assigned to the last category for which the condition evaluates to True\"\"\" dtype = pl . Enum ( category_condition_dict . keys ()) physical = np . zeros ( len ( df ), dtype = int ) for category_int , ( category_str , condition_expr ) in enumerate ( category_condition_dict . items ()): if condition_expr is True or condition_expr . meta . eq ( pl . lit ( True )): in_category = np . ones ( len ( df ), dtype = bool ) else : in_category = df . select ( condition_expr ) . fill_null ( False ) . to_numpy () . flatten () assert in_category . dtype == bool physical [ in_category ] = category_int series = pl . Series ( name = output_col , values = physical ) . cast ( dtype ) df = pl . DataFrame ({ output_col : series }) return df df2 = categorize_df ( df , self . category_condition_dict , output_col ) . with_columns ( df ) return df2","title":"calc_from_df"},{"location":"docstrings/#mass2.core.recipe.ColumnAsNumpyMapStep","text":"Bases: RecipeStep This step is meant for interactive exploration, it takes a column and applies a function to it, and makes a new column with the result. It makes it easy to test functions on a column without having to write a whole new step class, while maintaining the benefit of being able to use the step in a Recipe chain, like replaying steps on another channel. example usage: def my_function(x): ... return x * 2 step = ColumnAsNumpyMapStep(inputs=[\"my_column\"], output=[\"my_new_column\"], f=my_function) ch2 = ch.with_step(step) Source code in mass2/core/recipe.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 @dataclass ( frozen = True ) class ColumnAsNumpyMapStep ( RecipeStep ): \"\"\" This step is meant for interactive exploration, it takes a column and applies a function to it, and makes a new column with the result. It makes it easy to test functions on a column without having to write a whole new step class, while maintaining the benefit of being able to use the step in a Recipe chain, like replaying steps on another channel. example usage: >>> def my_function(x): ... return x * 2 >>> step = ColumnAsNumpyMapStep(inputs=[\"my_column\"], output=[\"my_new_column\"], f=my_function) >>> ch2 = ch.with_step(step) \"\"\" f : Callable [[ np . ndarray ], np . ndarray ] def __post_init__ ( self ) -> None : \"\"\"Check that inputs and outputs are valid (single column each) and that `f` is a callable object.\"\"\" assert len ( self . inputs ) == 1 , \"ColumnMapStep expects exactly one input\" assert len ( self . output ) == 1 , \"ColumnMapStep expects exactly one output\" if not callable ( self . f ): raise ValueError ( f \"f must be a callable, got { self . f } \" ) def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the new column by applying `f` to the input column, returning a new DataFrame.\"\"\" output_col = self . output [ 0 ] output_segments = [] for df_iter in df . select ( self . inputs ) . iter_slices (): series1 = df_iter [ self . inputs [ 0 ]] # Have to apply the function differently when series elements are arrays vs scalars if series1 . dtype . base_type () is pl . Array : output_numpy = np . array ([ self . f ( v . to_numpy ()) for v in series1 ]) else : output_numpy = self . f ( series1 . to_numpy ()) this_output_segment = pl . Series ( output_col , output_numpy ) output_segments . append ( this_output_segment ) combined = pl . concat ( output_segments ) # Put into a DataFrame with one column df2 = pl . DataFrame ({ output_col : combined }) . with_columns ( df ) return df2","title":"ColumnAsNumpyMapStep"},{"location":"docstrings/#mass2.core.recipe.ColumnAsNumpyMapStep.__post_init__","text":"Check that inputs and outputs are valid (single column each) and that f is a callable object. Source code in mass2/core/recipe.py 133 134 135 136 137 138 def __post_init__ ( self ) -> None : \"\"\"Check that inputs and outputs are valid (single column each) and that `f` is a callable object.\"\"\" assert len ( self . inputs ) == 1 , \"ColumnMapStep expects exactly one input\" assert len ( self . output ) == 1 , \"ColumnMapStep expects exactly one output\" if not callable ( self . f ): raise ValueError ( f \"f must be a callable, got { self . f } \" )","title":"__post_init__"},{"location":"docstrings/#mass2.core.recipe.ColumnAsNumpyMapStep.calc_from_df","text":"Calculate the new column by applying f to the input column, returning a new DataFrame. Source code in mass2/core/recipe.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the new column by applying `f` to the input column, returning a new DataFrame.\"\"\" output_col = self . output [ 0 ] output_segments = [] for df_iter in df . select ( self . inputs ) . iter_slices (): series1 = df_iter [ self . inputs [ 0 ]] # Have to apply the function differently when series elements are arrays vs scalars if series1 . dtype . base_type () is pl . Array : output_numpy = np . array ([ self . f ( v . to_numpy ()) for v in series1 ]) else : output_numpy = self . f ( series1 . to_numpy ()) this_output_segment = pl . Series ( output_col , output_numpy ) output_segments . append ( this_output_segment ) combined = pl . concat ( output_segments ) # Put into a DataFrame with one column df2 = pl . DataFrame ({ output_col : combined }) . with_columns ( df ) return df2","title":"calc_from_df"},{"location":"docstrings/#mass2.core.recipe.PretrigMeanJumpFixStep","text":"Bases: RecipeStep A step to fix jumps in the pretrigger mean by unwrapping the phase angle, a periodic quantity. Source code in mass2/core/recipe.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @dataclass ( frozen = True ) class PretrigMeanJumpFixStep ( RecipeStep ): \"\"\"A step to fix jumps in the pretrigger mean by unwrapping the phase angle, a periodic quantity.\"\"\" period : float def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the jump-corrected pretrigger mean and return a new DataFrame.\"\"\" ptm1 = df [ self . inputs [ 0 ]] . to_numpy () ptm2 = np . unwrap ( ptm1 % self . period , period = self . period ) df2 = pl . DataFrame ({ self . output [ 0 ]: ptm2 }) . with_columns ( df ) return df2 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a diagnostic plot of the pretrigger mean before and after the jump fix.\"\"\" plt . figure () plt . plot ( df_after [ \"timestamp\" ], df_after [ self . inputs [ 0 ]], \".\" , label = self . inputs [ 0 ], ** kwargs ) plt . plot ( df_after [ \"timestamp\" ], df_after [ self . output [ 0 ]], \".\" , label = self . output [ 0 ], ** kwargs ) plt . legend () plt . xlabel ( \"timestamp\" ) plt . ylabel ( \"pretrig mean\" ) plt . tight_layout () return plt . gca ()","title":"PretrigMeanJumpFixStep"},{"location":"docstrings/#mass2.core.recipe.PretrigMeanJumpFixStep.calc_from_df","text":"Calculate the jump-corrected pretrigger mean and return a new DataFrame. Source code in mass2/core/recipe.py 62 63 64 65 66 67 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the jump-corrected pretrigger mean and return a new DataFrame.\"\"\" ptm1 = df [ self . inputs [ 0 ]] . to_numpy () ptm2 = np . unwrap ( ptm1 % self . period , period = self . period ) df2 = pl . DataFrame ({ self . output [ 0 ]: ptm2 }) . with_columns ( df ) return df2","title":"calc_from_df"},{"location":"docstrings/#mass2.core.recipe.PretrigMeanJumpFixStep.dbg_plot","text":"Make a diagnostic plot of the pretrigger mean before and after the jump fix. Source code in mass2/core/recipe.py 69 70 71 72 73 74 75 76 77 78 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Make a diagnostic plot of the pretrigger mean before and after the jump fix.\"\"\" plt . figure () plt . plot ( df_after [ \"timestamp\" ], df_after [ self . inputs [ 0 ]], \".\" , label = self . inputs [ 0 ], ** kwargs ) plt . plot ( df_after [ \"timestamp\" ], df_after [ self . output [ 0 ]], \".\" , label = self . output [ 0 ], ** kwargs ) plt . legend () plt . xlabel ( \"timestamp\" ) plt . ylabel ( \"pretrig mean\" ) plt . tight_layout () return plt . gca ()","title":"dbg_plot"},{"location":"docstrings/#mass2.core.recipe.Recipe","text":"Bases: Sequence [ RecipeStep ] A sequence of RecipeStep objects to be applied in order to a DataFrame. Source code in mass2/core/recipe.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 @dataclass ( frozen = True ) class Recipe ( Sequence [ RecipeStep ]): \"\"\"A sequence of RecipeStep objects to be applied in order to a DataFrame.\"\"\" steps : list [ RecipeStep ] # TODO: leaves many optimizations on the table, but is very simple # 1. we could calculate filt_value_5lag and filt_phase_5lag at the same time # 2. we could calculate intermediate quantities optionally and not materialize all of them def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"return a dataframe with all the newly calculated info\" for step in self . steps : df = step . calc_from_df ( df ) . with_columns ( df ) return df @classmethod def new_empty ( cls ) -> \"Recipe\" : \"\"\"Create a new empty Recipe.\"\"\" return cls ([]) @overload def __getitem__ ( self , key : int ) -> RecipeStep : \"\"\"Return the step at a given index.\"\"\" ... @overload def __getitem__ ( self , key : slice ) -> Sequence [ RecipeStep ]: \"\"\"Return the steps at a given slice of indices.\"\"\" ... def __getitem__ ( self , key : int | slice ) -> RecipeStep | Sequence [ RecipeStep ]: \"\"\"Return the step at the given index, or the steps at a slice of steps.\"\"\" return self . steps [ key ] def __len__ ( self ) -> int : \"\"\"Return the number of steps in the recipe.\"\"\" return len ( self . steps ) def with_step ( self , step : RecipeStep ) -> \"Recipe\" : \"\"\"Create a new Recipe with the given step added to the end.\"\"\" # return a new Recipe with the step added, no mutation! return Recipe ( self . steps + [ step ]) def trim_dead_ends ( self , required_fields : Iterable [ str ] | str | None , drop_debug : bool = True ) -> \"Recipe\" : \"\"\"Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with `drop_debug=True`). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in `required_fields`. The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the `required_fields` (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters ---------- required_fields : Iterable[str] | str | None Steps will be preserved if any of their outputs are among `required_fields`, or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug : bool Whether to run `step.drop_debug()` to remove debugging information from the preserved steps. Returns ------- Recipe A copy of `self`, except that any steps not required to compute any of `required_fields` are omitted. \"\"\" if isinstance ( required_fields , str ): required_fields = [ required_fields ] nsteps = len ( self ) required = np . zeros ( nsteps , dtype = bool ) # The easiest approach is to traverse the steps from last to first to build our list of required # fields, because necessarily no later step can produce the inputs needed by an earlier step. if required_fields is None : required [:] = True else : all_fields_out : set [ str ] = set ( required_fields ) for istep in range ( nsteps - 1 , - 1 , - 1 ): step = self [ istep ] for field in step . output : if field in all_fields_out : required [ istep ] = True all_fields_out . update ( step . inputs ) break if not np . any ( required ): # If this error ever because a problem, where user _acutally_ wants an empty series of steps # to be a non-err, then add argument `error_on_empty_output=True` to this method. raise ValueError ( \"trim_dead_ends found no steps to be preserved\" ) steps = [] for i in range ( nsteps ): if required [ i ]: if drop_debug : steps . append ( self [ i ] . drop_debug ()) else : steps . append ( self [ i ]) return Recipe ( steps )","title":"Recipe"},{"location":"docstrings/#mass2.core.recipe.Recipe.__getitem__","text":"__getitem__ ( key : int ) -> RecipeStep __getitem__ ( key : slice ) -> Sequence [ RecipeStep ] Return the step at the given index, or the steps at a slice of steps. Source code in mass2/core/recipe.py 242 243 244 def __getitem__ ( self , key : int | slice ) -> RecipeStep | Sequence [ RecipeStep ]: \"\"\"Return the step at the given index, or the steps at a slice of steps.\"\"\" return self . steps [ key ]","title":"__getitem__"},{"location":"docstrings/#mass2.core.recipe.Recipe.__len__","text":"Return the number of steps in the recipe. Source code in mass2/core/recipe.py 246 247 248 def __len__ ( self ) -> int : \"\"\"Return the number of steps in the recipe.\"\"\" return len ( self . steps )","title":"__len__"},{"location":"docstrings/#mass2.core.recipe.Recipe.calc_from_df","text":"return a dataframe with all the newly calculated info Source code in mass2/core/recipe.py 221 222 223 224 225 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"return a dataframe with all the newly calculated info\" for step in self . steps : df = step . calc_from_df ( df ) . with_columns ( df ) return df","title":"calc_from_df"},{"location":"docstrings/#mass2.core.recipe.Recipe.new_empty","text":"Create a new empty Recipe. Source code in mass2/core/recipe.py 227 228 229 230 @classmethod def new_empty ( cls ) -> \"Recipe\" : \"\"\"Create a new empty Recipe.\"\"\" return cls ([])","title":"new_empty"},{"location":"docstrings/#mass2.core.recipe.Recipe.trim_dead_ends","text":"Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with drop_debug=True ). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in required_fields . The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the required_fields (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters: required_fields ( Iterable [ str ] | str | None ) \u2013 Steps will be preserved if any of their outputs are among required_fields , or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug ( bool , default: True ) \u2013 Whether to run step.drop_debug() to remove debugging information from the preserved steps. Returns: Recipe \u2013 A copy of self , except that any steps not required to compute any of required_fields are omitted. Source code in mass2/core/recipe.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def trim_dead_ends ( self , required_fields : Iterable [ str ] | str | None , drop_debug : bool = True ) -> \"Recipe\" : \"\"\"Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with `drop_debug=True`). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in `required_fields`. The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the `required_fields` (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters ---------- required_fields : Iterable[str] | str | None Steps will be preserved if any of their outputs are among `required_fields`, or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug : bool Whether to run `step.drop_debug()` to remove debugging information from the preserved steps. Returns ------- Recipe A copy of `self`, except that any steps not required to compute any of `required_fields` are omitted. \"\"\" if isinstance ( required_fields , str ): required_fields = [ required_fields ] nsteps = len ( self ) required = np . zeros ( nsteps , dtype = bool ) # The easiest approach is to traverse the steps from last to first to build our list of required # fields, because necessarily no later step can produce the inputs needed by an earlier step. if required_fields is None : required [:] = True else : all_fields_out : set [ str ] = set ( required_fields ) for istep in range ( nsteps - 1 , - 1 , - 1 ): step = self [ istep ] for field in step . output : if field in all_fields_out : required [ istep ] = True all_fields_out . update ( step . inputs ) break if not np . any ( required ): # If this error ever because a problem, where user _acutally_ wants an empty series of steps # to be a non-err, then add argument `error_on_empty_output=True` to this method. raise ValueError ( \"trim_dead_ends found no steps to be preserved\" ) steps = [] for i in range ( nsteps ): if required [ i ]: if drop_debug : steps . append ( self [ i ] . drop_debug ()) else : steps . append ( self [ i ]) return Recipe ( steps )","title":"trim_dead_ends"},{"location":"docstrings/#mass2.core.recipe.Recipe.with_step","text":"Create a new Recipe with the given step added to the end. Source code in mass2/core/recipe.py 250 251 252 253 def with_step ( self , step : RecipeStep ) -> \"Recipe\" : \"\"\"Create a new Recipe with the given step added to the end.\"\"\" # return a new Recipe with the step added, no mutation! return Recipe ( self . steps + [ step ])","title":"with_step"},{"location":"docstrings/#mass2.core.recipe.RecipeStep","text":"Represent one step in a data processing recipe. A step has inputs, outputs, and a calculation method. It also has a good_expr and use_expr that can be used to filter the data before processing. This is an abstract base class, subclasses should implement calc_from_df and dbg_plot. Source code in mass2/core/recipe.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @dataclass ( frozen = True ) class RecipeStep : \"\"\"Represent one step in a data processing recipe. A step has inputs, outputs, and a calculation method. It also has a good_expr and use_expr that can be used to filter the data before processing. This is an abstract base class, subclasses should implement calc_from_df and dbg_plot. \"\"\" inputs : list [ str ] output : list [ str ] good_expr : pl . Expr use_expr : pl . Expr @property def name ( self ) -> str : \"\"\"The name of this step, usually the class name.\"\"\" return str ( type ( self )) @property def description ( self ) -> str : \"\"\"A short description of this step, including its inputs and outputs.\"\"\" return f \" { type ( self ) . __name__ } inputs= { self . inputs } outputs= { self . output } \" def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the outputs from the inputs in the given DataFrame, returning a new DataFrame.\"\"\" # TODO: should this be an abstract method? return df . filter ( self . good_expr ) def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Generate a diagnostic plot of the results after this step.\"\"\" # this is a no-op, subclasses can override this to plot something plt . figure () plt . text ( 0.0 , 0.5 , f \"No plot defined for: { self . description } \" ) return plt . gca () def drop_debug ( self ) -> \"RecipeStep\" : \"Return self, or a copy of it with debug information removed\" return self","title":"RecipeStep"},{"location":"docstrings/#mass2.core.recipe.RecipeStep.description","text":"A short description of this step, including its inputs and outputs.","title":"description"},{"location":"docstrings/#mass2.core.recipe.RecipeStep.name","text":"The name of this step, usually the class name.","title":"name"},{"location":"docstrings/#mass2.core.recipe.RecipeStep.calc_from_df","text":"Calculate the outputs from the inputs in the given DataFrame, returning a new DataFrame. Source code in mass2/core/recipe.py 39 40 41 42 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the outputs from the inputs in the given DataFrame, returning a new DataFrame.\"\"\" # TODO: should this be an abstract method? return df . filter ( self . good_expr )","title":"calc_from_df"},{"location":"docstrings/#mass2.core.recipe.RecipeStep.dbg_plot","text":"Generate a diagnostic plot of the results after this step. Source code in mass2/core/recipe.py 44 45 46 47 48 49 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : \"\"\"Generate a diagnostic plot of the results after this step.\"\"\" # this is a no-op, subclasses can override this to plot something plt . figure () plt . text ( 0.0 , 0.5 , f \"No plot defined for: { self . description } \" ) return plt . gca ()","title":"dbg_plot"},{"location":"docstrings/#mass2.core.recipe.RecipeStep.drop_debug","text":"Return self, or a copy of it with debug information removed Source code in mass2/core/recipe.py 51 52 53 def drop_debug ( self ) -> \"RecipeStep\" : \"Return self, or a copy of it with debug information removed\" return self","title":"drop_debug"},{"location":"docstrings/#mass2.core.recipe.SelectStep","text":"Bases: RecipeStep This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. Source code in mass2/core/recipe.py 197 198 199 200 201 202 203 204 205 206 207 208 @dataclass ( frozen = True ) class SelectStep ( RecipeStep ): \"\"\" This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. \"\"\" col_expr_dict : dict [ str , pl . Expr ] def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Select the given columns and return a new DataFrame.\"\"\" df2 = df . select ( ** self . col_expr_dict ) . with_columns ( df ) return df2","title":"SelectStep"},{"location":"docstrings/#mass2.core.recipe.SelectStep.calc_from_df","text":"Select the given columns and return a new DataFrame. Source code in mass2/core/recipe.py 205 206 207 208 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Select the given columns and return a new DataFrame.\"\"\" df2 = df . select ( ** self . col_expr_dict ) . with_columns ( df ) return df2","title":"calc_from_df"},{"location":"docstrings/#mass2.core.recipe.SummarizeStep","text":"Bases: RecipeStep Summarize raw pulse data into summary statistics using numba-accelerated code. Source code in mass2/core/recipe.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 @dataclass ( frozen = True ) class SummarizeStep ( RecipeStep ): \"\"\"Summarize raw pulse data into summary statistics using numba-accelerated code.\"\"\" frametime_s : float peak_index : int pulse_col : str pretrigger_ignore_samples : int n_presamples : int transform_raw : Callable | None = None def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the summary statistics and return a new DataFrame.\"\"\" summaries = [] for df_iter in df . select ( self . inputs ) . iter_slices (): raw = df_iter [ self . pulse_col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) s = pl . from_numpy ( pulse_algorithms . summarize_data_numba ( raw , self . frametime_s , peak_samplenumber = self . peak_index , pretrigger_ignore_samples = self . pretrigger_ignore_samples , nPresamples = self . n_presamples , ) ) summaries . append ( s ) df2 = pl . concat ( summaries ) . with_columns ( df ) return df2","title":"SummarizeStep"},{"location":"docstrings/#mass2.core.recipe.SummarizeStep.calc_from_df","text":"Calculate the summary statistics and return a new DataFrame. Source code in mass2/core/recipe.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"\"\"Calculate the summary statistics and return a new DataFrame.\"\"\" summaries = [] for df_iter in df . select ( self . inputs ) . iter_slices (): raw = df_iter [ self . pulse_col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) s = pl . from_numpy ( pulse_algorithms . summarize_data_numba ( raw , self . frametime_s , peak_samplenumber = self . peak_index , pretrigger_ignore_samples = self . pretrigger_ignore_samples , nPresamples = self . n_presamples , ) ) summaries . append ( s ) df2 = pl . concat ( summaries ) . with_columns ( df ) return df2 Tools for rough calibration of pulse heights to energies","title":"calc_from_df"},{"location":"docstrings/#mass2.core.rough_cal.BestAssignmentPfitGainResult","text":"Result of finding the best assignment of pulse heights to energies and fitting a polynomial gain curve. Source code in mass2/core/rough_cal.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 @dataclass ( frozen = True ) class BestAssignmentPfitGainResult : \"\"\"Result of finding the best assignment of pulse heights to energies and fitting a polynomial gain curve.\"\"\" rms_residual : float ph_assigned : np . ndarray residual_e : np . ndarray | None assignment_inds : np . ndarray | None pfit_gain : np . polynomial . Polynomial energy_target : np . ndarray names_target : list [ str ] # list of strings with names for the energies in energy_target ph_target : np . ndarray # longer than energy target by 0-3 def ph_unassigned ( self ) -> ndarray : \"\"\"Which pulse heights were not assigned to any energy.\"\"\" return np . array ( list ( set ( self . ph_target ) - set ( self . ph_assigned ))) def plot ( self , ax : Axes | None = None ) -> None : \"\"\"Make a diagnostic plot of the gain fit.\"\"\" if ax is None : plt . figure () ax = plt . gca () gain = self . ph_assigned / self . energy_target ax . plot ( self . ph_assigned , self . ph_assigned / self . energy_target , \"o\" ) ph_large_range = np . linspace ( 0 , self . ph_assigned [ - 1 ] * 1.1 , 51 ) ax . plot ( ph_large_range , self . pfit_gain ( ph_large_range )) ax . set_xlabel ( \"pulse_height\" ) ax . set_ylabel ( \"gain\" ) ax . set_title ( f \"BestAssignmentPfitGainResult rms_residual= { self . rms_residual : .2f } eV\" ) assert len ( self . names_target ) == len ( self . ph_assigned ) for name , x , y in zip ( self . names_target , self . ph_assigned , gain ): ax . annotate ( str ( name ), ( x , y )) def phzerogain ( self ) -> float : \"\"\"Find the pulse height where the gain goes to zero. Quadratic fits should have two roots, we want the positive one; if they are complex, choose the real part.\"\"\" # the pulse height at which the gain is zero # for now I'm counting on the roots being ordered, we want the positive root where gain goes zero # since our function is invalid outside that range if self . pfit_gain . degree () == 2 : return np . real ( self . pfit_gain . roots ()[ 1 ]) elif self . pfit_gain . degree () == 1 : return self . pfit_gain . roots ()[ 0 ] else : raise ValueError () def ph2energy ( self , ph : ndarray | float ) -> float | ndarray : \"\"\"Convert pulse height to energy using the fitted gain curve.\"\"\" return ph / self . pfit_gain ( ph ) def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"Invert the gain curve to convert energy to pulse height.\"\"\" if self . pfit_gain . degree () == 2 : return self . _energy2ph_deg2 ( energy ) elif self . pfit_gain . degree () == 1 : return self . _energy2ph_deg1 ( energy ) elif self . pfit_gain . degree () == 0 : return self . _energy2ph_deg0 ( energy ) else : raise Exception ( \"degree out of range\" ) def _energy2ph_deg2 ( self , energy : ArrayLike ) -> NDArray : \"\"\"Invert a 2nd degree polynomial gain curve to convert energy to pulse height.\"\"\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(c + b*x + a*x^2) # so # y*c + (y*b-1)*x + a*x^2 = 0 # and given that we've selected for well formed calibrations, # we know which root we want cba = self . pfit_gain . convert () . coef c , bb , a = cba * np . asarray ( energy ) b = bb - 1 ph = ( - b - np . sqrt ( b ** 2 - 4 * a * c )) / ( 2 * a ) return ph def _energy2ph_deg1 ( self , energy : ArrayLike ) -> NDArray : \"\"\"Invert a 1st degree polynomial gain curve to convert energy to pulse height.\"\"\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(b + a*x) # so # x = y*b/(1-y*a) # and given that we've selected for well formed calibrations, # we know which root we want b , a = self . pfit_gain . convert () . coef y = energy ph = y * b / ( 1 - y * a ) return ph def _energy2ph_deg0 ( self , energy : ArrayLike ) -> NDArray : \"\"\"Invert a 0th degree polynomial gain curve to convert energy to pulse height.\"\"\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(a) # so # x = y*a ( a ,) = self . pfit_gain . convert () . coef y = energy ph = y * a return ph def predicted_energies ( self ) -> NDArray | float : \"\"\"Convert the assigned pulse heights to energies using the fitted gain curve.\"\"\" return self . ph2energy ( self . ph_assigned )","title":"BestAssignmentPfitGainResult"},{"location":"docstrings/#mass2.core.rough_cal.BestAssignmentPfitGainResult.energy2ph","text":"Invert the gain curve to convert energy to pulse height. Source code in mass2/core/rough_cal.py 135 136 137 138 139 140 141 142 143 144 def energy2ph ( self , energy : ArrayLike ) -> NDArray : \"\"\"Invert the gain curve to convert energy to pulse height.\"\"\" if self . pfit_gain . degree () == 2 : return self . _energy2ph_deg2 ( energy ) elif self . pfit_gain . degree () == 1 : return self . _energy2ph_deg1 ( energy ) elif self . pfit_gain . degree () == 0 : return self . _energy2ph_deg0 ( energy ) else : raise Exception ( \"degree out of range\" )","title":"energy2ph"},{"location":"docstrings/#mass2.core.rough_cal.BestAssignmentPfitGainResult.ph2energy","text":"Convert pulse height to energy using the fitted gain curve. Source code in mass2/core/rough_cal.py 131 132 133 def ph2energy ( self , ph : ndarray | float ) -> float | ndarray : \"\"\"Convert pulse height to energy using the fitted gain curve.\"\"\" return ph / self . pfit_gain ( ph )","title":"ph2energy"},{"location":"docstrings/#mass2.core.rough_cal.BestAssignmentPfitGainResult.ph_unassigned","text":"Which pulse heights were not assigned to any energy. Source code in mass2/core/rough_cal.py 98 99 100 def ph_unassigned ( self ) -> ndarray : \"\"\"Which pulse heights were not assigned to any energy.\"\"\" return np . array ( list ( set ( self . ph_target ) - set ( self . ph_assigned )))","title":"ph_unassigned"},{"location":"docstrings/#mass2.core.rough_cal.BestAssignmentPfitGainResult.phzerogain","text":"Find the pulse height where the gain goes to zero. Quadratic fits should have two roots, we want the positive one; if they are complex, choose the real part. Source code in mass2/core/rough_cal.py 118 119 120 121 122 123 124 125 126 127 128 129 def phzerogain ( self ) -> float : \"\"\"Find the pulse height where the gain goes to zero. Quadratic fits should have two roots, we want the positive one; if they are complex, choose the real part.\"\"\" # the pulse height at which the gain is zero # for now I'm counting on the roots being ordered, we want the positive root where gain goes zero # since our function is invalid outside that range if self . pfit_gain . degree () == 2 : return np . real ( self . pfit_gain . roots ()[ 1 ]) elif self . pfit_gain . degree () == 1 : return self . pfit_gain . roots ()[ 0 ] else : raise ValueError ()","title":"phzerogain"},{"location":"docstrings/#mass2.core.rough_cal.BestAssignmentPfitGainResult.plot","text":"Make a diagnostic plot of the gain fit. Source code in mass2/core/rough_cal.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def plot ( self , ax : Axes | None = None ) -> None : \"\"\"Make a diagnostic plot of the gain fit.\"\"\" if ax is None : plt . figure () ax = plt . gca () gain = self . ph_assigned / self . energy_target ax . plot ( self . ph_assigned , self . ph_assigned / self . energy_target , \"o\" ) ph_large_range = np . linspace ( 0 , self . ph_assigned [ - 1 ] * 1.1 , 51 ) ax . plot ( ph_large_range , self . pfit_gain ( ph_large_range )) ax . set_xlabel ( \"pulse_height\" ) ax . set_ylabel ( \"gain\" ) ax . set_title ( f \"BestAssignmentPfitGainResult rms_residual= { self . rms_residual : .2f } eV\" ) assert len ( self . names_target ) == len ( self . ph_assigned ) for name , x , y in zip ( self . names_target , self . ph_assigned , gain ): ax . annotate ( str ( name ), ( x , y ))","title":"plot"},{"location":"docstrings/#mass2.core.rough_cal.BestAssignmentPfitGainResult.predicted_energies","text":"Convert the assigned pulse heights to energies using the fitted gain curve. Source code in mass2/core/rough_cal.py 184 185 186 def predicted_energies ( self ) -> NDArray | float : \"\"\"Convert the assigned pulse heights to energies using the fitted gain curve.\"\"\" return self . ph2energy ( self . ph_assigned )","title":"predicted_energies"},{"location":"docstrings/#mass2.core.rough_cal.RoughCalibrationStep","text":"Bases: RecipeStep A step to perform a rough calibration of pulse heights to energies. Source code in mass2/core/rough_cal.py 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 @dataclass ( frozen = True ) class RoughCalibrationStep ( RecipeStep ): \"\"\"A step to perform a rough calibration of pulse heights to energies.\"\"\" pfresult : SmoothedLocalMaximaResult | None assignment_result : BestAssignmentPfitGainResult | None ph2energy : Callable success : bool def calc_from_df ( self , df : DataFrame ) -> DataFrame : \"\"\"Apply the rough calibration to a dataframe.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2 def drop_debug ( self ) -> \"RoughCalibrationStep\" : \"\"\"Return a copy of this step with debug information removed.\"\"\" return dataclasses . replace ( self , pfresult = None , assignment_result = None ) def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> None : \"\"\"Create diagnostic plots of the rough calibration step.\"\"\" if self . success : self . dbg_plot_success ( df_after , ** kwargs ) else : self . dbg_plot_failure ( df_after , ** kwargs ) def dbg_plot_success ( self , df : DataFrame , ** kwargs : Any ) -> None : \"\"\"Create diagnostic plots of the rough calibration step, if it succeeded.\"\"\" _ , axs = plt . subplots ( 2 , 1 , figsize = ( 11 , 6 )) if self . assignment_result : self . assignment_result . plot ( ax = axs [ 0 ]) if self . pfresult : self . pfresult . plot ( self . assignment_result , ax = axs [ 1 ]) plt . tight_layout () def dbg_plot_failure ( self , df : DataFrame , ** kwargs : None ) -> None : \"\"\"Create diagnostic plots of the rough calibration step, if it failed.\"\"\" _ , axs = plt . subplots ( 2 , 1 , figsize = ( 11 , 6 )) if self . pfresult : self . pfresult . plot ( self . assignment_result , ax = axs [ 1 ]) plt . tight_layout () def energy2ph ( self , energy : ArrayLike ) -> NDArray | float : \"\"\"Convert energy to pulse height using the fitted gain curve.\"\"\" if self . assignment_result : return self . assignment_result . energy2ph ( energy ) return 0.0 @classmethod def learn_combinatoric ( cls , ch : Channel , line_names : list [ str ], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test.\"\"\" ( names , ee ) = line_names_and_energies ( line_names ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () assert len ( uncalibrated ) > 10 , \"not enough pulses\" pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = ph_smoothing_fwhm ) assignment_result = find_optimal_assignment2 ( pfresult . ph_sorted_by_prominence ()[: len ( ee ) + n_extra ], ee , names ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr . and_ ( pl . col ( uncalibrated_col ) < assignment_result . phzerogain ()), use_expr = use_expr , pfresult = pfresult , assignment_result = assignment_result , ph2energy = assignment_result . ph2energy , success = True , ) return step @classmethod def learn_combinatoric_height_info ( cls , ch : Channel , line_names : list [ str ], line_heights_allowed : list [ list [ int ]], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies.\"\"\" ( names , ee ) = line_names_and_energies ( line_names ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () assert len ( uncalibrated ) > 10 , \"not enough pulses\" pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = ph_smoothing_fwhm ) assignment_result = find_optimal_assignment2_height_info ( pfresult . ph_sorted_by_prominence ()[: len ( ee ) + n_extra ], ee , names , line_heights_allowed , ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr = use_expr , pfresult = pfresult , assignment_result = assignment_result , ph2energy = assignment_result . ph2energy , success = True , ) return step @classmethod def learn_3peak ( # noqa: PLR0917 PLR0914, cls , ch : Channel , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning 3 of them to known energies in a way that minimizes the RMS error in a local linearity test, and then evaluating that assignment by fitting a 2nd degree polynomial gain curve to all possible pulse heights and returning the RMS error in energy after applying that gain curve to all possible pulse heights. If no good assignment is found, the step will be marked as unsuccessful.\"\"\" if calibrated_col is None : calibrated_col = f \"energy_ { uncalibrated_col } \" ( line_names_str , line_energies_list ) = line_names_and_energies ( line_names ) line_energies = np . asarray ( line_energies_list ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = fwhm_pulse_height_units ) possible_phs = pfresult . ph_sorted_by_prominence ()[: len ( line_names_str ) + n_extra_peaks ] df3peak , _dfe = rank_3peak_assignments ( possible_phs , line_energies , line_names_str , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , ) best_rms_residual = np . inf best_assignment_result = None for assignment_row in df3peak . select ( \"e0\" , \"ph0\" , \"e1\" , \"ph1\" , \"e2\" , \"ph2\" , \"e_err_at_ph2\" ) . iter_rows (): e0 , ph0 , e1 , ph1 , e2 , ph2 , _e_err_at_ph2 = assignment_row pharray = np . array ([ ph0 , ph1 , ph2 ]) earray = np . array ([ e0 , e1 , e2 ]) rms_residual , assignment_result = eval_3peak_assignment_pfit_gain ( pharray , earray , possible_phs , line_energies , line_names_str ) if rms_residual < best_rms_residual : best_rms_residual = rms_residual best_assignment_result = assignment_result if rms_residual < acceptable_rms_residual_e : break if ( best_assignment_result and isinstance ( best_assignment_result , BestAssignmentPfitGainResult ) and not np . isinf ( best_rms_residual ) ): success = True ph2energy = best_assignment_result . ph2energy # df3peak_on_failure = None else : success = False def nanenergy ( ph : NDArray | float ) -> NDArray | float : \"Return NaN for all pulse heights, indicating failure to calibrate.\" return ph * np . nan ph2energy = nanenergy # df3peak_on_failure = df3peak # df3peak_on_failure = df3peak if isinstance ( best_assignment_result , str ): best_assignment_result = None step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr = use_expr , pfresult = pfresult , assignment_result = best_assignment_result , ph2energy = ph2energy , success = success , ) return step","title":"RoughCalibrationStep"},{"location":"docstrings/#mass2.core.rough_cal.RoughCalibrationStep.calc_from_df","text":"Apply the rough calibration to a dataframe. Source code in mass2/core/rough_cal.py 756 757 758 759 760 761 762 763 def calc_from_df ( self , df : DataFrame ) -> DataFrame : \"\"\"Apply the rough calibration to a dataframe.\"\"\" # only works with in memory data, but just takes it as numpy data and calls function # is much faster than map_elements approach, but wouldn't work with out of core data without some extra book keeping inputs_np = [ df [ input ] . to_numpy () for input in self . inputs ] out = self . ph2energy ( inputs_np [ 0 ]) df2 = pl . DataFrame ({ self . output [ 0 ]: out }) . with_columns ( df ) return df2","title":"calc_from_df"},{"location":"docstrings/#mass2.core.rough_cal.RoughCalibrationStep.dbg_plot","text":"Create diagnostic plots of the rough calibration step. Source code in mass2/core/rough_cal.py 769 770 771 772 773 774 def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> None : \"\"\"Create diagnostic plots of the rough calibration step.\"\"\" if self . success : self . dbg_plot_success ( df_after , ** kwargs ) else : self . dbg_plot_failure ( df_after , ** kwargs )","title":"dbg_plot"},{"location":"docstrings/#mass2.core.rough_cal.RoughCalibrationStep.dbg_plot_failure","text":"Create diagnostic plots of the rough calibration step, if it failed. Source code in mass2/core/rough_cal.py 785 786 787 788 789 790 def dbg_plot_failure ( self , df : DataFrame , ** kwargs : None ) -> None : \"\"\"Create diagnostic plots of the rough calibration step, if it failed.\"\"\" _ , axs = plt . subplots ( 2 , 1 , figsize = ( 11 , 6 )) if self . pfresult : self . pfresult . plot ( self . assignment_result , ax = axs [ 1 ]) plt . tight_layout ()","title":"dbg_plot_failure"},{"location":"docstrings/#mass2.core.rough_cal.RoughCalibrationStep.dbg_plot_success","text":"Create diagnostic plots of the rough calibration step, if it succeeded. Source code in mass2/core/rough_cal.py 776 777 778 779 780 781 782 783 def dbg_plot_success ( self , df : DataFrame , ** kwargs : Any ) -> None : \"\"\"Create diagnostic plots of the rough calibration step, if it succeeded.\"\"\" _ , axs = plt . subplots ( 2 , 1 , figsize = ( 11 , 6 )) if self . assignment_result : self . assignment_result . plot ( ax = axs [ 0 ]) if self . pfresult : self . pfresult . plot ( self . assignment_result , ax = axs [ 1 ]) plt . tight_layout ()","title":"dbg_plot_success"},{"location":"docstrings/#mass2.core.rough_cal.RoughCalibrationStep.drop_debug","text":"Return a copy of this step with debug information removed. Source code in mass2/core/rough_cal.py 765 766 767 def drop_debug ( self ) -> \"RoughCalibrationStep\" : \"\"\"Return a copy of this step with debug information removed.\"\"\" return dataclasses . replace ( self , pfresult = None , assignment_result = None )","title":"drop_debug"},{"location":"docstrings/#mass2.core.rough_cal.RoughCalibrationStep.energy2ph","text":"Convert energy to pulse height using the fitted gain curve. Source code in mass2/core/rough_cal.py 792 793 794 795 796 def energy2ph ( self , energy : ArrayLike ) -> NDArray | float : \"\"\"Convert energy to pulse height using the fitted gain curve.\"\"\" if self . assignment_result : return self . assignment_result . energy2ph ( energy ) return 0.0","title":"energy2ph"},{"location":"docstrings/#mass2.core.rough_cal.RoughCalibrationStep.learn_3peak","text":"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning 3 of them to known energies in a way that minimizes the RMS error in a local linearity test, and then evaluating that assignment by fitting a 2nd degree polynomial gain curve to all possible pulse heights and returning the RMS error in energy after applying that gain curve to all possible pulse heights. If no good assignment is found, the step will be marked as unsuccessful. Source code in mass2/core/rough_cal.py 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 @classmethod def learn_3peak ( # noqa: PLR0917 PLR0914, cls , ch : Channel , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning 3 of them to known energies in a way that minimizes the RMS error in a local linearity test, and then evaluating that assignment by fitting a 2nd degree polynomial gain curve to all possible pulse heights and returning the RMS error in energy after applying that gain curve to all possible pulse heights. If no good assignment is found, the step will be marked as unsuccessful.\"\"\" if calibrated_col is None : calibrated_col = f \"energy_ { uncalibrated_col } \" ( line_names_str , line_energies_list ) = line_names_and_energies ( line_names ) line_energies = np . asarray ( line_energies_list ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = fwhm_pulse_height_units ) possible_phs = pfresult . ph_sorted_by_prominence ()[: len ( line_names_str ) + n_extra_peaks ] df3peak , _dfe = rank_3peak_assignments ( possible_phs , line_energies , line_names_str , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , ) best_rms_residual = np . inf best_assignment_result = None for assignment_row in df3peak . select ( \"e0\" , \"ph0\" , \"e1\" , \"ph1\" , \"e2\" , \"ph2\" , \"e_err_at_ph2\" ) . iter_rows (): e0 , ph0 , e1 , ph1 , e2 , ph2 , _e_err_at_ph2 = assignment_row pharray = np . array ([ ph0 , ph1 , ph2 ]) earray = np . array ([ e0 , e1 , e2 ]) rms_residual , assignment_result = eval_3peak_assignment_pfit_gain ( pharray , earray , possible_phs , line_energies , line_names_str ) if rms_residual < best_rms_residual : best_rms_residual = rms_residual best_assignment_result = assignment_result if rms_residual < acceptable_rms_residual_e : break if ( best_assignment_result and isinstance ( best_assignment_result , BestAssignmentPfitGainResult ) and not np . isinf ( best_rms_residual ) ): success = True ph2energy = best_assignment_result . ph2energy # df3peak_on_failure = None else : success = False def nanenergy ( ph : NDArray | float ) -> NDArray | float : \"Return NaN for all pulse heights, indicating failure to calibrate.\" return ph * np . nan ph2energy = nanenergy # df3peak_on_failure = df3peak # df3peak_on_failure = df3peak if isinstance ( best_assignment_result , str ): best_assignment_result = None step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr = use_expr , pfresult = pfresult , assignment_result = best_assignment_result , ph2energy = ph2energy , success = success , ) return step","title":"learn_3peak"},{"location":"docstrings/#mass2.core.rough_cal.RoughCalibrationStep.learn_combinatoric","text":"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test. Source code in mass2/core/rough_cal.py 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 @classmethod def learn_combinatoric ( cls , ch : Channel , line_names : list [ str ], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test.\"\"\" ( names , ee ) = line_names_and_energies ( line_names ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () assert len ( uncalibrated ) > 10 , \"not enough pulses\" pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = ph_smoothing_fwhm ) assignment_result = find_optimal_assignment2 ( pfresult . ph_sorted_by_prominence ()[: len ( ee ) + n_extra ], ee , names ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr . and_ ( pl . col ( uncalibrated_col ) < assignment_result . phzerogain ()), use_expr = use_expr , pfresult = pfresult , assignment_result = assignment_result , ph2energy = assignment_result . ph2energy , success = True , ) return step","title":"learn_combinatoric"},{"location":"docstrings/#mass2.core.rough_cal.RoughCalibrationStep.learn_combinatoric_height_info","text":"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies. Source code in mass2/core/rough_cal.py 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 @classmethod def learn_combinatoric_height_info ( cls , ch : Channel , line_names : list [ str ], line_heights_allowed : list [ list [ int ]], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), ) -> \"RoughCalibrationStep\" : \"\"\"Train a rough calibration step by finding peaks in a smoothed histogram of pulse heights, and assigning them to known energies in a way that minimizes the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies.\"\"\" ( names , ee ) = line_names_and_energies ( line_names ) uncalibrated = ch . good_series ( uncalibrated_col , use_expr = use_expr ) . to_numpy () assert len ( uncalibrated ) > 10 , \"not enough pulses\" pfresult = peakfind_local_maxima_of_smoothed_hist ( uncalibrated , fwhm_pulse_height_units = ph_smoothing_fwhm ) assignment_result = find_optimal_assignment2_height_info ( pfresult . ph_sorted_by_prominence ()[: len ( ee ) + n_extra ], ee , names , line_heights_allowed , ) step = cls ( [ uncalibrated_col ], [ calibrated_col ], ch . good_expr , use_expr = use_expr , pfresult = pfresult , assignment_result = assignment_result , ph2energy = assignment_result . ph2energy , success = True , ) return step","title":"learn_combinatoric_height_info"},{"location":"docstrings/#mass2.core.rough_cal.SmoothedLocalMaximaResult","text":"A set of local maxima found in a smoothed histogram of pulse heights. Source code in mass2/core/rough_cal.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 @dataclass ( frozen = True ) class SmoothedLocalMaximaResult : \"\"\"A set of local maxima found in a smoothed histogram of pulse heights.\"\"\" fwhm_pulse_height_units : float bin_centers : np . ndarray counts : np . ndarray smoothed_counts : np . ndarray local_maxima_inds : np . ndarray # inds into bin_centers local_minima_inds : np . ndarray # inds into bin_centers def inds_sorted_by_peak_height ( self ) -> NDArray : \"\"\"Indices of local maxima sorted by peak height, highest first.\"\"\" return self . local_maxima_inds [ np . argsort ( - self . peak_height ())] def inds_sorted_by_prominence ( self ) -> NDArray : \"\"\"Indices of local maxima sorted by prominence, most prominent first.\"\"\" return self . local_maxima_inds [ np . argsort ( - self . prominence ())] def ph_sorted_by_prominence ( self ) -> NDArray : \"\"\"Pulse heights of local maxima sorted by prominence, most prominent first.\"\"\" return self . bin_centers [ self . inds_sorted_by_prominence ()] def ph_sorted_by_peak_height ( self ) -> NDArray : \"\"\"Pulse heights of local maxima sorted by peak height, highest first.\"\"\" return self . bin_centers [ self . inds_sorted_by_peak_height ()] def peak_height ( self ) -> NDArray : \"\"\"Peak heights of local maxima.\"\"\" return self . smoothed_counts [ self . local_maxima_inds ] def prominence ( self ) -> NDArray : \"\"\"Prominence of local maxima, in aems order as `local_maxima_inds`.\"\"\" assert len ( self . local_minima_inds ) == len ( self . local_maxima_inds ) + 1 , ( \"peakfind_local_maxima_of_smoothed_hist must ensure this \" ) prominence = np . zeros_like ( self . local_maxima_inds , dtype = float ) for i in range ( len ( self . local_maxima_inds )): sc_max = self . smoothed_counts [ self . local_maxima_inds [ i ]] sc_min_before = self . smoothed_counts [ self . local_minima_inds [ i ]] sc_min_after = self . smoothed_counts [ self . local_minima_inds [ i + 1 ]] prominence [ i ] = ( 2 * sc_max - sc_min_before - sc_min_after ) / 2 assert np . all ( prominence >= 0 ), \"prominence should be non-negative\" return prominence def plot ( self , assignment_result : BestAssignmentPfitGainResult | None = None , n_highlight : int = 10 , plot_counts : bool = False , ax : Axes | None = None , ) -> Axes : \"\"\"Make a diagnostic plot of the smoothed histogram and local maxima.\"\"\" if ax is None : plt . figure () ax = plt . gca () inds_prominence = self . inds_sorted_by_prominence ()[: n_highlight ] inds_peak_height = self . inds_sorted_by_peak_height ()[: n_highlight ] if plot_counts : ax . plot ( self . bin_centers , self . counts , label = \"counts\" ) ax . plot ( self . bin_centers , self . smoothed_counts , label = \"smoothed_counts\" ) ax . plot ( self . bin_centers [ self . local_maxima_inds ], self . smoothed_counts [ self . local_maxima_inds ], \".\" , label = \"peaks\" , ) if assignment_result is not None : inds_assigned = np . searchsorted ( self . bin_centers , assignment_result . ph_assigned ) inds_unassigned = np . searchsorted ( self . bin_centers , assignment_result . ph_unassigned ()) bin_centers_assigned = self . bin_centers [ inds_assigned ] bin_centers_unassigned = self . bin_centers [ inds_unassigned ] smoothed_counts_assigned = self . smoothed_counts [ inds_assigned ] smoothed_counts_unassigned = self . smoothed_counts [ inds_unassigned ] ax . plot ( bin_centers_assigned , smoothed_counts_assigned , \"o\" , label = \"assigned\" ) ax . plot ( bin_centers_unassigned , smoothed_counts_unassigned , \"o\" , label = \"unassigned\" , ) for name , x , y in zip ( assignment_result . names_target , bin_centers_assigned , smoothed_counts_assigned , ): ax . annotate ( str ( name ), ( x , y ), rotation = 30 ) ax . set_title ( f \"SmoothedLocalMaximaResult rms_residual= { assignment_result . rms_residual : .2f } eV\" ) else : ax . plot ( self . bin_centers [ inds_prominence ], self . smoothed_counts [ inds_prominence ], \"o\" , label = f \" { n_highlight } most prominent\" , ) ax . plot ( self . bin_centers [ inds_peak_height ], self . smoothed_counts [ inds_peak_height ], \"v\" , label = f \" { n_highlight } highest\" , ) ax . set_title ( \"SmoothedLocalMaximaResult\" ) ax . legend () ax . set_xlabel ( \"pulse height\" ) ax . set_ylabel ( \"intensity\" ) # print(f\"{np.amax(self.smoothed_counts)=} {np.amin(self.smoothed_counts)=} \") # ax.set_ylim(1/self.fwhm_pulse_height_units, ax.get_ylim()[1]) return ax","title":"SmoothedLocalMaximaResult"},{"location":"docstrings/#mass2.core.rough_cal.SmoothedLocalMaximaResult.inds_sorted_by_peak_height","text":"Indices of local maxima sorted by peak height, highest first. Source code in mass2/core/rough_cal.py 200 201 202 def inds_sorted_by_peak_height ( self ) -> NDArray : \"\"\"Indices of local maxima sorted by peak height, highest first.\"\"\" return self . local_maxima_inds [ np . argsort ( - self . peak_height ())]","title":"inds_sorted_by_peak_height"},{"location":"docstrings/#mass2.core.rough_cal.SmoothedLocalMaximaResult.inds_sorted_by_prominence","text":"Indices of local maxima sorted by prominence, most prominent first. Source code in mass2/core/rough_cal.py 204 205 206 def inds_sorted_by_prominence ( self ) -> NDArray : \"\"\"Indices of local maxima sorted by prominence, most prominent first.\"\"\" return self . local_maxima_inds [ np . argsort ( - self . prominence ())]","title":"inds_sorted_by_prominence"},{"location":"docstrings/#mass2.core.rough_cal.SmoothedLocalMaximaResult.peak_height","text":"Peak heights of local maxima. Source code in mass2/core/rough_cal.py 216 217 218 def peak_height ( self ) -> NDArray : \"\"\"Peak heights of local maxima.\"\"\" return self . smoothed_counts [ self . local_maxima_inds ]","title":"peak_height"},{"location":"docstrings/#mass2.core.rough_cal.SmoothedLocalMaximaResult.ph_sorted_by_peak_height","text":"Pulse heights of local maxima sorted by peak height, highest first. Source code in mass2/core/rough_cal.py 212 213 214 def ph_sorted_by_peak_height ( self ) -> NDArray : \"\"\"Pulse heights of local maxima sorted by peak height, highest first.\"\"\" return self . bin_centers [ self . inds_sorted_by_peak_height ()]","title":"ph_sorted_by_peak_height"},{"location":"docstrings/#mass2.core.rough_cal.SmoothedLocalMaximaResult.ph_sorted_by_prominence","text":"Pulse heights of local maxima sorted by prominence, most prominent first. Source code in mass2/core/rough_cal.py 208 209 210 def ph_sorted_by_prominence ( self ) -> NDArray : \"\"\"Pulse heights of local maxima sorted by prominence, most prominent first.\"\"\" return self . bin_centers [ self . inds_sorted_by_prominence ()]","title":"ph_sorted_by_prominence"},{"location":"docstrings/#mass2.core.rough_cal.SmoothedLocalMaximaResult.plot","text":"Make a diagnostic plot of the smoothed histogram and local maxima. Source code in mass2/core/rough_cal.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def plot ( self , assignment_result : BestAssignmentPfitGainResult | None = None , n_highlight : int = 10 , plot_counts : bool = False , ax : Axes | None = None , ) -> Axes : \"\"\"Make a diagnostic plot of the smoothed histogram and local maxima.\"\"\" if ax is None : plt . figure () ax = plt . gca () inds_prominence = self . inds_sorted_by_prominence ()[: n_highlight ] inds_peak_height = self . inds_sorted_by_peak_height ()[: n_highlight ] if plot_counts : ax . plot ( self . bin_centers , self . counts , label = \"counts\" ) ax . plot ( self . bin_centers , self . smoothed_counts , label = \"smoothed_counts\" ) ax . plot ( self . bin_centers [ self . local_maxima_inds ], self . smoothed_counts [ self . local_maxima_inds ], \".\" , label = \"peaks\" , ) if assignment_result is not None : inds_assigned = np . searchsorted ( self . bin_centers , assignment_result . ph_assigned ) inds_unassigned = np . searchsorted ( self . bin_centers , assignment_result . ph_unassigned ()) bin_centers_assigned = self . bin_centers [ inds_assigned ] bin_centers_unassigned = self . bin_centers [ inds_unassigned ] smoothed_counts_assigned = self . smoothed_counts [ inds_assigned ] smoothed_counts_unassigned = self . smoothed_counts [ inds_unassigned ] ax . plot ( bin_centers_assigned , smoothed_counts_assigned , \"o\" , label = \"assigned\" ) ax . plot ( bin_centers_unassigned , smoothed_counts_unassigned , \"o\" , label = \"unassigned\" , ) for name , x , y in zip ( assignment_result . names_target , bin_centers_assigned , smoothed_counts_assigned , ): ax . annotate ( str ( name ), ( x , y ), rotation = 30 ) ax . set_title ( f \"SmoothedLocalMaximaResult rms_residual= { assignment_result . rms_residual : .2f } eV\" ) else : ax . plot ( self . bin_centers [ inds_prominence ], self . smoothed_counts [ inds_prominence ], \"o\" , label = f \" { n_highlight } most prominent\" , ) ax . plot ( self . bin_centers [ inds_peak_height ], self . smoothed_counts [ inds_peak_height ], \"v\" , label = f \" { n_highlight } highest\" , ) ax . set_title ( \"SmoothedLocalMaximaResult\" ) ax . legend () ax . set_xlabel ( \"pulse height\" ) ax . set_ylabel ( \"intensity\" ) # print(f\"{np.amax(self.smoothed_counts)=} {np.amin(self.smoothed_counts)=} \") # ax.set_ylim(1/self.fwhm_pulse_height_units, ax.get_ylim()[1]) return ax","title":"plot"},{"location":"docstrings/#mass2.core.rough_cal.SmoothedLocalMaximaResult.prominence","text":"Prominence of local maxima, in aems order as local_maxima_inds . Source code in mass2/core/rough_cal.py 220 221 222 223 224 225 226 227 228 229 230 231 232 def prominence ( self ) -> NDArray : \"\"\"Prominence of local maxima, in aems order as `local_maxima_inds`.\"\"\" assert len ( self . local_minima_inds ) == len ( self . local_maxima_inds ) + 1 , ( \"peakfind_local_maxima_of_smoothed_hist must ensure this \" ) prominence = np . zeros_like ( self . local_maxima_inds , dtype = float ) for i in range ( len ( self . local_maxima_inds )): sc_max = self . smoothed_counts [ self . local_maxima_inds [ i ]] sc_min_before = self . smoothed_counts [ self . local_minima_inds [ i ]] sc_min_after = self . smoothed_counts [ self . local_minima_inds [ i + 1 ]] prominence [ i ] = ( 2 * sc_max - sc_min_before - sc_min_after ) / 2 assert np . all ( prominence >= 0 ), \"prominence should be non-negative\" return prominence","title":"prominence"},{"location":"docstrings/#mass2.core.rough_cal.drift_correct_entropy","text":"Calculate the entropy of a histogram of drift-corrected pulse heights. Source code in mass2/core/rough_cal.py 634 635 636 637 638 639 640 641 642 643 644 645 def drift_correct_entropy ( slope : float , indicator_zero_mean : ndarray , uncorrected : ndarray , bin_edges : ndarray , fwhm_in_bin_number_units : int , ) -> float : \"\"\"Calculate the entropy of a histogram of drift-corrected pulse heights.\"\"\" corrected = uncorrected * ( 1 + indicator_zero_mean * slope ) smoothed_counts , bin_edges , _counts = hist_smoothed ( corrected , fwhm_in_bin_number_units , bin_edges ) w = smoothed_counts > 0 return - ( np . log ( smoothed_counts [ w ]) * smoothed_counts [ w ]) . sum ()","title":"drift_correct_entropy"},{"location":"docstrings/#mass2.core.rough_cal.eval_3peak_assignment_pfit_gain","text":"Evaluate a proposed assignment of 3 pulse heights to 3 energies by fitting a 2nd degree polynomial gain curve, and returning the RMS residual in energy after applying that gain curve to all possible pulse heights. If the proposed assignment does not lead to a well formed gain curve, return infinity and a string describing the problem. Source code in mass2/core/rough_cal.py 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 def eval_3peak_assignment_pfit_gain ( ph_assigned : NDArray , e_assigned : NDArray , possible_phs : NDArray , line_energies : NDArray , line_names : list [ str ] ) -> tuple [ float , BestAssignmentPfitGainResult | str ]: \"\"\"Evaluate a proposed assignment of 3 pulse heights to 3 energies by fitting a 2nd degree polynomial gain curve, and returning the RMS residual in energy after applying that gain curve to all possible pulse heights. If the proposed assignment does not lead to a well formed gain curve, return infinity and a string describing the problem.\"\"\" assert len ( np . unique ( ph_assigned )) == len ( ph_assigned ), \"assignments must be unique\" assert len ( np . unique ( e_assigned )) == len ( e_assigned ), \"assignments must be unique\" assert all ( np . diff ( ph_assigned ) > 0 ), \"assignments must be sorted\" assert all ( np . diff ( e_assigned ) > 0 ), \"assignments must be sorted\" gain_assigned = np . array ( ph_assigned ) / np . array ( e_assigned ) pfit_gain_3peak = np . polynomial . Polynomial . fit ( ph_assigned , gain_assigned , deg = 2 ) if pfit_gain_3peak . deriv ( 1 )( 0 ) > 0 : # well formed calibration have negative derivative at zero pulse height return np . inf , \"pfit_gain_3peak deriv at 0 should be <0\" if pfit_gain_3peak ( 1e5 ) < 0 : # well formed calibration have positive gain at 1e5 return np . inf , \"pfit_gain_3peak should be above zero at 100k ph\" if any ( np . iscomplex ( pfit_gain_3peak . roots ())): # well formed calibrations have real roots return np . inf , \"pfit_gain_3peak must have real roots\" def ph2energy ( ph : NDArray ) -> NDArray : \"Convert pulse height to energy using the fitted gain curve.\" gain = pfit_gain_3peak ( ph ) return ph / gain cba = pfit_gain_3peak . convert () . coef def energy2ph ( energy : NDArray ) -> NDArray : \"Invert the gain curve to convert energy to pulse height.\" # ph2energy is equivalent to this with y=energy, x=ph # y = x/(c + b*x + a*x^2) # so # y*c + (y*b-1)*x + a*x^2 = 0 # and given that we've selected for well formed calibrations, # we know which root we want c , bb , a = cba * energy b = bb - 1 ph = ( - b - np . sqrt ( b ** 2 - 4 * a * c )) / ( 2 * a ) return ph predicted_ph = [ energy2ph ( _e ) for _e in line_energies ] df = pl . DataFrame ({ \"line_energy\" : line_energies , \"line_name\" : line_names , \"predicted_ph\" : predicted_ph , }) . sort ( by = \"predicted_ph\" ) dfph = pl . DataFrame ({ \"possible_ph\" : possible_phs , \"ph_ind\" : np . arange ( len ( possible_phs ))}) . sort ( by = \"possible_ph\" ) # for each e find the closest possible_ph to the calculaed predicted_ph # we started with assignments for 3 energies # now we have assignments for all energies df = df . join_asof ( dfph , left_on = \"predicted_ph\" , right_on = \"possible_ph\" , strategy = \"nearest\" ) n_unique = len ( df [ \"possible_ph\" ] . unique ()) if n_unique < len ( df ): # assigned multiple energies to same pulseheight, not a good cal return np . inf , \"assignments should be unique\" # now we evaluate the assignment and create a result object residual_e , pfit_gain = find_pfit_gain_residual ( df [ \"possible_ph\" ] . to_numpy (), df [ \"line_energy\" ] . to_numpy ()) if pfit_gain ( 1e5 ) < 0 : # well formed calibration have positive gain at 1e5 return np . inf , \"pfit_gain should be above zero at 100k ph\" if any ( np . iscomplex ( pfit_gain . roots ())): # well formed calibrations have real roots return np . inf , \"pfit_gain should not have complex roots\" rms_residual_e = mass2 . misc . root_mean_squared ( residual_e ) result = BestAssignmentPfitGainResult ( rms_residual_e , ph_assigned = df [ \"possible_ph\" ] . to_numpy (), residual_e = residual_e , assignment_inds = df [ \"ph_ind\" ] . to_numpy (), pfit_gain = pfit_gain , energy_target = df [ \"line_energy\" ] . to_numpy (), names_target = df [ \"line_name\" ] . to_list (), ph_target = possible_phs , ) return rms_residual_e , result","title":"eval_3peak_assignment_pfit_gain"},{"location":"docstrings/#mass2.core.rough_cal.find_best_residual_among_all_possible_assignments","text":"Try all possible assignments of pulse heights to energies, and return the one with the lowest RMS residual in energy after fitting a 2nd degree polynomial gain curve. Source code in mass2/core/rough_cal.py 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 def find_best_residual_among_all_possible_assignments ( ph : ndarray , e : ndarray ) -> tuple [ float , ndarray , ndarray , ndarray , Polynomial ]: \"\"\"Try all possible assignments of pulse heights to energies, and return the one with the lowest RMS residual in energy after fitting a 2nd degree polynomial gain curve. \"\"\" assert len ( ph ) >= len ( e ) ph = np . sort ( ph ) assignments_inds = itertools . combinations ( np . arange ( len ( ph )), len ( e )) best_rms_residual = np . inf best_ph_assigned = np . array ([]) best_residual_e = np . array ([]) best_assignment_inds = np . array ([]) best_pfit = Polynomial ([ 0 ]) for i , indices in enumerate ( assignments_inds ): assignment_inds = np . array ( indices ) ph_assigned = np . array ( ph [ assignment_inds ]) residual_e , pfit_gain = find_pfit_gain_residual ( ph_assigned , e ) rms_residual = mass2 . misc . root_mean_squared ( residual_e ) if rms_residual < best_rms_residual : best_rms_residual = rms_residual best_ph_assigned = ph_assigned best_residual_e = residual_e best_assignment_inds = assignment_inds best_pfit = pfit_gain return ( best_rms_residual , best_ph_assigned , best_residual_e , best_assignment_inds , best_pfit , )","title":"find_best_residual_among_all_possible_assignments"},{"location":"docstrings/#mass2.core.rough_cal.find_best_residual_among_all_possible_assignments2","text":"Try all possible assignments of pulse heights to energies, and return the one with the lowest RMS residual in energy after fitting a 2nd degree polynomial gain curve. Return as a BestAssignmentPfitGainResult object. Source code in mass2/core/rough_cal.py 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 def find_best_residual_among_all_possible_assignments2 ( ph : ndarray , e : ndarray , names : list [ str ]) -> BestAssignmentPfitGainResult : \"\"\"Try all possible assignments of pulse heights to energies, and return the one with the lowest RMS residual in energy after fitting a 2nd degree polynomial gain curve. Return as a BestAssignmentPfitGainResult object. \"\"\" ( best_rms_residual , best_ph_assigned , best_residual_e , best_assignment_inds , best_pfit , ) = find_best_residual_among_all_possible_assignments ( ph , e ) return BestAssignmentPfitGainResult ( float ( best_rms_residual ), best_ph_assigned , best_residual_e , best_assignment_inds , best_pfit , e , names , ph , )","title":"find_best_residual_among_all_possible_assignments2"},{"location":"docstrings/#mass2.core.rough_cal.find_local_maxima","text":"Smears each pulse by a gaussian of gaussian_fhwm and finds local maxima, returns a list of their locations in pulse_height units (sorted by number of pulses in peak) AND their peak values as: (peak_locations, peak_intensities) Args: pulse_heights (np.array(dtype=float)): a list of pulse heights (eg p_filt_value) gaussian_fwhm = fwhm of a gaussian that each pulse is smeared with, in same units as pulse heights Source code in mass2/core/rough_cal.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def find_local_maxima ( pulse_heights : ArrayLike , gaussian_fwhm : float ) -> Any : \"\"\"Smears each pulse by a gaussian of gaussian_fhwm and finds local maxima, returns a list of their locations in pulse_height units (sorted by number of pulses in peak) AND their peak values as: (peak_locations, peak_intensities) Args: pulse_heights (np.array(dtype=float)): a list of pulse heights (eg p_filt_value) gaussian_fwhm = fwhm of a gaussian that each pulse is smeared with, in same units as pulse heights \"\"\" # kernel density estimation (with a gaussian kernel) n = 128 * 1024 gaussian_fwhm = float ( gaussian_fwhm ) # The above ensures that lo & hi are floats, so that (lo-hi)/n is always a float in python2 sigma = gaussian_fwhm / ( np . sqrt ( np . log ( 2 ) * 2 ) * 2 ) tbw = 1.0 / sigma / ( np . pi * 2 ) lo = np . min ( pulse_heights ) - 3 * gaussian_fwhm hi = np . max ( pulse_heights ) + 3 * gaussian_fwhm hist , bins = np . histogram ( pulse_heights , np . linspace ( lo , hi , n + 1 )) tx = np . fft . rfftfreq ( n , ( lo - hi ) / n ) ty = np . exp ( - ( tx ** 2 ) / 2 / tbw ** 2 ) x = ( bins [ 1 :] + bins [: - 1 ]) / 2 y = np . fft . irfft ( np . fft . rfft ( hist ) * ty ) flag = ( y [ 1 : - 1 ] > y [: - 2 ]) & ( y [ 1 : - 1 ] > y [ 2 :]) lm = np . arange ( 1 , n - 1 )[ flag ] lm = lm [ np . argsort ( - y [ lm ])] bin_centers , _step_size = mass2 . misc . midpoints_and_step_size ( bins ) return np . array ( x [ lm ]), np . array ( y [ lm ]), ( hist , bin_centers , y )","title":"find_local_maxima"},{"location":"docstrings/#mass2.core.rough_cal.find_optimal_assignment","text":"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test. Source code in mass2/core/rough_cal.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def find_optimal_assignment ( ph : ArrayLike , e : ArrayLike ) -> tuple [ float , NDArray ]: \"\"\"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test.\"\"\" # ph is a list of peak heights longer than e # e is a list of known peak energies # we want to find the set of peak heights from ph that are closest to being locally linear with the energies in e # when given 3 or less energies to match, use the largest peaks in peak order ph = np . asarray ( ph ) e = np . asarray ( e ) assert len ( e ) >= 1 if len ( e ) <= 2 : return 0 , np . array ( sorted ( ph [: len ( e )])) rms_e_residual , pha , _pha_inds = rank_assignments ( ph , e ) ind = np . argmin ( rms_e_residual ) return rms_e_residual [ ind ], pha [ ind ]","title":"find_optimal_assignment"},{"location":"docstrings/#mass2.core.rough_cal.find_optimal_assignment2","text":"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, and fit a polynomial gain curve to the result. Source code in mass2/core/rough_cal.py 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 def find_optimal_assignment2 ( ph : ArrayLike , e : ArrayLike , line_names : list [ str ]) -> BestAssignmentPfitGainResult : \"\"\"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, and fit a polynomial gain curve to the result.\"\"\" ph = np . asarray ( ph ) e = np . asarray ( e ) rms_e_residual , pha = find_optimal_assignment ( ph , e ) gain = pha / e deg = min ( len ( e ) - 1 , 2 ) if deg == 0 : pfit_gain = np . polynomial . Polynomial ( gain ) else : pfit_gain = np . polynomial . Polynomial . fit ( pha , gain , deg = min ( len ( e ) - 1 , 2 )) result = BestAssignmentPfitGainResult ( rms_e_residual , ph_assigned = pha , residual_e = None , assignment_inds = None , pfit_gain = pfit_gain , energy_target = e , names_target = line_names , ph_target = ph , ) return result","title":"find_optimal_assignment2"},{"location":"docstrings/#mass2.core.rough_cal.find_optimal_assignment2_height_info","text":"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies, and fit a polynomial gain curve to the result. Source code in mass2/core/rough_cal.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 def find_optimal_assignment2_height_info ( ph : ArrayLike , e : ArrayLike , line_names : list [ str ], line_heights_allowed : ArrayLike ) -> BestAssignmentPfitGainResult : \"\"\"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies, and fit a polynomial gain curve to the result.\"\"\" rms_e_residual , pha = find_optimal_assignment_height_info ( ph , e , line_heights_allowed ) ph = np . asarray ( ph ) e = np . asarray ( e ) gain = pha / e deg = min ( len ( e ) - 1 , 2 ) if deg == 0 : pfit_gain = np . polynomial . Polynomial ( gain ) else : pfit_gain = np . polynomial . Polynomial . fit ( pha , gain , deg = min ( len ( e ) - 1 , 2 )) result = BestAssignmentPfitGainResult ( rms_e_residual , ph_assigned = pha , residual_e = None , assignment_inds = None , pfit_gain = pfit_gain , energy_target = e , names_target = line_names , ph_target = ph , ) return result","title":"find_optimal_assignment2_height_info"},{"location":"docstrings/#mass2.core.rough_cal.find_optimal_assignment_height_info","text":"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies. Source code in mass2/core/rough_cal.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 def find_optimal_assignment_height_info ( ph : ArrayLike , e : ArrayLike , line_heights_allowed : ArrayLike ) -> tuple [ float , NDArray ]: \"\"\"Find the optimal assignment of pulse heights to energies by minimizing the RMS error in a local linearity test, while respecting constraints on which pulse heights can be assigned to which energies.\"\"\" # ph is a list of peak heights longer than e # e is a list of known peak energies # we want to find the set of peak heights from ph that are closest to being locally linear with the energies in e # when given 3 or less energies to match, use the largest peaks in peak order ph = np . asarray ( ph ) e = np . asarray ( e ) line_heights_allowed = np . asarray ( line_heights_allowed ) assert len ( e ) >= 1 if len ( e ) <= 2 : return 0 , np . array ( sorted ( ph [: len ( e )])) rms_e_residual , pha , pha_inds = rank_assignments ( ph , e ) best_ind = None best_rms_residual = np . inf print ( f \" { e =} \" ) for i_assign in range ( len ( rms_e_residual )): rms_e_candidate = rms_e_residual [ i_assign ] # pha[i,:] is one choice of len(e) values from ph to assign to e pha_inds_candidate = pha_inds [ i_assign , :] if rms_e_candidate > best_rms_residual : continue # check if peaks mathch height info print ( f \" { pha_inds_candidate =} \" ) print ( f \" { line_heights_allowed =} \" ) failed_line_height_check = False for j in range ( len ( e )): if pha_inds_candidate [ j ] not in line_heights_allowed [ j ]: failed_line_height_check = True print ( \"not allowed\" ) a = pha_inds_candidate [ j ] b = line_heights_allowed [ j ] print ( f \" { a =} { b =} \" ) break if failed_line_height_check : continue print ( \"is new best!\" ) best_rms_residual = rms_e_candidate best_ind = i_assign if best_ind is None : raise Exception ( \"no assignment found satisfying peak height info\" ) return rms_e_residual [ best_ind ], pha [ best_ind , :]","title":"find_optimal_assignment_height_info"},{"location":"docstrings/#mass2.core.rough_cal.find_pfit_gain_residual","text":"Find a 2nd degree polynomial fit to the gain curve defined by ph/e, and return the residuals in energy when using that gain curve to convert ph to energy. Source code in mass2/core/rough_cal.py 562 563 564 565 566 567 568 569 570 571 572 573 574 575 def find_pfit_gain_residual ( ph : ndarray , e : ndarray ) -> tuple [ ndarray , Polynomial ]: \"\"\"Find a 2nd degree polynomial fit to the gain curve defined by ph/e, and return the residuals in energy when using that gain curve to convert ph to energy.\"\"\" assert len ( ph ) == len ( e ) gain = ph / e pfit_gain = np . polynomial . Polynomial . fit ( ph , gain , deg = 2 ) def ph2energy ( ph : NDArray ) -> NDArray : \"\"\"Convert pulse height to energy using the fitted gain curve.\"\"\" return ph / pfit_gain ( ph ) predicted_e = ph2energy ( ph ) residual_e = e - predicted_e return residual_e , pfit_gain","title":"find_pfit_gain_residual"},{"location":"docstrings/#mass2.core.rough_cal.hist_smoothed","text":"Compute a histogram of pulse heights and smooth it with a Gaussian of given FWHM. Source code in mass2/core/rough_cal.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 def hist_smoothed ( pulse_heights : ndarray , fwhm_pulse_height_units : float , bin_edges : ndarray | None = None , ) -> tuple [ ndarray , ndarray , ndarray ]: \"\"\"Compute a histogram of pulse heights and smooth it with a Gaussian of given FWHM.\"\"\" pulse_heights = pulse_heights . astype ( np . float64 ) # convert to float64 to avoid warpping subtraction and platform specific behavior regarding uint16s # linux CI will throw errors, while windows does not, but maybe is just silently wrong? assert len ( pulse_heights > 10 ), \"not enough pulses\" if bin_edges is None : n = 128 * 1024 lo = ( np . min ( pulse_heights ) - 3 * fwhm_pulse_height_units ) . astype ( np . float64 ) hi = ( np . max ( pulse_heights ) + 3 * fwhm_pulse_height_units ) . astype ( np . float64 ) bin_edges = np . linspace ( lo , hi , n + 1 ) _ , step_size = mass2 . misc . midpoints_and_step_size ( bin_edges ) counts , _ = np . histogram ( pulse_heights , bin_edges ) fwhm_in_bin_number_units = fwhm_pulse_height_units / step_size smoothed_counts = smooth_hist_with_gauassian_by_fft ( counts , fwhm_in_bin_number_units ) return smoothed_counts , bin_edges , counts","title":"hist_smoothed"},{"location":"docstrings/#mass2.core.rough_cal.local_maxima","text":"Find local maxima and minima, as 1D arrays. Source code in mass2/core/rough_cal.py 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def local_maxima ( y : ndarray ) -> tuple [ ndarray , ndarray ]: \"Find local maxima and minima, as 1D arrays.\" local_maxima_inds = [] local_minima_inds = [] increasing = False for i in range ( len ( y ) - 1 ): if increasing and ( y [ i + 1 ] < y [ i ]): local_maxima_inds . append ( i ) increasing = False if not increasing and ( y [ i + 1 ] > y [ i ]): local_minima_inds . append ( i ) increasing = True # increasing starts false, so we always start with a miniumum return np . array ( local_maxima_inds ), np . array ( local_minima_inds )","title":"local_maxima"},{"location":"docstrings/#mass2.core.rough_cal.minimize_entropy_linear","text":"Minimize the entropy of a histogram of drift-corrected pulse heights by varying the slope of a linear correction based on the given indicator. Source code in mass2/core/rough_cal.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 def minimize_entropy_linear ( indicator : ndarray , uncorrected : ndarray , bin_edges : ndarray , fwhm_in_bin_number_units : int , ) -> tuple [ OptimizeResult , float32 ]: \"\"\"Minimize the entropy of a histogram of drift-corrected pulse heights by varying the slope of a linear correction based on the given indicator.\"\"\" indicator_mean = np . mean ( indicator ) indicator_zero_mean = indicator - indicator_mean def entropy_fun ( slope : float ) -> float : \"\"\"Return the entropy of a histogram of drift-corrected pulse heights, the optimization target.\"\"\" return drift_correct_entropy ( slope , indicator_zero_mean , uncorrected , bin_edges , fwhm_in_bin_number_units ) result = sp . optimize . minimize_scalar ( entropy_fun , bracket = [ 0 , 0.1 ]) return result , indicator_mean","title":"minimize_entropy_linear"},{"location":"docstrings/#mass2.core.rough_cal.peakfind_local_maxima_of_smoothed_hist","text":"Find local maxima in a smoothed histogram of pulse heights. Source code in mass2/core/rough_cal.py 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def peakfind_local_maxima_of_smoothed_hist ( pulse_heights : ndarray , fwhm_pulse_height_units : float , bin_edges : ndarray | None = None , ) -> SmoothedLocalMaximaResult : \"\"\"Find local maxima in a smoothed histogram of pulse heights.\"\"\" pulse_heights = pulse_heights . astype ( np . float64 ) assert len ( pulse_heights > 10 ), \"not enough pulses\" smoothed_counts , bin_edges , counts = hist_smoothed ( pulse_heights , fwhm_pulse_height_units , bin_edges ) bin_centers , _step_size = mass2 . misc . midpoints_and_step_size ( bin_edges ) local_maxima_inds , local_minima_inds = local_maxima ( smoothed_counts ) # require a minimum before and after a maximum (the first check is redundant with behavior of local_maxima) if local_maxima_inds [ 0 ] < local_minima_inds [ 0 ]: local_maxima_inds = local_maxima_inds [ 1 :] if local_maxima_inds [ - 1 ] > local_minima_inds [ - 1 ]: local_maxima_inds = local_maxima_inds [: - 1 ] return SmoothedLocalMaximaResult ( fwhm_pulse_height_units , bin_centers , counts , smoothed_counts , local_maxima_inds , local_minima_inds , )","title":"peakfind_local_maxima_of_smoothed_hist"},{"location":"docstrings/#mass2.core.rough_cal.rank_3peak_assignments","text":"Explore and rank possible assignments of pulse heights to energies when there are 3 or more lines. Source code in mass2/core/rough_cal.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def rank_3peak_assignments ( ph : NDArray , e : NDArray , line_names : Iterable [ str ], max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , ) -> tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\"Explore and rank possible assignments of pulse heights to energies when there are 3 or more lines.\"\"\" # we explore possible line assignments, and down select based on knowledge of gain curve shape # gain = ph/e, and we assume gain starts at zero, decreases with pulse height, and # that a 2nd order polynomial is a reasonably good approximation # with one assignment we model the gain as constant, and use that to find the most likely # 2nd assignments, then we model the gain as linear, and use that to rank 3rd assignments dfe = pl . DataFrame ({ \"e0_ind\" : np . arange ( len ( e )), \"e0\" : e , \"name\" : line_names }) dfph = pl . DataFrame ({ \"ph0_ind\" : np . arange ( len ( ph )), \"ph0\" : ph }) # dfph should know about peak_area and use it to weight choices somehow # 1st assignments #### # e0 and ph0 are the first assignment df0 = dfe . join ( dfph , how = \"cross\" ) . with_columns ( gain0 = pl . col ( \"ph0\" ) / pl . col ( \"e0\" )) # 2nd assignments #### # e1 and ph1 are the 2nd assignment df1 = ( df0 . join ( df0 , how = \"cross\" ) . rename ({ \"e0_right\" : \"e1\" , \"ph0_right\" : \"ph1\" }) . drop ( \"e0_ind_right\" , \"ph0_ind_right\" , \"gain0_right\" ) ) # 1) keep only assignments with e0<e1 and ph0<ph1 to avoid looking at the same pair in reverse df1 = df1 . filter (( pl . col ( \"e0\" ) < pl . col ( \"e1\" )) . and_ ( pl . col ( \"ph0\" ) < pl . col ( \"ph1\" ))) # 2) the gain slope must be negative df1 = ( df1 . with_columns ( gain1 = pl . col ( \"ph1\" ) / pl . col ( \"e1\" )) . with_columns ( gain_slope = ( pl . col ( \"gain1\" ) - pl . col ( \"gain0\" )) / ( pl . col ( \"ph1\" ) - pl . col ( \"ph0\" ))) . filter ( pl . col ( \"gain_slope\" ) < 0 ) ) # 3) the gain slope should not have too large a magnitude df1 = df1 . with_columns ( gain_at_0 = pl . col ( \"gain0\" ) - pl . col ( \"ph0\" ) * pl . col ( \"gain_slope\" )) df1 = df1 . with_columns ( gain_frac_at_ph30k = ( 1 + 30000 * pl . col ( \"gain_slope\" ) / pl . col ( \"gain_at_0\" ))) df1 = df1 . filter ( pl . col ( \"gain_frac_at_ph30k\" ) > min_gain_fraction_at_ph_30k ) # 3rd assignments #### # e2 and ph2 are the 3rd assignment df2 = df1 . join ( df0 . select ( e2 = \"e0\" , ph2 = \"ph0\" ), how = \"cross\" ) df2 = df2 . with_columns ( gain_at_ph2 = pl . col ( \"gain_at_0\" ) + pl . col ( \"gain_slope\" ) * pl . col ( \"ph2\" )) df2 = df2 . with_columns ( e_at_ph2 = pl . col ( \"ph2\" ) / pl . col ( \"gain_at_ph2\" )) df2 = df2 . filter (( pl . col ( \"e1\" ) < pl . col ( \"e2\" )) . and_ ( pl . col ( \"ph1\" ) < pl . col ( \"ph2\" ))) # 1) rank 3rd assignments by energy error at ph2 assuming gain = gain_slope*ph+gain_at_0 # where gain_slope and gain are calculated from assignments 1 and 2 df2 = df2 . with_columns ( e_err_at_ph2 = pl . col ( \"e_at_ph2\" ) - pl . col ( \"e2\" )) . sort ( by = np . abs ( pl . col ( \"e_err_at_ph2\" ))) # 2) return a dataframe downselected to the assignments and the ranking criteria # 3) throw away assignments with large (default 10%) energy errors df3peak = df2 . select ( \"e0\" , \"ph0\" , \"e1\" , \"ph1\" , \"e2\" , \"ph2\" , \"e_err_at_ph2\" ) . filter ( np . abs ( pl . col ( \"e_err_at_ph2\" ) / pl . col ( \"e2\" )) < max_fractional_energy_error_3rd_assignment ) return df3peak , dfe","title":"rank_3peak_assignments"},{"location":"docstrings/#mass2.core.rough_cal.rank_assignments","text":"Rank possible assignments of pulse heights to energies by how locally linear their implied gain curves are. Source code in mass2/core/rough_cal.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def rank_assignments ( ph : ArrayLike , e : ArrayLike ) -> tuple [ NDArray , NDArray , NDArray ]: \"\"\"Rank possible assignments of pulse heights to energies by how locally linear their implied gain curves are.\"\"\" # ph is a list of peak heights longer than e # e is a list of known peak energies # we want to find the set of peak heights from ph that are closest to being locally linear with the energies in e e = np . array ( e ) ph = np . array ( ph ) e . sort () ph . sort () pha = np . array ( list ( itertools . combinations ( ph , len ( e )))) pha_inds = np . array ( list ( itertools . combinations ( np . arange ( len ( ph )), len ( e )))) # pha[i,:] is one choice of len(e) values from ph to assign to e # we use linear interpolation of the form y = y0 + (y1-y0)*(x-x0)/(x1-x0) # on each set of 3 values # with y = e and x = ph # x is pha[:,1:-1], x0 is pha[:,:-2], x1 is pha[:,2:] x = pha [:, 1 : - 1 ] x0 = pha [:, : - 2 ] x1 = pha [:, 2 :] y0 = e [: - 2 ] y1 = e [ 2 :] x_m_x0_over_x1_m_x0 = ( x - x0 ) / ( x1 - x0 ) y = y0 + ( y1 - y0 ) * x_m_x0_over_x1_m_x0 y_expected = e [ 1 : - 1 ] rms_e_residual = np . asarray ( mass2 . misc . root_mean_squared ( y - y_expected , axis = 1 )) # prefer negative slopes for gain # gain_first = ph[0]/e[0] # gain_last = ph[-1]/e[-1] return rms_e_residual , pha , pha_inds","title":"rank_assignments"},{"location":"docstrings/#mass2.core.rough_cal.smooth_hist_with_gauassian_by_fft","text":"Smooth a histogram by convolution with a Gaussian, using FFTs. Source code in mass2/core/rough_cal.py 302 303 304 305 306 def smooth_hist_with_gauassian_by_fft ( hist : ndarray , fwhm_in_bin_number_units : float ) -> ndarray : \"\"\"Smooth a histogram by convolution with a Gaussian, using FFTs.\"\"\" kernel = smooth_hist_with_gauassian_by_fft_compute_kernel ( len ( hist ), fwhm_in_bin_number_units ) y = np . fft . irfft ( np . fft . rfft ( hist ) * kernel ) return y","title":"smooth_hist_with_gauassian_by_fft"},{"location":"docstrings/#mass2.core.rough_cal.smooth_hist_with_gauassian_by_fft_compute_kernel","text":"Compute the DFT of a Gaussian kernel for smoothing a histogram. Source code in mass2/core/rough_cal.py 309 310 311 312 313 314 315 def smooth_hist_with_gauassian_by_fft_compute_kernel ( nbins : int , fwhm_in_bin_number_units : float ) -> ndarray : \"\"\"Compute the DFT of a Gaussian kernel for smoothing a histogram.\"\"\" sigma = fwhm_in_bin_number_units / ( np . sqrt ( np . log ( 2 ) * 2 ) * 2 ) tbw = 1.0 / sigma / ( np . pi * 2 ) tx = np . fft . rfftfreq ( nbins ) kernel = np . exp ( - ( tx ** 2 ) / 2 / tbw ** 2 ) return kernel Tools for working with continuous data, as taken by the True Bequerel project.","title":"smooth_hist_with_gauassian_by_fft_compute_kernel"},{"location":"docstrings/#mass2.core.truebq_bin.TriggerResult","text":"A trigger result from applying a triggering filter and threshold to a TrueBqBin data source. Source code in mass2/core/truebq_bin.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 @dataclass ( frozen = True ) class TriggerResult : \"\"\"A trigger result from applying a triggering filter and threshold to a TrueBqBin data source.\"\"\" data_source : \"TrueBqBin\" filter_in : np . ndarray threshold : float trig_inds : np . ndarray limit_samples : int def plot ( self , decimate : int = 10 , n_limit : int = 100000 , offset_raw : int = 0 , x_axis_time_s : bool = False , ax : plt . Axes | None = None ) -> None : \"\"\"Make a diagnostic plot of the trigger result.\"\"\" if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # raw (full-resolution) index ranges raw_start = offset_raw raw_stop = raw_start + n_limit * decimate data = self . data_source . data # scaling for x-axis (applied after decimation) x_scale = self . data_source . frametime_s * decimate if x_axis_time_s else 1 # raw filter output filt_raw = fast_apply_filter ( data [ raw_start : raw_stop ], self . filter_in ) # decimated data and filter data_dec = data [ raw_start : raw_stop : decimate ] filt_dec = filt_raw [:: decimate ] # truncate to the same length n = min ( len ( data_dec ), len ( filt_dec )) data_dec = data_dec [: n ] filt_dec = filt_dec [: n ] # shared x-axis x_dec = np . arange ( n ) * x_scale # plot data + filter plt . plot ( x_dec , data_dec , \".\" , label = \"data\" ) plt . plot ( x_dec , filt_dec , label = \"filter_out\" ) plt . axhline ( self . threshold , label = \"threshold\" ) # trigger indices (raw) \u2192 restrict to plotted window \u2192 convert to decimated indices trig_inds_raw = ( pl . DataFrame ({ \"trig_inds\" : self . trig_inds }) . filter ( pl . col ( \"trig_inds\" ) . is_between ( raw_start , raw_stop )) . to_series () . to_numpy () ) trig_inds_dec = ( trig_inds_raw - raw_start ) // decimate # clip to avoid indexing past n trig_inds_dec = trig_inds_dec [ trig_inds_dec < n ] plt . plot ( x_dec [ trig_inds_dec ], filt_dec [ trig_inds_dec ], \"o\" , label = \"trig_inds filt\" ) plt . plot ( x_dec [ trig_inds_dec ], data_dec [ trig_inds_dec ], \"o\" , label = \"trig_inds data\" ) # labels plt . title ( f \" { self . data_source . description } , trigger result debug plot\" ) plt . legend () plt . xlabel ( \"time with arb offset / s\" if x_axis_time_s else \"sample number (decimated)\" ) plt . ylabel ( \"signal (arb)\" ) def get_noise ( self , n_dead_samples_after_pulse_trigger : int , n_record_samples : int , max_noise_triggers : int = 200 , ) -> NoiseChannel : \"\"\"Synthesize a NoiseChannel from the data source by finding time periods without pulse triggers.\"\"\" noise_trigger_inds = get_noise_trigger_inds ( self . trig_inds , n_dead_samples_after_pulse_trigger , n_record_samples , max_noise_triggers , ) inds = noise_trigger_inds [ noise_trigger_inds > 0 ] # ensure all inds are greater than 0 inds = inds [ inds < ( len ( self . data_source . data ) - n_record_samples )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous ( self . data_source . data , npre = 0 , nsamples = n_record_samples , inds = inds , ) df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) noise = NoiseChannel ( df , header_df = self . data_source . header_df , frametime_s = self . data_source . frametime_s , ) return noise def to_channel_copy_to_memory ( self , noise_n_dead_samples_after_pulse_trigger : int , npre : int , npost : int , invert : bool = False ) -> Channel : \"\"\"Create a Channel object by copying pulse data into memory.\"\"\" noise = self . get_noise ( noise_n_dead_samples_after_pulse_trigger , npre + npost , max_noise_triggers = 1000 , ) inds = self . trig_inds [ self . trig_inds > npre ] inds = inds [ inds < ( len ( self . data_source . data ) - npre - npost )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous ( self . data_source . data , npre = npre , nsamples = npre + npost , inds = inds ) assert pulses . shape [ 0 ] == len ( inds ), \"pulses and trig_inds must have the same length\" if invert : df = pl . DataFrame ({ \"pulse\" : pulses * - 1 , \"framecount\" : inds }) else : df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) ch_header = ChannelHeader ( self . data_source . description , self . data_source . channel_number , self . data_source . frametime_s , npre , npre + npost , self . data_source . header_df , ) ch = Channel ( df , ch_header , npulses = len ( pulses ), noise = noise ) return ch def to_channel_mmap ( self , noise_n_dead_samples_after_pulse_trigger : int , npre : int , npost : int , invert : bool = False , verbose : bool = True , ) -> Channel : \"\"\"Create a Channel object by memory-mapping pulse data from disk.\"\"\" noise = self . get_noise ( noise_n_dead_samples_after_pulse_trigger , npre + npost , max_noise_triggers = 1000 , ) inds = self . trig_inds [ self . trig_inds > npre ] # ensure all inds inbounds inds = inds [ inds < ( len ( self . data_source . data ) - npre - npost )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous_mmap_with_cache ( self . data_source . data , npre = npre , nsamples = npre + npost , inds = inds , bin_path = self . data_source . bin_path , verbose = verbose , ) if invert : df = pl . DataFrame ({ \"pulse\" : pulses * - 1 , \"framecount\" : inds }) else : df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) ch_header = ChannelHeader ( self . data_source . description , self . data_source . channel_number , self . data_source . frametime_s , npre , npre + npost , self . data_source . header_df , ) ch = Channel ( df , ch_header , npulses = len ( pulses ), noise = noise ) return ch","title":"TriggerResult"},{"location":"docstrings/#mass2.core.truebq_bin.TriggerResult.get_noise","text":"Synthesize a NoiseChannel from the data source by finding time periods without pulse triggers. Source code in mass2/core/truebq_bin.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def get_noise ( self , n_dead_samples_after_pulse_trigger : int , n_record_samples : int , max_noise_triggers : int = 200 , ) -> NoiseChannel : \"\"\"Synthesize a NoiseChannel from the data source by finding time periods without pulse triggers.\"\"\" noise_trigger_inds = get_noise_trigger_inds ( self . trig_inds , n_dead_samples_after_pulse_trigger , n_record_samples , max_noise_triggers , ) inds = noise_trigger_inds [ noise_trigger_inds > 0 ] # ensure all inds are greater than 0 inds = inds [ inds < ( len ( self . data_source . data ) - n_record_samples )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous ( self . data_source . data , npre = 0 , nsamples = n_record_samples , inds = inds , ) df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) noise = NoiseChannel ( df , header_df = self . data_source . header_df , frametime_s = self . data_source . frametime_s , ) return noise","title":"get_noise"},{"location":"docstrings/#mass2.core.truebq_bin.TriggerResult.plot","text":"Make a diagnostic plot of the trigger result. Source code in mass2/core/truebq_bin.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def plot ( self , decimate : int = 10 , n_limit : int = 100000 , offset_raw : int = 0 , x_axis_time_s : bool = False , ax : plt . Axes | None = None ) -> None : \"\"\"Make a diagnostic plot of the trigger result.\"\"\" if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # raw (full-resolution) index ranges raw_start = offset_raw raw_stop = raw_start + n_limit * decimate data = self . data_source . data # scaling for x-axis (applied after decimation) x_scale = self . data_source . frametime_s * decimate if x_axis_time_s else 1 # raw filter output filt_raw = fast_apply_filter ( data [ raw_start : raw_stop ], self . filter_in ) # decimated data and filter data_dec = data [ raw_start : raw_stop : decimate ] filt_dec = filt_raw [:: decimate ] # truncate to the same length n = min ( len ( data_dec ), len ( filt_dec )) data_dec = data_dec [: n ] filt_dec = filt_dec [: n ] # shared x-axis x_dec = np . arange ( n ) * x_scale # plot data + filter plt . plot ( x_dec , data_dec , \".\" , label = \"data\" ) plt . plot ( x_dec , filt_dec , label = \"filter_out\" ) plt . axhline ( self . threshold , label = \"threshold\" ) # trigger indices (raw) \u2192 restrict to plotted window \u2192 convert to decimated indices trig_inds_raw = ( pl . DataFrame ({ \"trig_inds\" : self . trig_inds }) . filter ( pl . col ( \"trig_inds\" ) . is_between ( raw_start , raw_stop )) . to_series () . to_numpy () ) trig_inds_dec = ( trig_inds_raw - raw_start ) // decimate # clip to avoid indexing past n trig_inds_dec = trig_inds_dec [ trig_inds_dec < n ] plt . plot ( x_dec [ trig_inds_dec ], filt_dec [ trig_inds_dec ], \"o\" , label = \"trig_inds filt\" ) plt . plot ( x_dec [ trig_inds_dec ], data_dec [ trig_inds_dec ], \"o\" , label = \"trig_inds data\" ) # labels plt . title ( f \" { self . data_source . description } , trigger result debug plot\" ) plt . legend () plt . xlabel ( \"time with arb offset / s\" if x_axis_time_s else \"sample number (decimated)\" ) plt . ylabel ( \"signal (arb)\" )","title":"plot"},{"location":"docstrings/#mass2.core.truebq_bin.TriggerResult.to_channel_copy_to_memory","text":"Create a Channel object by copying pulse data into memory. Source code in mass2/core/truebq_bin.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def to_channel_copy_to_memory ( self , noise_n_dead_samples_after_pulse_trigger : int , npre : int , npost : int , invert : bool = False ) -> Channel : \"\"\"Create a Channel object by copying pulse data into memory.\"\"\" noise = self . get_noise ( noise_n_dead_samples_after_pulse_trigger , npre + npost , max_noise_triggers = 1000 , ) inds = self . trig_inds [ self . trig_inds > npre ] inds = inds [ inds < ( len ( self . data_source . data ) - npre - npost )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous ( self . data_source . data , npre = npre , nsamples = npre + npost , inds = inds ) assert pulses . shape [ 0 ] == len ( inds ), \"pulses and trig_inds must have the same length\" if invert : df = pl . DataFrame ({ \"pulse\" : pulses * - 1 , \"framecount\" : inds }) else : df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) ch_header = ChannelHeader ( self . data_source . description , self . data_source . channel_number , self . data_source . frametime_s , npre , npre + npost , self . data_source . header_df , ) ch = Channel ( df , ch_header , npulses = len ( pulses ), noise = noise ) return ch","title":"to_channel_copy_to_memory"},{"location":"docstrings/#mass2.core.truebq_bin.TriggerResult.to_channel_mmap","text":"Create a Channel object by memory-mapping pulse data from disk. Source code in mass2/core/truebq_bin.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def to_channel_mmap ( self , noise_n_dead_samples_after_pulse_trigger : int , npre : int , npost : int , invert : bool = False , verbose : bool = True , ) -> Channel : \"\"\"Create a Channel object by memory-mapping pulse data from disk.\"\"\" noise = self . get_noise ( noise_n_dead_samples_after_pulse_trigger , npre + npost , max_noise_triggers = 1000 , ) inds = self . trig_inds [ self . trig_inds > npre ] # ensure all inds inbounds inds = inds [ inds < ( len ( self . data_source . data ) - npre - npost )] # ensure all inds inbounds pulses = gather_pulses_from_inds_numpy_contiguous_mmap_with_cache ( self . data_source . data , npre = npre , nsamples = npre + npost , inds = inds , bin_path = self . data_source . bin_path , verbose = verbose , ) if invert : df = pl . DataFrame ({ \"pulse\" : pulses * - 1 , \"framecount\" : inds }) else : df = pl . DataFrame ({ \"pulse\" : pulses , \"framecount\" : inds }) ch_header = ChannelHeader ( self . data_source . description , self . data_source . channel_number , self . data_source . frametime_s , npre , npre + npost , self . data_source . header_df , ) ch = Channel ( df , ch_header , npulses = len ( pulses ), noise = noise ) return ch","title":"to_channel_mmap"},{"location":"docstrings/#mass2.core.truebq_bin.TrueBqBin","text":"Represents a binary data file from the True Bequerel project. Source code in mass2/core/truebq_bin.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 @dataclass ( frozen = True ) class TrueBqBin : \"\"\"Represents a binary data file from the True Bequerel project.\"\"\" bin_path : Path description : str channel_number : int header_df : pl . DataFrame frametime_s : float voltage_scale : float data : np . ndarray # the bin file is a continuous data aqusition, untriggered @classmethod def load ( cls , bin_path : str | Path ) -> \"TrueBqBin\" : \"\"\"Create a TrueBqBin object by memory-mapping the given binary file.\"\"\" bin_path = Path ( bin_path ) try : # for when it's named like dev2_ai6 channel_number = int ( str ( bin_path . parent )[ - 1 ]) except ValueError : # for when it's named like 2A def bay2int ( bay : str ) -> int : \"\"\"Convert a bay name like '2A' to a channel number like 4.\"\"\" return ( int ( bay [ 0 ]) - 1 ) * 4 + \"ABCD\" . index ( bay [ 1 ] . upper ()) channel_number = bay2int ( str ( bin_path . parent . stem )) desc = str ( bin_path . parent . parent . stem ) + \"_\" + str ( bin_path . parent . stem ) header_np = np . memmap ( bin_path , dtype = header_dtype , mode = \"r\" , offset = 0 , shape = 1 ) sample_rate_hz = header_np [ \"sample_rate_hz\" ][ 0 ] header_df = pl . from_numpy ( header_np ) data = np . memmap ( bin_path , dtype = np . int16 , mode = \"r\" , offset = 68 ) return cls ( bin_path , desc , channel_number , header_df , 1 / sample_rate_hz , header_np [ \"voltage_scale\" ][ 0 ], data , ) def trigger ( self , filter_in : NDArray , threshold : float , limit_hours : float | None = None , verbose : bool = True ) -> TriggerResult : \"\"\"Compute trigger indices by applying the given filter and threshold to the data.\"\"\" if limit_hours is None : limit_samples = len ( self . data ) else : limit_samples = int ( limit_hours * 3600 / self . frametime_s ) trig_inds = _fasttrig_filter_trigger_with_cache ( self . data , filter_in , threshold , limit_samples , self . bin_path , verbose = verbose ) return TriggerResult ( self , filter_in , threshold , trig_inds , limit_samples )","title":"TrueBqBin"},{"location":"docstrings/#mass2.core.truebq_bin.TrueBqBin.load","text":"Create a TrueBqBin object by memory-mapping the given binary file. Source code in mass2/core/truebq_bin.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @classmethod def load ( cls , bin_path : str | Path ) -> \"TrueBqBin\" : \"\"\"Create a TrueBqBin object by memory-mapping the given binary file.\"\"\" bin_path = Path ( bin_path ) try : # for when it's named like dev2_ai6 channel_number = int ( str ( bin_path . parent )[ - 1 ]) except ValueError : # for when it's named like 2A def bay2int ( bay : str ) -> int : \"\"\"Convert a bay name like '2A' to a channel number like 4.\"\"\" return ( int ( bay [ 0 ]) - 1 ) * 4 + \"ABCD\" . index ( bay [ 1 ] . upper ()) channel_number = bay2int ( str ( bin_path . parent . stem )) desc = str ( bin_path . parent . parent . stem ) + \"_\" + str ( bin_path . parent . stem ) header_np = np . memmap ( bin_path , dtype = header_dtype , mode = \"r\" , offset = 0 , shape = 1 ) sample_rate_hz = header_np [ \"sample_rate_hz\" ][ 0 ] header_df = pl . from_numpy ( header_np ) data = np . memmap ( bin_path , dtype = np . int16 , mode = \"r\" , offset = 68 ) return cls ( bin_path , desc , channel_number , header_df , 1 / sample_rate_hz , header_np [ \"voltage_scale\" ][ 0 ], data , )","title":"load"},{"location":"docstrings/#mass2.core.truebq_bin.TrueBqBin.trigger","text":"Compute trigger indices by applying the given filter and threshold to the data. Source code in mass2/core/truebq_bin.py 302 303 304 305 306 307 308 309 def trigger ( self , filter_in : NDArray , threshold : float , limit_hours : float | None = None , verbose : bool = True ) -> TriggerResult : \"\"\"Compute trigger indices by applying the given filter and threshold to the data.\"\"\" if limit_hours is None : limit_samples = len ( self . data ) else : limit_samples = int ( limit_hours * 3600 / self . frametime_s ) trig_inds = _fasttrig_filter_trigger_with_cache ( self . data , filter_in , threshold , limit_samples , self . bin_path , verbose = verbose ) return TriggerResult ( self , filter_in , threshold , trig_inds , limit_samples )","title":"trigger"},{"location":"docstrings/#mass2.core.truebq_bin.fast_apply_filter","text":"Apply a filter to the data, returning the filter output. Source code in mass2/core/truebq_bin.py 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 @njit def fast_apply_filter ( data : NDArray , filter_in : NDArray ) -> NDArray : \"\"\"Apply a filter to the data, returning the filter output.\"\"\" cache = np . zeros ( len ( filter_in )) filter = np . zeros ( len ( filter_in )) filter [:] = filter_in filter_len = len ( filter ) filter_out = np . zeros ( len ( data ) - len ( filter )) j = 0 jmax = len ( data ) - filter_len - 1 while j <= jmax : cache [:] = data [ j : ( j + filter_len )] filter_out [ j ] = np . dot ( cache , filter ) j += 1 return filter_out","title":"fast_apply_filter"},{"location":"docstrings/#mass2.core.truebq_bin.fasttrig_filter_trigger","text":"Apply a filter to the data and return trigger indices where the filter output crosses the threshold. Source code in mass2/core/truebq_bin.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 @njit def fasttrig_filter_trigger ( data : NDArray , filter_in : NDArray , threshold : float , verbose : bool ) -> NDArray : \"\"\"Apply a filter to the data and return trigger indices where the filter output crosses the threshold.\"\"\" assert threshold > 0 , \"algorithm assumes we trigger with positive threshold, change sign of filter_in to accomodate\" filter_len = len ( filter_in ) inds = [] jmax = len ( data ) - filter_len - 1 # njit only likes float64s, so I'm trying to force float64 use without allocating a ton of memory cache = np . zeros ( len ( filter_in )) filter = np . zeros ( len ( filter_in )) filter [:] = filter_in # intitalize a,b,c j = 0 cache [:] = data [ j : ( j + filter_len )] b = np . dot ( cache , filter ) a = b # won't be used, just need same type j = 1 cache [:] = data [ j : ( j + filter_len )] c = np . dot ( cache , filter ) j = 2 ready = False prog_step = jmax // 100 prog_ticks = 0 while j <= jmax : if j % prog_step == 0 : prog_ticks += 1 if verbose : print ( f \"fasttrig_filter_trigger { prog_ticks } / { 100 } \" ) a , b = b , c cache [:] = data [ j : ( j + filter_len )] c = np . dot ( cache , filter ) if b > threshold and b >= c and b > a and ready : inds . append ( j ) ready = False if b < 0 : # hold off on retriggering until we see opposite sign slope ready = True j += 1 return np . array ( inds )","title":"fasttrig_filter_trigger"},{"location":"docstrings/#mass2.core.truebq_bin.filter_and_residual_rms","text":"Apply a filter to pulses extracted from data at the given trigger indices, returning filter values and residual RMS. Source code in mass2/core/truebq_bin.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 def filter_and_residual_rms ( data : NDArray , chosen_filter : NDArray , avg_pulse : NDArray , trig_inds : NDArray , npre : int , nsamples : int , polarity : int ) -> tuple [ NDArray , NDArray , NDArray ]: \"\"\"Apply a filter to pulses extracted from data at the given trigger indices, returning filter values and residual RMS.\"\"\" filt_value = np . zeros ( len ( trig_inds )) residual_rms = np . zeros ( len ( trig_inds )) filt_value_template = np . zeros ( len ( trig_inds )) template = avg_pulse - np . mean ( avg_pulse ) template /= np . sqrt ( np . dot ( template , template )) for i in range ( len ( trig_inds )): j = trig_inds [ i ] pulse = data [ j - npre : j + nsamples - npre ] * polarity pulse -= pulse . mean () filt_value [ i ] = np . dot ( chosen_filter , pulse ) filt_value_template [ i ] = np . dot ( template , pulse ) residual = pulse - template * filt_value_template [ i ] residual_rms_val = misc . root_mean_squared ( residual ) residual_rms [ i ] = residual_rms_val return filt_value , residual_rms , filt_value_template","title":"filter_and_residual_rms"},{"location":"docstrings/#mass2.core.truebq_bin.gather_pulses_from_inds_numpy_contiguous","text":"Gather pulses from data at the given trigger indices, returning a contiguous numpy array. Source code in mass2/core/truebq_bin.py 438 439 440 441 442 443 444 445 446 def gather_pulses_from_inds_numpy_contiguous ( data : NDArray , npre : int , nsamples : int , inds : NDArray ) -> NDArray : \"\"\"Gather pulses from data at the given trigger indices, returning a contiguous numpy array.\"\"\" assert all ( inds > npre ), \"all inds must be greater than npre\" assert all ( inds < ( len ( data ) - nsamples )), \"all inds must be less than len(data) - nsamples\" offsets = inds - npre # shift by npre to start at correct offset pulses = np . zeros (( len ( offsets ), nsamples ), dtype = np . int16 ) for i , offset in enumerate ( offsets ): pulses [ i , :] = data [ offset : offset + nsamples ] return pulses","title":"gather_pulses_from_inds_numpy_contiguous"},{"location":"docstrings/#mass2.core.truebq_bin.gather_pulses_from_inds_numpy_contiguous_mmap","text":"Gather pulses from data at the given trigger indices, returning a memory-mapped numpy array. Source code in mass2/core/truebq_bin.py 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def gather_pulses_from_inds_numpy_contiguous_mmap ( data : NDArray , npre : int , nsamples : int , inds : NDArray , filename : str | Path = \".mmapped_pulses.npy\" ) -> NDArray : \"\"\"Gather pulses from data at the given trigger indices, returning a memory-mapped numpy array.\"\"\" assert all ( inds > npre ), \"all inds must be greater than npre\" assert all ( inds < ( len ( data ) - nsamples )), \"all inds must be less than len(data) - nsamples\" offsets = inds - npre # shift by npre to start at correct offset pulses = np . memmap ( filename , dtype = np . int16 , mode = \"w+\" , shape = ( len ( offsets ), nsamples )) for i , offset in enumerate ( offsets ): pulses [ i , :] = data [ offset : offset + nsamples ] pulses . flush () # re-open the mmap to ensure it is read-only del pulses pulses = np . memmap ( filename , dtype = np . int16 , mode = \"r\" , shape = ( len ( offsets ), nsamples )) return pulses","title":"gather_pulses_from_inds_numpy_contiguous_mmap"},{"location":"docstrings/#mass2.core.truebq_bin.gather_pulses_from_inds_numpy_contiguous_mmap_with_cache","text":"Gather pulses from data at the given trigger indices, returning a memory-mapped numpy array, using a cache. Source code in mass2/core/truebq_bin.py 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 def gather_pulses_from_inds_numpy_contiguous_mmap_with_cache ( data : NDArray , npre : int , nsamples : int , inds : NDArray , bin_path : Path | str , verbose : bool = True ) -> NDArray | np . memmap : \"\"\"Gather pulses from data at the given trigger indices, returning a memory-mapped numpy array, using a cache.\"\"\" bin_full_path = Path ( bin_path ) . absolute () inds = inds [ inds > npre ] # ensure all inds inbounds inds = inds [ inds < ( len ( data ) - nsamples )] # ensure all inds inbounds inds_hash = hashlib . sha256 ( inds . tobytes ()) . hexdigest () to_hash_str = str ( npre ) + str ( nsamples ) + str ( bin_full_path ) + inds_hash key = hashlib . sha256 ( to_hash_str . encode ()) . hexdigest () fname = f \". { key } .truebq_pulse_cache.npy\" cache_dir_path = bin_full_path . parent / \"_truebq_bin_cache\" cache_dir_path . mkdir ( exist_ok = True ) file_path = cache_dir_path / fname inds = np . array ( inds ) if file_path . is_file (): # check if the file is the right size Nbytes = len ( inds ) * nsamples * 2 # 2 bytes per int16 if file_path . stat () . st_size != Nbytes : # on windows the error if the file is the wrong size makes it sound like you don't have enough memory # and python doesn't seem to catch the exception, so we check the size here if verbose : print ( f \"pulse cache is corrupted, re-gathering pulses for { file_path } \" ) file_path . unlink () cache_hit = False else : cache_hit = True else : cache_hit = False if cache_hit : if verbose : print ( f \"pulse cache hit for { file_path } \" ) return np . memmap ( file_path , dtype = np . int16 , mode = \"r\" , shape = ( len ( inds ), nsamples )) if verbose : print ( f \"pulse cache miss for { file_path } \" ) return gather_pulses_from_inds_numpy_contiguous_mmap ( data , npre , nsamples , inds , filename = file_path )","title":"gather_pulses_from_inds_numpy_contiguous_mmap_with_cache"},{"location":"docstrings/#mass2.core.truebq_bin.get_noise_trigger_inds","text":"Get trigger indices for noise periods, avoiding pulses. Source code in mass2/core/truebq_bin.py 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def get_noise_trigger_inds ( pulse_trigger_inds : ArrayLike , n_dead_samples_after_previous_pulse : int , n_record_samples : int , max_noise_triggers : int , ) -> NDArray : \"\"\"Get trigger indices for noise periods, avoiding pulses.\"\"\" pulse_trigger_inds = np . asarray ( pulse_trigger_inds ) diffs = np . diff ( pulse_trigger_inds ) inds = [] for i in range ( len ( diffs )): if diffs [ i ] > n_dead_samples_after_previous_pulse : n_make = ( diffs [ i ] - n_dead_samples_after_previous_pulse ) // n_record_samples ind0 = pulse_trigger_inds [ i ] + n_dead_samples_after_previous_pulse for j in range ( n_make ): inds . append ( ind0 + n_record_samples * j ) if len ( inds ) == max_noise_triggers : return np . array ( inds ) return np . array ( inds )","title":"get_noise_trigger_inds"},{"location":"docstrings/#mass2.core.truebq_bin.write_truebq_bin_file","text":"Write a binary file that can be opened by TrueBqBin.load(). This function writes data efficiently without copying by using memory mapping and direct file operations. Args: path: Output file path data: Data array to write (will be converted to int16 if not already) sample_rate_hz: Sample rate in Hz voltage_scale: Voltage scaling factor format_version: File format version (default: 1) schema_version: Schema version (default: 1) data_reduction_factor: Data reduction factor (default: 1) acquisition_flags: Acquisition flags (default: 0) start_time: Start time as uint64 array of length 2 (optional) stop_time: Stop time as uint64 array of length 2 (optional) Source code in mass2/core/truebq_bin.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 def write_truebq_bin_file ( path : str | Path , data : np . ndarray , sample_rate_hz : float , * , # force keyword only voltage_scale : float = 1.0 , format_version : int = 1 , schema_version : int = 1 , data_reduction_factor : int = 1 , acquisition_flags : int = 0 , start_time : np . ndarray | None = None , stop_time : np . ndarray | None = None , ) -> None : \"\"\" Write a binary file that can be opened by TrueBqBin.load(). This function writes data efficiently without copying by using memory mapping and direct file operations. Args: path: Output file path data: Data array to write (will be converted to int16 if not already) sample_rate_hz: Sample rate in Hz voltage_scale: Voltage scaling factor format_version: File format version (default: 1) schema_version: Schema version (default: 1) data_reduction_factor: Data reduction factor (default: 1) acquisition_flags: Acquisition flags (default: 0) start_time: Start time as uint64 array of length 2 (optional) stop_time: Stop time as uint64 array of length 2 (optional) \"\"\" path = Path ( path ) path . parent . mkdir ( parents = True , exist_ok = True ) # Ensure data is int16 (convert if necessary, but avoid unnecessary copying) if data . dtype != np . int16 : if not np . can_cast ( data . dtype , np . int16 , casting = \"safe\" ): print ( f \"Warning: Converting data from { data . dtype } to int16 may cause data loss\" ) data = data . astype ( np . int16 ) # Prepare header num_samples = len ( data ) # Default time values if not provided if start_time is None : start_time = np . array ([ 0 , 0 ], dtype = np . uint64 ) if stop_time is None : stop_time = np . array ([ 0 , 0 ], dtype = np . uint64 ) # Create header array header = np . array ( [ ( format_version , schema_version , sample_rate_hz , data_reduction_factor , voltage_scale , acquisition_flags , start_time , stop_time , num_samples , ) ], dtype = header_dtype , ) # Create the file with the correct size with open ( path , \"wb\" ) as f : # Write header f . write ( header . tobytes ()) # For large data arrays, write in chunks to avoid memory issues chunk_size = 1024 * 1024 # 1MB chunks if data . nbytes <= chunk_size : # Small data, write directly f . write ( data . tobytes ()) else : # Large data, write in chunks data_flat = data . ravel () # Flatten without copying if possible for i in range ( 0 , len ( data_flat ), chunk_size // data . itemsize ): chunk = data_flat [ i : i + chunk_size // data . itemsize ] f . write ( chunk . tobytes ()) Various utility functions and classes: MouseClickReader: a class to use as a callback for reading mouse click locations in matplotlib plots. InlineUpdater: a class that loops over a generator and prints a message to the terminal each time it yields.","title":"write_truebq_bin_file"},{"location":"docstrings/#mass2.core.utilities.InlineUpdater","text":"A class to print progress updates to the terminal. Source code in mass2/core/utilities.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class InlineUpdater : \"\"\"A class to print progress updates to the terminal.\"\"\" def __init__ ( self , baseString : str ): self . fracDone = 0.0 self . minElapseTimeForCalc = 1.0 self . startTime = time . time () self . baseString = baseString self . logger = logging . getLogger ( \"mass\" ) def update ( self , fracDone : float ) -> None : \"\"\"Update the progress to the given fraction done.\"\"\" if self . logger . getEffectiveLevel () >= logging . WARNING : return self . fracDone = fracDone sys . stdout . write ( f \" \\r { self . baseString } { self . fracDone * 100.0 : .1f } % done, estimated { self . timeRemainingStr } left\" ) sys . stdout . flush () if fracDone >= 1 : sys . stdout . write ( f \" \\n { self . baseString } finished in { self . elapsedTimeStr } \\n \" ) @property def timeRemaining ( self ) -> float : \"\"\"Estimate of time remaining in seconds, or -1 if not enough information yet.\"\"\" if self . elapsedTimeSec > self . minElapseTimeForCalc and self . fracDone > 0 : fracRemaining = 1 - self . fracDone rate = self . fracDone / self . elapsedTimeSec try : return fracRemaining / rate except ZeroDivisionError : return - 1 else : return - 1 @property def timeRemainingStr ( self ) -> str : \"\"\"String version of time-remaining estimate.\"\"\" timeRemaining = self . timeRemaining if timeRemaining == - 1 : return \"?\" else : return \" %.1f min\" % ( timeRemaining / 60.0 ) @property def elapsedTimeSec ( self ) -> float : \"\"\"Elapsed time in seconds since the creation of this object.\"\"\" return time . time () - self . startTime @property def elapsedTimeStr ( self ) -> str : \"\"\"String version of elapsed time.\"\"\" return \" %.1f min\" % ( self . elapsedTimeSec / 60.0 )","title":"InlineUpdater"},{"location":"docstrings/#mass2.core.utilities.InlineUpdater.elapsedTimeSec","text":"Elapsed time in seconds since the creation of this object.","title":"elapsedTimeSec"},{"location":"docstrings/#mass2.core.utilities.InlineUpdater.elapsedTimeStr","text":"String version of elapsed time.","title":"elapsedTimeStr"},{"location":"docstrings/#mass2.core.utilities.InlineUpdater.timeRemaining","text":"Estimate of time remaining in seconds, or -1 if not enough information yet.","title":"timeRemaining"},{"location":"docstrings/#mass2.core.utilities.InlineUpdater.timeRemainingStr","text":"String version of time-remaining estimate.","title":"timeRemainingStr"},{"location":"docstrings/#mass2.core.utilities.InlineUpdater.update","text":"Update the progress to the given fraction done. Source code in mass2/core/utilities.py 28 29 30 31 32 33 34 35 36 def update ( self , fracDone : float ) -> None : \"\"\"Update the progress to the given fraction done.\"\"\" if self . logger . getEffectiveLevel () >= logging . WARNING : return self . fracDone = fracDone sys . stdout . write ( f \" \\r { self . baseString } { self . fracDone * 100.0 : .1f } % done, estimated { self . timeRemainingStr } left\" ) sys . stdout . flush () if fracDone >= 1 : sys . stdout . write ( f \" \\n { self . baseString } finished in { self . elapsedTimeStr } \\n \" )","title":"update"},{"location":"docstrings/#mass2.core.utilities.NullUpdater","text":"A do-nothing updater class with the same API as InlineUpdater. Source code in mass2/core/utilities.py 71 72 73 74 75 76 class NullUpdater : \"\"\"A do-nothing updater class with the same API as InlineUpdater.\"\"\" def update ( self , f : float ) -> None : \"\"\"Do nothing.\"\"\" pass","title":"NullUpdater"},{"location":"docstrings/#mass2.core.utilities.NullUpdater.update","text":"Do nothing. Source code in mass2/core/utilities.py 74 75 76 def update ( self , f : float ) -> None : \"\"\"Do nothing.\"\"\" pass","title":"update"},{"location":"docstrings/#mass2.core.utilities.show_progress","text":"A decorator to show progress updates for another function. Source code in mass2/core/utilities.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def show_progress ( name : str ) -> Callable : \"\"\"A decorator to show progress updates for another function.\"\"\" def decorator ( func : Callable ) -> Callable : \"\"\"A decorator to show progress updates for another function.\"\"\" @functools . wraps ( func ) def work ( self : Any , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update the progress of the wrapped function.\"\"\" try : if \"sphinx\" in sys . modules : # supress output during doctests print_updater = NullUpdater () else : print_updater = self . updater ( name ) except TypeError : print_updater = NullUpdater () for d in func ( self , * args , ** kwargs ): print_updater . update ( d ) return work return decorator","title":"show_progress"},{"location":"docstrings/#other-docstrings","text":"See also Other docstrings for modules other than mass2.core .","title":"Other docstrings"},{"location":"docstrings2/","text":"Automatic documentation generated from docstrings This page is auto-generated from the docstrings of functions, methods, classes, and modules. Each lowest-level module in Mass2 (i.e., each python file) that you want documented and indexed for searching should be listed in this docstrings2.md file. The exception are the Core docstrings for the mass2.core docstrings. Calibration Energy to/from pulse heights Objects to assist with calibration from pulse heights to absolute energies. Created on May 16, 2011 Completely redesigned January 2025 Curvetypes Bases: Enum Enumerate the types of calibration curves supported by Mass2. Source code in mass2/calibration/energy_calibration.py 24 25 26 27 28 29 30 31 32 class Curvetypes ( Enum ): \"\"\"Enumerate the types of calibration curves supported by Mass2.\"\"\" LINEAR = auto () LINEAR_PLUS_ZERO = auto () LOGLOG = auto () GAIN = auto () INVGAIN = auto () LOGGAIN = auto () EnergyCalibration dataclass An energy calibration object that can convert pulse heights to (estimated) energies. Subclasses implement the math of either exact or approximating calibration curves. Methods allow you to convert between pulse heights and energies, estimate energy uncertainties, and estimate pulse heights for lines whose names are know, or estimate the cal curve slope. Methods allow you to plot the calibration curve with its anchor points. Returns: EnergyCalibration \u2013 Raises: ValueError \u2013 If there is not at least one anchor point. Source code in mass2/calibration/energy_calibration.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 @dataclass ( frozen = True ) class EnergyCalibration : \"\"\"An energy calibration object that can convert pulse heights to (estimated) energies. Subclasses implement the math of either exact or approximating calibration curves. Methods allow you to convert between pulse heights and energies, estimate energy uncertainties, and estimate pulse heights for lines whose names are know, or estimate the cal curve slope. Methods allow you to plot the calibration curve with its anchor points. Returns ------- EnergyCalibration Raises ------ ValueError If there is not at least one anchor point. \"\"\" ph : NDArray [ np . float64 ] energy : NDArray [ np . float64 ] dph : NDArray [ np . float64 ] de : NDArray [ np . float64 ] names : list [ str ] curvename : Curvetypes approximating : bool spline : Callable [ ... , NDArray [ np . float64 ]] energy2ph : Callable [[ ArrayLike ], NDArray [ np . float64 ]] ph2uncertainty : Callable [[ ArrayLike ], NDArray [ np . float64 ]] input_transform : Callable output_transform : Callable | None = None extra_info : dict [ str , Any ] | None = None def __post_init__ ( self ) -> None : \"\"\"Fail for inputs of length zero.\"\"\" assert self . npts > 0 def copy ( self , ** changes : Any ) -> EnergyCalibration : \"\"\"Make a copy of this object, optionally changing some attributes.\"\"\" return dataclasses . replace ( self , ** changes ) @property def npts ( self ) -> int : \"\"\"Return the number of calibration anchor points.\"\"\" return len ( self . ph ) @staticmethod def _ecal_input_identity ( ph : NDArray , der : int = 0 ) -> NDArray : \"Use ph as the argument to the spline\" assert der >= 0 if der == 0 : return ph elif der == 1 : return np . ones_like ( ph ) return np . zeros_like ( ph ) @staticmethod def _ecal_input_log ( ph : NDArray , der : int = 0 ) -> NDArray : \"Use log(ph) as the argument to the spline\" assert der >= 0 if der == 0 : return np . log ( ph ) elif der == 1 : return 1.0 / ph raise ValueError ( f \"der= { der } , should be one of (0,1)\" ) @staticmethod def _ecal_output_identity ( ph : NDArray , yspline : NDArray , der : int = 0 , dery : int = 0 ) -> NDArray : \"Use the spline result as E itself\" assert der >= 0 and dery >= 0 if der > 0 : return np . zeros_like ( ph ) if dery == 0 : return yspline elif dery == 1 : return np . ones_like ( ph ) else : return np . zeros_like ( ph ) @staticmethod def _ecal_output_log ( ph : NDArray , yspline : NDArray , der : int = 0 , dery : int = 0 ) -> NDArray : \"Use the spline result as log(E)\" assert der >= 0 and dery >= 0 if der == 0 : # Any order of d/dy equals E(y) itself, or exp(y). return np . exp ( yspline ) else : return np . zeros_like ( ph ) @staticmethod def _ecal_output_gain ( ph : NDArray , yspline : NDArray , der : int = 0 , dery : int = 0 ) -> NDArray : \"Use the spline result as gain = ph/E\" assert der >= 0 and dery >= 0 if dery == 0 : if der == 0 : return ph / yspline elif der == 1 : return 1.0 / yspline else : return np . zeros_like ( ph ) assert dery == 1 return - ph / yspline ** 2 @staticmethod def _ecal_output_invgain ( ph : NDArray , yspline : NDArray , der : int = 0 , dery : int = 0 ) -> NDArray : \"Use the spline result as the inverse gain = E/ph\" assert der >= 0 and dery >= 0 if dery == 0 : if der == 0 : return ph * yspline elif der == 1 : return yspline else : return np . zeros_like ( ph ) assert dery == 1 return ph @staticmethod def _ecal_output_loggain ( ph : NDArray , yspline : NDArray , der : int = 0 , dery : int = 0 ) -> NDArray : \"Use the spline result as the log of the gain, or log(ph/E)\" assert der >= 0 and dery >= 0 if dery == 0 : if der == 0 : return ph * np . exp ( - yspline ) elif der == 1 : return np . exp ( - yspline ) else : return np . zeros_like ( ph ) assert dery == 1 return - ph * np . exp ( - yspline ) @property def ismonotonic ( self ) -> np . bool : \"\"\"Is the curve monotonic from 0 to 1.05 times the max anchor point's pulse height? Test at 1001 points, equally spaced in pulse height.\"\"\" nsamples = 1001 ph = np . linspace ( 0 , 1.05 * self . ph . max (), nsamples ) e = self ( ph ) return np . all ( np . diff ( e ) > 0 ) def name2ph ( self , name : str ) -> NDArray [ np . float64 ]: \"\"\"Convert a named energy feature to pulse height. `name` need not be a calibration point.\"\"\" energy = STANDARD_FEATURES [ name ] return self . energy2ph ( energy ) def energy2dedph ( self , energies : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Calculate the slope at the given energies.\"\"\" return self . ph2dedph ( self . energy2ph ( energies )) def energy2uncertainty ( self , energies : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Cal uncertainty in eV at the given energies.\"\"\" ph = self . energy2ph ( energies ) return self . ph2uncertainty ( ph ) def __str__ ( self ) -> str : \"\"\"A full description of the calibration.\"\"\" seq = [ f \"EnergyCalibration( { self . curvename } )\" ] for name , pulse_ht , energy in zip ( self . names , self . ph , self . energy ): seq . append ( f \" energy(ph= { pulse_ht : 7.2f } ) --> { energy : 9.2f } eV ( { name } )\" ) return \" \\n \" . join ( seq ) def ph2energy ( self , ph : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Apply the calibration, converting pulse heights `ph` to energies. Parameters ---------- ph : ArrayLike The pulse heights to convert to energies. Returns ------- NDArray[np.float64] Energies in eV. \"\"\" ph = np . asarray ( ph ) x = self . input_transform ( ph ) y = self . spline ( x , der = 0 ) if self . output_transform is None : E = y else : E = self . output_transform ( ph , y ) return E __call__ = ph2energy def ph2dedph ( self , ph : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Calculate the calibration curve's slope at pulse heights `ph`.\"\"\" ph = np . asarray ( ph ) x = self . input_transform ( ph ) dgdP = self . input_transform ( ph , der = 1 ) dydx = self . spline ( x , der = 1 ) dEdP = dydx * dgdP if self . output_transform is not None : y = self . spline ( x ) dfdP = self . output_transform ( ph , y , der = 1 ) dfdy = self . output_transform ( ph , y , dery = 1 ) dEdP = dfdP + dfdy * dydx * dgdP return dEdP def energy2ph_exact ( self , E : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"An exact inversion of the calibration curve, converting energies `E` to pulse heights. This is still in TO DO status, as it simply uses the spline in the forward direction. Parameters ---------- E : ArrayLike Energies in eV to be converted back to pulse heihgts Returns ------- NDArray[np.float64] Pulse heights corresponding to the given energies. \"\"\" # TODO use the spline as a starting point for Brent's method return self . energy2ph ( E ) def save_to_hdf5 ( self , hdf5_group : h5py . Group , name : str ) -> None : \"\"\"Save this calibration to an HDF5 group in a new subordinate group with the given name.\"\"\" if name in hdf5_group : del hdf5_group [ name ] cal_group = hdf5_group . create_group ( name ) cal_group [ \"name\" ] = [ str ( n ) . encode () for n in self . names ] cal_group [ \"ph\" ] = self . ph cal_group [ \"energy\" ] = self . energy cal_group [ \"dph\" ] = self . dph cal_group [ \"de\" ] = self . de cal_group . attrs [ \"curvetype\" ] = self . curvename . name cal_group . attrs [ \"approximate\" ] = self . approximating @staticmethod def load_from_hdf5 ( hdf5_group : h5py . Group , name : str ) -> EnergyCalibration : \"\"\"Load a calibration from an HDF5 group with the given name.\"\"\" cal_group = hdf5_group [ name ] # Fix a behavior of h5py for writing in py2, reading in py3. ctype = cal_group . attrs [ \"curvetype\" ] if isinstance ( ctype , bytes ): ctype = ctype . decode ( \"utf-8\" ) curvetype = Curvetypes [ ctype ] maker = EnergyCalibrationMaker ( cal_group [ \"ph\" ][:], cal_group [ \"energy\" ][:], cal_group [ \"dph\" ][:], cal_group [ \"de\" ][:], cal_group [ \"name\" ][:] ) approximate = cal_group . attrs [ \"approximate\" ] return maker . make_calibration ( curvetype , approximate = approximate ) def plotgain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as gain (PH/eV) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"gain\" self . plot ( ** kwargs ) def plotinvgain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as inverse gain (eV/PH) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"invgain\" self . plot ( ** kwargs ) def plotloggain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as log-gain log(PH/eV) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"loggain\" self . plot ( ** kwargs ) def plot ( # noqa: PLR0917 self , axis : plt . Axes | None = None , color : str = \"blue\" , markercolor : str = \"red\" , plottype : str = \"linear\" , ph_rescale_power : float = 0.0 , removeslope : bool = False , energy_x : bool = False , showtext : bool = True , showerrors : bool = True , min_energy : float | None = None , max_energy : float | None = None , ) -> None : \"\"\"Plot the calibration curve, with options.\"\"\" # Plot smooth curve minph , maxph = self . ph . min () * 0.9 , self . ph . max () * 1.1 if min_energy is not None : minph = self . energy2ph ( min_energy ) if max_energy is not None : maxph = self . energy2ph ( max_energy ) phplot = np . linspace ( minph , maxph , 1000 ) eplot = self ( phplot ) gplot = phplot / eplot dyplot = None gains = self . ph / self . energy slope = 0.0 xplot = phplot x = self . ph xerr = self . dph if energy_x : xplot = eplot x = self . energy xerr = self . de if axis is None : plt . clf () axis = plt . subplot ( 111 ) # axis.set_xlim([x[0], x[-1]*1.1]) if energy_x : axis . set_xlabel ( \"Energy (eV)\" ) else : axis . set_xlabel ( \"Pulse height\" ) if plottype == \"linear\" : yplot = self ( phplot ) / ( phplot ** ph_rescale_power ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / ( phplot ** ph_rescale_power ) y = self . energy / ( self . ph ** ph_rescale_power ) if ph_rescale_power == 0.0 : ylabel = \"Energy (eV)\" axis . set_title ( \"Energy calibration curve\" ) else : ylabel = f \"Energy (eV) / PH^ { ph_rescale_power : .4f } \" axis . set_title ( f \"Energy calibration curve, scaled by { ph_rescale_power : .4f } power of PH\" ) elif plottype == \"gain\" : yplot = gplot if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot * gplot y = gains ylabel = \"Gain (PH/eV)\" axis . set_title ( \"Energy calibration curve, gain\" ) elif plottype == \"invgain\" : yplot = 1.0 / gplot if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / phplot y = 1.0 / gains ylabel = \"Inverse Gain (eV/PH)\" axis . set_title ( \"Energy calibration curve, inverse gain\" ) elif plottype == \"loggain\" : yplot = np . log ( gplot ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot y = np . log ( gains ) ylabel = \"Log Gain: log(eV/PH)\" axis . set_title ( \"Energy calibration curve, log gain\" ) elif plottype == \"loglog\" : yplot = np . log ( eplot ) xplot = np . log ( phplot ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot y = np . log ( self . energy ) x = np . log ( self . ph ) xerr = self . dph / self . ph ylabel = \"Log energy/1 eV\" axis . set_xlabel ( \"log(Pulse height/arbs)\" ) axis . set_title ( \"Energy calibration curve, log gain\" ) else : raise ValueError ( \"plottype must be one of ('linear', 'gain','loggain','invgain').\" ) if removeslope : slope = ( y [ - 1 ] - y [ 0 ]) / ( x [ - 1 ] - x [ 0 ]) yplot -= slope * xplot axis . plot ( xplot , yplot , color = color ) if dyplot is not None and showerrors : axis . plot ( xplot , yplot + dyplot , color = color , alpha = 0.35 ) axis . plot ( xplot , yplot - dyplot , color = color , alpha = 0.35 ) # Plot and label cal points dy = (( self . de / self . energy ) ** 2 + ( self . dph / self . ph ) ** 2 ) ** 0.5 * y axis . errorbar ( x , y - slope * x , yerr = dy , xerr = xerr , fmt = \"o\" , mec = \"black\" , mfc = markercolor , capsize = 0 ) axis . grid ( True ) if removeslope : ylabel = f \" { ylabel } slope removed\" axis . set_ylabel ( ylabel ) if showtext : for xval , name , yval in zip ( x , self . names , y ): axis . text ( xval , yval - slope * xval , name + \" \" , ha = \"right\" ) ismonotonic property Is the curve monotonic from 0 to 1.05 times the max anchor point's pulse height? Test at 1001 points, equally spaced in pulse height. npts property Return the number of calibration anchor points. __post_init__ () Fail for inputs of length zero. Source code in mass2/calibration/energy_calibration.py 507 508 509 def __post_init__ ( self ) -> None : \"\"\"Fail for inputs of length zero.\"\"\" assert self . npts > 0 __str__ () A full description of the calibration. Source code in mass2/calibration/energy_calibration.py 628 629 630 631 632 633 def __str__ ( self ) -> str : \"\"\"A full description of the calibration.\"\"\" seq = [ f \"EnergyCalibration( { self . curvename } )\" ] for name , pulse_ht , energy in zip ( self . names , self . ph , self . energy ): seq . append ( f \" energy(ph= { pulse_ht : 7.2f } ) --> { energy : 9.2f } eV ( { name } )\" ) return \" \\n \" . join ( seq ) copy ( ** changes ) Make a copy of this object, optionally changing some attributes. Source code in mass2/calibration/energy_calibration.py 511 512 513 def copy ( self , ** changes : Any ) -> EnergyCalibration : \"\"\"Make a copy of this object, optionally changing some attributes.\"\"\" return dataclasses . replace ( self , ** changes ) energy2dedph ( energies ) Calculate the slope at the given energies. Source code in mass2/calibration/energy_calibration.py 619 620 621 def energy2dedph ( self , energies : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Calculate the slope at the given energies.\"\"\" return self . ph2dedph ( self . energy2ph ( energies )) energy2ph_exact ( E ) An exact inversion of the calibration curve, converting energies E to pulse heights. This is still in TO DO status, as it simply uses the spline in the forward direction. Parameters: E ( ArrayLike ) \u2013 Energies in eV to be converted back to pulse heihgts Returns: NDArray [ float64 ] \u2013 Pulse heights corresponding to the given energies. Source code in mass2/calibration/energy_calibration.py 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 def energy2ph_exact ( self , E : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"An exact inversion of the calibration curve, converting energies `E` to pulse heights. This is still in TO DO status, as it simply uses the spline in the forward direction. Parameters ---------- E : ArrayLike Energies in eV to be converted back to pulse heihgts Returns ------- NDArray[np.float64] Pulse heights corresponding to the given energies. \"\"\" # TODO use the spline as a starting point for Brent's method return self . energy2ph ( E ) energy2uncertainty ( energies ) Cal uncertainty in eV at the given energies. Source code in mass2/calibration/energy_calibration.py 623 624 625 626 def energy2uncertainty ( self , energies : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Cal uncertainty in eV at the given energies.\"\"\" ph = self . energy2ph ( energies ) return self . ph2uncertainty ( ph ) load_from_hdf5 ( hdf5_group , name ) staticmethod Load a calibration from an HDF5 group with the given name. Source code in mass2/calibration/energy_calibration.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 @staticmethod def load_from_hdf5 ( hdf5_group : h5py . Group , name : str ) -> EnergyCalibration : \"\"\"Load a calibration from an HDF5 group with the given name.\"\"\" cal_group = hdf5_group [ name ] # Fix a behavior of h5py for writing in py2, reading in py3. ctype = cal_group . attrs [ \"curvetype\" ] if isinstance ( ctype , bytes ): ctype = ctype . decode ( \"utf-8\" ) curvetype = Curvetypes [ ctype ] maker = EnergyCalibrationMaker ( cal_group [ \"ph\" ][:], cal_group [ \"energy\" ][:], cal_group [ \"dph\" ][:], cal_group [ \"de\" ][:], cal_group [ \"name\" ][:] ) approximate = cal_group . attrs [ \"approximate\" ] return maker . make_calibration ( curvetype , approximate = approximate ) name2ph ( name ) Convert a named energy feature to pulse height. name need not be a calibration point. Source code in mass2/calibration/energy_calibration.py 614 615 616 617 def name2ph ( self , name : str ) -> NDArray [ np . float64 ]: \"\"\"Convert a named energy feature to pulse height. `name` need not be a calibration point.\"\"\" energy = STANDARD_FEATURES [ name ] return self . energy2ph ( energy ) ph2dedph ( ph ) Calculate the calibration curve's slope at pulse heights ph . Source code in mass2/calibration/energy_calibration.py 659 660 661 662 663 664 665 666 667 668 669 670 671 def ph2dedph ( self , ph : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Calculate the calibration curve's slope at pulse heights `ph`.\"\"\" ph = np . asarray ( ph ) x = self . input_transform ( ph ) dgdP = self . input_transform ( ph , der = 1 ) dydx = self . spline ( x , der = 1 ) dEdP = dydx * dgdP if self . output_transform is not None : y = self . spline ( x ) dfdP = self . output_transform ( ph , y , der = 1 ) dfdy = self . output_transform ( ph , y , dery = 1 ) dEdP = dfdP + dfdy * dydx * dgdP return dEdP ph2energy ( ph ) Apply the calibration, converting pulse heights ph to energies. Parameters: ph ( ArrayLike ) \u2013 The pulse heights to convert to energies. Returns: NDArray [ float64 ] \u2013 Energies in eV. Source code in mass2/calibration/energy_calibration.py 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 def ph2energy ( self , ph : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Apply the calibration, converting pulse heights `ph` to energies. Parameters ---------- ph : ArrayLike The pulse heights to convert to energies. Returns ------- NDArray[np.float64] Energies in eV. \"\"\" ph = np . asarray ( ph ) x = self . input_transform ( ph ) y = self . spline ( x , der = 0 ) if self . output_transform is None : E = y else : E = self . output_transform ( ph , y ) return E plot ( axis = None , color = 'blue' , markercolor = 'red' , plottype = 'linear' , ph_rescale_power = 0.0 , removeslope = False , energy_x = False , showtext = True , showerrors = True , min_energy = None , max_energy = None ) Plot the calibration curve, with options. Source code in mass2/calibration/energy_calibration.py 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 def plot ( # noqa: PLR0917 self , axis : plt . Axes | None = None , color : str = \"blue\" , markercolor : str = \"red\" , plottype : str = \"linear\" , ph_rescale_power : float = 0.0 , removeslope : bool = False , energy_x : bool = False , showtext : bool = True , showerrors : bool = True , min_energy : float | None = None , max_energy : float | None = None , ) -> None : \"\"\"Plot the calibration curve, with options.\"\"\" # Plot smooth curve minph , maxph = self . ph . min () * 0.9 , self . ph . max () * 1.1 if min_energy is not None : minph = self . energy2ph ( min_energy ) if max_energy is not None : maxph = self . energy2ph ( max_energy ) phplot = np . linspace ( minph , maxph , 1000 ) eplot = self ( phplot ) gplot = phplot / eplot dyplot = None gains = self . ph / self . energy slope = 0.0 xplot = phplot x = self . ph xerr = self . dph if energy_x : xplot = eplot x = self . energy xerr = self . de if axis is None : plt . clf () axis = plt . subplot ( 111 ) # axis.set_xlim([x[0], x[-1]*1.1]) if energy_x : axis . set_xlabel ( \"Energy (eV)\" ) else : axis . set_xlabel ( \"Pulse height\" ) if plottype == \"linear\" : yplot = self ( phplot ) / ( phplot ** ph_rescale_power ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / ( phplot ** ph_rescale_power ) y = self . energy / ( self . ph ** ph_rescale_power ) if ph_rescale_power == 0.0 : ylabel = \"Energy (eV)\" axis . set_title ( \"Energy calibration curve\" ) else : ylabel = f \"Energy (eV) / PH^ { ph_rescale_power : .4f } \" axis . set_title ( f \"Energy calibration curve, scaled by { ph_rescale_power : .4f } power of PH\" ) elif plottype == \"gain\" : yplot = gplot if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot * gplot y = gains ylabel = \"Gain (PH/eV)\" axis . set_title ( \"Energy calibration curve, gain\" ) elif plottype == \"invgain\" : yplot = 1.0 / gplot if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / phplot y = 1.0 / gains ylabel = \"Inverse Gain (eV/PH)\" axis . set_title ( \"Energy calibration curve, inverse gain\" ) elif plottype == \"loggain\" : yplot = np . log ( gplot ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot y = np . log ( gains ) ylabel = \"Log Gain: log(eV/PH)\" axis . set_title ( \"Energy calibration curve, log gain\" ) elif plottype == \"loglog\" : yplot = np . log ( eplot ) xplot = np . log ( phplot ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot y = np . log ( self . energy ) x = np . log ( self . ph ) xerr = self . dph / self . ph ylabel = \"Log energy/1 eV\" axis . set_xlabel ( \"log(Pulse height/arbs)\" ) axis . set_title ( \"Energy calibration curve, log gain\" ) else : raise ValueError ( \"plottype must be one of ('linear', 'gain','loggain','invgain').\" ) if removeslope : slope = ( y [ - 1 ] - y [ 0 ]) / ( x [ - 1 ] - x [ 0 ]) yplot -= slope * xplot axis . plot ( xplot , yplot , color = color ) if dyplot is not None and showerrors : axis . plot ( xplot , yplot + dyplot , color = color , alpha = 0.35 ) axis . plot ( xplot , yplot - dyplot , color = color , alpha = 0.35 ) # Plot and label cal points dy = (( self . de / self . energy ) ** 2 + ( self . dph / self . ph ) ** 2 ) ** 0.5 * y axis . errorbar ( x , y - slope * x , yerr = dy , xerr = xerr , fmt = \"o\" , mec = \"black\" , mfc = markercolor , capsize = 0 ) axis . grid ( True ) if removeslope : ylabel = f \" { ylabel } slope removed\" axis . set_ylabel ( ylabel ) if showtext : for xval , name , yval in zip ( x , self . names , y ): axis . text ( xval , yval - slope * xval , name + \" \" , ha = \"right\" ) plotgain ( ** kwargs ) Plot the calibration curve as gain (PH/eV) vs pulse height. Source code in mass2/calibration/energy_calibration.py 721 722 723 724 def plotgain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as gain (PH/eV) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"gain\" self . plot ( ** kwargs ) plotinvgain ( ** kwargs ) Plot the calibration curve as inverse gain (eV/PH) vs pulse height. Source code in mass2/calibration/energy_calibration.py 726 727 728 729 def plotinvgain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as inverse gain (eV/PH) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"invgain\" self . plot ( ** kwargs ) plotloggain ( ** kwargs ) Plot the calibration curve as log-gain log(PH/eV) vs pulse height. Source code in mass2/calibration/energy_calibration.py 731 732 733 734 def plotloggain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as log-gain log(PH/eV) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"loggain\" self . plot ( ** kwargs ) save_to_hdf5 ( hdf5_group , name ) Save this calibration to an HDF5 group in a new subordinate group with the given name. Source code in mass2/calibration/energy_calibration.py 690 691 692 693 694 695 696 697 698 699 700 701 702 def save_to_hdf5 ( self , hdf5_group : h5py . Group , name : str ) -> None : \"\"\"Save this calibration to an HDF5 group in a new subordinate group with the given name.\"\"\" if name in hdf5_group : del hdf5_group [ name ] cal_group = hdf5_group . create_group ( name ) cal_group [ \"name\" ] = [ str ( n ) . encode () for n in self . names ] cal_group [ \"ph\" ] = self . ph cal_group [ \"energy\" ] = self . energy cal_group [ \"dph\" ] = self . dph cal_group [ \"de\" ] = self . de cal_group . attrs [ \"curvetype\" ] = self . curvename . name cal_group . attrs [ \"approximate\" ] = self . approximating EnergyCalibrationMaker dataclass An object that can make energy calibration curves under various assumptions, but using a single set of calibration anchor points and uncertainties on them. Returns: EnergyCalibrationMaker \u2013 A factory for making various EnergyCalibration objects from the same anchor points. Raises: ValueError \u2013 When calibration data arrays have unequal length, or ph is not monotone in energy . Source code in mass2/calibration/energy_calibration.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 @dataclass ( frozen = True ) class EnergyCalibrationMaker : \"\"\"An object that can make energy calibration curves under various assumptions, but using a single set of calibration anchor points and uncertainties on them. Returns ------- EnergyCalibrationMaker A factory for making various `EnergyCalibration` objects from the same anchor points. Raises ------ ValueError When calibration data arrays have unequal length, or `ph` is not monotone in `energy`. \"\"\" ph : NDArray [ np . float64 ] energy : NDArray [ np . float64 ] dph : NDArray [ np . float64 ] de : NDArray [ np . float64 ] names : list [ str ] @classmethod def init ( cls , ph : ArrayLike | None = None , energy : ArrayLike | None = None , dph : ArrayLike | None = None , de : ArrayLike | None = None , names : list [ str ] | None = None , ) -> EnergyCalibrationMaker : \"\"\"Create an EnergyCalibrationMaker, filling in any missing requirements with empty arrays.\"\"\" if ph is None : ph = np . array ([], dtype = float ) else : ph = np . asarray ( ph ) if energy is None : energy = np . array ([], dtype = float ) else : energy = np . asarray ( energy ) if dph is None : dph = 1e-3 * ph else : dph = np . asarray ( dph ) if de is None : de = 1e-3 * energy else : de = np . asarray ( de ) if names is None : names = [ \"dummy\" ] * len ( dph ) return cls ( ph , energy , dph , de , names ) def __post_init__ ( self ) -> None : \"\"\"Check for inputs of unequal length. Check for monotone anchor points. Sort the input data by energy.\"\"\" N = len ( self . ph ) assert N == len ( self . energy ) assert N == len ( self . dph ) assert N == len ( self . de ) assert N == len ( self . names ) # First sort according to energy of the calibration point if not np . all ( np . diff ( self . energy ) > 0 ): sortkeys = np . argsort ( self . energy ) self . ph [:] = self . ph [ sortkeys ] self . energy [:] = self . energy [ sortkeys ] self . dph [:] = self . dph [ sortkeys ] self . de [:] = self . de [ sortkeys ] self . names [:] = [ self . names [ i ] for i in sortkeys ] # Then confirm that the pulse heights are also in order order_ph = self . ph . argsort () order_en = self . energy . argsort () if not np . all ( order_ph == order_en ): a = f \"PH: { self . ph [ order_ph ] } \" b = f \"Energy: { self . energy [ order_ph ] } \" raise ValueError ( f \"Calibration points are not monotone: \\n { a } \\n { b } \" ) @property def npts ( self ) -> int : \"\"\"The number of calibration anchor points.\"\"\" return len ( self . ph ) def _remove_cal_point_idx ( self , idx : int ) -> EnergyCalibrationMaker : \"\"\"Remove calibration point number `idx` from the calibration. Return a new maker.\"\"\" ph = np . delete ( self . ph , idx ) energy = np . delete ( self . energy , idx ) dph = np . delete ( self . dph , idx ) de = np . delete ( self . de , idx ) names = self . names . copy () names . pop ( idx ) return EnergyCalibrationMaker ( ph , energy , dph , de , names ) def remove_cal_point_name ( self , name : str ) -> EnergyCalibrationMaker : \"\"\"Remove calibration point named `name`. Return a new maker.\"\"\" idx = self . names . index ( name ) return self . _remove_cal_point_idx ( idx ) def remove_cal_point_prefix ( self , prefix : str ) -> EnergyCalibrationMaker : \"\"\"This removes all cal points whose name starts with `prefix`. Return a new maker.\"\"\" # Work recursively: remove the first match and make a new Maker, and repeat until none match. # This is clearly less efficient when removing N matches, as N copies are made. So what? # This feature is likely to be rarely used, and we favor clarity over performance here. for name in tuple ( self . names ): if name . startswith ( prefix ): return self . remove_cal_point_name ( name ) . remove_cal_point_prefix ( prefix ) return self def remove_cal_point_energy ( self , energy : float , de : float ) -> EnergyCalibrationMaker : \"\"\"Remove cal points at energies within \u00b1`de` of `energy`. Return a new maker.\"\"\" idxs = np . nonzero ( np . abs ( self . energy - energy ) < de )[ 0 ] if len ( idxs ) == 0 : return self # Also recursive and less efficient. See previous method's comment. return self . _remove_cal_point_idx ( idxs [ 0 ]) . remove_cal_point_energy ( energy , de ) def add_cal_point ( self , ph : float , energy : float | str , name : str = \"\" , ph_error : float | None = None , e_error : float | None = None , replace : bool = True , ) -> EnergyCalibrationMaker : \"\"\"Add a single energy calibration point. Can call as .add_cal_point(ph, energy, name) or if the \"energy\" is a line name, then .add_cal_point(ph, name) will find energy as `energy=mass2.STANDARD_FEATURES[name]`. Thus the following are equivalent: cal = cal.add_cal_point(12345.6, 5898.801, \"Mn Ka1\") cal = cal.add_cal_point(12456.6, \"Mn Ka1\") `ph` must be in units of the self.ph_field and `energy` is in eV. `ph_error` is the 1-sigma uncertainty on the pulse height. If None (the default), then assign ph_error = `ph`/1000. `e_error` is the 1-sigma uncertainty on the energy itself. If None (the default), then assign e_error=0.01 eV. Careful! If you give a name that's already in the list, or you add an equivalent energy but do NOT give a name, then this value replaces the previous one. You can prevent overwriting (and instead raise an error) by setting `replace`=False. \"\"\" # If <energy> is a string and a known spectral feature's name, use it as the name instead # Otherwise, it needs to be a numeric type convertible to float. try : energy = float ( energy ) except ValueError : try : if type ( energy ) is str : name = energy else : name = str ( energy ) energy = STANDARD_FEATURES [ name ] except Exception : raise ValueError ( \"2nd argument must be an energy or a known name\" + \" from mass2.energy_calibration.STANDARD_FEATURES\" ) if ph_error is None : ph_error = ph * 0.001 if e_error is None : e_error = 0.01 # Assume 0.01 eV error if none given update_index : int | None = None if self . npts > 0 : if name and name in self . names : # Update an existing point by name if not replace : raise ValueError ( f \"Calibration point ' { name } ' is already known and overwrite is False\" ) update_index = self . names . index ( name ) elif np . abs ( energy - self . energy ) . min () <= e_error : # Update existing point if not replace : raise ValueError ( f \"Calibration point at energy { energy : .2f } eV is already known and overwrite is False\" ) update_index = int ( np . abs ( energy - self . energy ) . argmin ()) if update_index is None : # Add a new calibration anchor point new_ph = np . hstack (( self . ph , ph )) new_energy = np . hstack (( self . energy , energy )) new_dph = np . hstack (( self . dph , ph_error )) new_de = np . hstack (( self . de , e_error )) new_names = self . names + [ name ] else : # Replace an existing calibration anchor point. new_ph = self . ph . copy () new_energy = self . energy . copy () new_dph = self . dph . copy () new_de = self . de . copy () new_names = self . names . copy () new_ph [ update_index ] = ph new_energy [ update_index ] = energy new_dph [ update_index ] = ph_error new_de [ update_index ] = e_error new_names [ update_index ] = name return EnergyCalibrationMaker ( new_ph , new_energy , new_dph , new_de , new_names ) @staticmethod def heuristic_samplepoints ( anchors : ArrayLike ) -> np . ndarray : \"\"\"Given a set of calibration anchor points, return a few hundred sample points, reasonably spaced below, between, and above the anchor points. Parameters ---------- anchors : ArrayLike The anchor points (in pulse height space) Returns ------- np.ndarray _description_ \"\"\" anchors = np . asarray ( anchors ) # Prescription is 50 points up to lowest anchor (but exclude 0): x = [ np . linspace ( 0 , anchors . min (), 51 )[ 1 :]] # Then one points, plus one extra per 1% spacing between (and at) each anchor for i in range ( len ( anchors ) - 1 ): low , high = anchors [ i : i + 2 ] n = 1 + int ( 100 * ( high / low - 1 ) + 0.5 ) x . append ( np . linspace ( low , high , n + 1 )[ 1 :]) # Finally, 100 more points between the highest anchor and 2x that. x . append ( anchors . max () * np . linspace ( 1 , 2 , 101 )[ 1 :]) return np . hstack ( x ) def make_calibration_loglog ( self , approximate : bool = False , powerlaw : float = 1.15 , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in log(energy) vs log(pulse height).\"\"\" return self . make_calibration ( Curvetypes . LOGLOG , approximate = approximate , powerlaw = powerlaw , extra_info = extra_info ) def make_calibration_gain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in (pulse height/energy) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . GAIN , approximate = approximate , extra_info = extra_info ) def make_calibration_invgain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in (energy/pulse height) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . INVGAIN , approximate = approximate , extra_info = extra_info ) def make_calibration_loggain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in log(pulse height/energy) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . LOGGAIN , approximate = approximate , extra_info = extra_info ) def make_calibration_linear ( self , approximate : bool = False , addzero : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in energy vs pulse height. If `addzero` include a (0,0) anchor point.\"\"\" curvename = Curvetypes . LINEAR_PLUS_ZERO if addzero else Curvetypes . LINEAR return self . make_calibration ( curvename , approximate = approximate , extra_info = extra_info ) def make_calibration ( self , curvename : Curvetypes = Curvetypes . LOGLOG , approximate : bool = False , powerlaw : float = 1.15 , extra_info : dict [ str , Any ] | None = None , ) -> EnergyCalibration : \"\"\"Create an energy calibration curve of the specified type. Parameters ---------- curvename : Curvetypes, optional Which curve type to use, by default Curvetypes.LOGLOG approximate : bool, optional Whether to approximate the anchor point data given the uncertainties, by default False powerlaw : float, optional An approximate powerlaw guess used by LOGLOG curves, by default 1.15 extra_info : dict[str, Any] | None, optional Extra text to store in the result, by default None Returns ------- EnergyCalibration The calibration object. Raises ------ ValueError If there are too few anchor points for an approximating curve, or if `curvename` is not in `Curvetypes`. \"\"\" if approximate and self . npts < 3 : raise ValueError ( f \"approximating curves require 3 or more cal anchor points, have { self . npts } \" ) if curvename not in Curvetypes : raise ValueError ( f \" { curvename =} , must be in { Curvetypes } \" ) # Use a heuristic to repair negative uncertainties. def regularize_uncertainties ( x : NDArray [ np . float64 ]) -> np . ndarray : \"\"\"Replace negative uncertainties with the minimum non-negative uncertainty, or zero.\"\"\" if not np . any ( x < 0 ): return x target = max ( 0.0 , x . min ()) x = x . copy () x [ x < 0 ] = target return x dph = regularize_uncertainties ( self . dph ) de = regularize_uncertainties ( self . de ) if curvename == Curvetypes . LOGLOG : input_transform = EnergyCalibration . _ecal_input_log output_transform = EnergyCalibration . _ecal_output_log x = np . log ( self . ph ) y = np . log ( self . energy ) # When there's only one point, enhance it by a fake point to enforce power-law behavior if self . npts == 1 : arboffset = 1.0 x = np . hstack ([ x , x + arboffset ]) y = np . hstack ([ y , y + arboffset / powerlaw ]) dx = dph / self . ph dy = de / self . energy elif curvename == Curvetypes . GAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_gain x = self . ph y = self . ph / self . energy # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * self . energy - 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph elif curvename == Curvetypes . INVGAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_invgain x = self . ph y = self . energy / self . ph # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * self . ph / y + 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph elif curvename in { Curvetypes . LINEAR , Curvetypes . LINEAR_PLUS_ZERO }: input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_identity x = self . ph y = self . energy dx = dph dy = de if ( curvename == Curvetypes . LINEAR_PLUS_ZERO ) and ( 0.0 not in x ): # Add a \"zero\"-energy and -PH point. But to avoid numerical problems, actually just use # 1e-3 times the lowest value, giving \u00b1100% uncertainty on the values. x = np . hstack (([ x . min () * 1e-3 ], x )) y = np . hstack (([ y . min () * 1e-3 ], y )) dx = np . hstack (([ x [ 0 ] * 1e-3 ], dx )) dy = np . hstack (( y [ 0 ] * 1e-3 , dy )) elif curvename == Curvetypes . LOGGAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_loggain x = self . ph y = np . log ( self . ph / self . energy ) # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * x - 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph else : raise ValueError ( f \"curvename=' { curvename } ' not recognized\" ) if approximate : internal_spline : CubicSpline = GPRSpline ( x , y , dy , dx ) elif len ( x ) > 1 : internal_spline = CubicSpline ( x , y ) else : internal_spline = CubicSpline ( x * [ 1 , 2 ], y * [ 1 , 2 ]) ph_samplepoints = EnergyCalibrationMaker . heuristic_samplepoints ( self . ph ) E_samplepoints = output_transform ( ph_samplepoints , internal_spline ( input_transform ( ph_samplepoints ))) energy2ph = CubicSpline ( E_samplepoints , ph_samplepoints ) if approximate : dspline = internal_spline . variance ( ph_samplepoints ) ** 0.5 if curvename == Curvetypes . LOGLOG : de_samplepoints = dspline * internal_spline ( input_transform ( ph_samplepoints )) elif curvename == Curvetypes . GAIN : de_samplepoints = dspline * E_samplepoints ** 2 / ph_samplepoints elif curvename == Curvetypes . INVGAIN : de_samplepoints = dspline * ph_samplepoints elif curvename in { Curvetypes . LINEAR , Curvetypes . LINEAR_PLUS_ZERO }: de_samplepoints = dspline elif curvename == Curvetypes . LOGGAIN : abs_dfdp = np . abs ( internal_spline ( ph_samplepoints , der = 1 )) de_samplepoints = dspline * E_samplepoints * abs_dfdp else : raise ValueError ( f \"curvename=' { curvename } ' not recognized\" ) uncertainty_spline : Callable = CubicSpline ( ph_samplepoints , de_samplepoints ) else : uncertainty_spline = np . zeros_like return EnergyCalibration ( self . ph , self . energy , self . dph , self . de , self . names , curvename = curvename , approximating = approximate , spline = internal_spline , energy2ph = energy2ph , ph2uncertainty = uncertainty_spline , input_transform = input_transform , output_transform = output_transform , extra_info = extra_info , ) def drop_one_errors ( self , curvename : Curvetypes = Curvetypes . LOGLOG , approximate : bool = False , powerlaw : float = 1.15 ) -> tuple [ NDArray [ np . float64 ], NDArray [ np . float64 ]]: \"\"\"For each calibration point, calculate the difference between the 'correct' energy and the energy predicted by creating a calibration without that point and using ph2energy to calculate the predicted energy Parameters ---------- curvename : Curvetypes, optional Calibration curve type to employ, by default Curvetypes.LOGLOG approximate : bool, optional Whether to approximate the anchor point data given the uncertainties, by default False powerlaw : float, optional An approximate powerlaw guess used by LOGLOG curves, by default 1.15 Returns ------- tuple[NDArray[np.float64], NDArray[np.float64]] An array of the anchor point energies, and an array of the differences between the predicted and actual energies for each anchor point. \"\"\" # \"\"\"For each calibration point, calculate the difference between the 'correct' energy # and the energy predicted by creating a calibration without that point and using # ph2energy to calculate the predicted energy, return (energies, drop_one_energy_diff)\"\"\" drop_one_energy_diff = np . zeros ( self . npts ) for i in range ( self . npts ): dropped_pulseheight = self . ph [ i ] dropped_energy = self . energy [ i ] drop_one_maker = self . _remove_cal_point_idx ( i ) drop_one_cal = drop_one_maker . make_calibration ( curvename = curvename , approximate = approximate , powerlaw = powerlaw ) predicted_energy = drop_one_cal . ph2energy ( dropped_pulseheight ) . item ( 0 ) drop_one_energy_diff [ i ] = predicted_energy - dropped_energy return self . energy , drop_one_energy_diff npts property The number of calibration anchor points. __post_init__ () Check for inputs of unequal length. Check for monotone anchor points. Sort the input data by energy. Source code in mass2/calibration/energy_calibration.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def __post_init__ ( self ) -> None : \"\"\"Check for inputs of unequal length. Check for monotone anchor points. Sort the input data by energy.\"\"\" N = len ( self . ph ) assert N == len ( self . energy ) assert N == len ( self . dph ) assert N == len ( self . de ) assert N == len ( self . names ) # First sort according to energy of the calibration point if not np . all ( np . diff ( self . energy ) > 0 ): sortkeys = np . argsort ( self . energy ) self . ph [:] = self . ph [ sortkeys ] self . energy [:] = self . energy [ sortkeys ] self . dph [:] = self . dph [ sortkeys ] self . de [:] = self . de [ sortkeys ] self . names [:] = [ self . names [ i ] for i in sortkeys ] # Then confirm that the pulse heights are also in order order_ph = self . ph . argsort () order_en = self . energy . argsort () if not np . all ( order_ph == order_en ): a = f \"PH: { self . ph [ order_ph ] } \" b = f \"Energy: { self . energy [ order_ph ] } \" raise ValueError ( f \"Calibration points are not monotone: \\n { a } \\n { b } \" ) add_cal_point ( ph , energy , name = '' , ph_error = None , e_error = None , replace = True ) Add a single energy calibration point. Can call as .add_cal_point(ph, energy, name) or if the \"energy\" is a line name, then .add_cal_point(ph, name) will find energy as energy=mass2.STANDARD_FEATURES[name] . Thus the following are equivalent: cal = cal.add_cal_point(12345.6, 5898.801, \"Mn Ka1\") cal = cal.add_cal_point(12456.6, \"Mn Ka1\") ph must be in units of the self.ph_field and energy is in eV. ph_error is the 1-sigma uncertainty on the pulse height. If None (the default), then assign ph_error = ph /1000. e_error is the 1-sigma uncertainty on the energy itself. If None (the default), then assign e_error=0.01 eV. Careful! If you give a name that's already in the list, or you add an equivalent energy but do NOT give a name, then this value replaces the previous one. You can prevent overwriting (and instead raise an error) by setting replace =False. Source code in mass2/calibration/energy_calibration.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def add_cal_point ( self , ph : float , energy : float | str , name : str = \"\" , ph_error : float | None = None , e_error : float | None = None , replace : bool = True , ) -> EnergyCalibrationMaker : \"\"\"Add a single energy calibration point. Can call as .add_cal_point(ph, energy, name) or if the \"energy\" is a line name, then .add_cal_point(ph, name) will find energy as `energy=mass2.STANDARD_FEATURES[name]`. Thus the following are equivalent: cal = cal.add_cal_point(12345.6, 5898.801, \"Mn Ka1\") cal = cal.add_cal_point(12456.6, \"Mn Ka1\") `ph` must be in units of the self.ph_field and `energy` is in eV. `ph_error` is the 1-sigma uncertainty on the pulse height. If None (the default), then assign ph_error = `ph`/1000. `e_error` is the 1-sigma uncertainty on the energy itself. If None (the default), then assign e_error=0.01 eV. Careful! If you give a name that's already in the list, or you add an equivalent energy but do NOT give a name, then this value replaces the previous one. You can prevent overwriting (and instead raise an error) by setting `replace`=False. \"\"\" # If <energy> is a string and a known spectral feature's name, use it as the name instead # Otherwise, it needs to be a numeric type convertible to float. try : energy = float ( energy ) except ValueError : try : if type ( energy ) is str : name = energy else : name = str ( energy ) energy = STANDARD_FEATURES [ name ] except Exception : raise ValueError ( \"2nd argument must be an energy or a known name\" + \" from mass2.energy_calibration.STANDARD_FEATURES\" ) if ph_error is None : ph_error = ph * 0.001 if e_error is None : e_error = 0.01 # Assume 0.01 eV error if none given update_index : int | None = None if self . npts > 0 : if name and name in self . names : # Update an existing point by name if not replace : raise ValueError ( f \"Calibration point ' { name } ' is already known and overwrite is False\" ) update_index = self . names . index ( name ) elif np . abs ( energy - self . energy ) . min () <= e_error : # Update existing point if not replace : raise ValueError ( f \"Calibration point at energy { energy : .2f } eV is already known and overwrite is False\" ) update_index = int ( np . abs ( energy - self . energy ) . argmin ()) if update_index is None : # Add a new calibration anchor point new_ph = np . hstack (( self . ph , ph )) new_energy = np . hstack (( self . energy , energy )) new_dph = np . hstack (( self . dph , ph_error )) new_de = np . hstack (( self . de , e_error )) new_names = self . names + [ name ] else : # Replace an existing calibration anchor point. new_ph = self . ph . copy () new_energy = self . energy . copy () new_dph = self . dph . copy () new_de = self . de . copy () new_names = self . names . copy () new_ph [ update_index ] = ph new_energy [ update_index ] = energy new_dph [ update_index ] = ph_error new_de [ update_index ] = e_error new_names [ update_index ] = name return EnergyCalibrationMaker ( new_ph , new_energy , new_dph , new_de , new_names ) drop_one_errors ( curvename = Curvetypes . LOGLOG , approximate = False , powerlaw = 1.15 ) For each calibration point, calculate the difference between the 'correct' energy and the energy predicted by creating a calibration without that point and using ph2energy to calculate the predicted energy Parameters: curvename ( Curvetypes , default: LOGLOG ) \u2013 Calibration curve type to employ, by default Curvetypes.LOGLOG approximate ( bool , default: False ) \u2013 Whether to approximate the anchor point data given the uncertainties, by default False powerlaw ( float , default: 1.15 ) \u2013 An approximate powerlaw guess used by LOGLOG curves, by default 1.15 Returns: tuple [ NDArray [ float64 ], NDArray [ float64 ]] \u2013 An array of the anchor point energies, and an array of the differences between the predicted and actual energies for each anchor point. Source code in mass2/calibration/energy_calibration.py 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def drop_one_errors ( self , curvename : Curvetypes = Curvetypes . LOGLOG , approximate : bool = False , powerlaw : float = 1.15 ) -> tuple [ NDArray [ np . float64 ], NDArray [ np . float64 ]]: \"\"\"For each calibration point, calculate the difference between the 'correct' energy and the energy predicted by creating a calibration without that point and using ph2energy to calculate the predicted energy Parameters ---------- curvename : Curvetypes, optional Calibration curve type to employ, by default Curvetypes.LOGLOG approximate : bool, optional Whether to approximate the anchor point data given the uncertainties, by default False powerlaw : float, optional An approximate powerlaw guess used by LOGLOG curves, by default 1.15 Returns ------- tuple[NDArray[np.float64], NDArray[np.float64]] An array of the anchor point energies, and an array of the differences between the predicted and actual energies for each anchor point. \"\"\" # \"\"\"For each calibration point, calculate the difference between the 'correct' energy # and the energy predicted by creating a calibration without that point and using # ph2energy to calculate the predicted energy, return (energies, drop_one_energy_diff)\"\"\" drop_one_energy_diff = np . zeros ( self . npts ) for i in range ( self . npts ): dropped_pulseheight = self . ph [ i ] dropped_energy = self . energy [ i ] drop_one_maker = self . _remove_cal_point_idx ( i ) drop_one_cal = drop_one_maker . make_calibration ( curvename = curvename , approximate = approximate , powerlaw = powerlaw ) predicted_energy = drop_one_cal . ph2energy ( dropped_pulseheight ) . item ( 0 ) drop_one_energy_diff [ i ] = predicted_energy - dropped_energy return self . energy , drop_one_energy_diff heuristic_samplepoints ( anchors ) staticmethod Given a set of calibration anchor points, return a few hundred sample points, reasonably spaced below, between, and above the anchor points. Parameters: anchors ( ArrayLike ) \u2013 The anchor points (in pulse height space) Returns: ndarray \u2013 description Source code in mass2/calibration/energy_calibration.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 @staticmethod def heuristic_samplepoints ( anchors : ArrayLike ) -> np . ndarray : \"\"\"Given a set of calibration anchor points, return a few hundred sample points, reasonably spaced below, between, and above the anchor points. Parameters ---------- anchors : ArrayLike The anchor points (in pulse height space) Returns ------- np.ndarray _description_ \"\"\" anchors = np . asarray ( anchors ) # Prescription is 50 points up to lowest anchor (but exclude 0): x = [ np . linspace ( 0 , anchors . min (), 51 )[ 1 :]] # Then one points, plus one extra per 1% spacing between (and at) each anchor for i in range ( len ( anchors ) - 1 ): low , high = anchors [ i : i + 2 ] n = 1 + int ( 100 * ( high / low - 1 ) + 0.5 ) x . append ( np . linspace ( low , high , n + 1 )[ 1 :]) # Finally, 100 more points between the highest anchor and 2x that. x . append ( anchors . max () * np . linspace ( 1 , 2 , 101 )[ 1 :]) return np . hstack ( x ) init ( ph = None , energy = None , dph = None , de = None , names = None ) classmethod Create an EnergyCalibrationMaker, filling in any missing requirements with empty arrays. Source code in mass2/calibration/energy_calibration.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 @classmethod def init ( cls , ph : ArrayLike | None = None , energy : ArrayLike | None = None , dph : ArrayLike | None = None , de : ArrayLike | None = None , names : list [ str ] | None = None , ) -> EnergyCalibrationMaker : \"\"\"Create an EnergyCalibrationMaker, filling in any missing requirements with empty arrays.\"\"\" if ph is None : ph = np . array ([], dtype = float ) else : ph = np . asarray ( ph ) if energy is None : energy = np . array ([], dtype = float ) else : energy = np . asarray ( energy ) if dph is None : dph = 1e-3 * ph else : dph = np . asarray ( dph ) if de is None : de = 1e-3 * energy else : de = np . asarray ( de ) if names is None : names = [ \"dummy\" ] * len ( dph ) return cls ( ph , energy , dph , de , names ) make_calibration ( curvename = Curvetypes . LOGLOG , approximate = False , powerlaw = 1.15 , extra_info = None ) Create an energy calibration curve of the specified type. Parameters: curvename ( Curvetypes , default: LOGLOG ) \u2013 Which curve type to use, by default Curvetypes.LOGLOG approximate ( bool , default: False ) \u2013 Whether to approximate the anchor point data given the uncertainties, by default False powerlaw ( float , default: 1.15 ) \u2013 An approximate powerlaw guess used by LOGLOG curves, by default 1.15 extra_info ( dict [ str , Any ] | None , default: None ) \u2013 Extra text to store in the result, by default None Returns: EnergyCalibration \u2013 The calibration object. Raises: ValueError \u2013 If there are too few anchor points for an approximating curve, or if curvename is not in Curvetypes . Source code in mass2/calibration/energy_calibration.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def make_calibration ( self , curvename : Curvetypes = Curvetypes . LOGLOG , approximate : bool = False , powerlaw : float = 1.15 , extra_info : dict [ str , Any ] | None = None , ) -> EnergyCalibration : \"\"\"Create an energy calibration curve of the specified type. Parameters ---------- curvename : Curvetypes, optional Which curve type to use, by default Curvetypes.LOGLOG approximate : bool, optional Whether to approximate the anchor point data given the uncertainties, by default False powerlaw : float, optional An approximate powerlaw guess used by LOGLOG curves, by default 1.15 extra_info : dict[str, Any] | None, optional Extra text to store in the result, by default None Returns ------- EnergyCalibration The calibration object. Raises ------ ValueError If there are too few anchor points for an approximating curve, or if `curvename` is not in `Curvetypes`. \"\"\" if approximate and self . npts < 3 : raise ValueError ( f \"approximating curves require 3 or more cal anchor points, have { self . npts } \" ) if curvename not in Curvetypes : raise ValueError ( f \" { curvename =} , must be in { Curvetypes } \" ) # Use a heuristic to repair negative uncertainties. def regularize_uncertainties ( x : NDArray [ np . float64 ]) -> np . ndarray : \"\"\"Replace negative uncertainties with the minimum non-negative uncertainty, or zero.\"\"\" if not np . any ( x < 0 ): return x target = max ( 0.0 , x . min ()) x = x . copy () x [ x < 0 ] = target return x dph = regularize_uncertainties ( self . dph ) de = regularize_uncertainties ( self . de ) if curvename == Curvetypes . LOGLOG : input_transform = EnergyCalibration . _ecal_input_log output_transform = EnergyCalibration . _ecal_output_log x = np . log ( self . ph ) y = np . log ( self . energy ) # When there's only one point, enhance it by a fake point to enforce power-law behavior if self . npts == 1 : arboffset = 1.0 x = np . hstack ([ x , x + arboffset ]) y = np . hstack ([ y , y + arboffset / powerlaw ]) dx = dph / self . ph dy = de / self . energy elif curvename == Curvetypes . GAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_gain x = self . ph y = self . ph / self . energy # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * self . energy - 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph elif curvename == Curvetypes . INVGAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_invgain x = self . ph y = self . energy / self . ph # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * self . ph / y + 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph elif curvename in { Curvetypes . LINEAR , Curvetypes . LINEAR_PLUS_ZERO }: input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_identity x = self . ph y = self . energy dx = dph dy = de if ( curvename == Curvetypes . LINEAR_PLUS_ZERO ) and ( 0.0 not in x ): # Add a \"zero\"-energy and -PH point. But to avoid numerical problems, actually just use # 1e-3 times the lowest value, giving \u00b1100% uncertainty on the values. x = np . hstack (([ x . min () * 1e-3 ], x )) y = np . hstack (([ y . min () * 1e-3 ], y )) dx = np . hstack (([ x [ 0 ] * 1e-3 ], dx )) dy = np . hstack (( y [ 0 ] * 1e-3 , dy )) elif curvename == Curvetypes . LOGGAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_loggain x = self . ph y = np . log ( self . ph / self . energy ) # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * x - 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph else : raise ValueError ( f \"curvename=' { curvename } ' not recognized\" ) if approximate : internal_spline : CubicSpline = GPRSpline ( x , y , dy , dx ) elif len ( x ) > 1 : internal_spline = CubicSpline ( x , y ) else : internal_spline = CubicSpline ( x * [ 1 , 2 ], y * [ 1 , 2 ]) ph_samplepoints = EnergyCalibrationMaker . heuristic_samplepoints ( self . ph ) E_samplepoints = output_transform ( ph_samplepoints , internal_spline ( input_transform ( ph_samplepoints ))) energy2ph = CubicSpline ( E_samplepoints , ph_samplepoints ) if approximate : dspline = internal_spline . variance ( ph_samplepoints ) ** 0.5 if curvename == Curvetypes . LOGLOG : de_samplepoints = dspline * internal_spline ( input_transform ( ph_samplepoints )) elif curvename == Curvetypes . GAIN : de_samplepoints = dspline * E_samplepoints ** 2 / ph_samplepoints elif curvename == Curvetypes . INVGAIN : de_samplepoints = dspline * ph_samplepoints elif curvename in { Curvetypes . LINEAR , Curvetypes . LINEAR_PLUS_ZERO }: de_samplepoints = dspline elif curvename == Curvetypes . LOGGAIN : abs_dfdp = np . abs ( internal_spline ( ph_samplepoints , der = 1 )) de_samplepoints = dspline * E_samplepoints * abs_dfdp else : raise ValueError ( f \"curvename=' { curvename } ' not recognized\" ) uncertainty_spline : Callable = CubicSpline ( ph_samplepoints , de_samplepoints ) else : uncertainty_spline = np . zeros_like return EnergyCalibration ( self . ph , self . energy , self . dph , self . de , self . names , curvename = curvename , approximating = approximate , spline = internal_spline , energy2ph = energy2ph , ph2uncertainty = uncertainty_spline , input_transform = input_transform , output_transform = output_transform , extra_info = extra_info , ) make_calibration_gain ( approximate = False , extra_info = None ) Create a calibration curve that is a spline in (pulse height/energy) vs pulse height. Source code in mass2/calibration/energy_calibration.py 263 264 265 def make_calibration_gain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in (pulse height/energy) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . GAIN , approximate = approximate , extra_info = extra_info ) make_calibration_invgain ( approximate = False , extra_info = None ) Create a calibration curve that is a spline in (energy/pulse height) vs pulse height. Source code in mass2/calibration/energy_calibration.py 267 268 269 def make_calibration_invgain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in (energy/pulse height) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . INVGAIN , approximate = approximate , extra_info = extra_info ) make_calibration_linear ( approximate = False , addzero = False , extra_info = None ) Create a calibration curve that is a spline in energy vs pulse height. If addzero include a (0,0) anchor point. Source code in mass2/calibration/energy_calibration.py 275 276 277 278 279 280 def make_calibration_linear ( self , approximate : bool = False , addzero : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in energy vs pulse height. If `addzero` include a (0,0) anchor point.\"\"\" curvename = Curvetypes . LINEAR_PLUS_ZERO if addzero else Curvetypes . LINEAR return self . make_calibration ( curvename , approximate = approximate , extra_info = extra_info ) make_calibration_loggain ( approximate = False , extra_info = None ) Create a calibration curve that is a spline in log(pulse height/energy) vs pulse height. Source code in mass2/calibration/energy_calibration.py 271 272 273 def make_calibration_loggain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in log(pulse height/energy) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . LOGGAIN , approximate = approximate , extra_info = extra_info ) make_calibration_loglog ( approximate = False , powerlaw = 1.15 , extra_info = None ) Create a calibration curve that is a spline in log(energy) vs log(pulse height). Source code in mass2/calibration/energy_calibration.py 257 258 259 260 261 def make_calibration_loglog ( self , approximate : bool = False , powerlaw : float = 1.15 , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in log(energy) vs log(pulse height).\"\"\" return self . make_calibration ( Curvetypes . LOGLOG , approximate = approximate , powerlaw = powerlaw , extra_info = extra_info ) remove_cal_point_energy ( energy , de ) Remove cal points at energies within \u00b1 de of energy . Return a new maker. Source code in mass2/calibration/energy_calibration.py 143 144 145 146 147 148 149 def remove_cal_point_energy ( self , energy : float , de : float ) -> EnergyCalibrationMaker : \"\"\"Remove cal points at energies within \u00b1`de` of `energy`. Return a new maker.\"\"\" idxs = np . nonzero ( np . abs ( self . energy - energy ) < de )[ 0 ] if len ( idxs ) == 0 : return self # Also recursive and less efficient. See previous method's comment. return self . _remove_cal_point_idx ( idxs [ 0 ]) . remove_cal_point_energy ( energy , de ) remove_cal_point_name ( name ) Remove calibration point named name . Return a new maker. Source code in mass2/calibration/energy_calibration.py 128 129 130 131 def remove_cal_point_name ( self , name : str ) -> EnergyCalibrationMaker : \"\"\"Remove calibration point named `name`. Return a new maker.\"\"\" idx = self . names . index ( name ) return self . _remove_cal_point_idx ( idx ) remove_cal_point_prefix ( prefix ) This removes all cal points whose name starts with prefix . Return a new maker. Source code in mass2/calibration/energy_calibration.py 133 134 135 136 137 138 139 140 141 def remove_cal_point_prefix ( self , prefix : str ) -> EnergyCalibrationMaker : \"\"\"This removes all cal points whose name starts with `prefix`. Return a new maker.\"\"\" # Work recursively: remove the first match and make a new Maker, and repeat until none match. # This is clearly less efficient when removing N matches, as N copies are made. So what? # This feature is likely to be rarely used, and we favor clarity over performance here. for name in tuple ( self . names ): if name . startswith ( prefix ): return self . remove_cal_point_name ( name ) . remove_cal_point_prefix ( prefix ) return self Fluorescence line shapes fluorescence_lines.py Tools for fitting and simulating X-ray fluorescence lines. AmplitudeType Bases: Enum AmplitudeType: which form of amplitude is used in the reference data. Source code in mass2/calibration/fluorescence_lines.py 76 77 78 79 80 81 class AmplitudeType ( Enum ): \"\"\"AmplitudeType: which form of amplitude is used in the reference data.\"\"\" LORENTZIAN_PEAK_HEIGHT = \"Peak height of Lorentzians\" LORENTZIAN_INTEGRAL_INTENSITY = \"Integrated intensity of Lorentzians\" VOIGT_PEAK_HEIGHT = \"Peak height of Voigts\" LineshapeReference dataclass Description of our source of information on a line shape. Might be a reference to the literature, or notes on conversations. They are stored in a YAML file mass2/data/fluorescence_line_references.yaml Source code in mass2/calibration/fluorescence_lines.py 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 @dataclass ( frozen = True ) class LineshapeReference : \"\"\"Description of our source of information on a line shape. Might be a reference to the literature, or notes on conversations. They are stored in a YAML file mass2/data/fluorescence_line_references.yaml \"\"\" tag : str description : str url : str @classmethod def load ( cls , filename : pathlib . Path | str | None = None ) -> dict : \"\"\"Load the reference comments from a YAML file. If filename is None, load the default file in mass2/data. Parameters ---------- filename : pathlib.Path | str | None, optional The file to read containing reference comments, by default None Returns ------- dict A dictionary of LineshapeReference objects, keyed by their tag. \"\"\" references = { \"unknown\" : LineshapeReference ( \"unknown\" , \"unknown\" , \"\" )} if filename is None : filename = str ( pkg_resources . files ( \"mass2\" ) / \"data\" / \"fluorescence_line_references.yaml\" ) with open ( filename , \"r\" , encoding = \"utf-8\" ) as file : d = yaml . safe_load ( file ) for item in d : url = item . get ( \"URL\" , \"\" ) references [ item [ \"tag\" ]] = LineshapeReference ( item [ \"tag\" ], item [ \"description\" ], url ) return references def __str__ ( self ) -> str : \"\"\"The citation string for this reference.\"\"\" lines = [ f 'lineshape_references[\" { self . tag } \"]:' ] lines . append ( self . description . rstrip ( \" \\n \" )) if len ( self . url ) > 1 : lines . append ( f \"url: { self . url } \" ) return \" \\n \" . join ( lines ) __str__ () The citation string for this reference. Source code in mass2/calibration/fluorescence_lines.py 427 428 429 430 431 432 433 def __str__ ( self ) -> str : \"\"\"The citation string for this reference.\"\"\" lines = [ f 'lineshape_references[\" { self . tag } \"]:' ] lines . append ( self . description . rstrip ( \" \\n \" )) if len ( self . url ) > 1 : lines . append ( f \"url: { self . url } \" ) return \" \\n \" . join ( lines ) load ( filename = None ) classmethod Load the reference comments from a YAML file. If filename is None, load the default file in mass2/data. Parameters: filename ( Path | str | None , default: None ) \u2013 The file to read containing reference comments, by default None Returns: dict \u2013 A dictionary of LineshapeReference objects, keyed by their tag. Source code in mass2/calibration/fluorescence_lines.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 @classmethod def load ( cls , filename : pathlib . Path | str | None = None ) -> dict : \"\"\"Load the reference comments from a YAML file. If filename is None, load the default file in mass2/data. Parameters ---------- filename : pathlib.Path | str | None, optional The file to read containing reference comments, by default None Returns ------- dict A dictionary of LineshapeReference objects, keyed by their tag. \"\"\" references = { \"unknown\" : LineshapeReference ( \"unknown\" , \"unknown\" , \"\" )} if filename is None : filename = str ( pkg_resources . files ( \"mass2\" ) / \"data\" / \"fluorescence_line_references.yaml\" ) with open ( filename , \"r\" , encoding = \"utf-8\" ) as file : d = yaml . safe_load ( file ) for item in d : url = item . get ( \"URL\" , \"\" ) references [ item [ \"tag\" ]] = LineshapeReference ( item [ \"tag\" ], item [ \"description\" ], url ) return references SpectralLine dataclass An abstract base class for modeling spectral lines as a sum of Voigt profiles (i.e., Gaussian-convolved Lorentzians). Call SpectralLine.addline(...) to create a new instance. The API follows scipy.stats.stats.rv_continuous and is kind of like rv_frozen. Calling this object with an argument evalutes the pdf at the argument, it does not return an rv_frozen. So far, we ony define rvs and pdf . Source code in mass2/calibration/fluorescence_lines.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 @dataclass ( frozen = True ) class SpectralLine : \"\"\"An abstract base class for modeling spectral lines as a sum of Voigt profiles (i.e., Gaussian-convolved Lorentzians). Call `SpectralLine.addline(...)` to create a new instance. The API follows scipy.stats.stats.rv_continuous and is kind of like rv_frozen. Calling this object with an argument evalutes the pdf at the argument, it does not return an rv_frozen. So far, we ony define `rvs` and `pdf`. \"\"\" element : str material : str linetype : str nominal_peak_energy : float energies : NDArray [ np . float64 ] lorentzian_fwhm : NDArray [ np . float64 ] reference_amplitude : NDArray [ np . float64 ] reference_amplitude_type : AmplitudeType = AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY reference_measurement_type : str | None = \"unknown\" intrinsic_sigma : float = 0.0 reference_plot_instrument_gaussian_fwhm : float | None = 0.0 reference_short : str = \"unknown\" position_uncertainty : float = 0.0 is_default_material : bool = True @cached_property def peak_energy ( self ) -> float : \"\"\"Find the peak energy of the line shape assuming ideal instrument resolution.\"\"\" try : peak_energy = sp . optimize . brent ( lambda x : - self . pdf ( x , instrument_gaussian_fwhm = 0 ), brack = np . array (( 0.5 , 1 , 1.5 )) * self . nominal_peak_energy ) except ValueError : peak_energy = self . nominal_peak_energy return peak_energy @property def cumulative_amplitudes ( self ) -> NDArray : \"\"\"Cumulative sum of the Lorentzian integral intensities.\"\"\" return self . lorentzian_integral_intensity . cumsum () @cached_property def lorentzian_integral_intensity ( self ) -> NDArray : \"\"\"Return (and cache) computed integrated intensities of the Lorentzian components.\"\"\" if self . reference_amplitude_type == AmplitudeType . VOIGT_PEAK_HEIGHT : sigma = self . reference_plot_instrument_gaussian_fwhm / FWHM_OVER_SIGMA return np . array ([ ph / voigt ( 0 , 0 , fwhm / 2.0 , sigma ) for ( ph , fwhm ) in zip ( self . reference_amplitude , self . lorentzian_fwhm ) ]) if self . reference_amplitude_type == AmplitudeType . LORENTZIAN_PEAK_HEIGHT : return self . reference_amplitude * ( 0.5 * np . pi * self . lorentzian_fwhm ) if self . reference_amplitude_type == AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY : return self . reference_amplitude @cached_property def normalized_lorentzian_integral_intensity ( self ) -> NDArray : \"\"\"Return (and cache) computed integrated intensities of the Lorentzian components, normalized so sum=1.\"\"\" x = self . lorentzian_integral_intensity return x / np . sum ( x ) @cached_property def lorentz_amplitude ( self ) -> NDArray : \"\"\"Return (and cache) computed Lorentzian peak heights of the components.\"\"\" return self . lorentzian_integral_intensity / self . lorentzian_fwhm def __call__ ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> NDArray : \"\"\"Make the class callable, returning the same value as the self.pdf method.\"\"\" return self . pdf ( x , instrument_gaussian_fwhm ) def pdf ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> NDArray : \"\"\"Spectrum (units of fraction per eV) as a function of <x>, the energy in eV\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) x = np . asarray ( x , dtype = float ) result = np . zeros_like ( x ) for energy , fwhm , ampl in zip ( self . energies , self . lorentzian_fwhm , self . normalized_lorentzian_integral_intensity ): result += ampl * voigt ( x , energy , hwhm = fwhm * 0.5 , sigma = gaussian_sigma ) # mass2.voigt() is normalized to have unit integrated intensity return result def components ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> list [ NDArray ]: \"\"\"List of spectrum components as a function of <x>, the energy in eV\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) x = np . asarray ( x , dtype = float ) components = [] for energy , fwhm , ampl in zip ( self . energies , self . lorentzian_fwhm , self . lorentzian_integral_intensity ): components . append ( ampl * voigt ( x , energy , hwhm = fwhm * 0.5 , sigma = gaussian_sigma )) return components def plot ( self , x : ArrayLike | None = None , instrument_gaussian_fwhm : float = 0 , axis : plt . Axes | None = None , components : bool = True , label : str | None = None , setylim : bool = True , color : str | None = None , ) -> plt . Axes : \"\"\"Plot the spectrum. x - np array of energy in eV to plot at (sensible default) axis - axis to plot on (default creates new figure) components - True plots each voigt component in addition to the spectrum label - a string to label the plot with (optional)\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) if x is None : width = max ( 2 * gaussian_sigma , 3 * float ( np . amax ( self . lorentzian_fwhm ))) lo = np . amin ( self . energies ) - width hi = np . amax ( self . energies ) + width x = np . linspace ( lo , hi , 500 ) x = np . asarray ( x ) if axis is None : plt . figure () axis = plt . gca () if components : for component in self . components ( x , instrument_gaussian_fwhm ): axis . plot ( x , component , \"--\" ) pdf = self . pdf ( x , instrument_gaussian_fwhm ) axis . plot ( x , pdf , lw = 2 , label = label , color = color ) axis . set_xlabel ( \"Energy (eV)\" ) axis . set_ylabel ( f \"Counts per { float ( x [ 1 ] - x [ 0 ]) : .2 } eV bin\" ) axis . set_xlim ( x [ 0 ], x [ - 1 ]) if setylim : axis . set_ylim ( 0 , np . amax ( pdf ) * 1.05 ) axis . set_title ( f \" { self . shortname } with resolution { instrument_gaussian_fwhm : .2f } eV FWHM\" ) return axis def plot_like_reference ( self , axis : plt . Axes | None = None ) -> plt . Axes : \"\"\"Plot the spectrum to match the instrument resolution used in the reference data publication, if known.\"\"\" if self . reference_plot_instrument_gaussian_fwhm is None : fwhm = 0.001 else : fwhm = self . reference_plot_instrument_gaussian_fwhm axis = self . plot ( axis = axis , instrument_gaussian_fwhm = fwhm ) return axis def rvs ( self , size : int | tuple [ int ] | None , instrument_gaussian_fwhm : float , rng : np . random . Generator | None = None ) -> NDArray : \"\"\"The CDF and PPF (cumulative distribution and percentile point functions) are hard to compute. But it's easy enough to generate the random variates themselves, so we override that method.\"\"\" if rng is None : rng = _rng gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) # Choose from among the N Lorentzian lines in proportion to the line amplitudes iline = self . cumulative_amplitudes . searchsorted ( rng . uniform ( 0 , self . cumulative_amplitudes [ - 1 ], size = size )) # Choose Lorentzian variates of the appropriate width (but centered on 0) lor = rng . standard_cauchy ( size = size ) * self . lorentzian_fwhm [ iline ] * 0.5 # If necessary, add a Gaussian variate to mimic finite resolution if gaussian_sigma > 0.0 : lor += rng . standard_normal ( size = size ) * gaussian_sigma # Finally, add the line centers. results = lor + self . energies [ iline ] # We must check for non-positive results and replace them by recursive call # to self.rvs(). not_positive = results <= 0.0 if np . any ( not_positive ): Nbad = not_positive . sum () results [ not_positive ] = self . rvs ( size = Nbad , instrument_gaussian_fwhm = instrument_gaussian_fwhm ) return results @property def shortname ( self ) -> str : \"\"\"A short name for the line, suitable for use as a dictionary key.\"\"\" if self . is_default_material : return f \" { self . element }{ self . linetype } \" else : return f \" { self . element }{ self . linetype } _ { self . material } \" @property def reference ( self ) -> str : \"\"\"The full comment and/or citation for the reference data.\"\"\" return lineshape_references [ self . reference_short ] def _gaussian_sigma ( self , instrument_gaussian_fwhm : float ) -> float : \"\"\"combined intrinstic_sigma and insturment_gaussian_fwhm in quadrature and return the result\"\"\" assert instrument_gaussian_fwhm >= 0 return (( instrument_gaussian_fwhm / FWHM_OVER_SIGMA ) ** 2 + self . intrinsic_sigma ** 2 ) ** 0.5 def __repr__ ( self ) -> str : \"\"\"String representation of the SpectralLine.\"\"\" return f \"SpectralLine: { self . shortname } \" def model ( self , has_linear_background : bool = True , has_tails : bool = False , prefix : str = \"\" , qemodel : Callable | None = None ) -> GenericLineModel : \"\"\"Generate a LineModel instance from a SpectralLine\"\"\" model_class = GenericLineModel name = f \" { self . element }{ self . linetype } \" m = model_class ( name = name , spect = self , has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix , qemodel = qemodel ) return m def fitter ( self ) -> GenericLineModel : \"\"\"Generate a GenericLineModel instance for fitting this SpectralLine.\"\"\" fitter_class = GenericLineModel f = fitter_class ( self ) f . name = f \" { self . element }{ self . linetype } \" return f def minimum_fwhm ( self , instrument_gaussian_fwhm : float ) -> float : \"\"\"for the narrowest lorentzian in the line model, calculate the combined fwhm including the lorentzian, intrinstic_sigma, and instrument_gaussian_fwhm\"\"\" fwhm2 = np . amin ( self . lorentzian_fwhm ) ** 2 + instrument_gaussian_fwhm ** 2 + ( self . intrinsic_sigma * FWHM_OVER_SIGMA ) ** 2 return np . sqrt ( fwhm2 ) @classmethod def quick_monochromatic_line ( cls , name : str , energy : float , lorentzian_fwhm : float , intrinsic_sigma : float = 0.0 ) -> \"SpectralLine\" : \"\"\" Create a quick monochromatic line. Intended for use in calibration when we know a line energy, but not a lineshape model. Returns and instrance of SpectralLine with most fields having contents like \"unknown: quick_line\". The line will have a single lorentzian element with the given energy, fwhm, and intrinsic_sigma values. \"\"\" energy = float ( energy ) element = name material = \"unknown: quick_line\" if lorentzian_fwhm <= 0 and intrinsic_sigma <= 1e-6 : intrinsic_sigma = 1e-6 linetype = \"Gaussian\" reference_short = \"unknown: quick_line\" reference_amplitude = np . array ([ 1.0 ]) reference_amplitude_type = AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY nominal_peak_energy = energy position_uncertainty = 0.0 reference_measurement_type = \"unkown: quick_line\" return cls ( element = element , material = material , linetype = linetype , energies = np . array ([ energy ]), lorentzian_fwhm = np . array ([ lorentzian_fwhm ]), intrinsic_sigma = intrinsic_sigma , reference_short = reference_short , reference_amplitude = reference_amplitude , reference_amplitude_type = reference_amplitude_type , nominal_peak_energy = nominal_peak_energy , position_uncertainty = position_uncertainty , reference_measurement_type = reference_measurement_type , ) @classmethod def addline ( # noqa: PLR0917 cls , element : str , linetype : str , material : str , reference_short : str , reference_plot_instrument_gaussian_fwhm : float | None , nominal_peak_energy : float , energies : ArrayLike , lorentzian_fwhm : ArrayLike , reference_amplitude : ArrayLike , reference_amplitude_type : AmplitudeType , ka12_energy_diff : float | None = None , position_uncertainty : float = np . nan , intrinsic_sigma : float = 0 , reference_measurement_type : str | None = None , is_default_material : bool = True , allow_replacement : bool = True , ) -> \"SpectralLine\" : \"\"\"Add a new SpectralLine to the `mass2.fluorescence_lines.spectra` dictionary, and as a variable in this module.\"\"\" # require exactly one method of specifying the amplitude of each component assert reference_amplitude_type in { AmplitudeType . LORENTZIAN_PEAK_HEIGHT , AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY , AmplitudeType . VOIGT_PEAK_HEIGHT , } # require the reference exists in lineshape_references assert reference_short in lineshape_references # require kalpha lines to have ka12_energy_diff if linetype . startswith ( \"KAlpha\" ) and ka12_energy_diff is not None : ka12_energy_diff = float ( ka12_energy_diff ) # require reference_plot_instrument_gaussian_fwhm to be a float or None assert reference_plot_instrument_gaussian_fwhm is None or isinstance ( reference_plot_instrument_gaussian_fwhm , float ) line = cls ( element = element , material = material , linetype = linetype , nominal_peak_energy = float ( nominal_peak_energy ), energies = np . array ( energies ), lorentzian_fwhm = np . array ( lorentzian_fwhm ), reference_amplitude = np . array ( reference_amplitude ), reference_amplitude_type = reference_amplitude_type , reference_measurement_type = reference_measurement_type , intrinsic_sigma = intrinsic_sigma , reference_plot_instrument_gaussian_fwhm = reference_plot_instrument_gaussian_fwhm , reference_short = reference_short , position_uncertainty = float ( position_uncertainty ), is_default_material = is_default_material , ) name = line . shortname if name in spectra . keys () and ( not allow_replacement ): raise ValueError ( f \"spectrum { name } already exists\" ) # Add this SpectralLine to spectra dict AND make it be a variable in the module spectra [ name ] = line globals ()[ name ] = line return line cumulative_amplitudes property Cumulative sum of the Lorentzian integral intensities. lorentz_amplitude cached property Return (and cache) computed Lorentzian peak heights of the components. lorentzian_integral_intensity cached property Return (and cache) computed integrated intensities of the Lorentzian components. normalized_lorentzian_integral_intensity cached property Return (and cache) computed integrated intensities of the Lorentzian components, normalized so sum=1. peak_energy cached property Find the peak energy of the line shape assuming ideal instrument resolution. reference property The full comment and/or citation for the reference data. shortname property A short name for the line, suitable for use as a dictionary key. __call__ ( x , instrument_gaussian_fwhm ) Make the class callable, returning the same value as the self.pdf method. Source code in mass2/calibration/fluorescence_lines.py 152 153 154 def __call__ ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> NDArray : \"\"\"Make the class callable, returning the same value as the self.pdf method.\"\"\" return self . pdf ( x , instrument_gaussian_fwhm ) __repr__ () String representation of the SpectralLine. Source code in mass2/calibration/fluorescence_lines.py 264 265 266 def __repr__ ( self ) -> str : \"\"\"String representation of the SpectralLine.\"\"\" return f \"SpectralLine: { self . shortname } \" addline ( element , linetype , material , reference_short , reference_plot_instrument_gaussian_fwhm , nominal_peak_energy , energies , lorentzian_fwhm , reference_amplitude , reference_amplitude_type , ka12_energy_diff = None , position_uncertainty = np . nan , intrinsic_sigma = 0 , reference_measurement_type = None , is_default_material = True , allow_replacement = True ) classmethod Add a new SpectralLine to the mass2.fluorescence_lines.spectra dictionary, and as a variable in this module. Source code in mass2/calibration/fluorescence_lines.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 @classmethod def addline ( # noqa: PLR0917 cls , element : str , linetype : str , material : str , reference_short : str , reference_plot_instrument_gaussian_fwhm : float | None , nominal_peak_energy : float , energies : ArrayLike , lorentzian_fwhm : ArrayLike , reference_amplitude : ArrayLike , reference_amplitude_type : AmplitudeType , ka12_energy_diff : float | None = None , position_uncertainty : float = np . nan , intrinsic_sigma : float = 0 , reference_measurement_type : str | None = None , is_default_material : bool = True , allow_replacement : bool = True , ) -> \"SpectralLine\" : \"\"\"Add a new SpectralLine to the `mass2.fluorescence_lines.spectra` dictionary, and as a variable in this module.\"\"\" # require exactly one method of specifying the amplitude of each component assert reference_amplitude_type in { AmplitudeType . LORENTZIAN_PEAK_HEIGHT , AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY , AmplitudeType . VOIGT_PEAK_HEIGHT , } # require the reference exists in lineshape_references assert reference_short in lineshape_references # require kalpha lines to have ka12_energy_diff if linetype . startswith ( \"KAlpha\" ) and ka12_energy_diff is not None : ka12_energy_diff = float ( ka12_energy_diff ) # require reference_plot_instrument_gaussian_fwhm to be a float or None assert reference_plot_instrument_gaussian_fwhm is None or isinstance ( reference_plot_instrument_gaussian_fwhm , float ) line = cls ( element = element , material = material , linetype = linetype , nominal_peak_energy = float ( nominal_peak_energy ), energies = np . array ( energies ), lorentzian_fwhm = np . array ( lorentzian_fwhm ), reference_amplitude = np . array ( reference_amplitude ), reference_amplitude_type = reference_amplitude_type , reference_measurement_type = reference_measurement_type , intrinsic_sigma = intrinsic_sigma , reference_plot_instrument_gaussian_fwhm = reference_plot_instrument_gaussian_fwhm , reference_short = reference_short , position_uncertainty = float ( position_uncertainty ), is_default_material = is_default_material , ) name = line . shortname if name in spectra . keys () and ( not allow_replacement ): raise ValueError ( f \"spectrum { name } already exists\" ) # Add this SpectralLine to spectra dict AND make it be a variable in the module spectra [ name ] = line globals ()[ name ] = line return line components ( x , instrument_gaussian_fwhm ) List of spectrum components as a function of , the energy in eV Source code in mass2/calibration/fluorescence_lines.py 166 167 168 169 170 171 172 173 def components ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> list [ NDArray ]: \"\"\"List of spectrum components as a function of <x>, the energy in eV\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) x = np . asarray ( x , dtype = float ) components = [] for energy , fwhm , ampl in zip ( self . energies , self . lorentzian_fwhm , self . lorentzian_integral_intensity ): components . append ( ampl * voigt ( x , energy , hwhm = fwhm * 0.5 , sigma = gaussian_sigma )) return components fitter () Generate a GenericLineModel instance for fitting this SpectralLine. Source code in mass2/calibration/fluorescence_lines.py 279 280 281 282 283 284 def fitter ( self ) -> GenericLineModel : \"\"\"Generate a GenericLineModel instance for fitting this SpectralLine.\"\"\" fitter_class = GenericLineModel f = fitter_class ( self ) f . name = f \" { self . element }{ self . linetype } \" return f minimum_fwhm ( instrument_gaussian_fwhm ) for the narrowest lorentzian in the line model, calculate the combined fwhm including the lorentzian, intrinstic_sigma, and instrument_gaussian_fwhm Source code in mass2/calibration/fluorescence_lines.py 286 287 288 289 290 def minimum_fwhm ( self , instrument_gaussian_fwhm : float ) -> float : \"\"\"for the narrowest lorentzian in the line model, calculate the combined fwhm including the lorentzian, intrinstic_sigma, and instrument_gaussian_fwhm\"\"\" fwhm2 = np . amin ( self . lorentzian_fwhm ) ** 2 + instrument_gaussian_fwhm ** 2 + ( self . intrinsic_sigma * FWHM_OVER_SIGMA ) ** 2 return np . sqrt ( fwhm2 ) model ( has_linear_background = True , has_tails = False , prefix = '' , qemodel = None ) Generate a LineModel instance from a SpectralLine Source code in mass2/calibration/fluorescence_lines.py 268 269 270 271 272 273 274 275 276 277 def model ( self , has_linear_background : bool = True , has_tails : bool = False , prefix : str = \"\" , qemodel : Callable | None = None ) -> GenericLineModel : \"\"\"Generate a LineModel instance from a SpectralLine\"\"\" model_class = GenericLineModel name = f \" { self . element }{ self . linetype } \" m = model_class ( name = name , spect = self , has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix , qemodel = qemodel ) return m pdf ( x , instrument_gaussian_fwhm ) Spectrum (units of fraction per eV) as a function of , the energy in eV Source code in mass2/calibration/fluorescence_lines.py 156 157 158 159 160 161 162 163 164 def pdf ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> NDArray : \"\"\"Spectrum (units of fraction per eV) as a function of <x>, the energy in eV\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) x = np . asarray ( x , dtype = float ) result = np . zeros_like ( x ) for energy , fwhm , ampl in zip ( self . energies , self . lorentzian_fwhm , self . normalized_lorentzian_integral_intensity ): result += ampl * voigt ( x , energy , hwhm = fwhm * 0.5 , sigma = gaussian_sigma ) # mass2.voigt() is normalized to have unit integrated intensity return result plot ( x = None , instrument_gaussian_fwhm = 0 , axis = None , components = True , label = None , setylim = True , color = None ) Plot the spectrum. x - np array of energy in eV to plot at (sensible default) axis - axis to plot on (default creates new figure) components - True plots each voigt component in addition to the spectrum label - a string to label the plot with (optional) Source code in mass2/calibration/fluorescence_lines.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def plot ( self , x : ArrayLike | None = None , instrument_gaussian_fwhm : float = 0 , axis : plt . Axes | None = None , components : bool = True , label : str | None = None , setylim : bool = True , color : str | None = None , ) -> plt . Axes : \"\"\"Plot the spectrum. x - np array of energy in eV to plot at (sensible default) axis - axis to plot on (default creates new figure) components - True plots each voigt component in addition to the spectrum label - a string to label the plot with (optional)\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) if x is None : width = max ( 2 * gaussian_sigma , 3 * float ( np . amax ( self . lorentzian_fwhm ))) lo = np . amin ( self . energies ) - width hi = np . amax ( self . energies ) + width x = np . linspace ( lo , hi , 500 ) x = np . asarray ( x ) if axis is None : plt . figure () axis = plt . gca () if components : for component in self . components ( x , instrument_gaussian_fwhm ): axis . plot ( x , component , \"--\" ) pdf = self . pdf ( x , instrument_gaussian_fwhm ) axis . plot ( x , pdf , lw = 2 , label = label , color = color ) axis . set_xlabel ( \"Energy (eV)\" ) axis . set_ylabel ( f \"Counts per { float ( x [ 1 ] - x [ 0 ]) : .2 } eV bin\" ) axis . set_xlim ( x [ 0 ], x [ - 1 ]) if setylim : axis . set_ylim ( 0 , np . amax ( pdf ) * 1.05 ) axis . set_title ( f \" { self . shortname } with resolution { instrument_gaussian_fwhm : .2f } eV FWHM\" ) return axis plot_like_reference ( axis = None ) Plot the spectrum to match the instrument resolution used in the reference data publication, if known. Source code in mass2/calibration/fluorescence_lines.py 213 214 215 216 217 218 219 220 def plot_like_reference ( self , axis : plt . Axes | None = None ) -> plt . Axes : \"\"\"Plot the spectrum to match the instrument resolution used in the reference data publication, if known.\"\"\" if self . reference_plot_instrument_gaussian_fwhm is None : fwhm = 0.001 else : fwhm = self . reference_plot_instrument_gaussian_fwhm axis = self . plot ( axis = axis , instrument_gaussian_fwhm = fwhm ) return axis quick_monochromatic_line ( name , energy , lorentzian_fwhm , intrinsic_sigma = 0.0 ) classmethod Create a quick monochromatic line. Intended for use in calibration when we know a line energy, but not a lineshape model. Returns and instrance of SpectralLine with most fields having contents like \"unknown: quick_line\". The line will have a single lorentzian element with the given energy, fwhm, and intrinsic_sigma values. Source code in mass2/calibration/fluorescence_lines.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 @classmethod def quick_monochromatic_line ( cls , name : str , energy : float , lorentzian_fwhm : float , intrinsic_sigma : float = 0.0 ) -> \"SpectralLine\" : \"\"\" Create a quick monochromatic line. Intended for use in calibration when we know a line energy, but not a lineshape model. Returns and instrance of SpectralLine with most fields having contents like \"unknown: quick_line\". The line will have a single lorentzian element with the given energy, fwhm, and intrinsic_sigma values. \"\"\" energy = float ( energy ) element = name material = \"unknown: quick_line\" if lorentzian_fwhm <= 0 and intrinsic_sigma <= 1e-6 : intrinsic_sigma = 1e-6 linetype = \"Gaussian\" reference_short = \"unknown: quick_line\" reference_amplitude = np . array ([ 1.0 ]) reference_amplitude_type = AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY nominal_peak_energy = energy position_uncertainty = 0.0 reference_measurement_type = \"unkown: quick_line\" return cls ( element = element , material = material , linetype = linetype , energies = np . array ([ energy ]), lorentzian_fwhm = np . array ([ lorentzian_fwhm ]), intrinsic_sigma = intrinsic_sigma , reference_short = reference_short , reference_amplitude = reference_amplitude , reference_amplitude_type = reference_amplitude_type , nominal_peak_energy = nominal_peak_energy , position_uncertainty = position_uncertainty , reference_measurement_type = reference_measurement_type , ) rvs ( size , instrument_gaussian_fwhm , rng = None ) The CDF and PPF (cumulative distribution and percentile point functions) are hard to compute. But it's easy enough to generate the random variates themselves, so we override that method. Source code in mass2/calibration/fluorescence_lines.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def rvs ( self , size : int | tuple [ int ] | None , instrument_gaussian_fwhm : float , rng : np . random . Generator | None = None ) -> NDArray : \"\"\"The CDF and PPF (cumulative distribution and percentile point functions) are hard to compute. But it's easy enough to generate the random variates themselves, so we override that method.\"\"\" if rng is None : rng = _rng gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) # Choose from among the N Lorentzian lines in proportion to the line amplitudes iline = self . cumulative_amplitudes . searchsorted ( rng . uniform ( 0 , self . cumulative_amplitudes [ - 1 ], size = size )) # Choose Lorentzian variates of the appropriate width (but centered on 0) lor = rng . standard_cauchy ( size = size ) * self . lorentzian_fwhm [ iline ] * 0.5 # If necessary, add a Gaussian variate to mimic finite resolution if gaussian_sigma > 0.0 : lor += rng . standard_normal ( size = size ) * gaussian_sigma # Finally, add the line centers. results = lor + self . energies [ iline ] # We must check for non-positive results and replace them by recursive call # to self.rvs(). not_positive = results <= 0.0 if np . any ( not_positive ): Nbad = not_positive . sum () results [ not_positive ] = self . rvs ( size = Nbad , instrument_gaussian_fwhm = instrument_gaussian_fwhm ) return results LineEnergies () A dictionary to know a lot of x-ray fluorescence line energies, based on Deslattes' database. It is built on facts from mass2.calibration.nist_xray_database module. It is a dictionary from peak name to energy, with several alternate names for the lines: E = Energies() print E[\"MnKAlpha\"] print E[\"MnKAlpha\"], E[\"MnKA\"], E[\"MnKA1\"], E[\"MnKL3\"] Source code in mass2/calibration/fluorescence_lines.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def LineEnergies () -> dict [ str , float ]: \"\"\" A dictionary to know a lot of x-ray fluorescence line energies, based on Deslattes' database. It is built on facts from mass2.calibration.nist_xray_database module. It is a dictionary from peak name to energy, with several alternate names for the lines: E = Energies() print E[\"MnKAlpha\"] print E[\"MnKAlpha\"], E[\"MnKA\"], E[\"MnKA1\"], E[\"MnKL3\"] \"\"\" db = NISTXrayDBFile () alternate_line_names = { v : k for ( k , v ) in db . LINE_NICKNAMES . items ()} data = {} for fullname , L in db . lines . items (): element , linename = fullname . split ( \" \" , 1 ) allnames = [ linename ] if linename in alternate_line_names : siegbahn_linename = alternate_line_names [ linename ] long_linename = siegbahn_linename . replace ( \"A\" , \"Alpha\" ) . replace ( \"B\" , \"Beta\" ) . replace ( \"G\" , \"Gamma\" ) allnames . append ( siegbahn_linename ) allnames . append ( long_linename ) if siegbahn_linename . endswith ( \"1\" ): allnames . append ( siegbahn_linename [: - 1 ]) allnames . append ( long_linename [: - 1 ]) for name in allnames : key = \"\" . join (( element , name )) data [ key ] = L . peak return data plot_all_spectra ( maxplots = 10 ) Makes plots showing the line shape and component parts for some lines. Intended to replicate plots in the literature giving spectral lineshapes. Source code in mass2/calibration/fluorescence_lines.py 1632 1633 1634 1635 1636 1637 1638 def plot_all_spectra ( maxplots : int = 10 ) -> None : \"\"\"Makes plots showing the line shape and component parts for some lines. Intended to replicate plots in the literature giving spectral lineshapes.\"\"\" keys = list ( spectra . keys ())[: maxplots ] for name in keys : spectrum = spectra [ name ] spectrum . plot_like_reference () Implements MLEModel, CompositeMLEModel, GenericLineModel CompositeMLEModel Bases: MLEModel , CompositeModel A version of lmfit.CompositeModel that uses Maximum Likelihood weights in place of chisq, as described in: doi:10.1007/s10909-014-1098-4 \"Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation\" Source code in mass2/calibration/line_models.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 class CompositeMLEModel ( MLEModel , lmfit . CompositeModel ): \"\"\"A version of lmfit.CompositeModel that uses Maximum Likelihood weights in place of chisq, as described in: doi:10.1007/s10909-014-1098-4 \"Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation\" \"\"\" def _residual ( self , params : lmfit . Parameters , data : NDArray | None , weights : NDArray | None , ** kwargs : Any ) -> NDArray : \"\"\"Calculate the chi_MLE^2 value from Joe Fowler's Paper doi:10.1007/s10909-014-1098-4 Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation \"\"\" y = self . eval ( params , ** kwargs ) if data is None : return y r2 = y - data nonzero = data > 0 r2 [ nonzero ] += data [ nonzero ] * np . log (( data / y )[ nonzero ]) vals = ( 2 * r2 ) ** 0.5 vals [ y < data ] *= - 1 return vals def __add__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Sum of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . add ) def __sub__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Difference of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . sub ) def __mul__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Product of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . mul ) def __truediv__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Ratio of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . truediv ) __add__ ( other ) Sum of two models Source code in mass2/calibration/line_models.py 236 237 238 def __add__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Sum of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . add ) __mul__ ( other ) Product of two models Source code in mass2/calibration/line_models.py 244 245 246 def __mul__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Product of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . mul ) __sub__ ( other ) Difference of two models Source code in mass2/calibration/line_models.py 240 241 242 def __sub__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Difference of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . sub ) __truediv__ ( other ) Ratio of two models Source code in mass2/calibration/line_models.py 248 249 250 def __truediv__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Ratio of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . truediv ) GenericLineModel Bases: MLEModel A generic line model for fitting spectral lines. Source code in mass2/calibration/line_models.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 class GenericLineModel ( MLEModel ): \"\"\"A generic line model for fitting spectral lines.\"\"\" def __init__ ( self , spect : \"SpectralLine\" , independent_vars : list [ str ] = [ \"bin_centers\" ], prefix : str = \"\" , nan_policy : str = \"raise\" , has_linear_background : bool = True , has_tails : bool = False , qemodel : Callable | None = None , ** kwargs : Any , ): \"\"\"Initialize a GenericLineModel Parameters ---------- spect : SpectralLine The line or feature to be modeled independent_vars : list[str], optional List of independent variable names, by default [\"bin_centers\"] prefix : str, optional Model, by default \"\" nan_policy : str, optional How to handle NaN results in the computed spectrum, by default \"raise\" has_linear_background : bool, optional Whether the background model can have a nonzero slope, by default True has_tails : bool, optional Whether exponential tails are included in the model, by default False qemodel : Callable | None, optional A model for the quantum efficiency (which changes the expected line shape), by default None Returns ------- GenericLineModel The initialized model Raises ------ ValueError If the spectral model produces negative or NaN values \"\"\" self . spect = spect self . _has_tails = has_tails self . _has_linear_background = has_linear_background param_names = [ \"fwhm\" , \"peak_ph\" , \"dph_de\" , \"integral\" ] if self . _has_linear_background : param_names += [ \"background\" , \"bg_slope\" ] if self . _has_tails : param_names += [ \"tail_frac\" , \"tail_tau\" , \"tail_share_hi\" , \"tail_tau_hi\" ] kwargs . update ({ \"prefix\" : prefix , \"nan_policy\" : nan_policy , \"independent_vars\" : independent_vars , \"param_names\" : param_names }) if has_tails : def modelfunctails ( # noqa: PLR0917 bin_centers : ArrayLike , fwhm : float , peak_ph : float , dph_de : float , integral : float , background : float = 0 , bg_slope : float = 0 , tail_frac : float = 0 , tail_tau : float = 8 , tail_share_hi : float = 0 , tail_tau_hi : float = 8 , ) -> NDArray : bin_centers = np . asarray ( bin_centers , dtype = float ) bin_width = bin_centers [ 1 ] - bin_centers [ 0 ] energy = ( bin_centers - peak_ph ) / dph_de + self . spect . peak_energy def cleanspectrum_fn ( x : ArrayLike ) -> NDArray : return self . spect . pdf ( x , instrument_gaussian_fwhm = fwhm ) # tail_tau* is in energy units but has to be converted to the same units as `bin_centers` tail_arbs_lo = tail_tau * dph_de tail_arbs_hi = tail_tau_hi * dph_de spectrum = _smear_exponential_tail ( cleanspectrum_fn , energy , fwhm , tail_frac , tail_arbs_lo , tail_share_hi , tail_arbs_hi ) scale_factor = integral * bin_width * dph_de r = _scale_add_bg ( spectrum , scale_factor , background , bg_slope ) if any ( np . isnan ( r )) or any ( r < 0 ): raise ValueError ( \"some entry in r is nan or negative\" ) if qemodel is None : return r return r * qemodel ( energy ) super () . __init__ ( modelfunctails , ** kwargs ) else : def modelfunc ( bin_centers : ArrayLike , fwhm : float , peak_ph : float , dph_de : float , integral : float , background : float = 0 , bg_slope : float = 0 , ) -> NDArray : bin_centers = np . asarray ( bin_centers , dtype = float ) bin_width = bin_centers [ 1 ] - bin_centers [ 0 ] energy = ( bin_centers - peak_ph ) / dph_de + self . spect . peak_energy spectrum = self . spect . pdf ( energy , fwhm ) scale_factor = integral * bin_width / dph_de r = _scale_add_bg ( spectrum , scale_factor , background , bg_slope ) if any ( np . isnan ( r )) or any ( r < 0 ): raise ValueError ( \"some entry in r is nan or negative\" ) if qemodel is None : return r return r * qemodel ( energy ) super () . __init__ ( modelfunc , ** kwargs ) self . _set_paramhints_prefix () def _set_paramhints_prefix ( self ) -> None : \"\"\"Set parameter hints with reasonable initial values and bounds.\"\"\" nominal_peak_energy = self . spect . nominal_peak_energy self . set_param_hint ( \"fwhm\" , value = nominal_peak_energy / 1000 , min = nominal_peak_energy / 10000 , max = nominal_peak_energy ) self . set_param_hint ( \"peak_ph\" , value = nominal_peak_energy , min = 0 ) self . set_param_hint ( \"dph_de\" , value = 1 , min = 0.01 , max = 100 ) self . set_param_hint ( \"integral\" , value = 100 , min = 0 ) if self . _has_linear_background : self . set_param_hint ( \"background\" , value = 1 , min = 0 ) self . set_param_hint ( \"bg_slope\" , value = 0 , vary = False ) if self . _has_tails : self . set_param_hint ( \"tail_frac\" , value = 0.05 , min = 0 , max = 1 , vary = True ) self . set_param_hint ( \"tail_tau\" , value = nominal_peak_energy / 200 , min = 0 , max = nominal_peak_energy / 10 , vary = True ) self . set_param_hint ( \"tail_share_hi\" , value = 0 , min = 0 , max = 1 , vary = False ) self . set_param_hint ( \"tail_tau_hi\" , value = nominal_peak_energy / 200 , min = 0 , max = nominal_peak_energy / 10 , vary = False ) def guess ( self , data : ArrayLike , bin_centers : ArrayLike , dph_de : float , ** kwargs : Any ) -> lmfit . Parameters : \"Guess values for the peak_ph, integral, and background.\" data = np . asarray ( data ) bin_centers = np . asarray ( bin_centers ) order_stat = np . array ( data . cumsum (), dtype = float ) / data . sum () def percentiles ( p : float ) -> NDArray : \"\"\"Find the p-th percentile of the data using histograms.\"\"\" return bin_centers [( order_stat > p ) . argmax ()] fwhm_arb = 0.7 * ( percentiles ( 0.75 ) - percentiles ( 0.25 )) peak_ph = bin_centers [ data . argmax ()] if len ( data ) > 20 : # Ensure baseline guess > 0 (see Issue #152). Guess at least 1 background across all bins baseline = max ( data [ 0 : 10 ] . mean (), 1.0 / len ( data )) else : baseline = 0.1 tcounts_above_bg = data . sum () - baseline * len ( data ) if tcounts_above_bg < 0 : tcounts_above_bg = data . sum () # lets avoid negative estimates for the integral pars = self . make_params ( peak_ph = peak_ph , background = baseline , integral = tcounts_above_bg , fwhm = fwhm_arb / dph_de , dph_de = dph_de ) return lmfit . models . update_param_vals ( pars , self . prefix , ** kwargs ) __init__ ( spect , independent_vars = [ 'bin_centers' ], prefix = '' , nan_policy = 'raise' , has_linear_background = True , has_tails = False , qemodel = None , ** kwargs ) Initialize a GenericLineModel Parameters: spect ( SpectralLine ) \u2013 The line or feature to be modeled independent_vars ( list [ str ] , default: ['bin_centers'] ) \u2013 List of independent variable names, by default [\"bin_centers\"] prefix ( str , default: '' ) \u2013 Model, by default \"\" nan_policy ( str , default: 'raise' ) \u2013 How to handle NaN results in the computed spectrum, by default \"raise\" has_linear_background ( bool , default: True ) \u2013 Whether the background model can have a nonzero slope, by default True has_tails ( bool , default: False ) \u2013 Whether exponential tails are included in the model, by default False qemodel ( Callable | None , default: None ) \u2013 A model for the quantum efficiency (which changes the expected line shape), by default None Returns: GenericLineModel \u2013 The initialized model Raises: ValueError \u2013 If the spectral model produces negative or NaN values Source code in mass2/calibration/line_models.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def __init__ ( self , spect : \"SpectralLine\" , independent_vars : list [ str ] = [ \"bin_centers\" ], prefix : str = \"\" , nan_policy : str = \"raise\" , has_linear_background : bool = True , has_tails : bool = False , qemodel : Callable | None = None , ** kwargs : Any , ): \"\"\"Initialize a GenericLineModel Parameters ---------- spect : SpectralLine The line or feature to be modeled independent_vars : list[str], optional List of independent variable names, by default [\"bin_centers\"] prefix : str, optional Model, by default \"\" nan_policy : str, optional How to handle NaN results in the computed spectrum, by default \"raise\" has_linear_background : bool, optional Whether the background model can have a nonzero slope, by default True has_tails : bool, optional Whether exponential tails are included in the model, by default False qemodel : Callable | None, optional A model for the quantum efficiency (which changes the expected line shape), by default None Returns ------- GenericLineModel The initialized model Raises ------ ValueError If the spectral model produces negative or NaN values \"\"\" self . spect = spect self . _has_tails = has_tails self . _has_linear_background = has_linear_background param_names = [ \"fwhm\" , \"peak_ph\" , \"dph_de\" , \"integral\" ] if self . _has_linear_background : param_names += [ \"background\" , \"bg_slope\" ] if self . _has_tails : param_names += [ \"tail_frac\" , \"tail_tau\" , \"tail_share_hi\" , \"tail_tau_hi\" ] kwargs . update ({ \"prefix\" : prefix , \"nan_policy\" : nan_policy , \"independent_vars\" : independent_vars , \"param_names\" : param_names }) if has_tails : def modelfunctails ( # noqa: PLR0917 bin_centers : ArrayLike , fwhm : float , peak_ph : float , dph_de : float , integral : float , background : float = 0 , bg_slope : float = 0 , tail_frac : float = 0 , tail_tau : float = 8 , tail_share_hi : float = 0 , tail_tau_hi : float = 8 , ) -> NDArray : bin_centers = np . asarray ( bin_centers , dtype = float ) bin_width = bin_centers [ 1 ] - bin_centers [ 0 ] energy = ( bin_centers - peak_ph ) / dph_de + self . spect . peak_energy def cleanspectrum_fn ( x : ArrayLike ) -> NDArray : return self . spect . pdf ( x , instrument_gaussian_fwhm = fwhm ) # tail_tau* is in energy units but has to be converted to the same units as `bin_centers` tail_arbs_lo = tail_tau * dph_de tail_arbs_hi = tail_tau_hi * dph_de spectrum = _smear_exponential_tail ( cleanspectrum_fn , energy , fwhm , tail_frac , tail_arbs_lo , tail_share_hi , tail_arbs_hi ) scale_factor = integral * bin_width * dph_de r = _scale_add_bg ( spectrum , scale_factor , background , bg_slope ) if any ( np . isnan ( r )) or any ( r < 0 ): raise ValueError ( \"some entry in r is nan or negative\" ) if qemodel is None : return r return r * qemodel ( energy ) super () . __init__ ( modelfunctails , ** kwargs ) else : def modelfunc ( bin_centers : ArrayLike , fwhm : float , peak_ph : float , dph_de : float , integral : float , background : float = 0 , bg_slope : float = 0 , ) -> NDArray : bin_centers = np . asarray ( bin_centers , dtype = float ) bin_width = bin_centers [ 1 ] - bin_centers [ 0 ] energy = ( bin_centers - peak_ph ) / dph_de + self . spect . peak_energy spectrum = self . spect . pdf ( energy , fwhm ) scale_factor = integral * bin_width / dph_de r = _scale_add_bg ( spectrum , scale_factor , background , bg_slope ) if any ( np . isnan ( r )) or any ( r < 0 ): raise ValueError ( \"some entry in r is nan or negative\" ) if qemodel is None : return r return r * qemodel ( energy ) super () . __init__ ( modelfunc , ** kwargs ) self . _set_paramhints_prefix () guess ( data , bin_centers , dph_de , ** kwargs ) Guess values for the peak_ph, integral, and background. Source code in mass2/calibration/line_models.py 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 def guess ( self , data : ArrayLike , bin_centers : ArrayLike , dph_de : float , ** kwargs : Any ) -> lmfit . Parameters : \"Guess values for the peak_ph, integral, and background.\" data = np . asarray ( data ) bin_centers = np . asarray ( bin_centers ) order_stat = np . array ( data . cumsum (), dtype = float ) / data . sum () def percentiles ( p : float ) -> NDArray : \"\"\"Find the p-th percentile of the data using histograms.\"\"\" return bin_centers [( order_stat > p ) . argmax ()] fwhm_arb = 0.7 * ( percentiles ( 0.75 ) - percentiles ( 0.25 )) peak_ph = bin_centers [ data . argmax ()] if len ( data ) > 20 : # Ensure baseline guess > 0 (see Issue #152). Guess at least 1 background across all bins baseline = max ( data [ 0 : 10 ] . mean (), 1.0 / len ( data )) else : baseline = 0.1 tcounts_above_bg = data . sum () - baseline * len ( data ) if tcounts_above_bg < 0 : tcounts_above_bg = data . sum () # lets avoid negative estimates for the integral pars = self . make_params ( peak_ph = peak_ph , background = baseline , integral = tcounts_above_bg , fwhm = fwhm_arb / dph_de , dph_de = dph_de ) return lmfit . models . update_param_vals ( pars , self . prefix , ** kwargs ) LineModelResult Bases: ModelResult like lmfit.model.Model result, but with some convenient plotting functions for line spectra fits Source code in mass2/calibration/line_models.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 class LineModelResult ( lmfit . model . ModelResult ): \"\"\"like lmfit.model.Model result, but with some convenient plotting functions for line spectra fits\"\"\" def _compact_fit_report ( self ) -> str : \"\"\"A compact fit report suitable for annotating a plot\"\"\" s = \"\" sn = { \"background\" : \"bg\" , \"integral\" : \"intgrl\" , \"bg_slope\" : \"bg_slp\" } for k in sorted ( self . params . keys ()): v = self . params [ k ] if v . vary : if v . stderr is None : sig_figs = 2 s += f \" { sn . get ( k , k ) : 7 } { v . value : . { sig_figs } g } \u00b1None \\n \" else : sig_figs = int ( np . ceil ( np . log10 ( np . abs ( v . value / v . stderr ))) + 1 ) sig_figs = max ( 1 , sig_figs ) s += f \" { sn . get ( k , k ) : 7 } { v . value : . { sig_figs } g } \u00b1 { v . stderr : .2g } \\n \" else : sig_figs = 2 s += f \" { sn . get ( k , k ) : 7 } { v . value : . { sig_figs } g } HELD \\n \" s += f \"redchi { self . redchi : .2g } \" return s def plotm ( self , ax : plt . Axes | None = None , title : str | None = None , xlabel : str | None = None , ylabel : str | None = None ) -> None : \"\"\"plot the data, the fit, and annotate the plot with the parameters\"\"\" title , xlabel , ylabel = self . _handle_default_labels ( title , xlabel , ylabel ) if ax is None : plt . figure () ax = plt . gca () ax = lmfit . model . ModelResult . plot_fit ( self , ax = ax , xlabel = xlabel , ylabel = ylabel ) if title is not None : plt . title ( title ) ax . text ( 0.05 , 0.95 , self . _compact_fit_report (), transform = ax . transAxes , verticalalignment = \"top\" , bbox = dict ( facecolor = \"w\" , alpha = 0.5 ), family = \"monospace\" , ) # ax.legend([\"data\", self._compact_fit_report()],loc='best', frameon=True, framealpha = 0.5) ax . legend ( loc = \"upper right\" ) def set_label_hints ( self , binsize : float , ds_shortname : str , attr_str : str , unit_str : str , cut_hint : str , states_hint : str = \"\" ) -> None : \"\"\"Set hints for axis labels and title for plotm().\"\"\" self . _binsize = binsize self . _ds_shortname = ds_shortname self . _attr_str = attr_str self . _unit_str = unit_str self . _cut_hint = cut_hint self . _states_hint = states_hint self . _has_label_hints = True def _handle_default_labels ( self , title : str | None , xlabel : str | None , ylabel : str | None ) -> tuple [ str , str , str ]: \"\"\"Handle default labels for plotm().\"\"\" if hasattr ( self , \"_has_label_hints\" ): if title is None : title = f \" { self . _ds_shortname } : { self . model . spect . shortname } \" if ylabel is None : ylabel = f \"counts per { self . _binsize : g } { self . _unit_str } bin\" if len ( self . _states_hint ) > 0 : ylabel += f \" \\n states= { self . _states_hint } : { self . _cut_hint } \" if xlabel is None : xlabel = f \" { self . _attr_str } ( { self . _unit_str } )\" elif ylabel is None and \"bin_centers\" in self . userkws : binsize = self . userkws [ \"bin_centers\" ][ 1 ] - self . userkws [ \"bin_centers\" ][ 0 ] ylabel = f \"counts per { binsize : g } unit bin\" if title is None : title = \"\" if xlabel is None : xlabel = \"\" if ylabel is None : ylabel = \"\" return title , xlabel , ylabel def _validate_bins_per_fwhm ( self , minimum_bins_per_fwhm : float ) -> None : \"\"\"Validate that the bin size is small enough compared to the fitted FWHM to prevent approximation problems.\"\"\" if \"bin_centers\" not in self . userkws : return # i guess someone used this for a non histogram fit if not VALIDATE_BIN_SIZE : return bin_centers = self . userkws [ \"bin_centers\" ] bin_size = bin_centers [ 1 ] - bin_centers [ 0 ] for iComp in self . components : prefix = iComp . prefix dphde = f \" { prefix } dph_de\" fwhm = f \" { prefix } fwhm\" if ( dphde in self . params ) and ( fwhm in self . params ): bin_size_energy = bin_size / self . params [ dphde ] instrument_gaussian_fwhm = self . params [ fwhm ] . value minimum_fwhm_energy = iComp . spect . minimum_fwhm ( instrument_gaussian_fwhm ) bins_per_fwhm = minimum_fwhm_energy / bin_size_energy if bins_per_fwhm < minimum_bins_per_fwhm : msg = f \"\"\"bins are too large. Bin size (energy units) = { bin_size_energy : .3g } , fit FWHM (energy units) = { instrument_gaussian_fwhm : .3g } Minimum FWHM accounting for narrowest Lorentzian in spectrum (energy units) = { minimum_fwhm_energy : .3g } Bins per FWHM = { bins_per_fwhm : .3g } , Minimum Bins per FWHM = { minimum_bins_per_fwhm : .3g } To avoid this error: 1. use smaller bins, or 2. pass a smaller value of `minimum_bins_per_fwhm` to .fit, or 3. set `mass2.calibration.line_models.VALIDATE_BIN_SIZE = False`. See https://github.com/usnistgov/mass/issues/162 for discussion on this issue\"\"\" raise ValueError ( msg ) plotm ( ax = None , title = None , xlabel = None , ylabel = None ) plot the data, the fit, and annotate the plot with the parameters Source code in mass2/calibration/line_models.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 def plotm ( self , ax : plt . Axes | None = None , title : str | None = None , xlabel : str | None = None , ylabel : str | None = None ) -> None : \"\"\"plot the data, the fit, and annotate the plot with the parameters\"\"\" title , xlabel , ylabel = self . _handle_default_labels ( title , xlabel , ylabel ) if ax is None : plt . figure () ax = plt . gca () ax = lmfit . model . ModelResult . plot_fit ( self , ax = ax , xlabel = xlabel , ylabel = ylabel ) if title is not None : plt . title ( title ) ax . text ( 0.05 , 0.95 , self . _compact_fit_report (), transform = ax . transAxes , verticalalignment = \"top\" , bbox = dict ( facecolor = \"w\" , alpha = 0.5 ), family = \"monospace\" , ) # ax.legend([\"data\", self._compact_fit_report()],loc='best', frameon=True, framealpha = 0.5) ax . legend ( loc = \"upper right\" ) set_label_hints ( binsize , ds_shortname , attr_str , unit_str , cut_hint , states_hint = '' ) Set hints for axis labels and title for plotm(). Source code in mass2/calibration/line_models.py 458 459 460 461 462 463 464 465 466 467 468 def set_label_hints ( self , binsize : float , ds_shortname : str , attr_str : str , unit_str : str , cut_hint : str , states_hint : str = \"\" ) -> None : \"\"\"Set hints for axis labels and title for plotm().\"\"\" self . _binsize = binsize self . _ds_shortname = ds_shortname self . _attr_str = attr_str self . _unit_str = unit_str self . _cut_hint = cut_hint self . _states_hint = states_hint self . _has_label_hints = True MLEModel Bases: Model A version of lmfit.Model that uses Maximum Likelihood weights in place of chisq, as described in: doi:10.1007/s10909-014-1098-4 \"Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation\" Source code in mass2/calibration/line_models.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class MLEModel ( lmfit . Model ): \"\"\"A version of lmfit.Model that uses Maximum Likelihood weights in place of chisq, as described in: doi:10.1007/s10909-014-1098-4 \"Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation\" \"\"\" def _residual ( self , params : lmfit . Parameters , data : NDArray | None , weights : NDArray | None , ** kwargs : Any ) -> NDArray : \"\"\"Calculate the chi_MLE^2 value from Joe Fowler's Paper doi:10.1007/s10909-014-1098-4 \"Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation\" \"\"\" y = self . eval ( params , ** kwargs ) if data is None : return y r2 = y - data nonzero = data > 0 r2 [ nonzero ] += data [ nonzero ] * np . log (( data / y )[ nonzero ]) # points that are zero do not effect the chisq value, so should not # be inlcuded in the calculate on ndegrees of freedome, and therefore reduced chisq # GCO tried setting self.ndata here, but it doesn't persist # not clear how to calculate reduced chisq correctly # Calculate the sqrt(2*r2) in place into vals. # The mask for r2>0 avoids the problem found in MASS issue #217. vals = np . zeros_like ( r2 ) nonneg = r2 > 0 vals [ nonneg ] = np . sqrt ( 2 * r2 [ nonneg ]) vals [ y < data ] *= - 1 return vals def __repr__ ( self ) -> str : \"\"\"Return representation of Model.\"\"\" return f \"< { type ( self ) . __name__ } : { self . name } >\" def _reprstring ( self , long : bool = False ) -> str : \"\"\"Return a longer string representation of Model, with its options.\"\"\" out = self . _name opts = [] if len ( self . _prefix ) > 0 : opts . append ( f \"prefix=' { self . _prefix } '\" ) if long : for k , v in self . opts . items (): opts . append ( f \" { k } =' { v } '\" ) if len ( opts ) > 0 : out = \" {} , {} \" . format ( out , \", \" . join ( opts )) return f \" { type ( self ) . __name__ } ( { out } )\" def __add__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Sum of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . add ) def __sub__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Difference of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . sub ) def __mul__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Product of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . mul ) def __truediv__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Ratio of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . truediv ) def fit ( self , * args : Any , minimum_bins_per_fwhm : float | None = 3 , ** kwargs : Any ) -> \"LineModelResult\" : \"\"\"as lmfit.Model.fit except 1. the default method is \"least_squares because it gives error bars more often at 1.5-2.0X speed penalty 2. supports \"leastsq_refit\" which uses \"leastsq\" to fit, but if there are no error bars, refits with \"least_squares\" call result.set_label_hints(...) then result.plotm() for a nice plot. \"\"\" if \"method\" not in kwargs : # change default method kwargs [ \"method\" ] = \"least_squares\" # least_squares always gives uncertainties, while the normal default leastsq often does not # leastsq fails to give uncertaities if parameters are near bounds or at their initial value # least_squares is about 1.5X to 2.0X slower based on two test case if minimum_bins_per_fwhm is None : minimum_bins_per_fwhm = 3 # provide default value if \"weights\" in kwargs and kwargs [ \"weights\" ] is not None : msg = \"MLEModel assumes Poisson-distributed data; cannot use weights other than None\" raise Exception ( msg ) result = self . _fit ( * args , ** kwargs ) result . __class__ = LineModelResult result . _validate_bins_per_fwhm ( minimum_bins_per_fwhm ) return result def _fit ( self , * args : Any , ** kwargs : Any ) -> \"LineModelResult\" : \"\"\"internal implementation of fit to add support for \"leastsq_refit\" method\"\"\" if kwargs [ \"method\" ] == \"leastsq_refit\" : # First fit with leastsq (the fastest method) kwargs [ \"method\" ] = \"leastsq\" result0 = lmfit . Model . fit ( self , * args , ** kwargs ) if result0 . success and result0 . errorbars : return result0 # If we didn't get uncertainties, fit again with least_squares kwargs [ \"method\" ] = \"least_squares\" if \"params\" in kwargs : kwargs [ \"params\" ] = result0 . params elif len ( args ) > 1 : args = tuple ([ result0 . params if i == 1 else arg for ( i , arg ) in enumerate ( args )]) result = lmfit . Model . fit ( self , * args , ** kwargs ) else : result = lmfit . Model . fit ( self , * args , ** kwargs ) return result __add__ ( other ) Sum of two models Source code in mass2/calibration/line_models.py 156 157 158 def __add__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Sum of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . add ) __mul__ ( other ) Product of two models Source code in mass2/calibration/line_models.py 164 165 166 def __mul__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Product of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . mul ) __repr__ () Return representation of Model. Source code in mass2/calibration/line_models.py 139 140 141 def __repr__ ( self ) -> str : \"\"\"Return representation of Model.\"\"\" return f \"< { type ( self ) . __name__ } : { self . name } >\" __sub__ ( other ) Difference of two models Source code in mass2/calibration/line_models.py 160 161 162 def __sub__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Difference of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . sub ) __truediv__ ( other ) Ratio of two models Source code in mass2/calibration/line_models.py 168 169 170 def __truediv__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Ratio of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . truediv ) fit ( * args , minimum_bins_per_fwhm = 3 , ** kwargs ) as lmfit.Model.fit except 1. the default method is \"least_squares because it gives error bars more often at 1.5-2.0X speed penalty 2. supports \"leastsq_refit\" which uses \"leastsq\" to fit, but if there are no error bars, refits with \"least_squares\" call result.set_label_hints(...) then result.plotm() for a nice plot. Source code in mass2/calibration/line_models.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def fit ( self , * args : Any , minimum_bins_per_fwhm : float | None = 3 , ** kwargs : Any ) -> \"LineModelResult\" : \"\"\"as lmfit.Model.fit except 1. the default method is \"least_squares because it gives error bars more often at 1.5-2.0X speed penalty 2. supports \"leastsq_refit\" which uses \"leastsq\" to fit, but if there are no error bars, refits with \"least_squares\" call result.set_label_hints(...) then result.plotm() for a nice plot. \"\"\" if \"method\" not in kwargs : # change default method kwargs [ \"method\" ] = \"least_squares\" # least_squares always gives uncertainties, while the normal default leastsq often does not # leastsq fails to give uncertaities if parameters are near bounds or at their initial value # least_squares is about 1.5X to 2.0X slower based on two test case if minimum_bins_per_fwhm is None : minimum_bins_per_fwhm = 3 # provide default value if \"weights\" in kwargs and kwargs [ \"weights\" ] is not None : msg = \"MLEModel assumes Poisson-distributed data; cannot use weights other than None\" raise Exception ( msg ) result = self . _fit ( * args , ** kwargs ) result . __class__ = LineModelResult result . _validate_bins_per_fwhm ( minimum_bins_per_fwhm ) return result Calibration algorithms This file is intended to include algorithms that could be generally useful for calibration. Mostly they are pulled out of the former mass.calibration.young module. FailedToGetModelException Bases: Exception Exception raised when get_model() fails to find a model for a line Source code in mass2/calibration/algorithms.py 176 177 178 179 class FailedToGetModelException ( Exception ): \"\"\"Exception raised when get_model() fails to find a model for a line\"\"\" pass build_fit_ranges ( line_names , excluded_line_names , approx_ecal , fit_width_ev ) Returns a list of (lo,hi) where lo and hi have units of energy of ranges to fit in for each energy in line_names. Args: line_names (list[str or float]): list or line names or energies excluded_line_names (list[str or float]): list of line_names or energies to avoid when making fit ranges approx_ecal: an EnergyCalibration object containing an approximate calibration fit_width_ev (float): full size in eV of fit ranges Source code in mass2/calibration/algorithms.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def build_fit_ranges ( line_names : Iterable [ str | float ], excluded_line_names : Iterable [ str | float ], approx_ecal : EnergyCalibration , fit_width_ev : float ) -> tuple [ list [ float ], list [ tuple [ float , float ]], list [ float ]]: \"\"\"Returns a list of (lo,hi) where lo and hi have units of energy of ranges to fit in for each energy in line_names. Args: line_names (list[str or float]): list or line names or energies excluded_line_names (list[str or float]): list of line_names or energies to avoid when making fit ranges approx_ecal: an EnergyCalibration object containing an approximate calibration fit_width_ev (float): full size in eV of fit ranges \"\"\" _names , e_e = line_names_and_energies ( line_names ) _excl_names , excl_e_e = line_names_and_energies ( excluded_line_names ) half_width_ev = fit_width_ev / 2.0 all_e = np . sort ( np . hstack (( e_e , excl_e_e ))) assert len ( all_e ) == len ( np . unique ( all_e )) fit_lo_hi_energy = [] slopes_de_dph = [] for e in e_e : slope_de_dph = cast ( float , approx_ecal . energy2dedph ( e )) if any ( all_e < e ): nearest_below = all_e [ all_e < e ][ - 1 ] else : nearest_below = - np . inf if any ( all_e > e ): nearest_above = all_e [ all_e > e ][ 0 ] else : nearest_above = np . inf lo = max ( e - half_width_ev , ( e + nearest_below ) / 2.0 ) hi = min ( e + half_width_ev , ( e + nearest_above ) / 2.0 ) fit_lo_hi_energy . append (( lo , hi )) slopes_de_dph . append ( slope_de_dph ) return e_e , fit_lo_hi_energy , slopes_de_dph build_fit_ranges_ph ( line_names , excluded_line_names , approx_ecal , fit_width_ev ) Call build_fit_ranges() to get (lo,hi) for fitranges in energy units, then convert to ph using approx_ecal Source code in mass2/calibration/algorithms.py 122 123 124 125 126 127 128 129 130 131 132 133 134 def build_fit_ranges_ph ( line_names : Iterable [ str | float ], excluded_line_names : Iterable [ str | float ], approx_ecal : EnergyCalibration , fit_width_ev : float ) -> tuple [ list [ float ], list [ tuple [ float , float ]], list [ float ]]: \"\"\"Call build_fit_ranges() to get (lo,hi) for fitranges in energy units, then convert to ph using approx_ecal\"\"\" e_e , fit_lo_hi_energy , slopes_de_dph = build_fit_ranges ( line_names , excluded_line_names , approx_ecal , fit_width_ev ) fit_lo_hi_ph = [] for lo , hi in fit_lo_hi_energy : lo_ph = cast ( float , approx_ecal . energy2ph ( lo )) hi_ph = cast ( float , approx_ecal . energy2ph ( hi )) fit_lo_hi_ph . append (( lo_ph , hi_ph )) return e_e , fit_lo_hi_ph , slopes_de_dph find_local_maxima ( pulse_heights , gaussian_fwhm ) Smears each pulse by a gaussian of gaussian_fhwm and finds local maxima, returns a list of their locations in pulse_height units (sorted by number of pulses in peak) AND their peak values as: (peak_locations, peak_intensities) Args: pulse_heights (np.array(dtype=float)): a list of pulse heights (eg p_filt_value) gaussian_fwhm = fwhm of a gaussian that each pulse is smeared with, in same units as pulse heights Source code in mass2/calibration/algorithms.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def find_local_maxima ( pulse_heights : ArrayLike , gaussian_fwhm : float ) -> tuple [ NDArray , NDArray ]: \"\"\"Smears each pulse by a gaussian of gaussian_fhwm and finds local maxima, returns a list of their locations in pulse_height units (sorted by number of pulses in peak) AND their peak values as: (peak_locations, peak_intensities) Args: pulse_heights (np.array(dtype=float)): a list of pulse heights (eg p_filt_value) gaussian_fwhm = fwhm of a gaussian that each pulse is smeared with, in same units as pulse heights \"\"\" # kernel density estimation (with a gaussian kernel) n = 128 * 1024 gaussian_fwhm = float ( gaussian_fwhm ) # The above ensures that lo & hi are floats, so that (lo-hi)/n is always a float in python2 sigma = gaussian_fwhm / ( np . sqrt ( np . log ( 2 ) * 2 ) * 2 ) tbw = 1.0 / sigma / ( np . pi * 2 ) lo = np . min ( pulse_heights ) - 3 * gaussian_fwhm hi = np . max ( pulse_heights ) + 3 * gaussian_fwhm hist , bins = np . histogram ( pulse_heights , np . linspace ( lo , hi , n + 1 )) tx = np . fft . rfftfreq ( n , ( lo - hi ) / n ) ty = np . exp ( - ( tx ** 2 ) / 2 / tbw ** 2 ) x = ( bins [ 1 :] + bins [: - 1 ]) / 2 y = np . fft . irfft ( np . fft . rfft ( hist ) * ty ) flag = ( y [ 1 : - 1 ] > y [: - 2 ]) & ( y [ 1 : - 1 ] > y [ 2 :]) lm = np . arange ( 1 , n - 1 )[ flag ] lm = lm [ np . argsort ( - y [ lm ])] return np . array ( x [ lm ]), np . array ( y [ lm ]) find_opt_assignment ( peak_positions , line_names , nextra = 2 , nincrement = 3 , nextramax = 8 , maxacc = 0.015 ) Tries to find an assignment of peaks to line names that is reasonably self consistent and smooth Args: peak_positions (np.array(dtype=float)): a list of peak locations in arb units, e.g. p_filt_value units line_names (list[str or float)]): a list of calibration lines either as number (which is energies in eV), or name to be looked up in STANDARD_FEATURES nextra (int): the algorithm starts with the first len(line_names) + nextra peak_positions nincrement (int): each the algorithm fails to find a satisfactory peak assignment, it uses nincrement more lines nextramax (int): the algorithm stops incrementint nextra past this value, instead failing with a ValueError saying \"no peak assignment succeeded\" maxacc (float): an empirical number that determines if an assignment is good enough. The default number works reasonably well for tupac data Source code in mass2/calibration/algorithms.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def find_opt_assignment ( peak_positions : ArrayLike , line_names : list [ str | float ], nextra : int = 2 , nincrement : int = 3 , nextramax : int = 8 , maxacc : float = 0.015 , ) -> tuple [ list [ str ], NDArray , list [ int ]]: \"\"\"Tries to find an assignment of peaks to line names that is reasonably self consistent and smooth Args: peak_positions (np.array(dtype=float)): a list of peak locations in arb units, e.g. p_filt_value units line_names (list[str or float)]): a list of calibration lines either as number (which is energies in eV), or name to be looked up in STANDARD_FEATURES nextra (int): the algorithm starts with the first len(line_names) + nextra peak_positions nincrement (int): each the algorithm fails to find a satisfactory peak assignment, it uses nincrement more lines nextramax (int): the algorithm stops incrementint nextra past this value, instead failing with a ValueError saying \"no peak assignment succeeded\" maxacc (float): an empirical number that determines if an assignment is good enough. The default number works reasonably well for tupac data \"\"\" name_e , e_e = line_names_and_energies ( line_names ) n_sel_pp = len ( line_names ) + nextra # number of peak_positions to use to line up to line_names nmax = len ( line_names ) + nextramax peak_positions = np . asarray ( peak_positions ) while True : sel_positions = np . asarray ( peak_positions [: n_sel_pp ], dtype = \"float\" ) energies = np . asarray ( e_e , dtype = \"float\" ) assign = np . array ( list ( itertools . combinations ( sel_positions , len ( line_names )))) assign . sort ( axis = 1 ) fracs = np . divide ( energies [ 1 : - 1 ] - energies [: - 2 ], energies [ 2 :] - energies [: - 2 ]) est_pos = assign [:, : - 2 ] * ( 1 - fracs ) + assign [:, 2 :] * fracs acc_est = np . linalg . norm ( np . divide ( est_pos - assign [:, 1 : - 1 ], assign [:, 2 :] - assign [:, : - 2 ]), axis = 1 ) opt_assign_i = np . argmin ( acc_est ) acc = acc_est [ opt_assign_i ] opt_assign = assign [ opt_assign_i ] if acc > maxacc * np . sqrt ( len ( energies )): n_sel_pp += nincrement if n_sel_pp > nmax : msg = f \"no peak assignment succeeded: acc { acc : g } , maxacc*sqrt(len(energies)) { maxacc * np . sqrt ( len ( energies )) : g } \" raise ValueError ( msg ) else : continue else : return name_e , energies , list ( opt_assign ) get_model ( lineNameOrEnergy , has_linear_background = True , has_tails = False , prefix = '' ) Get a GenericLineModel for a line, given either a line name or energy in eV Parameters: lineNameOrEnergy ( GenericLineModel | SpectralLine | str | float ) \u2013 A line name, or energy, or a SpectralLine, or a GenericLineModel has_linear_background ( bool , default: True ) \u2013 Whether to allow a background slope, by default True has_tails ( bool , default: False ) \u2013 Whether to allow exponential tails, by default False prefix ( str , default: '' ) \u2013 Line nae prefix, by default \"\" Returns: GenericLineModel \u2013 An appropriate line model Raises: FailedToGetModelException \u2013 When a matching line cannot be found Source code in mass2/calibration/algorithms.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def get_model ( lineNameOrEnergy : GenericLineModel | SpectralLine | str | float , has_linear_background : bool = True , has_tails : bool = False , prefix : str = \"\" , ) -> GenericLineModel : \"\"\"Get a GenericLineModel for a line, given either a line name or energy in eV Parameters ---------- lineNameOrEnergy : GenericLineModel | SpectralLine | str | float A line name, or energy, or a SpectralLine, or a GenericLineModel has_linear_background : bool, optional Whether to allow a background slope, by default True has_tails : bool, optional Whether to allow exponential tails, by default False prefix : str, optional Line nae prefix, by default \"\" Returns ------- GenericLineModel An appropriate line model Raises ------ FailedToGetModelException When a matching line cannot be found \"\"\" if isinstance ( lineNameOrEnergy , GenericLineModel ): line = lineNameOrEnergy . spect elif isinstance ( lineNameOrEnergy , SpectralLine ): line = lineNameOrEnergy elif isinstance ( lineNameOrEnergy , str ): if lineNameOrEnergy in mass2 . spectra : line = mass2 . spectra [ lineNameOrEnergy ] elif lineNameOrEnergy in mass2 . STANDARD_FEATURES : energy = mass2 . STANDARD_FEATURES [ lineNameOrEnergy ] line = SpectralLine . quick_monochromatic_line ( lineNameOrEnergy , energy , 0.001 , 0 ) else : raise FailedToGetModelException ( f \"failed to get line from lineNameOrEnergy= { lineNameOrEnergy } \" ) else : try : energy = float ( lineNameOrEnergy ) except Exception : raise FailedToGetModelException ( f \"lineNameOrEnergy = { lineNameOrEnergy } is not convertable\" \" to float or a str in mass2.spectra or mass2.STANDARD_FEATURES\" ) line = SpectralLine . quick_monochromatic_line ( f \" { lineNameOrEnergy } eV\" , float ( lineNameOrEnergy ), 0.001 , 0 ) return line . model ( has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix ) line_names_and_energies ( line_names ) Given a list of line_names, return (names, energies) in eV. Can also accept energies in eV directly and return (names, energies). Source code in mass2/calibration/algorithms.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def line_names_and_energies ( line_names : Iterable [ str | float ]) -> tuple [ list [ str ], list [ float ]]: \"\"\"Given a list of line_names, return (names, energies) in eV. Can also accept energies in eV directly and return (names, energies). \"\"\" energies : list [ float ] = [] for name_or_energy in line_names : if isinstance ( name_or_energy , str ): energies . append ( STANDARD_FEATURES [ name_or_energy ]) else : energies . append ( float ( name_or_energy )) order : NDArray = np . argsort ( energies ) names = list ( line_names ) sorted_names = [ str ( names [ i ]) for i in order ] energies . sort () return sorted_names , energies multifit ( ph , line_names , fit_lo_hi , binsize_ev , slopes_de_dph , hide_deprecation = False ) Args: ph (np.array(dtype=float)): list of pulse heights line_names: names of calibration lines fit_lo_hi (list[list[float]]): a list of (lo,hi) with units of ph, used as edges of histograms for fitting binsize_ev (list[float]): list of binsizes in eV for calibration lines slopes_de_dph (list[float]): - list of slopes de_dph (e in eV) hide_deprecation: whether to suppress deprecation warnings Source code in mass2/calibration/algorithms.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def multifit ( ph : ArrayLike , line_names : Iterable [ str ], fit_lo_hi : list [ list [ float ]], binsize_ev : list [ float ], slopes_de_dph : list [ float ], hide_deprecation : bool = False , ) -> dict [ str , Any ]: \"\"\" Args: ph (np.array(dtype=float)): list of pulse heights line_names: names of calibration lines fit_lo_hi (list[list[float]]): a list of (lo,hi) with units of ph, used as edges of histograms for fitting binsize_ev (list[float]): list of binsizes in eV for calibration lines slopes_de_dph (list[float]): - list of slopes de_dph (e in eV) hide_deprecation: whether to suppress deprecation warnings \"\"\" name_e , e_e = line_names_and_energies ( line_names ) results = [] peak_ph = [] eres = [] for i , name in enumerate ( name_e ): lo , hi = fit_lo_hi [ i ] dP_dE = 1 / slopes_de_dph [ i ] binsize_ph = binsize_ev [ i ] * dP_dE result = singlefit ( ph , name , lo , hi , binsize_ph , dP_dE ) results . append ( result ) peak_ph . append ( result . best_values [ \"peak_ph\" ]) eres . append ( result . best_values [ \"fwhm\" ]) return { \"results\" : results , \"peak_ph\" : peak_ph , \"eres\" : eres , \"line_names\" : name_e , \"energies\" : e_e } singlefit ( ph , name , lo , hi , binsize_ph , approx_dP_dE ) Performs a fit to a single line in pulse height units Parameters: ph ( ArrayLike ) \u2013 Measured pulse heights name ( GenericLineModel | SpectralLine | str | float ) \u2013 Spectral line to fit, either as a name, energy in eV, SpectralLine, or GenericLineModel lo ( float ) \u2013 minimum pulse height to include in fit hi ( float ) \u2013 maximum pulse height to include in fit binsize_ph ( float ) \u2013 bin size in pulse height units approx_dP_dE ( float ) \u2013 Estimate of the dph/dE at the line energy, used to constrain the fit Returns: LineModelResult \u2013 The best-fit result Raises: ValueError \u2013 When too many bins would be used in the fit Source code in mass2/calibration/algorithms.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def singlefit ( ph : ArrayLike , name : GenericLineModel | SpectralLine | str | float , lo : float , hi : float , binsize_ph : float , approx_dP_dE : float ) -> LineModelResult : \"\"\"Performs a fit to a single line in pulse height units Parameters ---------- ph : ArrayLike Measured pulse heights name : GenericLineModel | SpectralLine | str | float Spectral line to fit, either as a name, energy in eV, SpectralLine, or GenericLineModel lo : float minimum pulse height to include in fit hi : float maximum pulse height to include in fit binsize_ph : float bin size in pulse height units approx_dP_dE : float Estimate of the dph/dE at the line energy, used to constrain the fit Returns ------- LineModelResult The best-fit result Raises ------ ValueError When too many bins would be used in the fit \"\"\" nbins = ( hi - lo ) / binsize_ph if nbins > 5000 : raise ValueError ( \"too damn many bins, dont like running out of memory\" ) counts , bin_edges = np . histogram ( ph , np . arange ( lo , hi , binsize_ph )) e = bin_edges [: - 1 ] + 0.5 * ( bin_edges [ 1 ] - bin_edges [ 0 ]) model = get_model ( name ) guess_params = model . guess ( counts , bin_centers = e , dph_de = approx_dP_dE ) if \"Gaussian\" not in model . name : guess_params [ \"dph_de\" ] . set ( approx_dP_dE , vary = False ) result = model . fit ( counts , guess_params , bin_centers = e , minimum_bins_per_fwhm = 1.5 ) result . energies = e return result Highly charged ions hci_lines.py Uses pickle file containing NIST ASD levels data to generate some commonly used HCI lines in mass. Meant to be a replacement for _highly_charged_ion_lines.py, which hard codes in line parameters. The pickle file can be gzip-compressed, provided the compressed filename ends with \".gz\". February 2020 Paul Szypryt NIST_ASD Class for working with a pickled atomic spectra database Source code in mass2/calibration/hci_lines.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class NIST_ASD : \"\"\"Class for working with a pickled atomic spectra database\"\"\" def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy __init__ ( pickleFilename = None ) Loads ASD pickle file (optionally gzipped) Parameters: pickleFilename ( str | None , default: None ) \u2013 ASD pickle file name, as str, or if none then mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH (default None) Source code in mass2/calibration/hci_lines.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) getAvailableElements () Returns a list of all available elements from the ASD pickle file Source code in mass2/calibration/hci_lines.py 52 53 54 55 def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) getAvailableLevels ( element , spectralCharge , requiredConf = None , requiredTerm = None , requiredJVal = None , maxLevels = None , units = 'eV' , getUncertainty = True ) For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters: element ( str ) \u2013 Elemental atomic symbol, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf ( str | None , default: None ) \u2013 if not None, limits results to those with conf == requiredConf , by default None requiredTerm ( str | None , default: None ) \u2013 if not None, limits results to those with term == requiredTerm , by default None requiredJVal ( str | None , default: None ) \u2013 if not None, limits results to those with a == requiredJVal , by default None maxLevels ( int | None , default: None ) \u2013 the maximum number of levels (sorted by energy) to return, by default None units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 whether to return uncertain values, by default True Returns: dict \u2013 A dictionary of energy level strings to energy levels. Source code in mass2/calibration/hci_lines.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict getAvailableSpectralCharges ( element ) For a given element, returns a list of all available charge states from the ASD pickle file Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' Returns: list [ int ] \u2013 Available charge states Source code in mass2/calibration/hci_lines.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) getSingleLevel ( element , spectralCharge , conf , term , JVal , units = 'eV' , getUncertainty = True ) Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf ( str ) \u2013 nuclear configuration, e.g. '2p' term ( str ) \u2013 nuclear term, e.g. '2P*' JVal ( str ) \u2013 total angular momentum J, e.g. '3/2' units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 includes uncertainties in list of levels, by default True Returns: float \u2013 description Source code in mass2/calibration/hci_lines.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy add_H_like_lines_from_asd ( asd , element , maxLevels = None ) Add all known H-like lines for a given element from the ASD database Source code in mass2/calibration/hci_lines.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def add_H_like_lines_from_asd ( asd : NIST_ASD , element : str , maxLevels : int | None = None ) -> list [ SpectralLine ]: \"\"\"Add all known H-like lines for a given element from the ASD database\"\"\" spectr_ch = xraydb . atomic_number ( element ) added_lines = [] if maxLevels is not None : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch , maxLevels = maxLevels + 1 ) else : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch ) for iLevel in list ( levelsDict . keys ()): lineEnergy = levelsDict [ iLevel ][ 0 ] if lineEnergy != 0.0 : iLine = add_hci_line ( element = element , spectr_ch = spectr_ch , line_identifier = iLevel , energies = [ lineEnergy ], widths = [ 0.1 ], ratios = [ 1.0 ] ) added_lines . append ( iLine ) return added_lines add_He_like_lines_from_asd ( asd , element , maxLevels = None ) Add all known He-like lines for a given element from the ASD database Source code in mass2/calibration/hci_lines.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def add_He_like_lines_from_asd ( asd : NIST_ASD , element : str , maxLevels : int | None = None ) -> list [ SpectralLine ]: \"\"\"Add all known He-like lines for a given element from the ASD database\"\"\" spectr_ch = xraydb . atomic_number ( element ) - 1 added_lines = [] if maxLevels is not None : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch , maxLevels = maxLevels + 1 ) else : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch ) for iLevel in list ( levelsDict . keys ()): lineEnergy = levelsDict [ iLevel ][ 0 ] if lineEnergy != 0.0 : iLine = add_hci_line ( element = element , spectr_ch = spectr_ch , line_identifier = iLevel , energies = [ lineEnergy ], widths = [ 0.1 ], ratios = [ 1.0 ] ) added_lines . append ( iLine ) return added_lines add_hci_line ( element , spectr_ch , line_identifier , energies , widths , ratios , nominal_peak_energy = None ) Add a single HCI line to the fluorescence_lines database Parameters: element ( str ) \u2013 The element whose line is being added, e.g. 'Ne' spectr_ch ( int ) \u2013 The charge state of the ion whose line is being added, e.g. 9 for H-like Ne line_identifier ( str ) \u2013 The line identifier, e.g. '1s2S1/2 - 2p2P3/2' energies ( ArrayLike ) \u2013 The energies of the components of the line, in eV widths ( ArrayLike ) \u2013 The Lorentzian FWHM widths of the components of the line, in eV ratios ( ArrayLike ) \u2013 The relative intensities of the components of the line nominal_peak_energy ( float | None , default: None ) \u2013 The nominal spectral peak in eV, by default None Returns: SpectralLine \u2013 The newly added SpectralLine object Source code in mass2/calibration/hci_lines.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def add_hci_line ( element : str , spectr_ch : int , line_identifier : str , energies : ArrayLike , widths : ArrayLike , ratios : ArrayLike , nominal_peak_energy : float | None = None , ) -> SpectralLine : \"\"\"Add a single HCI line to the fluorescence_lines database Parameters ---------- element : str The element whose line is being added, e.g. 'Ne' spectr_ch : int The charge state of the ion whose line is being added, e.g. 9 for H-like Ne line_identifier : str The line identifier, e.g. '1s2S1/2 - 2p2P3/2' energies : ArrayLike The energies of the components of the line, in eV widths : ArrayLike The Lorentzian FWHM widths of the components of the line, in eV ratios : ArrayLike The relative intensities of the components of the line nominal_peak_energy : float | None, optional The nominal spectral peak in eV, by default None Returns ------- SpectralLine The newly added SpectralLine object \"\"\" energies = np . asarray ( energies ) widths = np . asarray ( widths ) ratios = np . asarray ( ratios ) if nominal_peak_energy is None : nominal_peak_energy = np . dot ( energies , ratios ) / np . sum ( ratios ) linetype = f \" { int ( spectr_ch ) } { line_identifier } \" spectrum_class = fluorescence_lines . addline ( element = element , material = \"Highly Charged Ion\" , linetype = linetype , reference_short = \"NIST ASD\" , reference_plot_instrument_gaussian_fwhm = 0.5 , nominal_peak_energy = nominal_peak_energy , energies = energies , lorentzian_fwhm = widths , reference_amplitude = ratios , reference_amplitude_type = AmplitudeType . LORENTZIAN_PEAK_HEIGHT , ka12_energy_diff = None , ) return spectrum_class hci_models.py Some useful methods for initializing GenericLineModel and CompositeMLEModel objects applied to HCI lines. June 2020 Paul Szypryt add_bg_model ( generic_model , vary_slope = False ) Adds a LinearBackgroundModel to a generic lmfit model Parameters: generic_model ( GenericLineModel ) \u2013 object to which to add a linear background model vary_slope ( bool , default: False ) \u2013 allows a varying linear slope rather than just constant value, by default False Returns: GenericLineModel \u2013 The input model, with background componets added Source code in mass2/calibration/hci_models.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def add_bg_model ( generic_model : GenericLineModel , vary_slope : bool = False ) -> GenericLineModel : \"\"\"Adds a LinearBackgroundModel to a generic lmfit model Parameters ---------- generic_model : GenericLineModel object to which to add a linear background model vary_slope : bool, optional allows a varying linear slope rather than just constant value, by default False Returns ------- GenericLineModel The input model, with background componets added \"\"\" # composite_name = generic_model._name # bg_prefix = f\"{composite_name}_\".replace(\" \", \"_\").replace(\"J=\", \"\").replace(\"/\", \"_\").replace(\"*\", \"\").replace(\".\", \"\") raise NotImplementedError ( \"No LinearBackgroundModel still exists in mass2\" ) initialize_HLike_2P_model ( element , conf , has_linear_background = False , has_tails = False , vary_amp_ratio = False ) Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' conf ( str ) \u2013 nuclear configuration as str, e.g. '2p' or '3p' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False vary_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def initialize_HLike_2P_model ( element : str , conf : str , has_linear_background : bool = False , has_tails : bool = False , vary_amp_ratio : bool = False ) -> GenericLineModel : \"\"\"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' conf : str nuclear configuration as str, e.g. '2p' or '3p' has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional include low energy tail in the model, by default False vary_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns ------- GenericLineModel The new composite line \"\"\" # Set up line names and lmfit prefixes charge = int ( xraydb . atomic_number ( element )) line_name_1_2 = f \" { element }{ charge } { conf } 2P* J=1/2\" line_name_3_2 = f \" { element }{ charge } { conf } 2P* J=3/2\" prefix_1_2 = f \" { line_name_1_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) prefix_3_2 = f \" { line_name_3_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) # Initialize individual lines and models line_1_2 = spectra [ line_name_1_2 ] line_3_2 = spectra [ line_name_3_2 ] model_1_2 = line_1_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_1_2 ) model_3_2 = line_3_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_3_2 ) # Initialize composite model and set addition H-like constraints composite_name = f \" { element }{ charge } { conf } \" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = [ model_1_2 , model_3_2 ], has_linear_background = has_linear_background , peak_component_name = line_name_3_2 , ) amp_ratio_param_name = f \" { element }{ charge } _ { conf } _amp_ratio\" composite_model . set_param_hint ( name = amp_ratio_param_name , value = 0.5 , min = 0.0 , vary = vary_amp_ratio ) composite_model . set_param_hint ( f \" { prefix_1_2 } integral\" , expr = f \" { prefix_3_2 } integral * { amp_ratio_param_name } \" ) return composite_model initialize_HeLike_complex_model ( element , has_linear_background = False , has_tails = False , additional_line_names = []) Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P J=1, and 1s.2p 1P J=1 lines. Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the Lorentzian models, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False additional_line_names ( list , default: [] ) \u2013 additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns: GenericLineModel \u2013 A model of the given HCI complex. Source code in mass2/calibration/hci_models.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def initialize_HeLike_complex_model ( element : str , has_linear_background : bool = False , has_tails : bool = False , additional_line_names : list = [] ) -> GenericLineModel : \"\"\"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P* J=1, and 1s.2p 1P* J=1 lines. Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background : bool, optional include a single linear background on top of the Lorentzian models, by default False has_tails : bool, optional include low energy tail in the model, by default False additional_line_names : list, optional additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns ------- GenericLineModel A model of the given HCI complex. \"\"\" # Set up line names charge = int ( xraydb . atomic_number ( element ) - 1 ) line_name_1s2s_3S = f \" { element }{ charge } 1s.2s 3S J=1\" line_name_1s2p_3P = f \" { element }{ charge } 1s.2p 3P* J=1\" line_name_1s2p_1P = f \" { element }{ charge } 1s.2p 1P* J=1\" line_names = np . hstack ([[ line_name_1s2s_3S , line_name_1s2p_3P , line_name_1s2p_1P ], additional_line_names ]) # Set up lines and models based on line_names # individual_lines = [spectra[i_line_name]() for i_line_name in line_names] individual_models = [ initialize_hci_line_model ( i_line_name , has_linear_background = False , has_tails = has_tails ) for i_line_name in line_names ] # Set up composite model composite_name = f \" { element }{ charge } 1s2s_2p Complex\" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = individual_models , has_linear_background = has_linear_background , peak_component_name = line_name_1s2p_1P , ) return composite_model initialize_hci_composite_model ( composite_name , individual_models , has_linear_background = False , peak_component_name = None ) Initializes composite lmfit model from the sum of input models Parameters: composite_name ( str ) \u2013 name given to composite line model individual_models ( list [ GenericLineModel ] ) \u2013 Models to sum into a composite has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of group of lorentzians, by default False peak_component_name ( str | None , default: None ) \u2013 designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def initialize_hci_composite_model ( composite_name : str , individual_models : list [ GenericLineModel ], has_linear_background : bool = False , peak_component_name : str | None = None , ) -> GenericLineModel : \"\"\"Initializes composite lmfit model from the sum of input models Parameters ---------- composite_name : str name given to composite line model individual_models : list[GenericLineModel] Models to sum into a composite has_linear_background : bool, optional include a single linear background on top of group of lorentzians, by default False peak_component_name : str | None, optional designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns ------- GenericLineModel The new composite line \"\"\" composite_model : GenericLineModel = np . sum ( individual_models ) composite_model . name = composite_name if has_linear_background : composite_model = add_bg_model ( composite_model ) # Workaround for energy calibration using composite models, pick 1st GenericLineModel component line_model_components = [ i_comp for i_comp in composite_model . components if isinstance ( i_comp , mass2 . calibration . line_models . GenericLineModel ) ] if peak_component_name is None : peak_component_name = line_model_components [ 0 ] . _name peak_component_index = [ i_comp . _name for i_comp in line_model_components ] . index ( peak_component_name ) peak_component = line_model_components [ peak_component_index ] composite_model . peak_prefix = peak_component . prefix composite_model . peak_energy = peak_component . spect . peak_energy # Set up some constraints relative to peak_component num_line_components = len ( line_model_components ) line_component_prefixes = [ iComp . prefix for iComp in line_model_components ] line_component_energies = [ iComp . spect . peak_energy for iComp in line_model_components ] for i in np . arange ( num_line_components ): if i != peak_component_index : # Single fwhm across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } fwhm\" , expr = f \" { composite_model . peak_prefix } fwhm\" ) # Single dph_de across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } dph_de\" , expr = f \" { composite_model . peak_prefix } dph_de\" ) # Fixed energy separation based on database values separation = line_component_energies [ i ] - composite_model . peak_energy hint = f \"( { separation } * { composite_model . peak_prefix } dph_de) + { composite_model . peak_prefix } peak_ph\" composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } peak_ph\" , expr = hint ) composite_model . shortname = composite_name return composite_model initialize_hci_line_model ( line_name , has_linear_background = False , has_tails = False ) Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters: line_name ( str ) \u2013 name of line to use in mass2.spectra has_linear_background ( bool , default: False ) \u2013 include linear background in the model, by default False has_tails ( bool , default: False ) \u2013 include low-energy tail in the model, by default False Returns: GenericLineModel \u2013 New HCI line. Source code in mass2/calibration/hci_models.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def initialize_hci_line_model ( line_name : str , has_linear_background : bool = False , has_tails : bool = False ) -> GenericLineModel : \"\"\"Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters ---------- line_name : str name of line to use in mass2.spectra has_linear_background : bool, optional include linear background in the model, by default False has_tails : bool, optional include low-energy tail in the model, by default False Returns ------- GenericLineModel New HCI line. \"\"\" line = spectra [ line_name ] prefix = f \" { line_name } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) line_model = line . model ( has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix ) line_model . shortname = line_name return line_model models ( has_linear_background = False , has_tails = False , vary_Hlike_amp_ratio = False , additional_Helike_complex_lines = []) Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters: has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines ( list , default: [] ) \u2013 additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns: _type_ \u2013 Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. Source code in mass2/calibration/hci_models.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def models ( has_linear_background : bool = False , has_tails : bool = False , vary_Hlike_amp_ratio : bool = False , additional_Helike_complex_lines : list = [], ) -> dict : \"\"\"Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters ---------- has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines : list, optional additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns ------- _type_ Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. \"\"\" models_dict = {} # Make some common H-like 2P* models conf_Hlike_2P_dict = {} conf_Hlike_2P_dict [ \"N\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"O\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ne\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ar\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] for i_element in list ( conf_Hlike_2P_dict . keys ()): for i_conf in conf_Hlike_2P_dict [ i_element ]: Hlike_model = initialize_HLike_2P_model ( i_element , i_conf , has_linear_background = has_linear_background , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio , ) models_dict [ Hlike_model . _name ] = Hlike_model # Make some common He-like 1s2s,2p complex and higher order 1p* models # He-like lines Helike_complex_elements = [ \"N\" , \"O\" , \"Ne\" , \"Ar\" ] for i_element in Helike_complex_elements : Helike_model = initialize_HeLike_complex_model ( i_element , has_linear_background = has_linear_background , has_tails = has_tails , additional_line_names = additional_Helike_complex_lines , ) models_dict [ Helike_model . _name ] = Helike_model # 1s.np 1P* lines for n>=3 conf_Helike_1P_dict = {} conf_Helike_1P_dict [ \"N\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"O\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ne\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ar\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] for i_element in list ( conf_Helike_1P_dict . keys ()): i_charge = int ( xraydb . atomic_number ( i_element ) - 1 ) for i_conf in conf_Helike_1P_dict [ i_element ]: Helike_line_name = f \" { i_element }{ i_charge } { i_conf } 1P* J=1\" Helike_model = initialize_hci_line_model ( Helike_line_name , has_linear_background = has_linear_background , has_tails = has_tails ) models_dict [ Helike_model . _name ] = Helike_model # Some more complicated cases # 500 eV region of H-/He-like N N6_1s3p_model = initialize_hci_line_model ( \"N6 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) N7_2p_model = initialize_HLike_2P_model ( \"N\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) N_500eV_model = initialize_hci_composite_model ( \"N 500eV Region\" , [ N6_1s3p_model , N7_2p_model ], has_linear_background = has_linear_background , peak_component_name = \"N7 2p 2P* J=3/2\" , ) models_dict [ N_500eV_model . _name ] = N_500eV_model # 660 eV region of H-/He-like O O8_2p_model = initialize_HLike_2P_model ( \"O\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) O7_1s3p_model = initialize_hci_line_model ( \"O7 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) O_660eV_model = initialize_hci_composite_model ( \"O 660eV Region\" , [ O8_2p_model , O7_1s3p_model ], has_linear_background = has_linear_background , peak_component_name = \"O8 2p 2P* J=3/2\" , ) models_dict [ O_660eV_model . _name ] = O_660eV_model return models_dict Bookkeeping import_asd.py Tool for converting a NIST ASD levels sql dump into a pickle file February 2020 Paul Szypryt parseLine ( energyLevelsDict , fieldNamesDict , formattedLine ) Parse a line from the ASD sql dump and add it to the energyLevelsDict Parameters: energyLevelsDict ( dict [ str , dict [ int , dict [ str , list [ float ]]]] ) \u2013 description fieldNamesDict ( dict [ str , Any ] ) \u2013 description formattedLine ( str ) \u2013 description Source code in mass2/calibration/import_asd.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def parseLine ( energyLevelsDict : dict [ str , dict [ int , dict [ str , list [ float ]]]], fieldNamesDict : dict [ str , Any ], formattedLine : str ) -> None : \"\"\"Parse a line from the ASD sql dump and add it to the energyLevelsDict Parameters ---------- energyLevelsDict : dict[str, dict[int, dict[str, list[float]]]] _description_ fieldNamesDict : dict[str, Any] _description_ formattedLine : str _description_ \"\"\" lineAsArray = np . array ( ast . literal_eval ( formattedLine )) for iEntry in lineAsArray : element = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"element\" )] spectr_charge = int ( iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"spectr_charge\" )]) # Pull information that will be used to name dictionary keys conf = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"conf\" )] term = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"term\" )] j_val = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"j_val\" )] # Pull energy and uncertainty energy = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"energy\" )] # cm^-1, str unc = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"unc\" )] # cm^-1, str try : energy_inv_cm = float ( energy ) # cm^-1 except ValueError : energy_inv_cm = np . nan try : unc_inv_cm = float ( unc ) # cm^-1 except ValueError : unc_inv_cm = np . nan if conf and term and term != \"*\" : # Set up upper level dictionary if element not in energyLevelsDict . keys (): energyLevelsDict [ element ] = {} if spectr_charge not in energyLevelsDict [ element ] . keys (): energyLevelsDict [ element ][ spectr_charge ] = {} levelName = f \" { conf } { term } J= { j_val } \" energyLevelsDict [ element ][ spectr_charge ][ levelName ] = [ energy_inv_cm , unc_inv_cm ] write_asd_pickle ( inputFilename , outputFilename ) Write the levels from a NIST Atomic Spectra Database SQL dump to a pickle file Parameters: inputFilename ( str ) \u2013 The ASD's sql dump file name outputFilename ( str ) \u2013 The pickle file name to write the output dictionary to Source code in mass2/calibration/import_asd.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def write_asd_pickle ( inputFilename : str , outputFilename : str ) -> None : \"\"\"Write the levels from a NIST Atomic Spectra Database SQL dump to a pickle file Parameters ---------- inputFilename : str The ASD's sql dump file name outputFilename : str The pickle file name to write the output dictionary to \"\"\" createTableString = \"CREATE TABLE\" valueSearchString = r \"\\`([^\\`]*)\\`\" tableName = \"\" fieldNamesDict : dict [ str , Any ] = {} energyLevelsDict : dict [ str , dict [ int , dict [ str , list [ float ]]]] = {} with open ( inputFilename , \"r\" , encoding = \"utf-8\" ) as ASD_file : for line in ASD_file : # Create dictionary of field names for various tables if line . startswith ( createTableString ): match = re . search ( valueSearchString , line ) if match is not None : fieldNamesDict [ match . groups ()[ 0 ]] = [] elif tableName and line . strip () . startswith ( \"`\" ): match = re . search ( valueSearchString , line ) if match is not None : fieldNamesDict [ tableName ] . append ( match . groups ()[ 0 ]) # Parse Levels portion elif line . startswith ( \"INSERT INTO `ASD_Levels` VALUES\" ): partitionedLine = line . partition ( \" VALUES \" )[ - 1 ] . strip () nullReplacedLine = partitionedLine . replace ( \"NULL\" , \"''\" ) formattedLine = nullReplacedLine if nullReplacedLine [ - 1 ] == \";\" : formattedLine = nullReplacedLine [: - 1 ] parseLine ( energyLevelsDict , fieldNamesDict , formattedLine ) # Sort levels within an element/charge state by energy outputDict : dict [ str , dict [ int , dict [ str , list [ float ]]]] = {} for iElement , element in energyLevelsDict . items (): for iCharge , chargestate in element . items (): energyOrder = np . argsort ( np . array ( list ( chargestate . values ()))[:, 0 ]) orderedKeys = np . array ( list ( chargestate . keys ()))[ energyOrder ] orderedValues = np . array ( list ( chargestate . values ()))[ energyOrder ] for i , iKey in enumerate ( list ( orderedKeys )): if iElement not in outputDict . keys (): outputDict [ iElement ] = {} if iCharge not in outputDict [ iElement ] . keys (): outputDict [ iElement ][ iCharge ] = {} outputDict [ iElement ][ iCharge ][ str ( iKey )] = orderedValues [ i ] . tolist () # Write dict to pickle file with open ( outputFilename , \"wb\" ) as handle : pickle . dump ( outputDict , handle , protocol = 2 ) nist_xray_database Download the NIST x-ray line database from the website, and parse the downloaded data into useable form. For loading a file (locally, from disk) and plotting some information: * NISTXrayDBFile * plot_line_uncertainties For updating the data files: * NISTXrayDBRetrieve * GetAllLines Basic usage (assuming you put the x-ray files in ${MASS_HOME}/mass2/calibration/nist_xray_data.dat): J. Fowler, NIST February 2014 NISTXrayDBFile A NIST X-ray database file, loaded from disk. Source code in mass2/calibration/nist_xray_database.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class NISTXrayDBFile : \"\"\"A NIST X-ray database file, loaded from disk.\"\"\" DEFAULT_FILENAMES = \"nist_xray_data.dat\" , \"low_z_xray_data.dat\" def __init__ ( self , * filenames : str ): \"\"\"Initialize the database from 1 or more <filenames>, which point to files downloaded using NISTXrayDBRetrieve. If the list is empty (the default), then the file named by self.DEFAULT_FILENAME will be used.\"\"\" self . lines = {} self . alllines = set () if not filenames : path = os . path . split ( __file__ )[ 0 ] filenames = tuple ([ os . path . join ( path , df ) for df in self . DEFAULT_FILENAMES ]) self . loaded_filenames = [] for filename in filenames : try : fp = open ( filename , \"r\" , encoding = \"utf-8\" ) except OSError : print ( f \"' { filename } ' is not a readable file with X-ray database info! Continuing...\" ) continue while True : line = fp . readline () if \"Theory\" in line and \"Blend\" in line and \"Ref.\" in line : break for textline in fp . readlines (): try : xrayline = NISTXrayLine ( textline ) self . lines [ xrayline . name ] = xrayline self . alllines . add ( xrayline ) except Exception : continue self . loaded_filenames . append ( filename ) fp . close () LINE_NICKNAMES = { \"KA1\" : \"KL3\" , \"KA2\" : \"KL2\" , \"KB1\" : \"KM3\" , \"KB3\" : \"KM2\" , \"KB5\" : \"KM5\" , \"LA1\" : \"L3M5\" , \"LA2\" : \"L3M4\" , \"Ll\" : \"L3M1\" , \"LB3\" : \"L1M3\" , \"LB1\" : \"L2M4\" , \"LB2\" : \"L3N5\" , \"LG1\" : \"L2N4\" , } def get_lines_by_type ( self , linetype : str ) -> tuple [ \"NISTXrayLine\" ]: \"\"\"Return a tuple containing all lines of a certain type, e.g., \"KL3\". See self.LINE_NICKNAMES for some known line \"nicknames\".\"\"\" linetype = linetype . upper () if \"ALPHA\" in linetype : linetype = linetype . replace ( \"ALPHA\" , \"A\" ) elif \"BETA\" in linetype : linetype = linetype . replace ( \"BETA\" , \"B\" ) elif \"GAMMA\" in linetype : linetype = linetype . replace ( \"GAMMA\" , \"G\" ) linetype = self . LINE_NICKNAMES . get ( linetype , linetype ) lines = [] for element in ELEMENTS : linename = f \" { element } { linetype } \" if linename in self . lines : lines . append ( self . lines [ linename ]) return tuple ( lines ) def __getitem__ ( self , key : str ) -> \"NISTXrayLine\" : \"\"\"Get a line by its full name, e.g., \"Fe KL3\", or by a nickname, e.g., \"Fe Kalpha1\". Parameters ---------- key : str The line name or nickname Returns ------- NISTXrayLine The matching NISTXrayLine object Raises ------ KeyError If not found \"\"\" element , line = key . split ()[: 2 ] element = element . capitalize () line = line . upper () key = f \" { element } { line } \" if key in self . lines : return self . lines [ key ] lcline = line . lower () lcline = lcline . replace ( \"alpha\" , \"a\" ) lcline = lcline . replace ( \"beta\" , \"b\" ) lcline = lcline . replace ( \"gamma\" , \"g\" ) if lcline in self . LINE_NICKNAMES : key = f \" { element } { self . LINE_NICKNAMES [ lcline ] } \" return self . lines [ key ] raise KeyError ( f \" { key } is not a known line or line nickname\" ) __getitem__ ( key ) Get a line by its full name, e.g., \"Fe KL3\", or by a nickname, e.g., \"Fe Kalpha1\". Parameters: key ( str ) \u2013 The line name or nickname Returns: NISTXrayLine \u2013 The matching NISTXrayLine object Raises: KeyError \u2013 If not found Source code in mass2/calibration/nist_xray_database.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def __getitem__ ( self , key : str ) -> \"NISTXrayLine\" : \"\"\"Get a line by its full name, e.g., \"Fe KL3\", or by a nickname, e.g., \"Fe Kalpha1\". Parameters ---------- key : str The line name or nickname Returns ------- NISTXrayLine The matching NISTXrayLine object Raises ------ KeyError If not found \"\"\" element , line = key . split ()[: 2 ] element = element . capitalize () line = line . upper () key = f \" { element } { line } \" if key in self . lines : return self . lines [ key ] lcline = line . lower () lcline = lcline . replace ( \"alpha\" , \"a\" ) lcline = lcline . replace ( \"beta\" , \"b\" ) lcline = lcline . replace ( \"gamma\" , \"g\" ) if lcline in self . LINE_NICKNAMES : key = f \" { element } { self . LINE_NICKNAMES [ lcline ] } \" return self . lines [ key ] raise KeyError ( f \" { key } is not a known line or line nickname\" ) __init__ ( * filenames ) Initialize the database from 1 or more , which point to files downloaded using NISTXrayDBRetrieve. If the list is empty (the default), then the file named by self.DEFAULT_FILENAME will be used. Source code in mass2/calibration/nist_xray_database.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def __init__ ( self , * filenames : str ): \"\"\"Initialize the database from 1 or more <filenames>, which point to files downloaded using NISTXrayDBRetrieve. If the list is empty (the default), then the file named by self.DEFAULT_FILENAME will be used.\"\"\" self . lines = {} self . alllines = set () if not filenames : path = os . path . split ( __file__ )[ 0 ] filenames = tuple ([ os . path . join ( path , df ) for df in self . DEFAULT_FILENAMES ]) self . loaded_filenames = [] for filename in filenames : try : fp = open ( filename , \"r\" , encoding = \"utf-8\" ) except OSError : print ( f \"' { filename } ' is not a readable file with X-ray database info! Continuing...\" ) continue while True : line = fp . readline () if \"Theory\" in line and \"Blend\" in line and \"Ref.\" in line : break for textline in fp . readlines (): try : xrayline = NISTXrayLine ( textline ) self . lines [ xrayline . name ] = xrayline self . alllines . add ( xrayline ) except Exception : continue self . loaded_filenames . append ( filename ) fp . close () get_lines_by_type ( linetype ) Return a tuple containing all lines of a certain type, e.g., \"KL3\". See self.LINE_NICKNAMES for some known line \"nicknames\". Source code in mass2/calibration/nist_xray_database.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def get_lines_by_type ( self , linetype : str ) -> tuple [ \"NISTXrayLine\" ]: \"\"\"Return a tuple containing all lines of a certain type, e.g., \"KL3\". See self.LINE_NICKNAMES for some known line \"nicknames\".\"\"\" linetype = linetype . upper () if \"ALPHA\" in linetype : linetype = linetype . replace ( \"ALPHA\" , \"A\" ) elif \"BETA\" in linetype : linetype = linetype . replace ( \"BETA\" , \"B\" ) elif \"GAMMA\" in linetype : linetype = linetype . replace ( \"GAMMA\" , \"G\" ) linetype = self . LINE_NICKNAMES . get ( linetype , linetype ) lines = [] for element in ELEMENTS : linename = f \" { element } { linetype } \" if linename in self . lines : lines . append ( self . lines [ linename ]) return tuple ( lines ) NISTXrayLine A single line from the NIST X-ray database. Source code in mass2/calibration/nist_xray_database.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class NISTXrayLine : \"\"\"A single line from the NIST X-ray database.\"\"\" DEFAULT_COLUMN_DEFS = { \"element\" : ( 1 , 4 ), \"transition\" : ( 10 , 16 ), \"peak\" : ( 45 , 59 ), \"peak_unc\" : ( 61 , 72 ), \"blend\" : ( 74 , 79 ), \"ref\" : ( 81 , 91 ), } def __init__ ( self , textline : str , column_defs : dict [ str , tuple [ int , int ]] | None = None ): \"\"\"Initialize a NISTXrayLine from a line of text found in the NIST x-ray database file. Parameters ---------- textline : str The text line from the database file column_defs : dict[str, tuple[int, int]] | None, optional The column boundaries of the relevant data, by default None \"\"\" self . element = \"\" self . transition = \"\" self . peak = 0.0 self . peak_unc = 0.0 self . blend = \"\" self . ref = \"\" if column_defs is None : column_defs = self . DEFAULT_COLUMN_DEFS for name , colrange in column_defs . items (): a = colrange [ 0 ] - 1 b = colrange [ 1 ] self . __dict__ [ name ] = textline [ a : b ] . rstrip () self . peak = float ( self . peak ) self . peak_unc = float ( self . peak_unc ) self . name = f \" { self . element } { self . transition } \" self . raw = textline . rstrip () def __str__ ( self ) -> str : \"\"\"The user-friendly string representation of the line\"\"\" return f \" { self . element } { self . transition } line: { self . peak : .3f } +- { self . peak_unc : .3f } eV\" def __repr__ ( self ) -> str : \"The code representation of the line\" return self . raw __init__ ( textline , column_defs = None ) Initialize a NISTXrayLine from a line of text found in the NIST x-ray database file. Parameters: textline ( str ) \u2013 The text line from the database file column_defs ( dict [ str , tuple [ int , int ]] | None , default: None ) \u2013 The column boundaries of the relevant data, by default None Source code in mass2/calibration/nist_xray_database.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def __init__ ( self , textline : str , column_defs : dict [ str , tuple [ int , int ]] | None = None ): \"\"\"Initialize a NISTXrayLine from a line of text found in the NIST x-ray database file. Parameters ---------- textline : str The text line from the database file column_defs : dict[str, tuple[int, int]] | None, optional The column boundaries of the relevant data, by default None \"\"\" self . element = \"\" self . transition = \"\" self . peak = 0.0 self . peak_unc = 0.0 self . blend = \"\" self . ref = \"\" if column_defs is None : column_defs = self . DEFAULT_COLUMN_DEFS for name , colrange in column_defs . items (): a = colrange [ 0 ] - 1 b = colrange [ 1 ] self . __dict__ [ name ] = textline [ a : b ] . rstrip () self . peak = float ( self . peak ) self . peak_unc = float ( self . peak_unc ) self . name = f \" { self . element } { self . transition } \" self . raw = textline . rstrip () __repr__ () The code representation of the line Source code in mass2/calibration/nist_xray_database.py 286 287 288 def __repr__ ( self ) -> str : \"The code representation of the line\" return self . raw __str__ () The user-friendly string representation of the line Source code in mass2/calibration/nist_xray_database.py 282 283 284 def __str__ ( self ) -> str : \"\"\"The user-friendly string representation of the line\"\"\" return f \" { self . element } { self . transition } line: { self . peak : .3f } +- { self . peak_unc : .3f } eV\" plot_line_energies () Plot the energies of some common families of lines from the NIST X-ray database. Source code in mass2/calibration/nist_xray_database.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def plot_line_energies () -> None : \"\"\"Plot the energies of some common families of lines from the NIST X-ray database.\"\"\" db = NISTXrayDBFile () plt . clf () cm = plt . cm . nipy_spectral transitions = ( \"KL2\" , \"KL3\" , \"KM5\" , \"KM3\" , \"KM2\" , \"L3M5\" , \"L3M4\" , \"L3M1\" , \"L2M4\" , \"L2N4\" , \"L3N5\" , \"L1M3\" , \"L3N7\" , \"M5N7\" , \"M5N6\" , \"M4N6\" , \"M3N5\" , \"M3N4\" , ) for i , linetype in enumerate ( transitions ): lines = db . get_lines_by_type ( linetype ) z = [ ATOMIC_NUMBERS [ line . element ] for line in lines ] e = [ line . peak for line in lines ] plt . loglog ( z , e , \"o-\" , color = cm ( float ( i ) / len ( transitions )), label = linetype ) plt . legend ( loc = \"upper left\" ) plt . xlim ([ 6 , 100 ]) plt . grid () r = list ( range ( 6 , 22 )) + list ( range ( 22 , 43 , 2 )) + list ( range ( 45 , 75 , 3 )) + list ( range ( 75 , 100 , 5 )) plt . xticks ( r , [ \" \\n \" . join ([ ELEMENTS [ i ], str ( i )]) for i in r ]) plot_line_uncertainties () Plot the uncertainties of some common families of lines from the NIST X-ray database. Source code in mass2/calibration/nist_xray_database.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def plot_line_uncertainties () -> None : \"\"\"Plot the uncertainties of some common families of lines from the NIST X-ray database.\"\"\" db = NISTXrayDBFile () transitions = ( \"KL3\" , \"KL2\" , \"KM3\" , \"KM5\" , \"L3M5\" , \"L3M4\" , \"L2M4\" , \"L3N5\" , \"L2N4\" , \"L1M3\" , \"L3N7\" , \"L3M1\" ) titles = { \"KL3\" : \"K$ \\\\ alpha_1$: Intense\" , \"KL2\" : \"K$ \\\\ alpha_2$: Intense, but not easily resolved\" , \"KM3\" : \"K$ \\\\ beta_1$: Intense\" , \"KM2\" : \"K$ \\\\ beta_3$: Intense, usually unresolvable\" , \"KM5\" : \"K$ \\\\ beta_5$: Weak line on high-E tail of K$ \\\\ beta_1$\" , \"L3M5\" : \"L$ \\\\ alpha_1$: Prominent\" , \"L3M4\" : \"L$ \\\\ alpha_2$: Small satellite\" , \"L2M4\" : \"L$ \\\\ beta_1$: Prominent\" , \"L3N5\" : \"L$ \\\\ beta_2$: Prominent\" , \"L2N4\" : \"K$ \\\\ gamma_1$: Weaker\" , \"L1M3\" : \"L$ \\\\ beta_3$: Weaker\" , \"L3N7\" : \"Lu: barely visible\" , \"L3M1\" : \"L$ \\\\ ell$: very weak\" , } axes = {} NX , NY = 3 , 4 plt . clf () for i , tr in enumerate ( transitions ): axes [ i ] = plt . subplot ( NY , NX , i + 1 ) plt . loglog () plt . grid ( True ) plt . title ( titles [ tr ]) if i >= NX * ( NY - 1 ): plt . xlabel ( \"Line energy (eV)\" ) if i % NX == 0 : plt . ylabel ( \"Line uncertainty (eV)\" ) plt . ylim ([ 1e-3 , 10 ]) plt . xlim ([ 100 , 3e4 ]) for line in db . lines . values (): if line . transition not in transitions : continue i = transitions . index ( line . transition ) plt . sca ( axes [ i ]) plt . plot ( line . peak , line . peak_unc , \"or\" ) plt . text ( line . peak , line . peak_unc , line . name ) Materials transmission Models for X-ray filter and detector efficiency. Filter dataclass Represent a single material layer in a FilterStack Source code in mass2/materials/efficiency_models.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 @dataclass ( frozen = True ) class Filter : \"\"\"Represent a single material layer in a FilterStack\"\"\" name : str material : NDArray atomic_number : NDArray density_g_per_cm3 : NDArray [ np . float64 ] thickness_cm : NDArray [ np . float64 ] fill_fraction : Variable = ufloat ( 1.0 , 1e-8 ) absorber : bool = False def get_efficiency ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Return the efficiency of this Filter at the given x-ray energies.\"\"\" optical_depth = np . vstack ([ xraydb . material_mu ( m , xray_energies_eV , density = d ) * t for ( m , d , t ) in zip ( self . material , self . density_g_per_cm3 , self . thickness_cm ) ]) individual_transmittance = unp . exp ( - optical_depth ) transmittance = np . prod ( individual_transmittance , axis = 0 ) if self . absorber : efficiency = ( 1.0 - transmittance ) * self . fill_fraction else : efficiency = ( transmittance * self . fill_fraction ) + ( 1.0 - self . fill_fraction ) if uncertain : return efficiency else : return unp . nominal_values ( efficiency ) def __repr__ ( self ) -> str : \"\"\"Return a string representation of the Filter object.\"\"\" s = f \" { type ( self ) } (\" for material , density , thick in zip ( self . material , self . density_g_per_cm3 , self . thickness_cm ): area_density = density * thick s += f \" { material } { area_density : .3g } g/cm^2, \" s += f \"fill_fraction= { self . fill_fraction : .3f } , absorber= { self . absorber } )\" return s @classmethod def newfilter ( cls , name : str , material : ArrayLike , area_density_g_per_cm2 : ArrayLike | None = None , thickness_nm : ArrayLike | None = None , density_g_per_cm3 : ArrayLike | None = None , fill_fraction : Variable = ufloat ( 1 , 1e-8 ), absorber : bool = False , ) -> \"Filter\" : \"\"\"Create a Filter from the given parameters, filling in defaults as needed.\"\"\" material = np . array ( material , ndmin = 1 ) atomic_number = np . array ([ xraydb . atomic_number ( iMaterial ) for iMaterial in material ], ndmin = 1 ) fill_fraction = ensure_uncertain ( fill_fraction ) # Save density, either default values for that element, or the given density. if density_g_per_cm3 is None : density_g_per_cm3 = np . array ([ xraydb . atomic_density ( int ( iAtomicNumber )) for iAtomicNumber in atomic_number ], ndmin = 1 ) else : density_g_per_cm3 = np . array ( density_g_per_cm3 , ndmin = 1 ) assert len ( material ) == len ( density_g_per_cm3 ) # Handle input value of areal density or thickness, but not both. assert np . logical_xor ( area_density_g_per_cm2 is None , thickness_nm is None ), ( \"must specify either areal density or thickness, not both\" ) if thickness_nm is not None : thickness_cm = np . array ( thickness_nm , ndmin = 1 ) * 1e-7 elif area_density_g_per_cm2 is not None : area_density_g_per_cm2 = np . array ( area_density_g_per_cm2 , ndmin = 1 ) thickness_cm = area_density_g_per_cm2 / density_g_per_cm3 if np . ndim == 0 : thickness_cm = np . array ( thickness_cm , ndmin = 1 ) else : raise ValueError ( \"must specify either areal density or thickness, not both\" ) thickness_cm = ensure_uncertain ( thickness_cm ) assert len ( thickness_cm ) >= 1 return cls ( name , material , atomic_number , density_g_per_cm3 , thickness_cm , fill_fraction , absorber ) __repr__ () Return a string representation of the Filter object. Source code in mass2/materials/efficiency_models.py 133 134 135 136 137 138 139 140 def __repr__ ( self ) -> str : \"\"\"Return a string representation of the Filter object.\"\"\" s = f \" { type ( self ) } (\" for material , density , thick in zip ( self . material , self . density_g_per_cm3 , self . thickness_cm ): area_density = density * thick s += f \" { material } { area_density : .3g } g/cm^2, \" s += f \"fill_fraction= { self . fill_fraction : .3f } , absorber= { self . absorber } )\" return s get_efficiency ( xray_energies_eV , uncertain = False ) Return the efficiency of this Filter at the given x-ray energies. Source code in mass2/materials/efficiency_models.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def get_efficiency ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Return the efficiency of this Filter at the given x-ray energies.\"\"\" optical_depth = np . vstack ([ xraydb . material_mu ( m , xray_energies_eV , density = d ) * t for ( m , d , t ) in zip ( self . material , self . density_g_per_cm3 , self . thickness_cm ) ]) individual_transmittance = unp . exp ( - optical_depth ) transmittance = np . prod ( individual_transmittance , axis = 0 ) if self . absorber : efficiency = ( 1.0 - transmittance ) * self . fill_fraction else : efficiency = ( transmittance * self . fill_fraction ) + ( 1.0 - self . fill_fraction ) if uncertain : return efficiency else : return unp . nominal_values ( efficiency ) newfilter ( name , material , area_density_g_per_cm2 = None , thickness_nm = None , density_g_per_cm3 = None , fill_fraction = ufloat ( 1 , 1e-08 ), absorber = False ) classmethod Create a Filter from the given parameters, filling in defaults as needed. Source code in mass2/materials/efficiency_models.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 @classmethod def newfilter ( cls , name : str , material : ArrayLike , area_density_g_per_cm2 : ArrayLike | None = None , thickness_nm : ArrayLike | None = None , density_g_per_cm3 : ArrayLike | None = None , fill_fraction : Variable = ufloat ( 1 , 1e-8 ), absorber : bool = False , ) -> \"Filter\" : \"\"\"Create a Filter from the given parameters, filling in defaults as needed.\"\"\" material = np . array ( material , ndmin = 1 ) atomic_number = np . array ([ xraydb . atomic_number ( iMaterial ) for iMaterial in material ], ndmin = 1 ) fill_fraction = ensure_uncertain ( fill_fraction ) # Save density, either default values for that element, or the given density. if density_g_per_cm3 is None : density_g_per_cm3 = np . array ([ xraydb . atomic_density ( int ( iAtomicNumber )) for iAtomicNumber in atomic_number ], ndmin = 1 ) else : density_g_per_cm3 = np . array ( density_g_per_cm3 , ndmin = 1 ) assert len ( material ) == len ( density_g_per_cm3 ) # Handle input value of areal density or thickness, but not both. assert np . logical_xor ( area_density_g_per_cm2 is None , thickness_nm is None ), ( \"must specify either areal density or thickness, not both\" ) if thickness_nm is not None : thickness_cm = np . array ( thickness_nm , ndmin = 1 ) * 1e-7 elif area_density_g_per_cm2 is not None : area_density_g_per_cm2 = np . array ( area_density_g_per_cm2 , ndmin = 1 ) thickness_cm = area_density_g_per_cm2 / density_g_per_cm3 if np . ndim == 0 : thickness_cm = np . array ( thickness_cm , ndmin = 1 ) else : raise ValueError ( \"must specify either areal density or thickness, not both\" ) thickness_cm = ensure_uncertain ( thickness_cm ) assert len ( thickness_cm ) >= 1 return cls ( name , material , atomic_number , density_g_per_cm3 , thickness_cm , fill_fraction , absorber ) FilterStack dataclass Represent a sequence of named materials Source code in mass2/materials/efficiency_models.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @dataclass () class FilterStack : \"\"\"Represent a sequence of named materials\"\"\" name : str components : list [ \"Filter | FilterStack\" ] = field ( default_factory = list ) def add ( self , film : \"Filter | FilterStack\" ) -> None : \"\"\"Add a Filter or FilterStack to this FilterStack.\"\"\" self . components . append ( film ) def add_filter ( self , name : str , material : str , area_density_g_per_cm2 : float | None = None , thickness_nm : float | None = None , density_g_per_cm3 : float | None = None , fill_fraction : Variable = ufloat ( 1 , 1e-8 ), absorber : bool = False , ) -> None : \"\"\"Create and add a Filter layer to this FilterStack.\"\"\" self . add ( Filter . newfilter ( name , material , area_density_g_per_cm2 = area_density_g_per_cm2 , thickness_nm = thickness_nm , density_g_per_cm3 = density_g_per_cm3 , fill_fraction = fill_fraction , absorber = absorber , ) ) def get_efficiency ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Return the overall efficiency of this FilterStack at the given x-ray energies.\"\"\" assert len ( self . components ) > 0 , f \" { self . name } has no components of which to calculate efficiency\" individual_efficiency = np . array ([ iComponent . get_efficiency ( xray_energies_eV , uncertain = uncertain ) for iComponent in self . components ]) efficiency = np . prod ( individual_efficiency , axis = 0 ) if uncertain : return efficiency else : return unp . nominal_values ( efficiency ) def __call__ ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Equivalent to get_efficiency.\"\"\" return self . get_efficiency ( xray_energies_eV , uncertain = uncertain ) def plot_efficiency ( self , xray_energies_eV : ArrayLike , ax : plt . Axes | None = None ) -> None : \"\"\"Plot the efficiency of this FilterStack and its components.\"\"\" efficiency = unp . nominal_values ( self . get_efficiency ( xray_energies_eV )) if ax is None : fig = plt . figure () ax = fig . add_subplot ( 111 ) ax . plot ( xray_energies_eV , efficiency * 100.0 , label = \"total\" , lw = 2 ) ax . set_xlabel ( \"Energy (keV)\" ) ax . set_ylabel ( \"Efficiency (%)\" ) ax . set_title ( self . name ) ax . set_title ( f \" { self . name } Efficiency\" ) for v in self . components : efficiency = v . get_efficiency ( xray_energies_eV ) ax . plot ( xray_energies_eV , efficiency * 100.0 , \"--\" , label = v . name ) ax . legend () def __repr__ ( self ) -> str : \"\"\"Return a string representation of the FilterStack object.\"\"\" s = f \" { type ( self ) } ( \\n \" for v in self . components : s += f \" { v . name } : { v } \\n \" s += \")\" return s __call__ ( xray_energies_eV , uncertain = False ) Equivalent to get_efficiency. Source code in mass2/materials/efficiency_models.py 72 73 74 def __call__ ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Equivalent to get_efficiency.\"\"\" return self . get_efficiency ( xray_energies_eV , uncertain = uncertain ) __repr__ () Return a string representation of the FilterStack object. Source code in mass2/materials/efficiency_models.py 95 96 97 98 99 100 101 def __repr__ ( self ) -> str : \"\"\"Return a string representation of the FilterStack object.\"\"\" s = f \" { type ( self ) } ( \\n \" for v in self . components : s += f \" { v . name } : { v } \\n \" s += \")\" return s add ( film ) Add a Filter or FilterStack to this FilterStack. Source code in mass2/materials/efficiency_models.py 33 34 35 def add ( self , film : \"Filter | FilterStack\" ) -> None : \"\"\"Add a Filter or FilterStack to this FilterStack.\"\"\" self . components . append ( film ) add_filter ( name , material , area_density_g_per_cm2 = None , thickness_nm = None , density_g_per_cm3 = None , fill_fraction = ufloat ( 1 , 1e-08 ), absorber = False ) Create and add a Filter layer to this FilterStack. Source code in mass2/materials/efficiency_models.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def add_filter ( self , name : str , material : str , area_density_g_per_cm2 : float | None = None , thickness_nm : float | None = None , density_g_per_cm3 : float | None = None , fill_fraction : Variable = ufloat ( 1 , 1e-8 ), absorber : bool = False , ) -> None : \"\"\"Create and add a Filter layer to this FilterStack.\"\"\" self . add ( Filter . newfilter ( name , material , area_density_g_per_cm2 = area_density_g_per_cm2 , thickness_nm = thickness_nm , density_g_per_cm3 = density_g_per_cm3 , fill_fraction = fill_fraction , absorber = absorber , ) ) get_efficiency ( xray_energies_eV , uncertain = False ) Return the overall efficiency of this FilterStack at the given x-ray energies. Source code in mass2/materials/efficiency_models.py 60 61 62 63 64 65 66 67 68 69 70 def get_efficiency ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Return the overall efficiency of this FilterStack at the given x-ray energies.\"\"\" assert len ( self . components ) > 0 , f \" { self . name } has no components of which to calculate efficiency\" individual_efficiency = np . array ([ iComponent . get_efficiency ( xray_energies_eV , uncertain = uncertain ) for iComponent in self . components ]) efficiency = np . prod ( individual_efficiency , axis = 0 ) if uncertain : return efficiency else : return unp . nominal_values ( efficiency ) plot_efficiency ( xray_energies_eV , ax = None ) Plot the efficiency of this FilterStack and its components. Source code in mass2/materials/efficiency_models.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def plot_efficiency ( self , xray_energies_eV : ArrayLike , ax : plt . Axes | None = None ) -> None : \"\"\"Plot the efficiency of this FilterStack and its components.\"\"\" efficiency = unp . nominal_values ( self . get_efficiency ( xray_energies_eV )) if ax is None : fig = plt . figure () ax = fig . add_subplot ( 111 ) ax . plot ( xray_energies_eV , efficiency * 100.0 , label = \"total\" , lw = 2 ) ax . set_xlabel ( \"Energy (keV)\" ) ax . set_ylabel ( \"Efficiency (%)\" ) ax . set_title ( self . name ) ax . set_title ( f \" { self . name } Efficiency\" ) for v in self . components : efficiency = v . get_efficiency ( xray_energies_eV ) ax . plot ( xray_energies_eV , efficiency * 100.0 , \"--\" , label = v . name ) ax . legend () AlFilmWithOxide ( name , Al_thickness_nm , Al_density_g_per_cm3 = None , num_oxidized_surfaces = 2 , oxide_density_g_per_cm3 = None ) Create a Filter made of an alumninum film with oxides on one or both surfaces Args: name: name given to filter object, e.g. '50K Filter'. Al_thickness_nm: thickness, in nm, of Al film Al_density_g_per_cm3: Al film density, in g/cm3, defaults to xraydb value num_oxidized_surfaces: Number of film surfaces that contain a native oxide, default 2 oxide_density_g_per_cm3: Al2O3 oxide density, in g/cm3, defaults to bulk xraydb value Source code in mass2/materials/efficiency_models.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def AlFilmWithOxide ( name : str , Al_thickness_nm : float , Al_density_g_per_cm3 : float | None = None , num_oxidized_surfaces : int = 2 , oxide_density_g_per_cm3 : ArrayLike | None = None , ) -> Filter : \"\"\"Create a Filter made of an alumninum film with oxides on one or both surfaces Args: name: name given to filter object, e.g. '50K Filter'. Al_thickness_nm: thickness, in nm, of Al film Al_density_g_per_cm3: Al film density, in g/cm3, defaults to xraydb value num_oxidized_surfaces: Number of film surfaces that contain a native oxide, default 2 oxide_density_g_per_cm3: Al2O3 oxide density, in g/cm3, defaults to bulk xraydb value \"\"\" assert num_oxidized_surfaces in { 1 , 2 }, \"only 1 or 2 oxidzed surfaces allowed\" if Al_density_g_per_cm3 is None : Al_density_g_per_cm3 = float ( xraydb . atomic_density ( \"Al\" )) arbE = 5000.0 # an arbitrary energy (5 keV) is used to get answers from material_mu_components() oxide_dict = xraydb . material_mu_components ( \"sapphire\" , arbE ) oxide_material = oxide_dict [ \"elements\" ] oxide_mass_fractions = [ oxide_dict [ x ][ 0 ] * oxide_dict [ x ][ 1 ] / oxide_dict [ \"mass\" ] for x in oxide_material ] # Assume oxidized surfaces are each 3 nm thick. num_oxide_elements = len ( oxide_material ) oxide_thickness_nm = np . repeat ( num_oxidized_surfaces * 3.0 , num_oxide_elements ) if oxide_density_g_per_cm3 is None : oxide_density_g_per_cm3 = np . repeat ( oxide_dict [ \"density\" ], num_oxide_elements ) else : oxide_density_g_per_cm3 = np . asarray ( oxide_density_g_per_cm3 ) material = np . hstack ([ \"Al\" , oxide_material ]) density_g_per_cm3 = np . hstack ([ Al_density_g_per_cm3 , oxide_density_g_per_cm3 * oxide_mass_fractions ]) thickness_nm = np . hstack ([ Al_thickness_nm , oxide_thickness_nm ]) return Filter . newfilter ( name , material , thickness_nm = thickness_nm , density_g_per_cm3 = density_g_per_cm3 ) AlFilmWithPolymer ( name , Al_thickness_nm , polymer_thickness_nm , Al_density_g_per_cm3 = None , num_oxidized_surfaces = 1 , oxide_density_g_per_cm3 = None , polymer_density_g_per_cm3 = None ) Create a Filter made of an alumninum film with polymer backing Args: name: name given to filter object, e.g. '50K Filter'. Al_thickness_nm: thickness, in nm, of Al film polymer_thickness_nm: thickness, in nm, of filter backside polymer Al_density_g_per_cm3: Al film density, in g/cm3, defaults to xraydb value num_oxidized_surfaces: Number of film surfaces that contain a native oxide, default 2 oxide_density_g_per_cm3: Al2O3 oxide density, in g/cm3, defaults to bulk xraydb value polymer_density_g_per_cm3: Polymer density, in g/cm3, defaults to Kapton Source code in mass2/materials/efficiency_models.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def AlFilmWithPolymer ( name : str , Al_thickness_nm : float , polymer_thickness_nm : float , Al_density_g_per_cm3 : float | None = None , num_oxidized_surfaces : int = 1 , oxide_density_g_per_cm3 : float | None = None , polymer_density_g_per_cm3 : float | None = None , ) -> Filter : \"\"\"Create a Filter made of an alumninum film with polymer backing Args: name: name given to filter object, e.g. '50K Filter'. Al_thickness_nm: thickness, in nm, of Al film polymer_thickness_nm: thickness, in nm, of filter backside polymer Al_density_g_per_cm3: Al film density, in g/cm3, defaults to xraydb value num_oxidized_surfaces: Number of film surfaces that contain a native oxide, default 2 oxide_density_g_per_cm3: Al2O3 oxide density, in g/cm3, defaults to bulk xraydb value polymer_density_g_per_cm3: Polymer density, in g/cm3, defaults to Kapton \"\"\" assert num_oxidized_surfaces in { 1 , 2 }, \"only 1 or 2 oxidzed surfaces allowed\" if Al_density_g_per_cm3 is None : Al_density_g_per_cm3 = xraydb . atomic_density ( \"Al\" ) arbE = 5000.0 # an arbitrary energy (5 keV) is used to get answers from material_mu_components() oxide_dict = xraydb . material_mu_components ( \"sapphire\" , arbE ) oxide_thickness_nm = num_oxidized_surfaces * 3.0 # assume 3 nm per oxidized surface oxide_material = oxide_dict [ \"elements\" ] oxide_mass_fractions = np . array ([ oxide_dict [ x ][ 0 ] * oxide_dict [ x ][ 1 ] / oxide_dict [ \"mass\" ] for x in oxide_material ]) if oxide_density_g_per_cm3 is None : oxide_density_g_per_cm3 = oxide_dict [ \"density\" ] * np . ones ( len ( oxide_material )) polymer_dict = xraydb . material_mu_components ( \"kapton\" , arbE ) polymer_material = polymer_dict [ \"elements\" ] polymer_thickness_nm_array = np . ones ( len ( polymer_material )) * polymer_thickness_nm polymer_mass_fractions = np . array ([ polymer_dict [ x ][ 0 ] * polymer_dict [ x ][ 1 ] / polymer_dict [ \"mass\" ] for x in polymer_material ]) if polymer_density_g_per_cm3 is None : polymer_density_g_per_cm3 = polymer_dict [ \"density\" ] * np . ones ( len ( polymer_material )) material = np . hstack ([ \"Al\" , oxide_material , polymer_material ]) density_g_per_cm3 = np . hstack ([ [ Al_density_g_per_cm3 ], oxide_density_g_per_cm3 * oxide_mass_fractions , polymer_density_g_per_cm3 * polymer_mass_fractions , ]) thickness_nm = np . hstack ([ Al_thickness_nm , oxide_thickness_nm , polymer_thickness_nm_array ]) return Filter . newfilter ( name = name , material = material , thickness_nm = thickness_nm , density_g_per_cm3 = density_g_per_cm3 ) LEX_HT ( name ) Create an Al film with polymer and stainless steel backing. Models the LEX-HT vacuum window. Args: name: name given to filter object, e.g. '50K Filter'. Source code in mass2/materials/efficiency_models.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def LEX_HT ( name : str ) -> FilterStack : \"\"\"Create an Al film with polymer and stainless steel backing. Models the LEX-HT vacuum window. Args: name: name given to filter object, e.g. '50K Filter'. \"\"\" # Set up Al + polyimide film film_material = [ \"C\" , \"H\" , \"N\" , \"O\" , \"Al\" ] film_area_density_g_per_cm2_given = np . array ([ 6.7e-5 , 2.6e-6 , 7.2e-6 , 1.7e-5 , 1.7e-5 ]) film_area_density_g_per_cm2 = with_fractional_uncertainty ( film_area_density_g_per_cm2_given , 0.03 ) film1 = Filter . newfilter ( name = \"LEX_HT Film\" , material = film_material , area_density_g_per_cm2 = film_area_density_g_per_cm2 ) # Set up mesh mesh_material = [ \"Fe\" , \"Cr\" , \"Ni\" , \"Mn\" , \"Si\" ] mesh_thickness = 100.0e-4 # cm mesh_density = 8.0 # g/cm^3 mesh_material_fractions = np . array ([ 0.705 , 0.19 , 0.09 , 0.01 , 0.005 ]) # fraction by weight mesh_area_density_g_per_cm2_scalar = mesh_material_fractions * mesh_density * mesh_thickness # g/cm^2 mesh_area_density_g_per_cm2 = with_fractional_uncertainty ( mesh_area_density_g_per_cm2_scalar , 0.02 ) mesh_fill_fraction = ufloat ( 0.19 , 0.01 ) film2 = Filter . newfilter ( name = \"LEX_HT Mesh\" , material = mesh_material , area_density_g_per_cm2 = mesh_area_density_g_per_cm2 , fill_fraction = mesh_fill_fraction , ) stack = FilterStack ( name ) stack . add ( film1 ) stack . add ( film2 ) return stack get_filter_stacks_dict () Create a dictionary with a few examples of FilterStack objects Returns: dict \u2013 A dictionary of named FilterStacks Source code in mass2/materials/efficiency_models.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 def get_filter_stacks_dict () -> dict [ str , FilterStack ]: \"\"\"Create a dictionary with a few examples of FilterStack objects Returns ------- dict A dictionary of named FilterStacks \"\"\" fs_dict : dict [ str , FilterStack ] = {} # EBIT Instrument EBIT_filter_stack = FilterStack ( name = \"EBIT 2018\" ) EBIT_filter_stack . add_filter ( name = \"Electroplated Au Absorber\" , material = \"Au\" , thickness_nm = with_fractional_uncertainty ( 965.5 , 0.03 ), absorber = True ) EBIT_filter_stack . add ( AlFilmWithOxide ( name = \"50mK Filter\" , Al_thickness_nm = with_fractional_uncertainty ( 112.5 , 0.02 ))) EBIT_filter_stack . add ( AlFilmWithOxide ( name = \"3K Filter\" , Al_thickness_nm = with_fractional_uncertainty ( 108.5 , 0.02 ))) filter_50K = FilterStack ( name = \"50K Filter\" ) filter_50K . add ( AlFilmWithOxide ( name = \"Al Film\" , Al_thickness_nm = with_fractional_uncertainty ( 102.6 , 0.02 ))) nickel = Filter . newfilter ( name = \"Ni Mesh\" , material = \"Ni\" , thickness_nm = ufloat ( 15.0e3 , 2e3 ), fill_fraction = ufloat ( 0.17 , 0.01 )) filter_50K . add ( nickel ) EBIT_filter_stack . add ( filter_50K ) luxel1 = LEX_HT ( \"Luxel Window TES\" ) luxel2 = LEX_HT ( \"Luxel Window EBIT\" ) EBIT_filter_stack . add ( luxel1 ) EBIT_filter_stack . add ( luxel2 ) fs_dict [ EBIT_filter_stack . name ] = EBIT_filter_stack # RAVEN Instrument RAVEN1_fs = FilterStack ( name = \"RAVEN1 2019\" ) RAVEN1_fs . add_filter ( name = \"Evaporated Bi Absorber\" , material = \"Bi\" , thickness_nm = 4.4e3 , absorber = True ) RAVEN1_fs . add ( AlFilmWithPolymer ( name = \"50mK Filter\" , Al_thickness_nm = 108.4 , polymer_thickness_nm = 206.4 )) RAVEN1_fs . add ( AlFilmWithPolymer ( name = \"3K Filter\" , Al_thickness_nm = 108.4 , polymer_thickness_nm = 206.4 )) RAVEN1_fs . add ( AlFilmWithOxide ( name = \"50K Filter\" , Al_thickness_nm = 1.0e3 )) RAVEN1_fs . add_filter ( name = \"Be TES Vacuum Window\" , material = \"Be\" , thickness_nm = 200.0e3 ) RAVEN1_fs . add ( AlFilmWithOxide ( name = \"e- Filter\" , Al_thickness_nm = 5.0e3 )) RAVEN1_fs . add_filter ( name = \"Be SEM Vacuum Window\" , material = \"Be\" , thickness_nm = 200.0e3 ) fs_dict [ RAVEN1_fs . name ] = RAVEN1_fs # Horton spring 2018, for metrology campaign. Horton_filter_stack = FilterStack ( name = \"Horton 2018\" ) Horton_filter_stack . add_filter ( name = \"Electroplated Au Absorber\" , material = \"Au\" , thickness_nm = 965.5 , absorber = True ) Horton_filter_stack . add ( AlFilmWithOxide ( name = \"50mK Filter\" , Al_thickness_nm = 5000 )) Horton_filter_stack . add ( AlFilmWithOxide ( name = \"3K Filter\" , Al_thickness_nm = 5000 )) Horton_filter_stack . add ( AlFilmWithOxide ( name = \"50K Filter\" , Al_thickness_nm = 12700 )) Horton_filter_stack . add ( LEX_HT ( \"Luxel Window TES\" )) fs_dict [ Horton_filter_stack . name ] = Horton_filter_stack return fs_dict Math and statistics functions These live in mass2.mathstat . Entropy entropy.py Estimates of the distribution entropy computed using kernel-density estimates of the distribution. laplace_entropy(x, w=1.0) - Compute the entropy H(p) of data set x where the kernel used to estimate p from x is the Laplace kernel k(x) \\propto exp(-abs(x-x0)/w). laplace_cross_entropy(x, y, w=1.0) - Compute the cross entropy of q from p, where q and p are the kernel-density estimates taken from data set y and data set x , and where the kernel is the Laplace kernel. KL_divergence(x, y, w=1.0) - Compute the Kullback-Leibler Divergence of data set y from x , where the kernel is the Laplace kernel. The K-L divergence of Q(x) from P(x) is defined as the integral over the full x domain of P(x) log[P(x)/Q(x)]. This equals the cross-entropy H(P,Q) - H(P). Note that cross-entropy and K-L divergence are not symmetric with respect to reversal of x and y . laplace_KL_divergence ( x , y , w = 1.0 , approx_mode = 'size' ) Compute the Kullback-Leibler divergence of data set y from data set x . Use kernel-density estimation, where the kernel is the Laplace kernel k(x) \\propto exp(-abs(x-x0)/w). The approx_mode can be one of: exact The exact integral is computed (can take ~0.25 sec per 10^6 values). approx The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. size Uses \"approx\" if len(x)+len(y)>200000, or \"exact\" otherwise. Source code in mass2/mathstat/entropy.py 254 255 256 257 258 259 260 261 262 263 264 265 266 def laplace_KL_divergence ( x : ArrayLike , y : ArrayLike , w : float = 1.0 , approx_mode : str = \"size\" ) -> float : r \"\"\"Compute the Kullback-Leibler divergence of data set `y` from data set `x`. Use kernel-density estimation, where the kernel is the Laplace kernel k(x) \\propto exp(-abs(x-x0)/w). The `approx_mode` can be one of: ``exact`` The exact integral is computed (can take ~0.25 sec per 10^6 values). ``approx`` The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. ``size`` Uses \"approx\" if len(x)+len(y)>200000, or \"exact\" otherwise. \"\"\" return laplace_cross_entropy ( x , y , w , approx_mode = approx_mode ) - laplace_entropy ( x , w , approx_mode = approx_mode ) laplace_cross_entropy ( x , y , w = 1.0 , approx_mode = 'size' ) laplace_cross_entropy(x, y, w: float = 1.0, approx_mode=\"size\") Compute the cross-entropy of data set x from data set y , where the kernel for x is the Laplace kernel k(x) \\propto exp(-abs(x-x0)/w). The kernel for the y data is the piecewise-constant (top-hat) kernel. We choose this because a Laplace kernel for y led to possible divergences when the y-distribtion q is exceedingly small, but the x-distribution p nevertheless is non-zero because of a random x-value lying far from any random y-values. The constant kernel is given a non-zero floor value, so that q is never so small as to make any x-value impossible. Args: x (array): One vector of data. y (array): The other vector of data. w (double): The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation. approx_mode (string): How to balance execution speed and accuracy (default \"size\"). The approx_mode can be one of: exact The exact integral is computed (can take ~0.25 sec per 10^6 values). approx The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. size Uses \"approx\" if len(x)+len(y)>200000, or \"exact\" otherwise. Source code in mass2/mathstat/entropy.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def laplace_cross_entropy ( x : ArrayLike , y : ArrayLike , w : float = 1.0 , approx_mode : str = \"size\" ) -> float : r \"\"\"`laplace_cross_entropy(x, y, w: float = 1.0, approx_mode=\"size\")` Compute the cross-entropy of data set `x` from data set `y`, where the kernel for x is the Laplace kernel k(x) \\propto exp(-abs(x-x0)/w). The kernel for the y data is the piecewise-constant (top-hat) kernel. We choose this because a Laplace kernel for y led to possible divergences when the y-distribtion q is exceedingly small, but the x-distribution p nevertheless is non-zero because of a random x-value lying far from any random y-values. The constant kernel is given a non-zero floor value, so that q is never so small as to make any x-value impossible. Args: x (array): One vector of data. y (array): The other vector of data. w (double): The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation. approx_mode (string): How to balance execution speed and accuracy (default \"size\"). The `approx_mode` can be one of: ``exact`` The exact integral is computed (can take ~0.25 sec per 10^6 values). ``approx`` The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. ``size`` Uses \"approx\" if len(x)+len(y)>200000, or \"exact\" otherwise. \"\"\" if w <= 0.0 : raise ValueError ( \"laplace_cross_entropy(x, y, w) needs `w>0`.\" ) x = np . asarray ( x ) y = np . asarray ( y ) Nx = len ( x ) Ny = len ( y ) if Nx == 0 or Ny == 0 : raise ValueError ( \"laplace_cross_entropy(x, y) needs at least 1 element apiece in `x` and `y`.\" ) if approx_mode == \"size\" : if Nx + Ny <= 200000 : approx_mode = \"exact\" else : approx_mode = \"approx\" if approx_mode . startswith ( \"exact\" ): xsorted = np . asarray ( np . sort ( x ) / w , dtype = DTYPE ) ysorted = np . asarray ( np . sort ( y ) / w , dtype = DTYPE ) return laplace_cross_entropy_arrays ( xsorted , ysorted ) + np . log ( w ) else : return laplace_cross_entropy_approx ( np . asarray ( x , dtype = DTYPE ), np . asarray ( y , dtype = DTYPE ), w ) laplace_cross_entropy_approx ( x , y , w = 1.0 ) Approximate the cross-entropy H(P, Q) between two empirical distributions P and Q, where P is estimated from data x and Q from data y using Laplace kernel-density estimation and binned histograms. This method uses histograms and convolution with a Laplace kernel to estimate the probability distributions, then computes the cross-entropy using numerical integration. Parameters: x ( ArrayLike ) \u2013 Data points for the P distribution. y ( ArrayLike ) \u2013 Data points for the Q distribution. w ( float , default: 1.0 ) \u2013 The width (exponential scale length) of the Laplace kernel, by default 1.0. Returns: float \u2013 The approximate cross-entropy H(P, Q) between the two distributions. Notes This is an approximate method, suitable for large data sets. The Laplace kernel is defined as k(x) \u221d exp(-abs(x-x0)/w). Uses Simpson's rule for numerical integration. Source code in mass2/mathstat/entropy.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def laplace_cross_entropy_approx ( x : ArrayLike , y : ArrayLike , w : float = 1.0 ) -> float : \"\"\" Approximate the cross-entropy H(P, Q) between two empirical distributions P and Q, where P is estimated from data `x` and Q from data `y` using Laplace kernel-density estimation and binned histograms. This method uses histograms and convolution with a Laplace kernel to estimate the probability distributions, then computes the cross-entropy using numerical integration. Parameters ---------- x : ArrayLike Data points for the P distribution. y : ArrayLike Data points for the Q distribution. w : float, optional The width (exponential scale length) of the Laplace kernel, by default 1.0. Returns ------- float The approximate cross-entropy H(P, Q) between the two distributions. Notes ----- - This is an approximate method, suitable for large data sets. - The Laplace kernel is defined as k(x) \u221d exp(-abs(x-x0)/w). - Uses Simpson's rule for numerical integration. \"\"\" EXTEND_DATA = 5 * w BINS_PER_W = 20 KERNEL_WIDTH_IN_WS = 15.0 xmin = min ( np . min ( x ), np . min ( y )) - EXTEND_DATA xmax = max ( np . max ( x ), np . max ( y )) + EXTEND_DATA nbins = int ( 0.5 + ( xmax - xmin ) * BINS_PER_W / w ) cx , b = np . histogram ( x , nbins , ( xmin , xmax )) cy , b = np . histogram ( y , nbins , ( xmin , xmax )) db = b [ 1 ] - b [ 0 ] nx = int ( 0.5 + KERNEL_WIDTH_IN_WS * w / db ) kernel = np . zeros ( 2 * nx + 1 ) for i in range ( 2 * nx + 1 ): kx = ( i - nx ) * db kernel [ i ] = np . exp ( - abs ( kx / w )) # kde = unnormalized kernel-density estimator. kde = sp . signal . fftconvolve ( cx , kernel , mode = \"full\" )[ nx : - nx ] kde [ kde < kernel . min ()] = kernel . min () # p = normalized probability distribution. norm = 1.0 / sp . integrate . simpson ( kde , dx = db ) p = kde * norm kde = sp . signal . fftconvolve ( cy , kernel , mode = \"full\" )[ nx : - nx ] kde [ kde < kernel . min ()] = kernel . min () norm = 1.0 / sp . integrate . simpson ( kde , dx = db ) q = kde * norm return - sp . integrate . simpson ( p * np . log ( q ), dx = db ) laplace_cross_entropy_arrays ( x , y ) Compute the cross-entropy H(P, Q) between two empirical distributions P and Q, where P is estimated from data x using a Laplace kernel, and Q is estimated from data y using a piecewise-constant (top-hat) kernel. This function assumes both x and y are sorted and scaled by the kernel width. The cross-entropy is computed exactly by integrating over all points where the estimated densities change due to the presence of data points in x or y . Parameters: x ( ArrayLike ) \u2013 Sorted array of data points for the P distribution, scaled by kernel width. y ( ArrayLike ) \u2013 Sorted array of data points for the Q distribution, scaled by kernel width. Returns: float \u2013 The exact cross-entropy H(P, Q) between the two distributions. Notes The Laplace kernel is defined as k(x) \u221d exp(-abs(x-x0)/w). The Q distribution uses a top-hat kernel with a nonzero floor to avoid divergences. This function is intended for internal use; see laplace_cross_entropy for the public API. Source code in mass2/mathstat/entropy.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def laplace_cross_entropy_arrays ( x : ArrayLike , y : ArrayLike ) -> float : # noqa: PLR0914 \"\"\"Compute the cross-entropy H(P, Q) between two empirical distributions P and Q, where P is estimated from data `x` using a Laplace kernel, and Q is estimated from data `y` using a piecewise-constant (top-hat) kernel. This function assumes both `x` and `y` are sorted and scaled by the kernel width. The cross-entropy is computed exactly by integrating over all points where the estimated densities change due to the presence of data points in `x` or `y`. Parameters ---------- x : ArrayLike Sorted array of data points for the P distribution, scaled by kernel width. y : ArrayLike Sorted array of data points for the Q distribution, scaled by kernel width. Returns ------- float The exact cross-entropy H(P, Q) between the two distributions. Notes ----- - The Laplace kernel is defined as k(x) \u221d exp(-abs(x-x0)/w). - The Q distribution uses a top-hat kernel with a nonzero floor to avoid divergences. - This function is intended for internal use; see `laplace_cross_entropy` for the public API. \"\"\" # List of all places where q(u) increases or decreases because of a y-point. Qstepwidth = 2 * np . sqrt ( 6 ) ynodes , qstep_is_up = _merge_orderedlists ( y - 0.5 * Qstepwidth , y + 0.5 * Qstepwidth ) # List of all places where p(u) or q(u) changes because of an x- a y-point. nodes , isx = _merge_orderedlists ( x , ynodes ) x = np . asarray ( x ) y = np . asarray ( y ) Nx = len ( x ) Ny = len ( y ) N = Nx + Ny * 2 # Pretend q(u) is never lower than this value, and spread this probability across # the range 10 less than the lowest to 10 more than the highest node. Qmin_sum = 1.0 / np . sqrt ( Ny + 3 ) Qmin = Qmin_sum / ( nodes [ - 1 ] + 10 - ( nodes [ 0 ] - 10 )) Qstep = ( 1.0 - Qmin_sum ) / ( Ny * Qstepwidth ) # Initialize the vectors decayfactor, c, and d. decayfactor = np . zeros ( N , dtype = DTYPE ) for i in range ( 1 , N ): decayfactor [ i ] = np . exp ( nodes [ i - 1 ] - nodes [ i ]) # c requires a left-right pass over all nodes. c = np . zeros ( N , dtype = DTYPE ) stepX = 1.0 / ( 2 * Nx ) j = 0 if isx [ 0 ]: c [ 0 ] = stepX else : j = 1 for i in range ( 1 , N ): factor = decayfactor [ i ] c [ i ] = factor * c [ i - 1 ] if isx [ i ]: c [ i ] += stepX # d requires a right-left pass over all nodes. d = np . zeros ( N , dtype = DTYPE ) if isx [ N - 1 ]: d [ N - 1 ] = stepX for i in range ( N - 2 , - 1 , - 1 ): factor = decayfactor [ i + 1 ] d [ i ] = factor * d [ i + 1 ] if isx [ i ]: d [ i ] += stepX # Now a left-right pass over all nodes to compute the H integral. net_up_qsteps = 0 if not isx [ 0 ]: net_up_qsteps = 1 H = - d [ 0 ] * np . log ( Qmin ) # H due to the open first interval [-inf, nodes[0]] for i in range ( 1 , N ): factor = decayfactor [ i ] q = Qmin + Qstep * net_up_qsteps H -= ( c [ i - 1 ] + d [ i ]) * ( 1 - factor ) * np . log ( q ) if not isx [ i ]: if qstep_is_up [ j ]: net_up_qsteps += 1 else : net_up_qsteps -= 1 j += 1 H -= c [ - 1 ] * np . log ( Qmin ) # H due to the open last interval [nodes[-1], +inf] return H laplace_entropy ( x_in , w = 1.0 , approx_mode = 'size' ) Compute the entropy of data set x where the kernel is the Laplace kernel, $k(x) \\propto$ exp(-abs(x-x0)/w). Parameters: x_in ( ArrayLike ) \u2013 The vector of data of which we want the entropy. w ( float , default: 1.0 ) \u2013 The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 approx_mode ( str , default: 'size' ) \u2013 How to balance execution speed and accuracy, by default \"size\" The approx_mode can be one of: exact The exact integral is computed (can take ~0.25 sec per 10^6 values). approx The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. size Uses \"approx\" if len(x)>200000, or \"exact\" otherwise. Returns: float \u2013 The Laplace-kernel entropy. Raises: ValueError \u2013 If the input array x has no values, or w is not positive. Source code in mass2/mathstat/entropy.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def laplace_entropy ( x_in : ArrayLike , w : float = 1.0 , approx_mode : str = \"size\" ) -> float : \"\"\"Compute the entropy of data set `x` where the kernel is the Laplace kernel, $k(x) \\\\propto$ exp(-abs(x-x0)/w). Parameters ---------- x_in : ArrayLike The vector of data of which we want the entropy. w : float, optional The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 approx_mode : str, optional How to balance execution speed and accuracy, by default \"size\" The `approx_mode` can be one of: ``exact`` The exact integral is computed (can take ~0.25 sec per 10^6 values). ``approx`` The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. ``size`` Uses \"approx\" if len(x)>200000, or \"exact\" otherwise. Returns ------- float The Laplace-kernel entropy. Raises ------ ValueError If the input array `x` has no values, or `w` is not positive. \"\"\" x_in = np . asarray ( x_in ) N = len ( x_in ) if N == 0 : raise ValueError ( \"laplace_entropy(x) needs at least 1 element in `x`.\" ) if w <= 0.0 : raise ValueError ( \"laplace_entropy(x, w) needs `w>0`.\" ) x = np . asarray ( x_in , dtype = DTYPE ) if approx_mode == \"size\" : if N <= 200000 : approx_mode = \"exact\" else : approx_mode = \"approx\" if approx_mode . startswith ( \"exact\" ): return laplace_entropy_array ( x , w ) else : return laplace_entropy_approx ( x , w ) laplace_entropy_approx ( x , w = 1.0 ) Approximage the entropy of data set x with a binned histogram and the Laplace-distribution kernel-density estimator of the probability distribtion. Parameters: x_in ( ArrayLike ) \u2013 The vector of data of which we want the entropy. w ( float , default: 1.0 ) \u2013 The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 Returns: float \u2013 The approximate Laplace-kernel entropy. Source code in mass2/mathstat/entropy.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def laplace_entropy_approx ( x : ArrayLike , w : float = 1.0 ) -> float : \"\"\"Approximage the entropy of data set `x` with a binned histogram and the Laplace-distribution kernel-density estimator of the probability distribtion. Parameters ---------- x_in : ArrayLike The vector of data of which we want the entropy. w : float, optional The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 Returns ------- float The approximate Laplace-kernel entropy. \"\"\" EXTEND_DATA = 5 * w BINS_PER_W = 20 KERNEL_WIDTH_IN_WS = 15.0 xmin = np . min ( x ) - EXTEND_DATA xmax = np . max ( x ) + EXTEND_DATA nbins = int ( 0.5 + ( xmax - xmin ) * BINS_PER_W / w ) c , b = np . histogram ( x , nbins , ( xmin , xmax )) db = b [ 1 ] - b [ 0 ] nx = int ( 0.5 + KERNEL_WIDTH_IN_WS * w / db ) kernel = np . zeros ( 2 * nx + 1 ) for i in range ( 2 * nx + 1 ): kx = ( i - nx ) * db kernel [ i ] = np . exp ( - np . abs ( kx / w )) # kde = unnormalized kernel-density estimator. kde = sp . signal . fftconvolve ( c , kernel , mode = \"full\" )[ nx : - nx ] minkern = kernel . min () kde [ kde < minkern ] = minkern # p = normalized probability distribution. norm = 1.0 / sp . integrate . simpson ( kde , dx = db ) p = kde * norm return - sp . integrate . simpson ( p * np . log ( p ), dx = db ) laplace_entropy_array ( x , w = 1.0 ) Compute the entropy of data set x where the kernel is the Laplace kernel, $k(x) \\propto$ exp(-abs(x-x0)/w). Parameters: x_in ( ArrayLike ) \u2013 The vector of data of which we want the entropy. w ( float , default: 1.0 ) \u2013 The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 Returns: float \u2013 The exact Laplace-kernel entropy, regardless of the input array size. Source code in mass2/mathstat/entropy.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 @njit def laplace_entropy_array ( x : ArrayLike , w : float = 1.0 ) -> float : \"\"\"Compute the entropy of data set `x` where the kernel is the Laplace kernel, $k(x) \\\\propto$ exp(-abs(x-x0)/w). Parameters ---------- x_in : ArrayLike The vector of data of which we want the entropy. w : float, optional The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 Returns ------- float The exact Laplace-kernel entropy, regardless of the input array size. \"\"\" x = np . asarray ( x ) N = len ( x ) c = np . zeros ( N , dtype = DTYPE ) d = np . zeros ( N , dtype = DTYPE ) y = np . sort ( x ) / w e = np . exp ( - np . diff ( y )) stepsize = 1.0 / ( 2 * w * N ) c [ 0 ] = stepsize for i in range ( 1 , N ): c [ i ] = e [ i - 1 ] * c [ i - 1 ] + stepsize d [ N - 1 ] = stepsize for i in range ( N - 2 , - 1 , - 1 ): d [ i ] = e [ i ] * d [ i + 1 ] + stepsize H = w * d [ 0 ] * ( 1 - np . log ( d [ 0 ])) + w * c [ N - 1 ] * ( 1 - np . log ( c [ N - 1 ])) for i in range ( N - 1 ): dp = d [ i + 1 ] * e [ i ] r1 = c [ i ] / d [ i + 1 ] e2 = np . sqrt ( e [ i ]) H += 4 * w * np . sqrt ( c [ i ] * dp ) * np . atan (( e2 - 1.0 / e2 ) * r1 ** 0.5 / ( r1 + 1.0 )) H += w * ( dp - c [ i ]) * ( np . log ( c [ i ] + dp ) - 1 ) A , B = d [ i + 1 ], c [ i ] * e [ i ] H -= w * ( A - B ) * ( np . log ( A + B ) - 1 ) return H Fitting mass2.mathstat.fitting Model-fitting utilities. Joe Fowler, NIST fit_kink_model ( x , y , kbounds = None ) Find the linear least-squares solution for a kinked-linear model. The model is f(x) = a+b(x-k) for x =k, where the 4 parameters are {k,a,b,c}, representing the kink at (x,y)=(k,a) and slopes of b and c for x = k. Given k, the model is linear in the other parameters, which can thus be found exactly by linear algebra. The best value of k is found by use of the Bounded method of the sp.optimize.minimize_scalar() routine. Parameters: x ( ArrayLike ) \u2013 The input data x-values y ( ArrayLike ) \u2013 The input data y-values kbounds ( Optional [ tuple [ float , float ]] , default: None ) \u2013 Bounds on k, by default None. If (u,v), then the minimize_scalar is used to find the best k strictly in u<=k<=v. If None, then use the Brent method, which will start with (b1,b2) as a search bracket where b1 and b2 are the 2nd lowest and 2nd highest values of x. Returns: model_y, abc, X2) where: \u2013 model_y : NDArray[float] an array of the model y-values; kabc : NDArray[float] the best-fit values of the kink location and the 3 linear parameters; X2 : float is the sum of square differences between y and model_y. Raises: ValueError \u2013 if k doesn't satisfy x.min() < k < x.max() Examples: x = np.arange(10, dtype=float) y = np.array(x) truek = 4.6 y[x>truek] = truek y += np.random.default_rng().standard_normal(len(x)) .15 model, (kbest,a,b,c), X2 = fit_kink_model(x, y, kbounds=(3,6)) plt.clf() plt.plot(x, y, \"or\", label=\"Noisy data to be fit\") xi = np.linspace(x[0], kbest, 200) xj = np.linspace(kbest, x[-1], 200) plt.plot(xi, a+b (xi-kbest), \"--k\", label=\"Best-fit kinked model\") plt.plot(xj, a+c*(xj-kbest), \"--k\") plt.legend() Source code in mass2/mathstat/fitting.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def fit_kink_model ( x : ArrayLike , y : ArrayLike , kbounds : tuple [ float , float ] | None = None ) -> tuple [ NDArray , NDArray , float ]: \"\"\"Find the linear least-squares solution for a kinked-linear model. The model is f(x) = a+b(x-k) for x<k and f(x)=a+c(x-k) for x>=k, where the 4 parameters are {k,a,b,c}, representing the kink at (x,y)=(k,a) and slopes of b and c for x<k and x>= k. Given k, the model is linear in the other parameters, which can thus be found exactly by linear algebra. The best value of k is found by use of the Bounded method of the sp.optimize.minimize_scalar() routine. Parameters ---------- x : ArrayLike The input data x-values y : ArrayLike The input data y-values kbounds : Optional[tuple[float, float]], optional Bounds on k, by default None. If (u,v), then the minimize_scalar is used to find the best k strictly in u<=k<=v. If None, then use the Brent method, which will start with (b1,b2) as a search bracket where b1 and b2 are the 2nd lowest and 2nd highest values of x. Returns ------- model_y, abc, X2) where: model_y : NDArray[float] an array of the model y-values; kabc : NDArray[float] the best-fit values of the kink location and the 3 linear parameters; X2 : float is the sum of square differences between y and model_y. Raises ------ ValueError if k doesn't satisfy x.min() < k < x.max() Examples -------- x = np.arange(10, dtype=float) y = np.array(x) truek = 4.6 y[x>truek] = truek y += np.random.default_rng().standard_normal(len(x))*.15 model, (kbest,a,b,c), X2 = fit_kink_model(x, y, kbounds=(3,6)) plt.clf() plt.plot(x, y, \"or\", label=\"Noisy data to be fit\") xi = np.linspace(x[0], kbest, 200) xj = np.linspace(kbest, x[-1], 200) plt.plot(xi, a+b*(xi-kbest), \"--k\", label=\"Best-fit kinked model\") plt.plot(xj, a+c*(xj-kbest), \"--k\") plt.legend() \"\"\" x = np . asarray ( x ) y = np . asarray ( y ) def penalty ( k : float , x : NDArray , y : NDArray ) -> float : \"Extract only the cost function from kink_model()\" _ , _ , X2 = kink_model ( k , x , y ) return X2 if kbounds is None : kbounds = ( x . min (), x . max ()) elif kbounds [ 0 ] < x . min () or kbounds [ 1 ] > x . max (): raise ValueError ( f \"kbounds ( { kbounds } ) must be within the range of x data\" ) optimum = sp . optimize . minimize_scalar ( penalty , args = ( x , y ), method = \"Bounded\" , bounds = kbounds ) kbest = optimum . x model , abc , X2 = kink_model ( kbest , x , y ) return model , np . hstack ([ kbest , abc ]), X2 kink_model ( k , x , y ) Compute a kinked-linear model on data {x,y} with kink at x=k. The model is f(x) = a+b(x-k) for x =k, where the 4 parameters are {k,a,b,c}, representing the kink at (x,y)=(k,a) and slopes of b and c for x = k. For a fixed k, the model is linear in the other parameters, whose linear least-squares values can thus be found exactly by linear algebra. This function computes them. Returns (model_y, (a,b,c), X2) where: model_y is an array of the model y-values; (a,b,c) are the best-fit values of the linear parameters; X2 is the sum of square differences between y and model_y. Parameters: k ( float ) \u2013 Location of the kink, in x coordinates x ( ArrayLike ) \u2013 The input data x-values y ( ArrayLike ) \u2013 The input data y-values Returns: model_y, abc, X2) where: \u2013 model_y : NDArray[float] an array of the model y-values; abc : NDArray[float] the best-fit values of the linear parameters; X2 : float is the sum of square differences between y and model_y. Raises: ValueError \u2013 if k doesn't satisfy x.min() < k < x.max() Source code in mass2/mathstat/fitting.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def kink_model ( k : float , x : ArrayLike , y : ArrayLike ) -> tuple [ NDArray , NDArray , float ]: \"\"\"Compute a kinked-linear model on data {x,y} with kink at x=k. The model is f(x) = a+b(x-k) for x<k and f(x)=a+c(x-k) for x>=k, where the 4 parameters are {k,a,b,c}, representing the kink at (x,y)=(k,a) and slopes of b and c for x<k and x>= k. For a fixed k, the model is linear in the other parameters, whose linear least-squares values can thus be found exactly by linear algebra. This function computes them. Returns (model_y, (a,b,c), X2) where: model_y is an array of the model y-values; (a,b,c) are the best-fit values of the linear parameters; X2 is the sum of square differences between y and model_y. Parameters ---------- k : float Location of the kink, in x coordinates x : ArrayLike The input data x-values y : ArrayLike The input data y-values Returns ------- model_y, abc, X2) where: model_y : NDArray[float] an array of the model y-values; abc : NDArray[float] the best-fit values of the linear parameters; X2 : float is the sum of square differences between y and model_y. Raises ------ ValueError if k doesn't satisfy x.min() < k < x.max() \"\"\" x = np . asarray ( x ) y = np . asarray ( y ) xi = x [ x < k ] yi = y [ x < k ] xj = x [ x >= k ] yj = y [ x >= k ] N = len ( x ) if len ( xi ) == 0 or len ( xj ) == 0 : xmin = x . min () xmax = x . max () raise ValueError ( f \"k= { k : g } should be in range [xmin,xmax], or [ { xmin : g } , { xmax : g } ].\" ) dxi = xi - k dxj = xj - k si = dxi . sum () sj = dxj . sum () si2 = ( dxi ** 2 ) . sum () sj2 = ( dxj ** 2 ) . sum () A = np . array ([[ N , si , sj ], [ si , si2 , 0 ], [ sj , 0 , sj2 ]]) v = np . array ([ y . sum (), ( yi * dxi ) . sum (), ( yj * dxj ) . sum ()]) abc = np . linalg . solve ( A , v ) model = np . hstack ([ abc [ 0 ] + abc [ 1 ] * dxi , abc [ 0 ] + abc [ 2 ] * dxj ]) X2 = (( model - y ) ** 2 ) . sum () return model , abc , X2 Interpolation interpolate.py Module mass2.mathstat.interpolate Contains interpolations functions not readily available elsewhere. CubicSpline - Perform an exact cubic spline through the data, with either specified slope at the end of the interval or 'natural boundary conditions' (y''=0 at ends). GPRSpline - Create a smoothing spline based on the theory of Gaussian process regression. Finds the curvature penalty by maximizing the Bayesian marginal likelihood. Intended to supercede SmoothingSpline , but very similar. Differs in how the curvature and data fidelity are balanced. SmoothingSpline - Create a smoothing spline that does not exactly interpolate the data, but finds the cubic spline with lowest \"curvature energy\" among all splines that meet the maximum allowed value of chi-squared. SmoothingSplineLog - Create a SmoothingSpline using the log of the x,y points. NaturalBsplineBasis - A tool for expressing a spline basis using B-splines but also enforcing 'natural boundary conditions'. Joe Fowler, NIST Created Feb 2014 CubicSpline An exact cubic spline, with either a specified slope or 'natural boundary conditions' (y''=0) at ends of interval. Note that the interface is similar to scipy.interpolate.InterpolatedUnivariateSpline, but the behavior is different. The scipy version will remove the 2nd and 2nd-to-last data points from the set of knots as a way of using the 2 extra degrees of freedom. This class instead sets the 1st or 2nd derivatives at the end of the interval to use the extra degrees of freedom. This code is inspired by section 3.3. of Numerical Recipes, 3rd Edition. Usage: x=np.linspace(4,12,20) y=(x-6)**2+np.random.standard_normal(20) cs = mass2.CubicSpline(x, y) plt.clf() plt.plot(x,y,'ok') xa = np.linspace(0,16,200) plt.plot(xa, cs(xa), 'b-') Source code in mass2/mathstat/interpolate.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 class CubicSpline : \"\"\"An exact cubic spline, with either a specified slope or 'natural boundary conditions' (y''=0) at ends of interval. Note that the interface is similar to scipy.interpolate.InterpolatedUnivariateSpline, but the behavior is different. The scipy version will remove the 2nd and 2nd-to-last data points from the set of knots as a way of using the 2 extra degrees of freedom. This class instead sets the 1st or 2nd derivatives at the end of the interval to use the extra degrees of freedom. This code is inspired by section 3.3. of Numerical Recipes, 3rd Edition. Usage: x=np.linspace(4,12,20) y=(x-6)**2+np.random.standard_normal(20) cs = mass2.CubicSpline(x, y) plt.clf() plt.plot(x,y,'ok') xa = np.linspace(0,16,200) plt.plot(xa, cs(xa), 'b-') \"\"\" def __init__ ( self , x : ArrayLike , y : ArrayLike , yprime1 : float | None = None , yprimeN : float | None = None ): \"\"\"Create an exact cubic spline representation for the function y(x). Parameters ---------- x : ArrayLike Indepdendent variable values. Will be sorted if not increasing. y : ArrayLike Dependent variable values. yprime1 : float | None, optional First derivative at the lower-x boundary, by default None yprimeN : float | None, optional First derivative at the upper-x boundary, by default None. slope of None means to use 'natural boundary conditions' by fixing the second derivative to zero at that boundary. \"\"\" argsort = np . argsort ( x ) self . _x = np . array ( x , dtype = float )[ argsort ] self . _y = np . array ( y , dtype = float )[ argsort ] self . _n = len ( argsort ) self . _y2 = np . zeros ( self . _n , dtype = float ) self . yprime1 = yprime1 self . yprimeN = yprimeN self . _compute_y2 () def _compute_y2 ( self ) -> None : \"\"\"Compute the second derivatives at the knots.\"\"\" self . ystep = self . _y [ 1 :] - self . _y [: - 1 ] self . xstep = self . _x [ 1 :] - self . _x [: - 1 ] u = self . ystep / self . xstep u [ 1 :] -= u [: - 1 ] # For natural boundary conditions, u[0]=y2[0]=0. if self . yprime1 is None : u [ 0 ] = 0 self . _y2 [ 0 ] = 0 else : u [ 0 ] = ( 3.0 / self . xstep [ 0 ]) * ( self . ystep [ 0 ] / self . xstep [ 0 ] - self . yprime1 ) self . _y2 [ 0 ] = - 0.5 for i in range ( 1 , self . _n - 1 ): sig = self . xstep [ i - 1 ] / ( self . _x [ i + 1 ] - self . _x [ i - 1 ]) p = sig * self . _y2 [ i - 1 ] + 2.0 self . _y2 [ i ] = ( sig - 1.0 ) / p u [ i ] = ( 6 * u [ i ] / ( self . _x [ i + 1 ] - self . _x [ i - 1 ]) - sig * u [ i - 1 ]) / p # Again, the following is only for natural boundary conditions if self . yprimeN is None : qn = un = 0.0 else : qn = 0.5 un = ( 3.0 / self . xstep [ - 1 ]) * ( self . yprimeN - self . ystep [ - 1 ] / self . xstep [ - 1 ]) self . _y2 [ self . _n - 1 ] = ( un - qn * u [ self . _n - 2 ]) / ( qn * self . _y2 [ self . _n - 2 ] + 1.0 ) # Backsubstitution: for k in range ( self . _n - 2 , - 1 , - 1 ): self . _y2 [ k ] = self . _y2 [ k ] * self . _y2 [ k + 1 ] + u [ k ] if self . yprime1 is None : self . yprime1 = self . ystep [ 0 ] / self . xstep [ 0 ] - self . xstep [ 0 ] * ( self . _y2 [ 0 ] / 3.0 + self . _y2 [ 1 ] / 6.0 ) if self . yprimeN is None : self . yprimeN = self . ystep [ - 1 ] / self . xstep [ - 1 ] + self . xstep [ - 1 ] * ( self . _y2 [ - 2 ] / 6.0 + self . _y2 [ - 1 ] / 3.0 ) def __call__ ( self , x : ArrayLike | float , der : int = 0 ) -> NDArray : \"\"\"Return the value of the cubic spline (or its derivative) at x. Parameters ---------- x : ArrayLike | float Independent variable value(s) at which to evaluate the spline. der : int, optional Derivative order, by default 0 Returns ------- NDArray Spline result \"\"\" scalar = np . isscalar ( x ) x = np . asarray ( x ) if x . size == 0 : return np . array ([]) elif x . size == 1 : x . shape = ( 1 ,) result = np . zeros_like ( x , dtype = float ) # Find which interval 0,...self._n-2 contains the points (or extrapolates to the points) position = np . searchsorted ( self . _x , x ) - 1 # Here, position == -1 means extrapolate below the first interval. extrap_low = position < 0 if extrap_low . any (): if der == 0 : h = x [ extrap_low ] - self . _x [ 0 ] # will be negative result [ extrap_low ] = self . _y [ 0 ] + h * self . yprime1 elif der == 1 : result [ extrap_low ] = self . yprime1 elif der > 1 : result [ extrap_low ] = 0.0 # position = self._n-1 means extrapolate above the last interval. extrap_hi = position >= self . _n - 1 if extrap_hi . any (): if der == 0 : h = x [ extrap_hi ] - self . _x [ - 1 ] # will be positive result [ extrap_hi ] = self . _y [ - 1 ] + h * self . yprimeN elif der == 1 : result [ extrap_hi ] = self . yprimeN elif der > 1 : result [ extrap_hi ] = 0.0 interp = np . logical_and ( position >= 0 , position < self . _n - 1 ) if interp . any (): klo = position [ interp ] khi = klo + 1 dx = self . xstep [ klo ] a = ( self . _x [ khi ] - x [ interp ]) / dx b = ( x [ interp ] - self . _x [ klo ]) / dx if der == 0 : result [ interp ] = ( a * self . _y [ klo ] + b * self . _y [ khi ] + (( a ** 3 - a ) * self . _y2 [ klo ] + ( b ** 3 - b ) * self . _y2 [ khi ]) * dx * dx / 6.0 ) elif der == 1 : result [ interp ] = ( - self . _y [ klo ] / dx + self . _y [ khi ] / dx + (( - ( a ** 2 ) + 1.0 / 3 ) * self . _y2 [ klo ] + ( b ** 2 - 1.0 / 3 ) * self . _y2 [ khi ]) * dx / 2.0 ) elif der == 2 : result [ interp ] = a * self . _y2 [ klo ] + b * self . _y2 [ khi ] elif der == 3 : result [ interp ] = ( - self . _y2 [ klo ] + self . _y2 [ khi ]) * dx elif der > 3 : result [ interp ] = 0.0 if scalar : result = result [ 0 ] return result def variance ( self , xtest : ArrayLike ) -> NDArray : # noqa: PLR6301 \"\"\"Return a dummy estimate of the variance at points `xtest`.\"\"\" return np . zeros_like ( xtest ) __call__ ( x , der = 0 ) Return the value of the cubic spline (or its derivative) at x. Parameters: x ( ArrayLike | float ) \u2013 Independent variable value(s) at which to evaluate the spline. der ( int , default: 0 ) \u2013 Derivative order, by default 0 Returns: NDArray \u2013 Spline result Source code in mass2/mathstat/interpolate.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def __call__ ( self , x : ArrayLike | float , der : int = 0 ) -> NDArray : \"\"\"Return the value of the cubic spline (or its derivative) at x. Parameters ---------- x : ArrayLike | float Independent variable value(s) at which to evaluate the spline. der : int, optional Derivative order, by default 0 Returns ------- NDArray Spline result \"\"\" scalar = np . isscalar ( x ) x = np . asarray ( x ) if x . size == 0 : return np . array ([]) elif x . size == 1 : x . shape = ( 1 ,) result = np . zeros_like ( x , dtype = float ) # Find which interval 0,...self._n-2 contains the points (or extrapolates to the points) position = np . searchsorted ( self . _x , x ) - 1 # Here, position == -1 means extrapolate below the first interval. extrap_low = position < 0 if extrap_low . any (): if der == 0 : h = x [ extrap_low ] - self . _x [ 0 ] # will be negative result [ extrap_low ] = self . _y [ 0 ] + h * self . yprime1 elif der == 1 : result [ extrap_low ] = self . yprime1 elif der > 1 : result [ extrap_low ] = 0.0 # position = self._n-1 means extrapolate above the last interval. extrap_hi = position >= self . _n - 1 if extrap_hi . any (): if der == 0 : h = x [ extrap_hi ] - self . _x [ - 1 ] # will be positive result [ extrap_hi ] = self . _y [ - 1 ] + h * self . yprimeN elif der == 1 : result [ extrap_hi ] = self . yprimeN elif der > 1 : result [ extrap_hi ] = 0.0 interp = np . logical_and ( position >= 0 , position < self . _n - 1 ) if interp . any (): klo = position [ interp ] khi = klo + 1 dx = self . xstep [ klo ] a = ( self . _x [ khi ] - x [ interp ]) / dx b = ( x [ interp ] - self . _x [ klo ]) / dx if der == 0 : result [ interp ] = ( a * self . _y [ klo ] + b * self . _y [ khi ] + (( a ** 3 - a ) * self . _y2 [ klo ] + ( b ** 3 - b ) * self . _y2 [ khi ]) * dx * dx / 6.0 ) elif der == 1 : result [ interp ] = ( - self . _y [ klo ] / dx + self . _y [ khi ] / dx + (( - ( a ** 2 ) + 1.0 / 3 ) * self . _y2 [ klo ] + ( b ** 2 - 1.0 / 3 ) * self . _y2 [ khi ]) * dx / 2.0 ) elif der == 2 : result [ interp ] = a * self . _y2 [ klo ] + b * self . _y2 [ khi ] elif der == 3 : result [ interp ] = ( - self . _y2 [ klo ] + self . _y2 [ khi ]) * dx elif der > 3 : result [ interp ] = 0.0 if scalar : result = result [ 0 ] return result __init__ ( x , y , yprime1 = None , yprimeN = None ) Create an exact cubic spline representation for the function y(x). Parameters: x ( ArrayLike ) \u2013 Indepdendent variable values. Will be sorted if not increasing. y ( ArrayLike ) \u2013 Dependent variable values. yprime1 ( float | None , default: None ) \u2013 First derivative at the lower-x boundary, by default None yprimeN ( float | None , default: None ) \u2013 First derivative at the upper-x boundary, by default None. slope of None means to use 'natural boundary conditions' by fixing the second derivative to zero at that boundary. Source code in mass2/mathstat/interpolate.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , x : ArrayLike , y : ArrayLike , yprime1 : float | None = None , yprimeN : float | None = None ): \"\"\"Create an exact cubic spline representation for the function y(x). Parameters ---------- x : ArrayLike Indepdendent variable values. Will be sorted if not increasing. y : ArrayLike Dependent variable values. yprime1 : float | None, optional First derivative at the lower-x boundary, by default None yprimeN : float | None, optional First derivative at the upper-x boundary, by default None. slope of None means to use 'natural boundary conditions' by fixing the second derivative to zero at that boundary. \"\"\" argsort = np . argsort ( x ) self . _x = np . array ( x , dtype = float )[ argsort ] self . _y = np . array ( y , dtype = float )[ argsort ] self . _n = len ( argsort ) self . _y2 = np . zeros ( self . _n , dtype = float ) self . yprime1 = yprime1 self . yprimeN = yprimeN self . _compute_y2 () variance ( xtest ) Return a dummy estimate of the variance at points xtest . Source code in mass2/mathstat/interpolate.py 201 202 203 def variance ( self , xtest : ArrayLike ) -> NDArray : # noqa: PLR6301 \"\"\"Return a dummy estimate of the variance at points `xtest`.\"\"\" return np . zeros_like ( xtest ) GPRSpline Bases: CubicSpline A callable object that performs a smoothing cubic spline operation The smoothing spline is the cubic spline minimizing the \"curvature energy\" subject to a constraint that the maximum allowed chi-squared is equal to the number of data points. Here curvature energy is defined as the integral of the square of the second derivative from the lowest to the highest knots. The value of sigmaf fixes the square root of the \"function variance\". Small values of sigmaf correspond to large penalties on the curvature, so they emphasize low curvature. Large sigmaf places emphasis on fidelity to the data and will have relatively higher curvature (and a higher uncertainty on the derived curve). Setting sigmaf=None (the default) will choose the value that maximizes the Bayesian marginal likelihood of the data and is probably smart. For further discussion, see Sections 2.2, 2.7, and 6.3 of Rasmussen, C. E., & Williams, K. I. (2006). Gaussian Processes for Machine Learning. Retrieved from http://www.gaussianprocess.org/gpml/chapters/ This object is very similar to SmoothingSpline in this module but is based on Gaussian Process Regression theory. It improves on SmoothingSpline in that: 1. The curvature/data fidelity trade-off is chosen by more principaled, Bayesian means. 2. The uncertainty in the spline curve is estimated by GPR theory. Source code in mass2/mathstat/interpolate.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 class GPRSpline ( CubicSpline ): \"\"\"A callable object that performs a smoothing cubic spline operation The smoothing spline is the cubic spline minimizing the \"curvature energy\" subject to a constraint that the maximum allowed chi-squared is equal to the number of data points. Here curvature energy is defined as the integral of the square of the second derivative from the lowest to the highest knots. The value of `sigmaf` fixes the square root of the \"function variance\". Small values of `sigmaf` correspond to large penalties on the curvature, so they emphasize low curvature. Large `sigmaf` places emphasis on fidelity to the data and will have relatively higher curvature (and a higher uncertainty on the derived curve). Setting `sigmaf=None` (the default) will choose the value that maximizes the Bayesian marginal likelihood of the data and is probably smart. For further discussion, see Sections 2.2, 2.7, and 6.3 of Rasmussen, C. E., & Williams, K. I. (2006). Gaussian Processes for Machine Learning. Retrieved from http://www.gaussianprocess.org/gpml/chapters/ This object is very similar to `SmoothingSpline` in this module but is based on Gaussian Process Regression theory. It improves on `SmoothingSpline` in that: 1. The curvature/data fidelity trade-off is chosen by more principaled, Bayesian means. 2. The uncertainty in the spline curve is estimated by GPR theory. \"\"\" def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , sigmaf : float | None = None ): \"\"\"Set up the Gaussian Process Regression spline. Parameters ---------- x : ArrayLike Indepdendent variable values. Will be sorted if not increasing. y : ArrayLike Dependent variable values. dy : ArrayLike Uncertainties in y values. dx : ArrayLike | None, optional Uncertainties in x values, by default None sigmaf : float | None, optional Allowed function variance, or None to maximize the data likelihood, by default None \"\"\" self . x = np . array ( x ) self . y = np . array ( y ) self . dy = np . array ( dy ) self . Nk = len ( self . x ) assert self . Nk == len ( self . y ) assert self . Nk == len ( self . dy ) if dx is None : self . dx = np . zeros_like ( dy ) self . err = np . array ( np . abs ( dy )) else : self . dx = np . array ( dx ) roughfit = np . polyfit ( self . x , self . y , 2 ) slope = np . poly1d ( np . polyder ( roughfit , 1 ))( self . x ) self . err = np . sqrt (( self . dx * slope ) ** 2 + self . dy ** 2 ) assert self . Nk == len ( self . dx ) assert self . Nk == len ( self . err ) if sigmaf is None : sigmaf = self . best_sigmaf () self . sigmaf = sigmaf H = np . vstack (( np . ones_like ( self . x ), self . x )) K = np . zeros (( self . Nk , self . Nk ), dtype = float ) sf2 = sigmaf ** 2 for i in range ( self . Nk ): K [ i , i ] = sf2 * k_spline ( self . x [ i ], self . x [ i ]) for j in range ( i + 1 , self . Nk ): K [ i , j ] = K [ j , i ] = sf2 * k_spline ( self . x [ i ], self . x [ j ]) Ky = K + np . diag ( self . err ** 2 ) L = np . linalg . cholesky ( Ky ) LH = np . linalg . solve ( L , H . T ) A = LH . T . dot ( LH ) KinvHT = np . linalg . solve ( L . T , LH ) self . L = L self . A = A self . KinvHT = KinvHT beta = np . linalg . solve ( A , KinvHT . T ) . dot ( self . y ) # Compute at test points = self.x # We know that these are the knots of a natural cubic spline R = H - KinvHT . T . dot ( K ) fbar = np . linalg . solve ( L . T , np . linalg . solve ( L , K )) . T . dot ( y ) gbar = fbar + R . T . dot ( beta ) CubicSpline . __init__ ( self , self . x , gbar ) def best_sigmaf ( self ) -> float : \"\"\"Return the sigmaf value that maximizes the marginal Bayesian likelihood.\"\"\" guess = np . median ( self . err / self . y ) result = sp . optimize . minimize_scalar ( lambda x : - self . _marginal_like ( x ), [ guess / 1e4 , guess * 1e4 ]) if result . success : # _marginal_like depends only on the abs(argument), so take minimizer as positive. return np . abs ( result . x ) raise ( ValueError ( \"Could not maximimze the marginal likelihood\" )) def _marginal_like ( self , sigmaf : float ) -> float : \"\"\"Compute the marginal likelihood of the data given sigmaf. Parameters ---------- sigmaf : float The square root of the function variance Returns ------- float The marginal likelihood (up to an additive constant) \"\"\" H = np . vstack (( np . ones_like ( self . x ), self . x )) K = np . zeros (( self . Nk , self . Nk ), dtype = float ) sf2 = sigmaf ** 2 for i in range ( self . Nk ): K [ i , i ] = sf2 * k_spline ( self . x [ i ], self . x [ i ]) for j in range ( i + 1 , self . Nk ): K [ i , j ] = K [ j , i ] = sf2 * k_spline ( self . x [ i ], self . x [ j ]) Ky = K + np . diag ( self . err ** 2 ) L = np . linalg . cholesky ( Ky ) LH = np . linalg . solve ( L , H . T ) A = LH . T . dot ( LH ) KinvHT = np . linalg . solve ( L . T , LH ) C = KinvHT . dot ( np . linalg . solve ( A , KinvHT . T )) yCy = self . y . dot ( C . dot ( self . y )) Linvy = np . linalg . solve ( L , self . y ) yKinvy = Linvy . dot ( Linvy ) return - 0.5 * (( self . Nk - 2 ) * np . log ( 2 * np . pi ) + np . linalg . slogdet ( A )[ 1 ] + np . linalg . slogdet ( Ky )[ 1 ] - yCy + yKinvy ) def variance ( self , xtest : ArrayLike ) -> NDArray : \"\"\"Returns the variance for function evaluations at the test points `xtest`. This equals the diagonal of `self.covariance(xtest)`, but for large test sets, this method computes only the diagonal and should therefore be faster.\"\"\" v = [] xtest = np . asarray ( xtest ) for x in np . asarray ( xtest ): Ktest = self . sigmaf ** 2 * k_spline ( x , self . x ) LinvKtest = np . linalg . solve ( self . L , Ktest ) cov_ftest = self . sigmaf ** 2 * k_spline ( x , x ) - ( LinvKtest ** 2 ) . sum () R = np . array (( 1 , x )) - self . KinvHT . T . dot ( Ktest ) v . append ( cov_ftest + R . dot ( np . linalg . solve ( self . A , R ))) if np . isscalar ( xtest ): return v [ 0 ] return np . array ( v ) def covariance ( self , xtest : ArrayLike ) -> NDArray : \"\"\"Returns the covariance between function evaluations at the test points `xtest`.\"\"\" if np . isscalar ( xtest ): return self . variance ( xtest ) xtest = np . asarray ( xtest ) Ktest = self . sigmaf ** 2 * np . vstack ([ k_spline ( x , self . x ) for x in xtest ]) . T LinvKtest = np . linalg . solve ( self . L , Ktest ) cov_ftest = self . sigmaf ** 2 * np . vstack ([ k_spline ( x , xtest ) for x in xtest ]) cov_ftest -= LinvKtest . T . dot ( LinvKtest ) R = np . vstack (( np . ones ( len ( xtest )), xtest )) R -= self . KinvHT . T . dot ( Ktest ) return cov_ftest + R . T . dot ( np . linalg . solve ( self . A , R )) __init__ ( x , y , dy , dx = None , sigmaf = None ) Set up the Gaussian Process Regression spline. Parameters: x ( ArrayLike ) \u2013 Indepdendent variable values. Will be sorted if not increasing. y ( ArrayLike ) \u2013 Dependent variable values. dy ( ArrayLike ) \u2013 Uncertainties in y values. dx ( ArrayLike | None , default: None ) \u2013 Uncertainties in x values, by default None sigmaf ( float | None , default: None ) \u2013 Allowed function variance, or None to maximize the data likelihood, by default None Source code in mass2/mathstat/interpolate.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , sigmaf : float | None = None ): \"\"\"Set up the Gaussian Process Regression spline. Parameters ---------- x : ArrayLike Indepdendent variable values. Will be sorted if not increasing. y : ArrayLike Dependent variable values. dy : ArrayLike Uncertainties in y values. dx : ArrayLike | None, optional Uncertainties in x values, by default None sigmaf : float | None, optional Allowed function variance, or None to maximize the data likelihood, by default None \"\"\" self . x = np . array ( x ) self . y = np . array ( y ) self . dy = np . array ( dy ) self . Nk = len ( self . x ) assert self . Nk == len ( self . y ) assert self . Nk == len ( self . dy ) if dx is None : self . dx = np . zeros_like ( dy ) self . err = np . array ( np . abs ( dy )) else : self . dx = np . array ( dx ) roughfit = np . polyfit ( self . x , self . y , 2 ) slope = np . poly1d ( np . polyder ( roughfit , 1 ))( self . x ) self . err = np . sqrt (( self . dx * slope ) ** 2 + self . dy ** 2 ) assert self . Nk == len ( self . dx ) assert self . Nk == len ( self . err ) if sigmaf is None : sigmaf = self . best_sigmaf () self . sigmaf = sigmaf H = np . vstack (( np . ones_like ( self . x ), self . x )) K = np . zeros (( self . Nk , self . Nk ), dtype = float ) sf2 = sigmaf ** 2 for i in range ( self . Nk ): K [ i , i ] = sf2 * k_spline ( self . x [ i ], self . x [ i ]) for j in range ( i + 1 , self . Nk ): K [ i , j ] = K [ j , i ] = sf2 * k_spline ( self . x [ i ], self . x [ j ]) Ky = K + np . diag ( self . err ** 2 ) L = np . linalg . cholesky ( Ky ) LH = np . linalg . solve ( L , H . T ) A = LH . T . dot ( LH ) KinvHT = np . linalg . solve ( L . T , LH ) self . L = L self . A = A self . KinvHT = KinvHT beta = np . linalg . solve ( A , KinvHT . T ) . dot ( self . y ) # Compute at test points = self.x # We know that these are the knots of a natural cubic spline R = H - KinvHT . T . dot ( K ) fbar = np . linalg . solve ( L . T , np . linalg . solve ( L , K )) . T . dot ( y ) gbar = fbar + R . T . dot ( beta ) CubicSpline . __init__ ( self , self . x , gbar ) best_sigmaf () Return the sigmaf value that maximizes the marginal Bayesian likelihood. Source code in mass2/mathstat/interpolate.py 300 301 302 303 304 305 306 307 def best_sigmaf ( self ) -> float : \"\"\"Return the sigmaf value that maximizes the marginal Bayesian likelihood.\"\"\" guess = np . median ( self . err / self . y ) result = sp . optimize . minimize_scalar ( lambda x : - self . _marginal_like ( x ), [ guess / 1e4 , guess * 1e4 ]) if result . success : # _marginal_like depends only on the abs(argument), so take minimizer as positive. return np . abs ( result . x ) raise ( ValueError ( \"Could not maximimze the marginal likelihood\" )) covariance ( xtest ) Returns the covariance between function evaluations at the test points xtest . Source code in mass2/mathstat/interpolate.py 357 358 359 360 361 362 363 364 365 366 367 368 369 def covariance ( self , xtest : ArrayLike ) -> NDArray : \"\"\"Returns the covariance between function evaluations at the test points `xtest`.\"\"\" if np . isscalar ( xtest ): return self . variance ( xtest ) xtest = np . asarray ( xtest ) Ktest = self . sigmaf ** 2 * np . vstack ([ k_spline ( x , self . x ) for x in xtest ]) . T LinvKtest = np . linalg . solve ( self . L , Ktest ) cov_ftest = self . sigmaf ** 2 * np . vstack ([ k_spline ( x , xtest ) for x in xtest ]) cov_ftest -= LinvKtest . T . dot ( LinvKtest ) R = np . vstack (( np . ones ( len ( xtest )), xtest )) R -= self . KinvHT . T . dot ( Ktest ) return cov_ftest + R . T . dot ( np . linalg . solve ( self . A , R )) variance ( xtest ) Returns the variance for function evaluations at the test points xtest . This equals the diagonal of self.covariance(xtest) , but for large test sets, this method computes only the diagonal and should therefore be faster. Source code in mass2/mathstat/interpolate.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 def variance ( self , xtest : ArrayLike ) -> NDArray : \"\"\"Returns the variance for function evaluations at the test points `xtest`. This equals the diagonal of `self.covariance(xtest)`, but for large test sets, this method computes only the diagonal and should therefore be faster.\"\"\" v = [] xtest = np . asarray ( xtest ) for x in np . asarray ( xtest ): Ktest = self . sigmaf ** 2 * k_spline ( x , self . x ) LinvKtest = np . linalg . solve ( self . L , Ktest ) cov_ftest = self . sigmaf ** 2 * k_spline ( x , x ) - ( LinvKtest ** 2 ) . sum () R = np . array (( 1 , x )) - self . KinvHT . T . dot ( Ktest ) v . append ( cov_ftest + R . dot ( np . linalg . solve ( self . A , R ))) if np . isscalar ( xtest ): return v [ 0 ] return np . array ( v ) NaturalBsplineBasis Represent a cubic B-spline basis in 1D with natural boundary conditions. That is, f''(x)=0 at the first and last knots. This constraint reduces the effective number of basis functions from (2+Nknots) to Nknots. Usage: knots = [0,5,8,9,10,12] basis = NaturalBsplineBasis(knots) x = np.linspace(0, 12, 200) plt.clf() for id in range(len(knots)): plt.plot(x, basis(x, id)) Source code in mass2/mathstat/interpolate.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 class NaturalBsplineBasis : \"\"\"Represent a cubic B-spline basis in 1D with natural boundary conditions. That is, f''(x)=0 at the first and last knots. This constraint reduces the effective number of basis functions from (2+Nknots) to Nknots. Usage: knots = [0,5,8,9,10,12] basis = NaturalBsplineBasis(knots) x = np.linspace(0, 12, 200) plt.clf() for id in range(len(knots)): plt.plot(x, basis(x, id)) \"\"\" def __init__ ( self , knots : ArrayLike ): \"\"\"Initialization requires only the list of knots.\"\"\" knots = np . asarray ( knots ) Nk = len ( knots ) b , e = knots [ 0 ], knots [ - 1 ] padknots = np . hstack ([[ b , b , b ], knots , [ e , e , e ]]) # Combinations of basis function #1 into 2 and 3 (and #N+2 into N+1 # and N) are used to enforce the natural B.C. of f''(x)=0 at the ends. lowfpp = np . zeros ( 3 , dtype = float ) hifpp = np . zeros ( 3 , dtype = float ) for i in ( 0 , 1 , 2 ): scoef = np . zeros ( Nk + 2 , dtype = float ) scoef [ i ] = 1.0 lowfpp [ i ] = splev ( b , sp . interpolate . BSpline ( padknots , scoef , 3 ), der = 2 ) for i in ( 0 , 1 , 2 ): scoef = np . zeros ( Nk + 2 , dtype = float ) scoef [ Nk + 1 - i ] = 1.0 # go from last to 3rd-to-last hifpp [ i ] = splev ( e , sp . interpolate . BSpline ( padknots , scoef , 3 ), der = 2 ) self . coef_b = - lowfpp [ 1 : 3 ] / lowfpp [ 0 ] self . coef_e = - hifpp [ 1 : 3 ] / hifpp [ 0 ] self . Nk = Nk self . knots = np . array ( knots ) self . padknots = padknots def __call__ ( self , x : ArrayLike , id : int , der : int = 0 ) -> NDArray : \"\"\"Compute the Basis-spline at the points `x` for basis function `id`, or its derivative of degree `der`. Parameters ---------- x : ArrayLike Independent variable values at which to evaluate the basis function. id : int Which basis function to evaluate, 0 <= id < Nk der : int, optional Derivative degree, by default 0 Returns ------- NDArray Basis function values (or derivative values) at `x`. Raises ------ ValueError If `id` is not in the range 0 <= id < Nk \"\"\" if id < 0 or id >= self . Nk : raise ValueError ( f \"Require 0 <= id < Nk= { self . Nk } \" ) coef = np . zeros ( self . Nk + 2 , dtype = float ) coef [ id + 1 ] = 1.0 if id < 2 : coef [ 0 ] = self . coef_b [ id ] elif id >= self . Nk - 2 : coef [ - 1 ] = self . coef_e [ self . Nk - id - 1 ] return splev ( x , ( self . padknots , coef , 3 ), der = der ) def values_matrix ( self , der : int = 0 ) -> NDArray : \"\"\"Return matrix M where M_ij = value at knot i for basis function j. If der>0, then return the derivative of that order instead of the value.\"\"\" # Note the array is naturally built by vstack as the Transpose of what we want. return np . vstack ([ self ( self . knots , id , der = der ) for id in range ( self . Nk )]) . T def expand_coeff ( self , beta : NDArray ) -> NDArray : \"\"\"Given coefficients of this length-Nk basis, return the coefficients needed by FITPACK, which are of length Nk+2.\"\"\" first = beta [ 0 ] * self . coef_b [ 0 ] + beta [ 1 ] * self . coef_b [ 1 ] last = beta [ - 1 ] * self . coef_e [ 0 ] + beta [ - 2 ] * self . coef_e [ 1 ] return np . hstack ([ first , beta , last ]) __call__ ( x , id , der = 0 ) Compute the Basis-spline at the points x for basis function id , or its derivative of degree der . Parameters: x ( ArrayLike ) \u2013 Independent variable values at which to evaluate the basis function. id ( int ) \u2013 Which basis function to evaluate, 0 <= id < Nk der ( int , default: 0 ) \u2013 Derivative degree, by default 0 Returns: NDArray \u2013 Basis function values (or derivative values) at x . Raises: ValueError \u2013 If id is not in the range 0 <= id < Nk Source code in mass2/mathstat/interpolate.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 def __call__ ( self , x : ArrayLike , id : int , der : int = 0 ) -> NDArray : \"\"\"Compute the Basis-spline at the points `x` for basis function `id`, or its derivative of degree `der`. Parameters ---------- x : ArrayLike Independent variable values at which to evaluate the basis function. id : int Which basis function to evaluate, 0 <= id < Nk der : int, optional Derivative degree, by default 0 Returns ------- NDArray Basis function values (or derivative values) at `x`. Raises ------ ValueError If `id` is not in the range 0 <= id < Nk \"\"\" if id < 0 or id >= self . Nk : raise ValueError ( f \"Require 0 <= id < Nk= { self . Nk } \" ) coef = np . zeros ( self . Nk + 2 , dtype = float ) coef [ id + 1 ] = 1.0 if id < 2 : coef [ 0 ] = self . coef_b [ id ] elif id >= self . Nk - 2 : coef [ - 1 ] = self . coef_e [ self . Nk - id - 1 ] return splev ( x , ( self . padknots , coef , 3 ), der = der ) __init__ ( knots ) Initialization requires only the list of knots. Source code in mass2/mathstat/interpolate.py 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def __init__ ( self , knots : ArrayLike ): \"\"\"Initialization requires only the list of knots.\"\"\" knots = np . asarray ( knots ) Nk = len ( knots ) b , e = knots [ 0 ], knots [ - 1 ] padknots = np . hstack ([[ b , b , b ], knots , [ e , e , e ]]) # Combinations of basis function #1 into 2 and 3 (and #N+2 into N+1 # and N) are used to enforce the natural B.C. of f''(x)=0 at the ends. lowfpp = np . zeros ( 3 , dtype = float ) hifpp = np . zeros ( 3 , dtype = float ) for i in ( 0 , 1 , 2 ): scoef = np . zeros ( Nk + 2 , dtype = float ) scoef [ i ] = 1.0 lowfpp [ i ] = splev ( b , sp . interpolate . BSpline ( padknots , scoef , 3 ), der = 2 ) for i in ( 0 , 1 , 2 ): scoef = np . zeros ( Nk + 2 , dtype = float ) scoef [ Nk + 1 - i ] = 1.0 # go from last to 3rd-to-last hifpp [ i ] = splev ( e , sp . interpolate . BSpline ( padknots , scoef , 3 ), der = 2 ) self . coef_b = - lowfpp [ 1 : 3 ] / lowfpp [ 0 ] self . coef_e = - hifpp [ 1 : 3 ] / hifpp [ 0 ] self . Nk = Nk self . knots = np . array ( knots ) self . padknots = padknots expand_coeff ( beta ) Given coefficients of this length-Nk basis, return the coefficients needed by FITPACK, which are of length Nk+2. Source code in mass2/mathstat/interpolate.py 451 452 453 454 455 456 def expand_coeff ( self , beta : NDArray ) -> NDArray : \"\"\"Given coefficients of this length-Nk basis, return the coefficients needed by FITPACK, which are of length Nk+2.\"\"\" first = beta [ 0 ] * self . coef_b [ 0 ] + beta [ 1 ] * self . coef_b [ 1 ] last = beta [ - 1 ] * self . coef_e [ 0 ] + beta [ - 2 ] * self . coef_e [ 1 ] return np . hstack ([ first , beta , last ]) values_matrix ( der = 0 ) Return matrix M where M_ij = value at knot i for basis function j. If der>0, then return the derivative of that order instead of the value. Source code in mass2/mathstat/interpolate.py 445 446 447 448 449 def values_matrix ( self , der : int = 0 ) -> NDArray : \"\"\"Return matrix M where M_ij = value at knot i for basis function j. If der>0, then return the derivative of that order instead of the value.\"\"\" # Note the array is naturally built by vstack as the Transpose of what we want. return np . vstack ([ self ( self . knots , id , der = der ) for id in range ( self . Nk )]) . T SmoothingSpline A callable object that performs a smoothing cubic spline operation, using the NaturalBsplineBasis object for the basis representation of splines. The smoothing spline is the cubic spline minimizing the \"curvature energy\" subject to a constraint that the maximum allowed chi-squared is equal to the number of data points. Here curvature energy is defined as the integral of the square of the second derivative from the lowest to the highest knots. For a proof see Reinsch, C. H. (1967). \"Smoothing by spline functions.\" Numerische Mathematik, 10(3), 177-183. http://doi.org/10.1007/BF02162161 Source code in mass2/mathstat/interpolate.py 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 class SmoothingSpline : \"\"\"A callable object that performs a smoothing cubic spline operation, using the NaturalBsplineBasis object for the basis representation of splines. The smoothing spline is the cubic spline minimizing the \"curvature energy\" subject to a constraint that the maximum allowed chi-squared is equal to the number of data points. Here curvature energy is defined as the integral of the square of the second derivative from the lowest to the highest knots. For a proof see Reinsch, C. H. (1967). \"Smoothing by spline functions.\" Numerische Mathematik, 10(3), 177-183. http://doi.org/10.1007/BF02162161 \"\"\" def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , maxchisq : float | None = None ): \"\"\"Smoothing spline for data {x,y} with errors {dy} on the y values and {dx} on the x values (or zero if not given). If dx errors are given, a global quadratic fit is done to the data to estimate the local slope. If that's a poor choice, then you should combine your dx and dy errors to create a sensible single error list, and you should pass that in as dy. maxchisq specifies the largest allowed value of chi-squared (the sum of the squares of the differences y_i-f(x_i), divided by the variance v_i). If not given, this defaults to the number of data values. When a (weighted) least squares fit of a line to the data meets the maxchisq constraint, then the actual chi-squared will be less than maxchisq. \"\"\" self . x = np . array ( x ) # copy self . y = np . array ( y ) self . dy = np . array ( dy ) if dx is None : err = np . array ( np . abs ( dy )) dx = np . zeros_like ( err ) else : roughfit = np . polyfit ( self . x , self . y , 2 ) slope = np . poly1d ( np . polyder ( roughfit , 1 ))( x ) err = np . sqrt (( np . asarray ( dx ) * slope ) ** 2 + self . dy ** 2 ) self . xscale = ( self . x ** 2 ) . mean () ** 0.5 self . x /= self . xscale self . dx = np . array ( dx ) / self . xscale self . err = err self . Nk = len ( self . x ) if maxchisq is None : self . maxchisq = float ( self . Nk ) else : self . maxchisq = maxchisq self . basis = NaturalBsplineBasis ( self . x ) self . N0 = self . basis . values_matrix ( 0 ) self . N2 = self . basis . values_matrix ( 2 ) self . Omega = self . _compute_Omega ( self . x , self . N2 ) self . smooth ( chisq = self . maxchisq ) @staticmethod def _compute_Omega ( knots : NDArray , N2 : NDArray ) -> NDArray : \"\"\"Given the matrix M2 of second derivates at the knots (that is, M2_ij is the value of B_j''(x_i), second derivative of basis function #j at knot i), compute the matrix Omega, where Omega_ij is the integral over the entire domain of the product B_i''(x) B_j''(x). This can be done because each B_i(x) is piecewise linear, with the slope changes at each knot location.\"\"\" Nk = len ( knots ) assert N2 . shape [ 0 ] == Nk assert N2 . shape [ 1 ] == Nk Omega = np . zeros_like ( N2 ) for i in range ( Nk ): for j in range ( i + 1 ): for k in range ( Nk - 1 ): Omega [ i , j ] += ( N2 [ k + 1 , i ] * N2 [ k , j ] + N2 [ k + 1 , j ] * N2 [ k , i ]) * ( knots [ k + 1 ] - knots [ k ]) / 6.0 for k in range ( Nk ): Omega [ i , j ] += N2 [ k , i ] * N2 [ k , j ] * ( knots [ min ( k + 1 , Nk - 1 )] - knots [ max ( 0 , k - 1 )]) / 3.0 Omega [ j , i ] = Omega [ i , j ] return Omega def smooth ( self , chisq : float | None = None ) -> None : \"\"\"Choose the value of the curve at the knots so as to achieve the smallest possible curvature subject to the constraint that the sum over all {x,y} pairs S = [(y-f(x))/dy]^2 <= chisq\"\"\" if chisq is None : chisq = self . Nk Dinv = self . err ** ( - 2 ) # Vector but stands for diagonals of a diagonal matrix. NTDinv = self . N0 . T * Dinv lhs = np . dot ( NTDinv , self . N0 ) rhs = np . dot ( self . N0 . T , Dinv * self . y ) def best_params ( p : NDArray ) -> NDArray : \"\"\"Return the best-fit parameters for a given curvature penalty p.\"\"\" return np . linalg . solve ( p * ( lhs - self . Omega ) + self . Omega , p * rhs ) def chisq_difference ( p : NDArray , target_chisq : float ) -> float : \"\"\"Return the difference between the chi-squared for curvature penalty p and the target chi-squared.\"\"\" # If curvature is too small, the computation can become singular. # Avoid this by returning a crazy-high chisquared, as needed. try : beta = best_params ( p ) except np . linalg . LinAlgError : return 1e99 ys = np . dot ( self . N0 , beta ) chisq = np . sum ((( self . y - ys ) / self . err ) ** 2 ) return chisq - target_chisq mincurvature = 1e-20 pbest = sp . optimize . brentq ( chisq_difference , mincurvature , 1 , args = ( chisq ,)) beta = best_params ( pbest ) self . coeff = self . basis . expand_coeff ( beta ) ys = np . dot ( self . N0 , beta ) self . actualchisq = np . sum ((( self . y - ys ) / self . err ) ** 2 ) # Store the linear extrapolation outside the knotted region. endpoints = np . array ([ self . x [ 0 ], self . x [ - 1 ]]) * self . xscale val = self . __eval ( endpoints , 0 , allow_extrapolate = False ) slope = self . __eval ( endpoints , 1 , allow_extrapolate = False ) * self . xscale self . lowline = np . poly1d ([ slope [ 0 ], val [ 0 ]]) self . highline = np . poly1d ([ slope [ 1 ], val [ 1 ]]) def __eval ( self , x : ArrayLike , der : int = 0 , allow_extrapolate : bool = True ) -> NDArray : \"\"\"Return the value of (the `der`th derivative of) the smoothing spline at data points `x`.\"\"\" scalar = np . isscalar ( x ) x = np . asarray ( x ) x /= self . xscale splresult = splev ( x , ( self . basis . padknots , self . coeff , 3 ), der = der ) low = x < self . x [ 0 ] high = x > self . x [ - 1 ] if np . any ( low ) and allow_extrapolate : if der == 0 : splresult [ low ] = self . lowline ( x [ low ] - self . x [ 0 ]) elif der == 1 : splresult [ low ] = self . lowline . coeffs [ 0 ] elif der >= 2 : splresult [ low ] = 0.0 if np . any ( high ) and allow_extrapolate : if der == 0 : splresult [ high ] = self . highline ( x [ high ] - self . x [ - 1 ]) elif der == 1 : splresult [ high ] = self . highline . coeffs [ 0 ] elif der >= 2 : splresult [ high ] = 0.0 if der > 0 : splresult /= self . xscale ** der if scalar : splresult = splresult [()] return splresult def __call__ ( self , x : ArrayLike , der : int = 0 ) -> NDArray : \"\"\"Return the value of (the `der`th derivative of) the smoothing spline at data points `x`.\"\"\" return self . __eval ( x , der = der ) __call__ ( x , der = 0 ) Return the value of (the der th derivative of) the smoothing spline at data points x . Source code in mass2/mathstat/interpolate.py 608 609 610 611 def __call__ ( self , x : ArrayLike , der : int = 0 ) -> NDArray : \"\"\"Return the value of (the `der`th derivative of) the smoothing spline at data points `x`.\"\"\" return self . __eval ( x , der = der ) __eval ( x , der = 0 , allow_extrapolate = True ) Return the value of (the der th derivative of) the smoothing spline at data points x . Source code in mass2/mathstat/interpolate.py 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 def __eval ( self , x : ArrayLike , der : int = 0 , allow_extrapolate : bool = True ) -> NDArray : \"\"\"Return the value of (the `der`th derivative of) the smoothing spline at data points `x`.\"\"\" scalar = np . isscalar ( x ) x = np . asarray ( x ) x /= self . xscale splresult = splev ( x , ( self . basis . padknots , self . coeff , 3 ), der = der ) low = x < self . x [ 0 ] high = x > self . x [ - 1 ] if np . any ( low ) and allow_extrapolate : if der == 0 : splresult [ low ] = self . lowline ( x [ low ] - self . x [ 0 ]) elif der == 1 : splresult [ low ] = self . lowline . coeffs [ 0 ] elif der >= 2 : splresult [ low ] = 0.0 if np . any ( high ) and allow_extrapolate : if der == 0 : splresult [ high ] = self . highline ( x [ high ] - self . x [ - 1 ]) elif der == 1 : splresult [ high ] = self . highline . coeffs [ 0 ] elif der >= 2 : splresult [ high ] = 0.0 if der > 0 : splresult /= self . xscale ** der if scalar : splresult = splresult [()] return splresult __init__ ( x , y , dy , dx = None , maxchisq = None ) Smoothing spline for data {x,y} with errors {dy} on the y values and {dx} on the x values (or zero if not given). If dx errors are given, a global quadratic fit is done to the data to estimate the local slope. If that's a poor choice, then you should combine your dx and dy errors to create a sensible single error list, and you should pass that in as dy. maxchisq specifies the largest allowed value of chi-squared (the sum of the squares of the differences y_i-f(x_i), divided by the variance v_i). If not given, this defaults to the number of data values. When a (weighted) least squares fit of a line to the data meets the maxchisq constraint, then the actual chi-squared will be less than maxchisq. Source code in mass2/mathstat/interpolate.py 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , maxchisq : float | None = None ): \"\"\"Smoothing spline for data {x,y} with errors {dy} on the y values and {dx} on the x values (or zero if not given). If dx errors are given, a global quadratic fit is done to the data to estimate the local slope. If that's a poor choice, then you should combine your dx and dy errors to create a sensible single error list, and you should pass that in as dy. maxchisq specifies the largest allowed value of chi-squared (the sum of the squares of the differences y_i-f(x_i), divided by the variance v_i). If not given, this defaults to the number of data values. When a (weighted) least squares fit of a line to the data meets the maxchisq constraint, then the actual chi-squared will be less than maxchisq. \"\"\" self . x = np . array ( x ) # copy self . y = np . array ( y ) self . dy = np . array ( dy ) if dx is None : err = np . array ( np . abs ( dy )) dx = np . zeros_like ( err ) else : roughfit = np . polyfit ( self . x , self . y , 2 ) slope = np . poly1d ( np . polyder ( roughfit , 1 ))( x ) err = np . sqrt (( np . asarray ( dx ) * slope ) ** 2 + self . dy ** 2 ) self . xscale = ( self . x ** 2 ) . mean () ** 0.5 self . x /= self . xscale self . dx = np . array ( dx ) / self . xscale self . err = err self . Nk = len ( self . x ) if maxchisq is None : self . maxchisq = float ( self . Nk ) else : self . maxchisq = maxchisq self . basis = NaturalBsplineBasis ( self . x ) self . N0 = self . basis . values_matrix ( 0 ) self . N2 = self . basis . values_matrix ( 2 ) self . Omega = self . _compute_Omega ( self . x , self . N2 ) self . smooth ( chisq = self . maxchisq ) smooth ( chisq = None ) Choose the value of the curve at the knots so as to achieve the smallest possible curvature subject to the constraint that the sum over all {x,y} pairs S = [(y-f(x))/dy]^2 <= chisq Source code in mass2/mathstat/interpolate.py 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 def smooth ( self , chisq : float | None = None ) -> None : \"\"\"Choose the value of the curve at the knots so as to achieve the smallest possible curvature subject to the constraint that the sum over all {x,y} pairs S = [(y-f(x))/dy]^2 <= chisq\"\"\" if chisq is None : chisq = self . Nk Dinv = self . err ** ( - 2 ) # Vector but stands for diagonals of a diagonal matrix. NTDinv = self . N0 . T * Dinv lhs = np . dot ( NTDinv , self . N0 ) rhs = np . dot ( self . N0 . T , Dinv * self . y ) def best_params ( p : NDArray ) -> NDArray : \"\"\"Return the best-fit parameters for a given curvature penalty p.\"\"\" return np . linalg . solve ( p * ( lhs - self . Omega ) + self . Omega , p * rhs ) def chisq_difference ( p : NDArray , target_chisq : float ) -> float : \"\"\"Return the difference between the chi-squared for curvature penalty p and the target chi-squared.\"\"\" # If curvature is too small, the computation can become singular. # Avoid this by returning a crazy-high chisquared, as needed. try : beta = best_params ( p ) except np . linalg . LinAlgError : return 1e99 ys = np . dot ( self . N0 , beta ) chisq = np . sum ((( self . y - ys ) / self . err ) ** 2 ) return chisq - target_chisq mincurvature = 1e-20 pbest = sp . optimize . brentq ( chisq_difference , mincurvature , 1 , args = ( chisq ,)) beta = best_params ( pbest ) self . coeff = self . basis . expand_coeff ( beta ) ys = np . dot ( self . N0 , beta ) self . actualchisq = np . sum ((( self . y - ys ) / self . err ) ** 2 ) # Store the linear extrapolation outside the knotted region. endpoints = np . array ([ self . x [ 0 ], self . x [ - 1 ]]) * self . xscale val = self . __eval ( endpoints , 0 , allow_extrapolate = False ) slope = self . __eval ( endpoints , 1 , allow_extrapolate = False ) * self . xscale self . lowline = np . poly1d ([ slope [ 0 ], val [ 0 ]]) self . highline = np . poly1d ([ slope [ 1 ], val [ 1 ]]) SmoothingSplineLog A smoothing spline in log-log space. Source code in mass2/mathstat/interpolate.py 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 class SmoothingSplineLog : \"\"\"A smoothing spline in log-log space.\"\"\" def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , maxchisq : float | None = None ): \"\"\"Set up a smoothing spline in log-log space. Parameters ---------- x : ArrayLike Independent variable values. Must be positive and will be sorted if not increasing. y : ArrayLike Dependent variable values. Must be positive. dy : ArrayLike Uncertainties in y values. dx : ArrayLike | None, optional Uncertainties in x values, by default None maxchisq : float | None, optional Maximum allowed chi^2 value, by default None Raises ------ ValueError If any x or y values are not positive. \"\"\" x = np . asarray ( x ) y = np . asarray ( y ) dy = np . asarray ( dy ) if np . any ( x <= 0 ) or np . any ( y <= 0 ): raise ValueError ( \"The x and y data must all be positive to use a SmoothingSplineLog\" ) if dx is not None : dx = np . asarray ( dx ) / x self . linear_model = SmoothingSpline ( np . log ( x ), np . log ( y ), dy / y , dx , maxchisq = maxchisq ) def __call__ ( self , x : ArrayLike , der : int = 0 ) -> NDArray : \"\"\"Compute the log-log smoothing spline or its derivative at the points `x`. Parameters ---------- x : ArrayLike Independent variable values at which to evaluate the spline. der : int, optional Derivative degree, by default 0 Returns ------- NDArray Smoothing spline values (or derivative values) at `x`. \"\"\" return np . exp ( self . linear_model ( np . log ( x ), der = der )) __call__ ( x , der = 0 ) Compute the log-log smoothing spline or its derivative at the points x . Parameters: x ( ArrayLike ) \u2013 Independent variable values at which to evaluate the spline. der ( int , default: 0 ) \u2013 Derivative degree, by default 0 Returns: NDArray \u2013 Smoothing spline values (or derivative values) at x . Source code in mass2/mathstat/interpolate.py 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 def __call__ ( self , x : ArrayLike , der : int = 0 ) -> NDArray : \"\"\"Compute the log-log smoothing spline or its derivative at the points `x`. Parameters ---------- x : ArrayLike Independent variable values at which to evaluate the spline. der : int, optional Derivative degree, by default 0 Returns ------- NDArray Smoothing spline values (or derivative values) at `x`. \"\"\" return np . exp ( self . linear_model ( np . log ( x ), der = der )) __init__ ( x , y , dy , dx = None , maxchisq = None ) Set up a smoothing spline in log-log space. Parameters: x ( ArrayLike ) \u2013 Independent variable values. Must be positive and will be sorted if not increasing. y ( ArrayLike ) \u2013 Dependent variable values. Must be positive. dy ( ArrayLike ) \u2013 Uncertainties in y values. dx ( ArrayLike | None , default: None ) \u2013 Uncertainties in x values, by default None maxchisq ( float | None , default: None ) \u2013 Maximum allowed chi^2 value, by default None Raises: ValueError \u2013 If any x or y values are not positive. Source code in mass2/mathstat/interpolate.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , maxchisq : float | None = None ): \"\"\"Set up a smoothing spline in log-log space. Parameters ---------- x : ArrayLike Independent variable values. Must be positive and will be sorted if not increasing. y : ArrayLike Dependent variable values. Must be positive. dy : ArrayLike Uncertainties in y values. dx : ArrayLike | None, optional Uncertainties in x values, by default None maxchisq : float | None, optional Maximum allowed chi^2 value, by default None Raises ------ ValueError If any x or y values are not positive. \"\"\" x = np . asarray ( x ) y = np . asarray ( y ) dy = np . asarray ( dy ) if np . any ( x <= 0 ) or np . any ( y <= 0 ): raise ValueError ( \"The x and y data must all be positive to use a SmoothingSplineLog\" ) if dx is not None : dx = np . asarray ( dx ) / x self . linear_model = SmoothingSpline ( np . log ( x ), np . log ( y ), dy / y , dx , maxchisq = maxchisq ) k_spline ( x , y ) Compute the spline covariance kernel, R&W eq 6.28. Source code in mass2/mathstat/interpolate.py 206 207 208 209 def k_spline ( x : NDArray , y : NDArray ) -> NDArray : \"\"\"Compute the spline covariance kernel, R&W eq 6.28.\"\"\" v = np . minimum ( x , y ) return v ** 3 / 3 + v ** 2 / 2 * np . abs ( x - y ) Power spectra A class and functions to compute a power spectrum using some of the sophistications given in Numerical Recipes, including windowing and overlapping data segments. Use the class PowerSpectrum in the case that you are compute-limited and PowerSpectrumOverlap in the case that you are data-limited. The latter uses k segments of data two segments at a time to make (k-1) estimates and makes fuller use of all data (except in the first and last segment). Joe Fowler, NIST October 13, 2010 Usage: import power_spectrum as ps import pylab as plt N=1024 M=N/4 data=np.random.default_rng().standard_normal(N) spec = ps.PowerSpectrum(M, dt=1e-6) window = ps.hann(2 M) for i in range(3): spec.addDataSegment(data[i M : (i+2)*M], window=window) plt.plot(spec.frequencies(), spec.spectrum()) Or you can use the convenience function that hides the class objects from you and simply returns a (frequency,spectrum) pair of arrays: N=1024 data=np.random.default_rng().standard_normal(N) plt.clf() for i in (2,4,8,1): f,s = ps.computeSpectrum(data, segfactor=i, dt=1e-6, window=np.hanning) plt.plot(f, s) Window choices are: bartlett - Triangle shape hann - Sine-squared hamming - 0.08 + 0.92*(sine-squared) welch - Parabolic None - Square (no windowing) *** - Any other vector of length 2m OR any callable accepting 2m as an argument and returning a sequence of that length. Each window take an argument (n), the number of data points per segment. When using the PowerSpectrum or PowerSpectrumOverlap classes or the convenience function computeSpectrum, you have a choice. You can call the window and pass in the resulting vector, or you can pass in the callable function itself. It is allowed to use different windows on different data segments, though honestly that would be really weird. PowerSpectrum Object for accumulating power spectrum estimates from one or more data segments. If you want to use multiple overlapping segments, use class PowerSpectrumOvelap. Based on Num Rec 3rd Ed section 13.4 Source code in mass2/mathstat/power_spectrum.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class PowerSpectrum : \"\"\"Object for accumulating power spectrum estimates from one or more data segments. If you want to use multiple overlapping segments, use class PowerSpectrumOvelap. Based on Num Rec 3rd Ed section 13.4\"\"\" def __init__ ( self , m : int , dt : float | None = 1.0 ): \"\"\"Sets up to estimate PSD at m+1 frequencies (counting DC) given data segments of length 2m. Optional dt is the time step Delta\"\"\" self . m = m self . m2 = 2 * m self . nsegments = 0 self . specsum = np . zeros ( m + 1 , dtype = float ) if dt is None : self . dt = 1.0 else : self . dt = dt def copy ( self ) -> \"PowerSpectrum\" : \"\"\"Return a copy of the object. Handy when coding and you don't want to recompute everything, but you do want to update the method definitions.\"\"\" c = PowerSpectrum ( self . m , dt = self . dt ) c . __dict__ . update ( self . __dict__ ) return c def addDataSegment ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a data segment of length 2m using the window function given. window can be None (square window), a callable taking the length and returning a sequence, or a sequence.\"\"\" if len ( data ) != self . m2 : raise ValueError ( f \"wrong size data segment. len(data)= { len ( data ) } but require { self . m2 } \" ) if np . isnan ( data ) . any (): raise ValueError ( \"data contains NaN\" ) if window is None : w = np . ones ( self . m2 ) elif callable ( window ): w = window ( self . m2 ) elif isinstance ( window , np . ndarray ): assert len ( window ) == self . m2 w = window else : raise TypeError ( \"Window not understood\" ) wksp = w * data sum_window = ( w ** 2 ) . sum () scale_factor = 2.0 / ( sum_window * self . m2 ) if True : # we want real units scale_factor *= self . dt * self . m2 wksp = np . fft . rfft ( wksp ) # The first line adds 2x too much to the first/last bins. ps = np . abs ( wksp ) ** 2 self . specsum += scale_factor * ps self . nsegments += 1 def addLongData ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a long vector of data as non-overlapping segments of length 2m.\"\"\" data = np . asarray ( data ) nt = len ( data ) nk = nt // self . m2 for k in range ( nk ): noff = k * self . m2 self . addDataSegment ( data [ noff : noff + self . m2 ], window = window ) def spectrum ( self , nbins : int | None = None ) -> NDArray : \"\"\"If <nbins> is given, the data are averaged into <nbins> bins.\"\"\" if nbins is None : return self . specsum / self . nsegments if nbins > self . m : raise ValueError ( f \"Cannot rebin into more than m= { self . m } bins\" ) newbin = np . asarray ( 0.5 + np . arange ( self . m + 1 , dtype = float ) / ( self . m + 1 ) * nbins , dtype = int ) result = np . zeros ( nbins + 1 , dtype = float ) for i in range ( nbins + 1 ): result [ i ] = self . specsum [ newbin == i ] . mean () return result / self . nsegments def autocorrelation ( self ) -> None : \"\"\"Return the autocorrelation (the DFT of this power spectrum)\"\"\" raise NotImplementedError ( \"The autocorrelation method is not yet implemented.\" ) def frequencies ( self , nbins : int | None = None ) -> NDArray : \"\"\"If <nbins> is given, the data are averaged into <nbins> bins.\"\"\" if nbins is None : nbins = self . m if nbins > self . m : raise ValueError ( f \"Cannot rebin into more than m= { self . m } bins\" ) return np . arange ( nbins + 1 , dtype = float ) / ( 2 * self . dt * nbins ) def plot ( self , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , ** plotkwarg : Any , ) -> None : \"\"\"Plot the power spectrum (or its square root) on a log-log plot. Parameters ---------- axis : plt.Axes | None, optional Axes to plot on, or if None create a new figure, by default None arb_to_unit_scale_and_label : tuple[int, str], optional rescale the sqrt(PSD) by this amoutn and label it such, by default (1, \"arb\") sqrt_psd : bool, optional Whether to take the square root of the PSD, by default True \"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . spectrum ()[ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies ()[ 1 :] if sqrt_psd : axis . plot ( freq , np . sqrt ( psd ), ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( freq , psd , ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) plt . loglog () axis . grid () axis . set_xlabel ( \"Frequency (Hz)\" ) __init__ ( m , dt = 1.0 ) Sets up to estimate PSD at m+1 frequencies (counting DC) given data segments of length 2m. Optional dt is the time step Delta Source code in mass2/mathstat/power_spectrum.py 74 75 76 77 78 79 80 81 82 83 84 def __init__ ( self , m : int , dt : float | None = 1.0 ): \"\"\"Sets up to estimate PSD at m+1 frequencies (counting DC) given data segments of length 2m. Optional dt is the time step Delta\"\"\" self . m = m self . m2 = 2 * m self . nsegments = 0 self . specsum = np . zeros ( m + 1 , dtype = float ) if dt is None : self . dt = 1.0 else : self . dt = dt addDataSegment ( data , window = None ) Process a data segment of length 2m using the window function given. window can be None (square window), a callable taking the length and returning a sequence, or a sequence. Source code in mass2/mathstat/power_spectrum.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def addDataSegment ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a data segment of length 2m using the window function given. window can be None (square window), a callable taking the length and returning a sequence, or a sequence.\"\"\" if len ( data ) != self . m2 : raise ValueError ( f \"wrong size data segment. len(data)= { len ( data ) } but require { self . m2 } \" ) if np . isnan ( data ) . any (): raise ValueError ( \"data contains NaN\" ) if window is None : w = np . ones ( self . m2 ) elif callable ( window ): w = window ( self . m2 ) elif isinstance ( window , np . ndarray ): assert len ( window ) == self . m2 w = window else : raise TypeError ( \"Window not understood\" ) wksp = w * data sum_window = ( w ** 2 ) . sum () scale_factor = 2.0 / ( sum_window * self . m2 ) if True : # we want real units scale_factor *= self . dt * self . m2 wksp = np . fft . rfft ( wksp ) # The first line adds 2x too much to the first/last bins. ps = np . abs ( wksp ) ** 2 self . specsum += scale_factor * ps self . nsegments += 1 addLongData ( data , window = None ) Process a long vector of data as non-overlapping segments of length 2m. Source code in mass2/mathstat/power_spectrum.py 125 126 127 128 129 130 131 132 def addLongData ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a long vector of data as non-overlapping segments of length 2m.\"\"\" data = np . asarray ( data ) nt = len ( data ) nk = nt // self . m2 for k in range ( nk ): noff = k * self . m2 self . addDataSegment ( data [ noff : noff + self . m2 ], window = window ) autocorrelation () Return the autocorrelation (the DFT of this power spectrum) Source code in mass2/mathstat/power_spectrum.py 147 148 149 def autocorrelation ( self ) -> None : \"\"\"Return the autocorrelation (the DFT of this power spectrum)\"\"\" raise NotImplementedError ( \"The autocorrelation method is not yet implemented.\" ) copy () Return a copy of the object. Handy when coding and you don't want to recompute everything, but you do want to update the method definitions. Source code in mass2/mathstat/power_spectrum.py 86 87 88 89 90 91 92 93 def copy ( self ) -> \"PowerSpectrum\" : \"\"\"Return a copy of the object. Handy when coding and you don't want to recompute everything, but you do want to update the method definitions.\"\"\" c = PowerSpectrum ( self . m , dt = self . dt ) c . __dict__ . update ( self . __dict__ ) return c frequencies ( nbins = None ) If is given, the data are averaged into bins. Source code in mass2/mathstat/power_spectrum.py 151 152 153 154 155 156 157 def frequencies ( self , nbins : int | None = None ) -> NDArray : \"\"\"If <nbins> is given, the data are averaged into <nbins> bins.\"\"\" if nbins is None : nbins = self . m if nbins > self . m : raise ValueError ( f \"Cannot rebin into more than m= { self . m } bins\" ) return np . arange ( nbins + 1 , dtype = float ) / ( 2 * self . dt * nbins ) plot ( axis = None , arb_to_unit_scale_and_label = ( 1 , 'arb' ), sqrt_psd = True , ** plotkwarg ) Plot the power spectrum (or its square root) on a log-log plot. Parameters: axis ( Axes | None , default: None ) \u2013 Axes to plot on, or if None create a new figure, by default None arb_to_unit_scale_and_label ( tuple [ int , str ] , default: (1, 'arb') ) \u2013 rescale the sqrt(PSD) by this amoutn and label it such, by default (1, \"arb\") sqrt_psd ( bool , default: True ) \u2013 Whether to take the square root of the PSD, by default True Source code in mass2/mathstat/power_spectrum.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def plot ( self , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , ** plotkwarg : Any , ) -> None : \"\"\"Plot the power spectrum (or its square root) on a log-log plot. Parameters ---------- axis : plt.Axes | None, optional Axes to plot on, or if None create a new figure, by default None arb_to_unit_scale_and_label : tuple[int, str], optional rescale the sqrt(PSD) by this amoutn and label it such, by default (1, \"arb\") sqrt_psd : bool, optional Whether to take the square root of the PSD, by default True \"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . spectrum ()[ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies ()[ 1 :] if sqrt_psd : axis . plot ( freq , np . sqrt ( psd ), ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( freq , psd , ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) plt . loglog () axis . grid () axis . set_xlabel ( \"Frequency (Hz)\" ) spectrum ( nbins = None ) If is given, the data are averaged into bins. Source code in mass2/mathstat/power_spectrum.py 134 135 136 137 138 139 140 141 142 143 144 145 def spectrum ( self , nbins : int | None = None ) -> NDArray : \"\"\"If <nbins> is given, the data are averaged into <nbins> bins.\"\"\" if nbins is None : return self . specsum / self . nsegments if nbins > self . m : raise ValueError ( f \"Cannot rebin into more than m= { self . m } bins\" ) newbin = np . asarray ( 0.5 + np . arange ( self . m + 1 , dtype = float ) / ( self . m + 1 ) * nbins , dtype = int ) result = np . zeros ( nbins + 1 , dtype = float ) for i in range ( nbins + 1 ): result [ i ] = self . specsum [ newbin == i ] . mean () return result / self . nsegments PowerSpectrumOverlap Bases: PowerSpectrum Object for power spectral estimation using overlapping data segments. User sends non-overlapping segments of length m, and they are processed in pairs of length 2m with overlap (except on the first and last segment). Source code in mass2/mathstat/power_spectrum.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class PowerSpectrumOverlap ( PowerSpectrum ): \"\"\"Object for power spectral estimation using overlapping data segments. User sends non-overlapping segments of length m, and they are processed in pairs of length 2m with overlap (except on the first and last segment). \"\"\" def __init__ ( self , m : int , dt : float | None = 1.0 ): \"\"\"Sets up an object to accumulate a power spectrum estimate. Parameters ---------- m : int Create a PSD estimate with m+1 frequency bins (counting DC) dt : float | None, optional Time sample period in seconds, by default 1.0 \"\"\" PowerSpectrum . __init__ ( self , m , dt = dt ) self . first = True def addDataSegment ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"Process a data segment of length m using window.\" if self . first : self . first = False self . fullseg = np . concatenate (( np . zeros_like ( data ), np . array ( data ))) else : self . fullseg [ 0 : self . m ] = self . fullseg [ self . m :] self . fullseg [ self . m :] = data PowerSpectrum . addDataSegment ( self , self . fullseg , window = window ) def addLongData ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a long vector of data as overlapping segments of length 2m.\"\"\" nt = len ( data ) nk = ( nt - 1 ) // self . m if nk > 1 : delta_el = ( nt - self . m2 ) / ( nk - 1.0 ) else : delta_el = 0.0 for k in range ( nk ): noff = int ( k * delta_el + 0.5 ) PowerSpectrum . addDataSegment ( self , data [ noff : noff + self . m2 ], window = window ) __init__ ( m , dt = 1.0 ) Sets up an object to accumulate a power spectrum estimate. Parameters: m ( int ) \u2013 Create a PSD estimate with m+1 frequency bins (counting DC) dt ( float | None , default: 1.0 ) \u2013 Time sample period in seconds, by default 1.0 Source code in mass2/mathstat/power_spectrum.py 202 203 204 205 206 207 208 209 210 211 212 213 def __init__ ( self , m : int , dt : float | None = 1.0 ): \"\"\"Sets up an object to accumulate a power spectrum estimate. Parameters ---------- m : int Create a PSD estimate with m+1 frequency bins (counting DC) dt : float | None, optional Time sample period in seconds, by default 1.0 \"\"\" PowerSpectrum . __init__ ( self , m , dt = dt ) self . first = True addDataSegment ( data , window = None ) Process a data segment of length m using window. Source code in mass2/mathstat/power_spectrum.py 215 216 217 218 219 220 221 222 223 def addDataSegment ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"Process a data segment of length m using window.\" if self . first : self . first = False self . fullseg = np . concatenate (( np . zeros_like ( data ), np . array ( data ))) else : self . fullseg [ 0 : self . m ] = self . fullseg [ self . m :] self . fullseg [ self . m :] = data PowerSpectrum . addDataSegment ( self , self . fullseg , window = window ) addLongData ( data , window = None ) Process a long vector of data as overlapping segments of length 2m. Source code in mass2/mathstat/power_spectrum.py 225 226 227 228 229 230 231 232 233 234 235 236 def addLongData ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a long vector of data as overlapping segments of length 2m.\"\"\" nt = len ( data ) nk = ( nt - 1 ) // self . m if nk > 1 : delta_el = ( nt - self . m2 ) / ( nk - 1.0 ) else : delta_el = 0.0 for k in range ( nk ): noff = int ( k * delta_el + 0.5 ) PowerSpectrum . addDataSegment ( self , data [ noff : noff + self . m2 ], window = window ) bartlett ( n ) A Bartlett window (triangle shape) of length n Source code in mass2/mathstat/power_spectrum.py 242 243 244 def bartlett ( n : int ) -> NDArray : \"\"\"A Bartlett window (triangle shape) of length n\"\"\" return np . bartlett ( n ) computeSpectrum ( data , segfactor = 1 , dt = None , window = None ) Convenience function to compute the power spectrum of a single data array. Args: Data for finding the spectrum How many segments to break up the data into. The spectrum will be found on each consecutive pair of segments and will be averaged over all pairs. The sample spacing, in time. The window function to apply. Should be a function that accepts a number of samples and returns an array of that length. Possible values are bartlett, welch, hann, and hamming in this module, or use a function of your choosing. Returns: Either the PSD estimate as an array (non-negative frequencies only), OR the tuple (frequencies, PSD). The latter returns when is not None. Source code in mass2/mathstat/power_spectrum.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def computeSpectrum ( data : ArrayLike , segfactor : int = 1 , dt : float | None = None , window : Callable | ArrayLike | None = None ) -> tuple [ NDArray , NDArray ]: \"\"\"Convenience function to compute the power spectrum of a single data array. Args: <data> Data for finding the spectrum <segfactor> How many segments to break up the data into. The spectrum will be found on each consecutive pair of segments and will be averaged over all pairs. <dt> The sample spacing, in time. <window> The window function to apply. Should be a function that accepts a number of samples and returns an array of that length. Possible values are bartlett, welch, hann, and hamming in this module, or use a function of your choosing. Returns: Either the PSD estimate as an array (non-negative frequencies only), *OR* the tuple (frequencies, PSD). The latter returns when <dt> is not None. \"\"\" data = np . asarray ( data ) N = len ( data ) M = N // ( 2 * segfactor ) window_length = 2 * M if window is None : w = np . ones ( window_length , dtype = float ) elif callable ( window ): w = window ( window_length ) elif isinstance ( window , np . ndarray ): assert len ( window ) == window_length w = np . array ( window ) else : raise TypeError ( \"Window not understood\" ) if segfactor == 1 : spec = PowerSpectrum ( M , dt = dt ) # Ensure that the datasegment has even length spec . addDataSegment ( data [: 2 * M ], window = w ) else : spec = PowerSpectrumOverlap ( M , dt = dt ) for i in range ( 2 * segfactor - 1 ): spec . addDataSegment ( data [ i * M : ( i + 1 ) * M ], window = w ) if dt is None : return np . zeros ( M ), spec . spectrum () else : return spec . frequencies (), spec . spectrum () demo ( N = 1024 , window = np . hanning ) Plot a demonstration power spectrum with different segmentations. Parameters: N ( int , default: 1024 ) \u2013 Length of the white-noise random data vector, by default 1024 window ( Callable | ArrayLike | None , default: hanning ) \u2013 Window function to apply, by default np.hanning Source code in mass2/mathstat/power_spectrum.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def demo ( N : int = 1024 , window : Callable | ArrayLike | None = np . hanning ) -> None : \"\"\"Plot a demonstration power spectrum with different segmentations. Parameters ---------- N : int, optional Length of the white-noise random data vector, by default 1024 window : Callable | ArrayLike | None, optional Window function to apply, by default np.hanning \"\"\" data = np . random . default_rng () . standard_normal ( N ) plt . clf () for i in ( 2 , 4 , 8 , 1 ): f , s = computeSpectrum ( data , segfactor = i , dt = 1e0 , window = window ) plt . plot ( f , s ) hamming ( n ) A Hamming window (0.08 + 0.92*sine-squared) of length n Source code in mass2/mathstat/power_spectrum.py 260 261 262 def hamming ( n : int ) -> NDArray : \"\"\"A Hamming window (0.08 + 0.92*sine-squared) of length n\"\"\" return np . hamming ( n ) hann ( n ) A Hann window (sine-squared) of length n Source code in mass2/mathstat/power_spectrum.py 252 253 254 255 256 257 def hann ( n : int ) -> NDArray : \"\"\"A Hann window (sine-squared) of length n\"\"\" # twopi = np.pi*2 # i = np.arange(n, dtype=float) # return 0.5*(1.0-np.cos(i*twopi/(n-1))) return np . hanning ( n ) welch ( n ) A Welch window (parabolic) of length n Source code in mass2/mathstat/power_spectrum.py 247 248 249 def welch ( n : int ) -> NDArray : \"\"\"A Welch window (parabolic) of length n\"\"\" return 1 - ( 2 * np . arange ( n , dtype = float ) / ( n - 1.0 ) - 1 ) ** 2 Robust statistics mass2.mathstat.robust Functions from the field of robust statistics. Location estimators: bisquare_weighted_mean - Mean with weights given by the bisquare rho function. huber_weighted_mean - Mean with weights given by Huber's rho function. trimean - Tukey's trimean, the average of the median and the midhinge. shorth_range - Primarily a dispersion estimator, but location=True gives a (poor) location. Dispersion estimators: median_abs_dev - Median absolute deviation from the median. shorth_range - Length of the shortest closed interval containing at least half the data. Qscale - Normalized Rousseeuw & Croux Q statistic, from the 25%ile of all 2-point distances. Utility functions: high_median - Weighted median Recommendations: For location, suggest the bisquare_weighted_mean with k=3.9*sigma, if you can make any reasonable guess as to the Gaussian-like width sigma. If not, trimean is a good second choice, though less efficient. For dispersion, the Qscale is very efficient for nearly Gaussian data. The median_abs_dev is the most robust though less efficient. If Qscale doesn't work, then short_range is a good second choice. Created on Feb 9, 2012 Rewritten with Numba Jan 23, 2025 @author: fowlerj Qscale ( x , sort_inplace = False ) Compute the robust estimator of scale Q of Rousseeuw and Croux using only O(n log n) memory and computations. A naive implementation is O(n^2) in both memory and computations. Args: x: The data set, an unsorted sequence of values. sort_inplace: Whether it is okay for the function to reorder the set . If True, must be a np.ndarray (or ValueError is raised). Q is defined as d_n * 2.2219 * {|xi-xj|; i<j}_k, where {a}_k means the kth order-statistic of the set {a}, this set is that of the distances between all (n 2) possible pairs of data in {x} n=# of observations in set {x}, k = (n choose 2)/4, 2.2219 makes Q consistent for sigma in normal distributions as n-->infinity, and d_n is a correction factor to the 2.2219 when n is not large. This function does apply the correction factors to make Q consistent with sigma for a Gaussian distribution. Technique from C. Croux & P. Rousseeuw in Comp. Stat Vol 1 (1992) ed. Dodge & Whittaker, Heidelberg: Physica-Verlag pages 411-428. Available at ftp://ftp.win.ua.ac.be/pub/preprints/92/Timeff92.pdf The estimator is further studied in Rousseeuw & Croux, J Am. Stat. Assoc 88 (1993), pp 1273-1283. Source code in mass2/mathstat/robust.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def Qscale ( x : ArrayLike , sort_inplace : bool = False ) -> float : \"\"\"Compute the robust estimator of scale Q of Rousseeuw and Croux using only O(n log n) memory and computations. A naive implementation is O(n^2) in both memory and computations. Args: x: The data set, an unsorted sequence of values. sort_inplace: Whether it is okay for the function to reorder the set <x>. If True, <x> must be a np.ndarray (or ValueError is raised). Q is defined as d_n * 2.2219 * {|xi-xj|; i<j}_k, where {a}_k means the kth order-statistic of the set {a}, this set is that of the distances between all (n 2) possible pairs of data in {x} n=# of observations in set {x}, k = (n choose 2)/4, 2.2219 makes Q consistent for sigma in normal distributions as n-->infinity, and d_n is a correction factor to the 2.2219 when n is not large. This function does apply the correction factors to make Q consistent with sigma for a Gaussian distribution. Technique from C. Croux & P. Rousseeuw in Comp. Stat Vol 1 (1992) ed. Dodge & Whittaker, Heidelberg: Physica-Verlag pages 411-428. Available at ftp://ftp.win.ua.ac.be/pub/preprints/92/Timeff92.pdf The estimator is further studied in Rousseeuw & Croux, J Am. Stat. Assoc 88 (1993), pp 1273-1283. \"\"\" if not sort_inplace : x = np . array ( x ) elif not isinstance ( x , np . ndarray ): raise ValueError ( \"sort_inplace cannot be True unless the data set x is a np.ndarray.\" ) x . sort () n = len ( x ) if n < 2 : raise ValueError ( \"Data set <x> must contain at least 2 values!\" ) h = n // 2 + 1 target_k = h * ( h - 1 ) // 2 - 1 # -1 so that order count can start at 0 instead of conventional 1,2,3... # Compute the n-dependent prefactor to make Q consistent with sigma of a Gaussian. prefactor = 2.2219 if n <= 9 : prefactor *= [ 0 , 0 , 0.399 , 0.994 , 0.512 , 0.844 , 0.611 , 0.857 , 0.669 , 0.872 ][ n ] elif n % 2 == 1 : prefactor *= n / ( n + 1.4 ) else : prefactor *= n / ( n + 3.8 ) # Now down to business finding the 25%ile of |xi - xj| for i<j (or equivalently, for i != j) # Imagine the upper triangle of the matrix Aij = xj - xi (upper means j>i). # If the set is sorted such that xi <= x(i+1) for any i, then the upper triangle of Aij contains # exactly those distances from which we need the k=n/4 order statistic. # For small lists, too many boundary problems arise. Just do it the slow way: if n <= 5 : dist = np . hstack ([ x [ j ] - x [: j ] for j in range ( 1 , n )]) assert len ( dist ) == ( n * ( n - 1 )) // 2 dist . sort () return dist [ target_k ] * prefactor q , npasses = _Qscale_subroutine ( x , n , target_k ) if npasses > n : raise RuntimeError ( f \"Qscale tried { npasses } distances, which is too many\" ) return q * prefactor bisquare_weighted_mean ( x , k , center = None , tol = None ) The bisquare weighted mean of the data with a k-value of . Args: x (array): data values to be summarized k (number): give zero weight to values at least distance k from the weighted mean. center (number): an initial guess at the weighted mean. If None, then the data median will be used (default None). tol (number): tolerance on the estimator (see below; default None) A sensible choice of is 3 to 5 times the rms width or 1.3 to 2 times the full width at half max of a peak. For strictly Gaussian data, the choices of k= 3.14, 3.88, and 4.68 times sigma will be 80%, 90%, and 95% efficient. The answer is found iteratively, revised until it changes by less than . If is None (the default), then will use 1e-5 times the median absolute deviation of about its median. Data values a distance of more than from the weighted mean are given no weight. Source code in mass2/mathstat/robust.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def bisquare_weighted_mean ( x : ArrayLike , k : float , center : float | None = None , tol : float | None = None ) -> float : \"\"\"The bisquare weighted mean of the data <x> with a k-value of <k>. Args: x (array): data values to be summarized k (number): give zero weight to values at least distance k from the weighted mean. center (number): an initial guess at the weighted mean. If None, then the data median will be used (default None). tol (number): tolerance on the estimator (see below; default None) A sensible choice of <k> is 3 to 5 times the rms width or 1.3 to 2 times the full width at half max of a peak. For strictly Gaussian data, the choices of k= 3.14, 3.88, and 4.68 times sigma will be 80%, 90%, and 95% efficient. The answer is found iteratively, revised until it changes by less than <tol>. If <tol> is None (the default), then <tol> will use 1e-5 times the median absolute deviation of <x> about its median. Data values a distance of more than <k> from the weighted mean are given no weight. \"\"\" x = np . asarray ( x ) if center is None : center = np . median ( x ) if tol is None : tol = 1e-5 * median_abs_dev ( x , normalize = True ) for _iteration in range ( 100 ): weights = ( 1 - (( x - center ) / k ) ** 2.0 ) ** 2.0 weights [ np . abs ( x - center ) > k ] = 0.0 newcenter = ( weights * x ) . sum () / weights . sum () if abs ( newcenter - center ) < tol : return newcenter center = newcenter raise RuntimeError ( \"bisquare_weighted_mean used too many iterations. \\n \" + \"Consider using higher <tol> or better <center>, or change to trimean(x).\" ) high_median ( x , weights = None , return_index = False ) Compute the weighted high median of data set x with weights . Returns: The smallest x[j] such that the sum of all weights for data with x[i] <= x[j] is strictly greater than half the total weight. If return_index is True, then the chosen index is returned also as (highmed, index). Source code in mass2/mathstat/robust.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def high_median ( x : ArrayLike , weights : ArrayLike | None = None , return_index : bool = False ) -> float | tuple [ float , int ]: \"\"\"Compute the weighted high median of data set x with weights <weights>. Returns: The smallest x[j] such that the sum of all weights for data with x[i] <= x[j] is strictly greater than half the total weight. If return_index is True, then the chosen index is returned also as (highmed, index). \"\"\" x = np . asarray ( x ) sort_idx = x . argsort () # now x[sort_idx] is sorted n = len ( x ) if weights is None : weights = np . ones ( n , dtype = float ) else : weights = np . asarray ( weights , dtype = float ) ri = _high_median ( sort_idx , weights , n ) if return_index : return x [ ri ], ri return x [ ri ] huber_weighted_mean ( x , k , center = None , tol = None ) Huber's weighted mean of the data with a k-value of . Args: x (array): data values to be summarized k (number): give zero weight to values at least distance k from the weighted mean. center (number): an initial guess at the weighted mean. If None, then the data median will be used (default None). tol (number): tolerance on the estimator (see below; default None) A sensible choice of is 1 to 1.5 times the rms width or 0.4 to 0.6 times the full width at half max of a peak. For strictly Gaussian data, the choices of k=1.0 and 1.4 sigma give ... The answer is found iteratively, revised until it changes by less than . If is None (the default), then will use 1e-5 times the median absolute deviation of about its median. Data values a distance of more than from the weighted mean are given no weight. Source code in mass2/mathstat/robust.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def huber_weighted_mean ( x : ArrayLike , k : float , center : float | None = None , tol : float | None = None ) -> float : \"\"\"Huber's weighted mean of the data <x> with a k-value of <k>. Args: x (array): data values to be summarized k (number): give zero weight to values at least distance k from the weighted mean. center (number): an initial guess at the weighted mean. If None, then the data median will be used (default None). tol (number): tolerance on the estimator (see below; default None) A sensible choice of <k> is 1 to 1.5 times the rms width or 0.4 to 0.6 times the full width at half max of a peak. For strictly Gaussian data, the choices of k=1.0 and 1.4 sigma give ... The answer is found iteratively, revised until it changes by less than <tol>. If <tol> is None (the default), then <tol> will use 1e-5 times the median absolute deviation of <x> about its median. Data values a distance of more than <k> from the weighted mean are given no weight. \"\"\" x = np . asarray ( x ) if center is None : center = np . median ( x ) if tol is None : tol = 1e-5 * median_abs_dev ( x , normalize = True ) for _iteration in range ( 100 ): weights = np . asarray (( 1.0 * k ) / np . abs ( x - center )) weights [ weights > 1.0 ] = 1.0 newcenter = ( weights * x ) . sum () / weights . sum () if abs ( newcenter - center ) < tol : return newcenter center = newcenter raise RuntimeError ( \"huber_weighted_mean used too many iterations. \\n \" + \"Consider using higher <tol> or better <center>, or change to trimean(x).\" ) median_abs_dev ( x , normalize = False ) Median absolute deviation (from the median) of data vector. Args: x (array): data to be summarized. normalize (bool): if True, then return MAD/0.675, which scaling makes the statistic consistent with the standard deviation for an asymptotically large sample of Gaussian deviates (default False). Source code in mass2/mathstat/robust.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def median_abs_dev ( x : ArrayLike , normalize : bool = False ) -> float : \"\"\"Median absolute deviation (from the median) of data vector. Args: x (array): data to be summarized. normalize (bool): if True, then return MAD/0.675, which scaling makes the statistic consistent with the standard deviation for an asymptotically large sample of Gaussian deviates (default False). \"\"\" x = np . asarray ( x ) mad = np . median ( np . abs ( x - np . median ( x ))) if normalize : return mad / 0.674480 # Half of the normal distribution has abs(x-mu) < 0.674480*sigma return mad shorth_range ( x , normalize = False , sort_inplace = False , location = False ) Returns the Shortest Half (shorth) Range, a robust estimator of dispersion. The Shortest Half of a data set {x} means that closed interval [a,b] where (1) a and b are both elements of the data set, (2) at least half of the elements are in the closed interval, and (3) which minimizes the length of the closed interval (b-a). The shorth range is (b-a). See mass2.mathstat.robust.shorth_information for further explanation and references in the literature. Args: x (array): The data set under study. Must be a sequence of values. normalize (bool): If False (default), then return the actual range b-a. If True, then the range will be divided by 1.348960, which normalizes the range to be a consistent estimator of the parameter sigma in the case of an exact Gaussian distribution. (A small correction of order 1/N is applied, too, which mostly corrects for bias at modest values of the sample size N.) sort_inplace - Permit this function to reorder the data set . If False (default), then x will be copied and the copy will be sorted. (Note that if is not a np.ndarray, an error will be raised if is True.) location - Whether to return two location estimators in addition to the dispersion estimator. (default False). Returns: shorth range if evaluates to False; otherwise returns: (shorth range, shorth mean, shorth center) In this, shorth mean is the mean of all samples in the closed range [a,b], and shorth center = (a+b)/2. Beware that both of these location estimators have the undesirable property that their asymptotic standard deviation improves only as N^(-1/3) rather than the more usual N^(-1/2). So it is not a very good idea to use them as location estimators. They are really only included here for testing just how useless they are. Source code in mass2/mathstat/robust.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def shorth_range ( x : ArrayLike , normalize : bool = False , sort_inplace : bool = False , location : bool = False ) -> float | tuple [ float , float , float ]: \"\"\"Returns the Shortest Half (shorth) Range, a robust estimator of dispersion. The Shortest Half of a data set {x} means that closed interval [a,b] where (1) a and b are both elements of the data set, (2) at least half of the elements are in the closed interval, and (3) which minimizes the length of the closed interval (b-a). The shorth range is (b-a). See mass2.mathstat.robust.shorth_information for further explanation and references in the literature. Args: x (array): The data set under study. Must be a sequence of values. normalize (bool): If False (default), then return the actual range b-a. If True, then the range will be divided by 1.348960, which normalizes the range to be a consistent estimator of the parameter sigma in the case of an exact Gaussian distribution. (A small correction of order 1/N is applied, too, which mostly corrects for bias at modest values of the sample size N.) sort_inplace - Permit this function to reorder the data set <x>. If False (default), then x will be copied and the copy will be sorted. (Note that if <x> is not a np.ndarray, an error will be raised if <sort_inplace> is True.) location - Whether to return two location estimators in addition to the dispersion estimator. (default False). Returns: shorth range if <location> evaluates to False; otherwise returns: (shorth range, shorth mean, shorth center) In this, shorth mean is the mean of all samples in the closed range [a,b], and shorth center = (a+b)/2. Beware that both of these location estimators have the undesirable property that their asymptotic standard deviation improves only as N^(-1/3) rather than the more usual N^(-1/2). So it is not a very good idea to use them as location estimators. They are really only included here for testing just how useless they are. \"\"\" if not sort_inplace : x = np . array ( x ) elif not isinstance ( x , np . ndarray ): raise ValueError ( \"sort_inplace cannot be True unless the data set x is a np.ndarray.\" ) x . sort () n = len ( x ) # Number of data values nhalves = int (( n + 1 ) / 2 ) # Number of minimal intervals containing at least half the data nobs = 1 + int ( n / 2 ) # Number of data values in each minimal interval range_each_half = x [ n - nhalves : n ] - x [ 0 : nhalves ] idxa = range_each_half . argmin () a , b = x [ idxa ], x [ idxa + nobs - 1 ] shorth_range = b - a if normalize : shorth_range /= 2 * 0.674480 # Asymptotic expectation for normal data: sigma*2*0.674480 # The value 2*0.674480 is twice the inverse cumulative normal distribution at 0.75. That is, # the middle 50% of a normal distribution are within \u00b10.674480*sigma of the mean. # The small-n corrections depend on n mod 4. See Rousseeuw & Lerow 1988. # These are not at all clear from the text of the paper (see table on p. 115 # if you want to try to decode them). if n % 4 == 0 : shorth_range *= ( n + 1.0 ) / n elif n % 4 == 1 : shorth_range *= ( n + 1.0 ) / ( n - 1.0 ) elif n % 4 == 2 : shorth_range *= ( n + 1.0 ) / n else : shorth_range *= ( n + 1.0 ) / ( n - 1.0 ) if location : return shorth_range , x [ idxa : idxa + nobs ] . mean (), 0.5 * ( a + b ) return shorth_range trimean ( x ) Return Tukey's trimean for a data set , a measure of its central tendency (\"location\" or \"center\"). If (q1,q2,q3) are the quartiles (i.e., the 25%ile, median, and 75 %ile), the trimean is (q1+q3)/4 + q2/2. Source code in mass2/mathstat/robust.py 121 122 123 124 125 126 127 128 129 130 131 def trimean ( x : ArrayLike ) -> float : \"\"\"Return Tukey's trimean for a data set <x>, a measure of its central tendency (\"location\" or \"center\"). If (q1,q2,q3) are the quartiles (i.e., the 25%ile, median, and 75 %ile), the trimean is (q1+q3)/4 + q2/2. \"\"\" x = np . asarray ( x ) q1 , q2 , q3 = [ np . percentile ( x , per ) for per in ( 25 , 50 , 75 )] trimean = 0.25 * ( q1 + q3 ) + 0.5 * q2 return trimean","title":"Other docstrings"},{"location":"docstrings2/#automatic-documentation-generated-from-docstrings","text":"This page is auto-generated from the docstrings of functions, methods, classes, and modules. Each lowest-level module in Mass2 (i.e., each python file) that you want documented and indexed for searching should be listed in this docstrings2.md file. The exception are the Core docstrings for the mass2.core docstrings.","title":"Automatic documentation generated from docstrings"},{"location":"docstrings2/#calibration","text":"","title":"Calibration"},{"location":"docstrings2/#energy-tofrom-pulse-heights","text":"Objects to assist with calibration from pulse heights to absolute energies. Created on May 16, 2011 Completely redesigned January 2025","title":"Energy to/from pulse heights"},{"location":"docstrings2/#mass2.calibration.energy_calibration.Curvetypes","text":"Bases: Enum Enumerate the types of calibration curves supported by Mass2. Source code in mass2/calibration/energy_calibration.py 24 25 26 27 28 29 30 31 32 class Curvetypes ( Enum ): \"\"\"Enumerate the types of calibration curves supported by Mass2.\"\"\" LINEAR = auto () LINEAR_PLUS_ZERO = auto () LOGLOG = auto () GAIN = auto () INVGAIN = auto () LOGGAIN = auto ()","title":"Curvetypes"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration","text":"An energy calibration object that can convert pulse heights to (estimated) energies. Subclasses implement the math of either exact or approximating calibration curves. Methods allow you to convert between pulse heights and energies, estimate energy uncertainties, and estimate pulse heights for lines whose names are know, or estimate the cal curve slope. Methods allow you to plot the calibration curve with its anchor points. Returns: EnergyCalibration \u2013 Raises: ValueError \u2013 If there is not at least one anchor point. Source code in mass2/calibration/energy_calibration.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 @dataclass ( frozen = True ) class EnergyCalibration : \"\"\"An energy calibration object that can convert pulse heights to (estimated) energies. Subclasses implement the math of either exact or approximating calibration curves. Methods allow you to convert between pulse heights and energies, estimate energy uncertainties, and estimate pulse heights for lines whose names are know, or estimate the cal curve slope. Methods allow you to plot the calibration curve with its anchor points. Returns ------- EnergyCalibration Raises ------ ValueError If there is not at least one anchor point. \"\"\" ph : NDArray [ np . float64 ] energy : NDArray [ np . float64 ] dph : NDArray [ np . float64 ] de : NDArray [ np . float64 ] names : list [ str ] curvename : Curvetypes approximating : bool spline : Callable [ ... , NDArray [ np . float64 ]] energy2ph : Callable [[ ArrayLike ], NDArray [ np . float64 ]] ph2uncertainty : Callable [[ ArrayLike ], NDArray [ np . float64 ]] input_transform : Callable output_transform : Callable | None = None extra_info : dict [ str , Any ] | None = None def __post_init__ ( self ) -> None : \"\"\"Fail for inputs of length zero.\"\"\" assert self . npts > 0 def copy ( self , ** changes : Any ) -> EnergyCalibration : \"\"\"Make a copy of this object, optionally changing some attributes.\"\"\" return dataclasses . replace ( self , ** changes ) @property def npts ( self ) -> int : \"\"\"Return the number of calibration anchor points.\"\"\" return len ( self . ph ) @staticmethod def _ecal_input_identity ( ph : NDArray , der : int = 0 ) -> NDArray : \"Use ph as the argument to the spline\" assert der >= 0 if der == 0 : return ph elif der == 1 : return np . ones_like ( ph ) return np . zeros_like ( ph ) @staticmethod def _ecal_input_log ( ph : NDArray , der : int = 0 ) -> NDArray : \"Use log(ph) as the argument to the spline\" assert der >= 0 if der == 0 : return np . log ( ph ) elif der == 1 : return 1.0 / ph raise ValueError ( f \"der= { der } , should be one of (0,1)\" ) @staticmethod def _ecal_output_identity ( ph : NDArray , yspline : NDArray , der : int = 0 , dery : int = 0 ) -> NDArray : \"Use the spline result as E itself\" assert der >= 0 and dery >= 0 if der > 0 : return np . zeros_like ( ph ) if dery == 0 : return yspline elif dery == 1 : return np . ones_like ( ph ) else : return np . zeros_like ( ph ) @staticmethod def _ecal_output_log ( ph : NDArray , yspline : NDArray , der : int = 0 , dery : int = 0 ) -> NDArray : \"Use the spline result as log(E)\" assert der >= 0 and dery >= 0 if der == 0 : # Any order of d/dy equals E(y) itself, or exp(y). return np . exp ( yspline ) else : return np . zeros_like ( ph ) @staticmethod def _ecal_output_gain ( ph : NDArray , yspline : NDArray , der : int = 0 , dery : int = 0 ) -> NDArray : \"Use the spline result as gain = ph/E\" assert der >= 0 and dery >= 0 if dery == 0 : if der == 0 : return ph / yspline elif der == 1 : return 1.0 / yspline else : return np . zeros_like ( ph ) assert dery == 1 return - ph / yspline ** 2 @staticmethod def _ecal_output_invgain ( ph : NDArray , yspline : NDArray , der : int = 0 , dery : int = 0 ) -> NDArray : \"Use the spline result as the inverse gain = E/ph\" assert der >= 0 and dery >= 0 if dery == 0 : if der == 0 : return ph * yspline elif der == 1 : return yspline else : return np . zeros_like ( ph ) assert dery == 1 return ph @staticmethod def _ecal_output_loggain ( ph : NDArray , yspline : NDArray , der : int = 0 , dery : int = 0 ) -> NDArray : \"Use the spline result as the log of the gain, or log(ph/E)\" assert der >= 0 and dery >= 0 if dery == 0 : if der == 0 : return ph * np . exp ( - yspline ) elif der == 1 : return np . exp ( - yspline ) else : return np . zeros_like ( ph ) assert dery == 1 return - ph * np . exp ( - yspline ) @property def ismonotonic ( self ) -> np . bool : \"\"\"Is the curve monotonic from 0 to 1.05 times the max anchor point's pulse height? Test at 1001 points, equally spaced in pulse height.\"\"\" nsamples = 1001 ph = np . linspace ( 0 , 1.05 * self . ph . max (), nsamples ) e = self ( ph ) return np . all ( np . diff ( e ) > 0 ) def name2ph ( self , name : str ) -> NDArray [ np . float64 ]: \"\"\"Convert a named energy feature to pulse height. `name` need not be a calibration point.\"\"\" energy = STANDARD_FEATURES [ name ] return self . energy2ph ( energy ) def energy2dedph ( self , energies : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Calculate the slope at the given energies.\"\"\" return self . ph2dedph ( self . energy2ph ( energies )) def energy2uncertainty ( self , energies : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Cal uncertainty in eV at the given energies.\"\"\" ph = self . energy2ph ( energies ) return self . ph2uncertainty ( ph ) def __str__ ( self ) -> str : \"\"\"A full description of the calibration.\"\"\" seq = [ f \"EnergyCalibration( { self . curvename } )\" ] for name , pulse_ht , energy in zip ( self . names , self . ph , self . energy ): seq . append ( f \" energy(ph= { pulse_ht : 7.2f } ) --> { energy : 9.2f } eV ( { name } )\" ) return \" \\n \" . join ( seq ) def ph2energy ( self , ph : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Apply the calibration, converting pulse heights `ph` to energies. Parameters ---------- ph : ArrayLike The pulse heights to convert to energies. Returns ------- NDArray[np.float64] Energies in eV. \"\"\" ph = np . asarray ( ph ) x = self . input_transform ( ph ) y = self . spline ( x , der = 0 ) if self . output_transform is None : E = y else : E = self . output_transform ( ph , y ) return E __call__ = ph2energy def ph2dedph ( self , ph : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Calculate the calibration curve's slope at pulse heights `ph`.\"\"\" ph = np . asarray ( ph ) x = self . input_transform ( ph ) dgdP = self . input_transform ( ph , der = 1 ) dydx = self . spline ( x , der = 1 ) dEdP = dydx * dgdP if self . output_transform is not None : y = self . spline ( x ) dfdP = self . output_transform ( ph , y , der = 1 ) dfdy = self . output_transform ( ph , y , dery = 1 ) dEdP = dfdP + dfdy * dydx * dgdP return dEdP def energy2ph_exact ( self , E : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"An exact inversion of the calibration curve, converting energies `E` to pulse heights. This is still in TO DO status, as it simply uses the spline in the forward direction. Parameters ---------- E : ArrayLike Energies in eV to be converted back to pulse heihgts Returns ------- NDArray[np.float64] Pulse heights corresponding to the given energies. \"\"\" # TODO use the spline as a starting point for Brent's method return self . energy2ph ( E ) def save_to_hdf5 ( self , hdf5_group : h5py . Group , name : str ) -> None : \"\"\"Save this calibration to an HDF5 group in a new subordinate group with the given name.\"\"\" if name in hdf5_group : del hdf5_group [ name ] cal_group = hdf5_group . create_group ( name ) cal_group [ \"name\" ] = [ str ( n ) . encode () for n in self . names ] cal_group [ \"ph\" ] = self . ph cal_group [ \"energy\" ] = self . energy cal_group [ \"dph\" ] = self . dph cal_group [ \"de\" ] = self . de cal_group . attrs [ \"curvetype\" ] = self . curvename . name cal_group . attrs [ \"approximate\" ] = self . approximating @staticmethod def load_from_hdf5 ( hdf5_group : h5py . Group , name : str ) -> EnergyCalibration : \"\"\"Load a calibration from an HDF5 group with the given name.\"\"\" cal_group = hdf5_group [ name ] # Fix a behavior of h5py for writing in py2, reading in py3. ctype = cal_group . attrs [ \"curvetype\" ] if isinstance ( ctype , bytes ): ctype = ctype . decode ( \"utf-8\" ) curvetype = Curvetypes [ ctype ] maker = EnergyCalibrationMaker ( cal_group [ \"ph\" ][:], cal_group [ \"energy\" ][:], cal_group [ \"dph\" ][:], cal_group [ \"de\" ][:], cal_group [ \"name\" ][:] ) approximate = cal_group . attrs [ \"approximate\" ] return maker . make_calibration ( curvetype , approximate = approximate ) def plotgain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as gain (PH/eV) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"gain\" self . plot ( ** kwargs ) def plotinvgain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as inverse gain (eV/PH) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"invgain\" self . plot ( ** kwargs ) def plotloggain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as log-gain log(PH/eV) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"loggain\" self . plot ( ** kwargs ) def plot ( # noqa: PLR0917 self , axis : plt . Axes | None = None , color : str = \"blue\" , markercolor : str = \"red\" , plottype : str = \"linear\" , ph_rescale_power : float = 0.0 , removeslope : bool = False , energy_x : bool = False , showtext : bool = True , showerrors : bool = True , min_energy : float | None = None , max_energy : float | None = None , ) -> None : \"\"\"Plot the calibration curve, with options.\"\"\" # Plot smooth curve minph , maxph = self . ph . min () * 0.9 , self . ph . max () * 1.1 if min_energy is not None : minph = self . energy2ph ( min_energy ) if max_energy is not None : maxph = self . energy2ph ( max_energy ) phplot = np . linspace ( minph , maxph , 1000 ) eplot = self ( phplot ) gplot = phplot / eplot dyplot = None gains = self . ph / self . energy slope = 0.0 xplot = phplot x = self . ph xerr = self . dph if energy_x : xplot = eplot x = self . energy xerr = self . de if axis is None : plt . clf () axis = plt . subplot ( 111 ) # axis.set_xlim([x[0], x[-1]*1.1]) if energy_x : axis . set_xlabel ( \"Energy (eV)\" ) else : axis . set_xlabel ( \"Pulse height\" ) if plottype == \"linear\" : yplot = self ( phplot ) / ( phplot ** ph_rescale_power ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / ( phplot ** ph_rescale_power ) y = self . energy / ( self . ph ** ph_rescale_power ) if ph_rescale_power == 0.0 : ylabel = \"Energy (eV)\" axis . set_title ( \"Energy calibration curve\" ) else : ylabel = f \"Energy (eV) / PH^ { ph_rescale_power : .4f } \" axis . set_title ( f \"Energy calibration curve, scaled by { ph_rescale_power : .4f } power of PH\" ) elif plottype == \"gain\" : yplot = gplot if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot * gplot y = gains ylabel = \"Gain (PH/eV)\" axis . set_title ( \"Energy calibration curve, gain\" ) elif plottype == \"invgain\" : yplot = 1.0 / gplot if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / phplot y = 1.0 / gains ylabel = \"Inverse Gain (eV/PH)\" axis . set_title ( \"Energy calibration curve, inverse gain\" ) elif plottype == \"loggain\" : yplot = np . log ( gplot ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot y = np . log ( gains ) ylabel = \"Log Gain: log(eV/PH)\" axis . set_title ( \"Energy calibration curve, log gain\" ) elif plottype == \"loglog\" : yplot = np . log ( eplot ) xplot = np . log ( phplot ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot y = np . log ( self . energy ) x = np . log ( self . ph ) xerr = self . dph / self . ph ylabel = \"Log energy/1 eV\" axis . set_xlabel ( \"log(Pulse height/arbs)\" ) axis . set_title ( \"Energy calibration curve, log gain\" ) else : raise ValueError ( \"plottype must be one of ('linear', 'gain','loggain','invgain').\" ) if removeslope : slope = ( y [ - 1 ] - y [ 0 ]) / ( x [ - 1 ] - x [ 0 ]) yplot -= slope * xplot axis . plot ( xplot , yplot , color = color ) if dyplot is not None and showerrors : axis . plot ( xplot , yplot + dyplot , color = color , alpha = 0.35 ) axis . plot ( xplot , yplot - dyplot , color = color , alpha = 0.35 ) # Plot and label cal points dy = (( self . de / self . energy ) ** 2 + ( self . dph / self . ph ) ** 2 ) ** 0.5 * y axis . errorbar ( x , y - slope * x , yerr = dy , xerr = xerr , fmt = \"o\" , mec = \"black\" , mfc = markercolor , capsize = 0 ) axis . grid ( True ) if removeslope : ylabel = f \" { ylabel } slope removed\" axis . set_ylabel ( ylabel ) if showtext : for xval , name , yval in zip ( x , self . names , y ): axis . text ( xval , yval - slope * xval , name + \" \" , ha = \"right\" )","title":"EnergyCalibration"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.ismonotonic","text":"Is the curve monotonic from 0 to 1.05 times the max anchor point's pulse height? Test at 1001 points, equally spaced in pulse height.","title":"ismonotonic"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.npts","text":"Return the number of calibration anchor points.","title":"npts"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.__post_init__","text":"Fail for inputs of length zero. Source code in mass2/calibration/energy_calibration.py 507 508 509 def __post_init__ ( self ) -> None : \"\"\"Fail for inputs of length zero.\"\"\" assert self . npts > 0","title":"__post_init__"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.__str__","text":"A full description of the calibration. Source code in mass2/calibration/energy_calibration.py 628 629 630 631 632 633 def __str__ ( self ) -> str : \"\"\"A full description of the calibration.\"\"\" seq = [ f \"EnergyCalibration( { self . curvename } )\" ] for name , pulse_ht , energy in zip ( self . names , self . ph , self . energy ): seq . append ( f \" energy(ph= { pulse_ht : 7.2f } ) --> { energy : 9.2f } eV ( { name } )\" ) return \" \\n \" . join ( seq )","title":"__str__"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.copy","text":"Make a copy of this object, optionally changing some attributes. Source code in mass2/calibration/energy_calibration.py 511 512 513 def copy ( self , ** changes : Any ) -> EnergyCalibration : \"\"\"Make a copy of this object, optionally changing some attributes.\"\"\" return dataclasses . replace ( self , ** changes )","title":"copy"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.energy2dedph","text":"Calculate the slope at the given energies. Source code in mass2/calibration/energy_calibration.py 619 620 621 def energy2dedph ( self , energies : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Calculate the slope at the given energies.\"\"\" return self . ph2dedph ( self . energy2ph ( energies ))","title":"energy2dedph"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.energy2ph_exact","text":"An exact inversion of the calibration curve, converting energies E to pulse heights. This is still in TO DO status, as it simply uses the spline in the forward direction. Parameters: E ( ArrayLike ) \u2013 Energies in eV to be converted back to pulse heihgts Returns: NDArray [ float64 ] \u2013 Pulse heights corresponding to the given energies. Source code in mass2/calibration/energy_calibration.py 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 def energy2ph_exact ( self , E : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"An exact inversion of the calibration curve, converting energies `E` to pulse heights. This is still in TO DO status, as it simply uses the spline in the forward direction. Parameters ---------- E : ArrayLike Energies in eV to be converted back to pulse heihgts Returns ------- NDArray[np.float64] Pulse heights corresponding to the given energies. \"\"\" # TODO use the spline as a starting point for Brent's method return self . energy2ph ( E )","title":"energy2ph_exact"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.energy2uncertainty","text":"Cal uncertainty in eV at the given energies. Source code in mass2/calibration/energy_calibration.py 623 624 625 626 def energy2uncertainty ( self , energies : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Cal uncertainty in eV at the given energies.\"\"\" ph = self . energy2ph ( energies ) return self . ph2uncertainty ( ph )","title":"energy2uncertainty"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.load_from_hdf5","text":"Load a calibration from an HDF5 group with the given name. Source code in mass2/calibration/energy_calibration.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 @staticmethod def load_from_hdf5 ( hdf5_group : h5py . Group , name : str ) -> EnergyCalibration : \"\"\"Load a calibration from an HDF5 group with the given name.\"\"\" cal_group = hdf5_group [ name ] # Fix a behavior of h5py for writing in py2, reading in py3. ctype = cal_group . attrs [ \"curvetype\" ] if isinstance ( ctype , bytes ): ctype = ctype . decode ( \"utf-8\" ) curvetype = Curvetypes [ ctype ] maker = EnergyCalibrationMaker ( cal_group [ \"ph\" ][:], cal_group [ \"energy\" ][:], cal_group [ \"dph\" ][:], cal_group [ \"de\" ][:], cal_group [ \"name\" ][:] ) approximate = cal_group . attrs [ \"approximate\" ] return maker . make_calibration ( curvetype , approximate = approximate )","title":"load_from_hdf5"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.name2ph","text":"Convert a named energy feature to pulse height. name need not be a calibration point. Source code in mass2/calibration/energy_calibration.py 614 615 616 617 def name2ph ( self , name : str ) -> NDArray [ np . float64 ]: \"\"\"Convert a named energy feature to pulse height. `name` need not be a calibration point.\"\"\" energy = STANDARD_FEATURES [ name ] return self . energy2ph ( energy )","title":"name2ph"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.ph2dedph","text":"Calculate the calibration curve's slope at pulse heights ph . Source code in mass2/calibration/energy_calibration.py 659 660 661 662 663 664 665 666 667 668 669 670 671 def ph2dedph ( self , ph : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Calculate the calibration curve's slope at pulse heights `ph`.\"\"\" ph = np . asarray ( ph ) x = self . input_transform ( ph ) dgdP = self . input_transform ( ph , der = 1 ) dydx = self . spline ( x , der = 1 ) dEdP = dydx * dgdP if self . output_transform is not None : y = self . spline ( x ) dfdP = self . output_transform ( ph , y , der = 1 ) dfdy = self . output_transform ( ph , y , dery = 1 ) dEdP = dfdP + dfdy * dydx * dgdP return dEdP","title":"ph2dedph"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.ph2energy","text":"Apply the calibration, converting pulse heights ph to energies. Parameters: ph ( ArrayLike ) \u2013 The pulse heights to convert to energies. Returns: NDArray [ float64 ] \u2013 Energies in eV. Source code in mass2/calibration/energy_calibration.py 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 def ph2energy ( self , ph : ArrayLike ) -> NDArray [ np . float64 ]: \"\"\"Apply the calibration, converting pulse heights `ph` to energies. Parameters ---------- ph : ArrayLike The pulse heights to convert to energies. Returns ------- NDArray[np.float64] Energies in eV. \"\"\" ph = np . asarray ( ph ) x = self . input_transform ( ph ) y = self . spline ( x , der = 0 ) if self . output_transform is None : E = y else : E = self . output_transform ( ph , y ) return E","title":"ph2energy"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.plot","text":"Plot the calibration curve, with options. Source code in mass2/calibration/energy_calibration.py 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 def plot ( # noqa: PLR0917 self , axis : plt . Axes | None = None , color : str = \"blue\" , markercolor : str = \"red\" , plottype : str = \"linear\" , ph_rescale_power : float = 0.0 , removeslope : bool = False , energy_x : bool = False , showtext : bool = True , showerrors : bool = True , min_energy : float | None = None , max_energy : float | None = None , ) -> None : \"\"\"Plot the calibration curve, with options.\"\"\" # Plot smooth curve minph , maxph = self . ph . min () * 0.9 , self . ph . max () * 1.1 if min_energy is not None : minph = self . energy2ph ( min_energy ) if max_energy is not None : maxph = self . energy2ph ( max_energy ) phplot = np . linspace ( minph , maxph , 1000 ) eplot = self ( phplot ) gplot = phplot / eplot dyplot = None gains = self . ph / self . energy slope = 0.0 xplot = phplot x = self . ph xerr = self . dph if energy_x : xplot = eplot x = self . energy xerr = self . de if axis is None : plt . clf () axis = plt . subplot ( 111 ) # axis.set_xlim([x[0], x[-1]*1.1]) if energy_x : axis . set_xlabel ( \"Energy (eV)\" ) else : axis . set_xlabel ( \"Pulse height\" ) if plottype == \"linear\" : yplot = self ( phplot ) / ( phplot ** ph_rescale_power ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / ( phplot ** ph_rescale_power ) y = self . energy / ( self . ph ** ph_rescale_power ) if ph_rescale_power == 0.0 : ylabel = \"Energy (eV)\" axis . set_title ( \"Energy calibration curve\" ) else : ylabel = f \"Energy (eV) / PH^ { ph_rescale_power : .4f } \" axis . set_title ( f \"Energy calibration curve, scaled by { ph_rescale_power : .4f } power of PH\" ) elif plottype == \"gain\" : yplot = gplot if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot * gplot y = gains ylabel = \"Gain (PH/eV)\" axis . set_title ( \"Energy calibration curve, gain\" ) elif plottype == \"invgain\" : yplot = 1.0 / gplot if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / phplot y = 1.0 / gains ylabel = \"Inverse Gain (eV/PH)\" axis . set_title ( \"Energy calibration curve, inverse gain\" ) elif plottype == \"loggain\" : yplot = np . log ( gplot ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot y = np . log ( gains ) ylabel = \"Log Gain: log(eV/PH)\" axis . set_title ( \"Energy calibration curve, log gain\" ) elif plottype == \"loglog\" : yplot = np . log ( eplot ) xplot = np . log ( phplot ) if self . approximating : dyplot = self . ph2uncertainty ( phplot ) / eplot y = np . log ( self . energy ) x = np . log ( self . ph ) xerr = self . dph / self . ph ylabel = \"Log energy/1 eV\" axis . set_xlabel ( \"log(Pulse height/arbs)\" ) axis . set_title ( \"Energy calibration curve, log gain\" ) else : raise ValueError ( \"plottype must be one of ('linear', 'gain','loggain','invgain').\" ) if removeslope : slope = ( y [ - 1 ] - y [ 0 ]) / ( x [ - 1 ] - x [ 0 ]) yplot -= slope * xplot axis . plot ( xplot , yplot , color = color ) if dyplot is not None and showerrors : axis . plot ( xplot , yplot + dyplot , color = color , alpha = 0.35 ) axis . plot ( xplot , yplot - dyplot , color = color , alpha = 0.35 ) # Plot and label cal points dy = (( self . de / self . energy ) ** 2 + ( self . dph / self . ph ) ** 2 ) ** 0.5 * y axis . errorbar ( x , y - slope * x , yerr = dy , xerr = xerr , fmt = \"o\" , mec = \"black\" , mfc = markercolor , capsize = 0 ) axis . grid ( True ) if removeslope : ylabel = f \" { ylabel } slope removed\" axis . set_ylabel ( ylabel ) if showtext : for xval , name , yval in zip ( x , self . names , y ): axis . text ( xval , yval - slope * xval , name + \" \" , ha = \"right\" )","title":"plot"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.plotgain","text":"Plot the calibration curve as gain (PH/eV) vs pulse height. Source code in mass2/calibration/energy_calibration.py 721 722 723 724 def plotgain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as gain (PH/eV) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"gain\" self . plot ( ** kwargs )","title":"plotgain"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.plotinvgain","text":"Plot the calibration curve as inverse gain (eV/PH) vs pulse height. Source code in mass2/calibration/energy_calibration.py 726 727 728 729 def plotinvgain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as inverse gain (eV/PH) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"invgain\" self . plot ( ** kwargs )","title":"plotinvgain"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.plotloggain","text":"Plot the calibration curve as log-gain log(PH/eV) vs pulse height. Source code in mass2/calibration/energy_calibration.py 731 732 733 734 def plotloggain ( self , ** kwargs : Any ) -> None : \"\"\"Plot the calibration curve as log-gain log(PH/eV) vs pulse height.\"\"\" kwargs [ \"plottype\" ] = \"loggain\" self . plot ( ** kwargs )","title":"plotloggain"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibration.save_to_hdf5","text":"Save this calibration to an HDF5 group in a new subordinate group with the given name. Source code in mass2/calibration/energy_calibration.py 690 691 692 693 694 695 696 697 698 699 700 701 702 def save_to_hdf5 ( self , hdf5_group : h5py . Group , name : str ) -> None : \"\"\"Save this calibration to an HDF5 group in a new subordinate group with the given name.\"\"\" if name in hdf5_group : del hdf5_group [ name ] cal_group = hdf5_group . create_group ( name ) cal_group [ \"name\" ] = [ str ( n ) . encode () for n in self . names ] cal_group [ \"ph\" ] = self . ph cal_group [ \"energy\" ] = self . energy cal_group [ \"dph\" ] = self . dph cal_group [ \"de\" ] = self . de cal_group . attrs [ \"curvetype\" ] = self . curvename . name cal_group . attrs [ \"approximate\" ] = self . approximating","title":"save_to_hdf5"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker","text":"An object that can make energy calibration curves under various assumptions, but using a single set of calibration anchor points and uncertainties on them. Returns: EnergyCalibrationMaker \u2013 A factory for making various EnergyCalibration objects from the same anchor points. Raises: ValueError \u2013 When calibration data arrays have unequal length, or ph is not monotone in energy . Source code in mass2/calibration/energy_calibration.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 @dataclass ( frozen = True ) class EnergyCalibrationMaker : \"\"\"An object that can make energy calibration curves under various assumptions, but using a single set of calibration anchor points and uncertainties on them. Returns ------- EnergyCalibrationMaker A factory for making various `EnergyCalibration` objects from the same anchor points. Raises ------ ValueError When calibration data arrays have unequal length, or `ph` is not monotone in `energy`. \"\"\" ph : NDArray [ np . float64 ] energy : NDArray [ np . float64 ] dph : NDArray [ np . float64 ] de : NDArray [ np . float64 ] names : list [ str ] @classmethod def init ( cls , ph : ArrayLike | None = None , energy : ArrayLike | None = None , dph : ArrayLike | None = None , de : ArrayLike | None = None , names : list [ str ] | None = None , ) -> EnergyCalibrationMaker : \"\"\"Create an EnergyCalibrationMaker, filling in any missing requirements with empty arrays.\"\"\" if ph is None : ph = np . array ([], dtype = float ) else : ph = np . asarray ( ph ) if energy is None : energy = np . array ([], dtype = float ) else : energy = np . asarray ( energy ) if dph is None : dph = 1e-3 * ph else : dph = np . asarray ( dph ) if de is None : de = 1e-3 * energy else : de = np . asarray ( de ) if names is None : names = [ \"dummy\" ] * len ( dph ) return cls ( ph , energy , dph , de , names ) def __post_init__ ( self ) -> None : \"\"\"Check for inputs of unequal length. Check for monotone anchor points. Sort the input data by energy.\"\"\" N = len ( self . ph ) assert N == len ( self . energy ) assert N == len ( self . dph ) assert N == len ( self . de ) assert N == len ( self . names ) # First sort according to energy of the calibration point if not np . all ( np . diff ( self . energy ) > 0 ): sortkeys = np . argsort ( self . energy ) self . ph [:] = self . ph [ sortkeys ] self . energy [:] = self . energy [ sortkeys ] self . dph [:] = self . dph [ sortkeys ] self . de [:] = self . de [ sortkeys ] self . names [:] = [ self . names [ i ] for i in sortkeys ] # Then confirm that the pulse heights are also in order order_ph = self . ph . argsort () order_en = self . energy . argsort () if not np . all ( order_ph == order_en ): a = f \"PH: { self . ph [ order_ph ] } \" b = f \"Energy: { self . energy [ order_ph ] } \" raise ValueError ( f \"Calibration points are not monotone: \\n { a } \\n { b } \" ) @property def npts ( self ) -> int : \"\"\"The number of calibration anchor points.\"\"\" return len ( self . ph ) def _remove_cal_point_idx ( self , idx : int ) -> EnergyCalibrationMaker : \"\"\"Remove calibration point number `idx` from the calibration. Return a new maker.\"\"\" ph = np . delete ( self . ph , idx ) energy = np . delete ( self . energy , idx ) dph = np . delete ( self . dph , idx ) de = np . delete ( self . de , idx ) names = self . names . copy () names . pop ( idx ) return EnergyCalibrationMaker ( ph , energy , dph , de , names ) def remove_cal_point_name ( self , name : str ) -> EnergyCalibrationMaker : \"\"\"Remove calibration point named `name`. Return a new maker.\"\"\" idx = self . names . index ( name ) return self . _remove_cal_point_idx ( idx ) def remove_cal_point_prefix ( self , prefix : str ) -> EnergyCalibrationMaker : \"\"\"This removes all cal points whose name starts with `prefix`. Return a new maker.\"\"\" # Work recursively: remove the first match and make a new Maker, and repeat until none match. # This is clearly less efficient when removing N matches, as N copies are made. So what? # This feature is likely to be rarely used, and we favor clarity over performance here. for name in tuple ( self . names ): if name . startswith ( prefix ): return self . remove_cal_point_name ( name ) . remove_cal_point_prefix ( prefix ) return self def remove_cal_point_energy ( self , energy : float , de : float ) -> EnergyCalibrationMaker : \"\"\"Remove cal points at energies within \u00b1`de` of `energy`. Return a new maker.\"\"\" idxs = np . nonzero ( np . abs ( self . energy - energy ) < de )[ 0 ] if len ( idxs ) == 0 : return self # Also recursive and less efficient. See previous method's comment. return self . _remove_cal_point_idx ( idxs [ 0 ]) . remove_cal_point_energy ( energy , de ) def add_cal_point ( self , ph : float , energy : float | str , name : str = \"\" , ph_error : float | None = None , e_error : float | None = None , replace : bool = True , ) -> EnergyCalibrationMaker : \"\"\"Add a single energy calibration point. Can call as .add_cal_point(ph, energy, name) or if the \"energy\" is a line name, then .add_cal_point(ph, name) will find energy as `energy=mass2.STANDARD_FEATURES[name]`. Thus the following are equivalent: cal = cal.add_cal_point(12345.6, 5898.801, \"Mn Ka1\") cal = cal.add_cal_point(12456.6, \"Mn Ka1\") `ph` must be in units of the self.ph_field and `energy` is in eV. `ph_error` is the 1-sigma uncertainty on the pulse height. If None (the default), then assign ph_error = `ph`/1000. `e_error` is the 1-sigma uncertainty on the energy itself. If None (the default), then assign e_error=0.01 eV. Careful! If you give a name that's already in the list, or you add an equivalent energy but do NOT give a name, then this value replaces the previous one. You can prevent overwriting (and instead raise an error) by setting `replace`=False. \"\"\" # If <energy> is a string and a known spectral feature's name, use it as the name instead # Otherwise, it needs to be a numeric type convertible to float. try : energy = float ( energy ) except ValueError : try : if type ( energy ) is str : name = energy else : name = str ( energy ) energy = STANDARD_FEATURES [ name ] except Exception : raise ValueError ( \"2nd argument must be an energy or a known name\" + \" from mass2.energy_calibration.STANDARD_FEATURES\" ) if ph_error is None : ph_error = ph * 0.001 if e_error is None : e_error = 0.01 # Assume 0.01 eV error if none given update_index : int | None = None if self . npts > 0 : if name and name in self . names : # Update an existing point by name if not replace : raise ValueError ( f \"Calibration point ' { name } ' is already known and overwrite is False\" ) update_index = self . names . index ( name ) elif np . abs ( energy - self . energy ) . min () <= e_error : # Update existing point if not replace : raise ValueError ( f \"Calibration point at energy { energy : .2f } eV is already known and overwrite is False\" ) update_index = int ( np . abs ( energy - self . energy ) . argmin ()) if update_index is None : # Add a new calibration anchor point new_ph = np . hstack (( self . ph , ph )) new_energy = np . hstack (( self . energy , energy )) new_dph = np . hstack (( self . dph , ph_error )) new_de = np . hstack (( self . de , e_error )) new_names = self . names + [ name ] else : # Replace an existing calibration anchor point. new_ph = self . ph . copy () new_energy = self . energy . copy () new_dph = self . dph . copy () new_de = self . de . copy () new_names = self . names . copy () new_ph [ update_index ] = ph new_energy [ update_index ] = energy new_dph [ update_index ] = ph_error new_de [ update_index ] = e_error new_names [ update_index ] = name return EnergyCalibrationMaker ( new_ph , new_energy , new_dph , new_de , new_names ) @staticmethod def heuristic_samplepoints ( anchors : ArrayLike ) -> np . ndarray : \"\"\"Given a set of calibration anchor points, return a few hundred sample points, reasonably spaced below, between, and above the anchor points. Parameters ---------- anchors : ArrayLike The anchor points (in pulse height space) Returns ------- np.ndarray _description_ \"\"\" anchors = np . asarray ( anchors ) # Prescription is 50 points up to lowest anchor (but exclude 0): x = [ np . linspace ( 0 , anchors . min (), 51 )[ 1 :]] # Then one points, plus one extra per 1% spacing between (and at) each anchor for i in range ( len ( anchors ) - 1 ): low , high = anchors [ i : i + 2 ] n = 1 + int ( 100 * ( high / low - 1 ) + 0.5 ) x . append ( np . linspace ( low , high , n + 1 )[ 1 :]) # Finally, 100 more points between the highest anchor and 2x that. x . append ( anchors . max () * np . linspace ( 1 , 2 , 101 )[ 1 :]) return np . hstack ( x ) def make_calibration_loglog ( self , approximate : bool = False , powerlaw : float = 1.15 , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in log(energy) vs log(pulse height).\"\"\" return self . make_calibration ( Curvetypes . LOGLOG , approximate = approximate , powerlaw = powerlaw , extra_info = extra_info ) def make_calibration_gain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in (pulse height/energy) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . GAIN , approximate = approximate , extra_info = extra_info ) def make_calibration_invgain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in (energy/pulse height) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . INVGAIN , approximate = approximate , extra_info = extra_info ) def make_calibration_loggain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in log(pulse height/energy) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . LOGGAIN , approximate = approximate , extra_info = extra_info ) def make_calibration_linear ( self , approximate : bool = False , addzero : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in energy vs pulse height. If `addzero` include a (0,0) anchor point.\"\"\" curvename = Curvetypes . LINEAR_PLUS_ZERO if addzero else Curvetypes . LINEAR return self . make_calibration ( curvename , approximate = approximate , extra_info = extra_info ) def make_calibration ( self , curvename : Curvetypes = Curvetypes . LOGLOG , approximate : bool = False , powerlaw : float = 1.15 , extra_info : dict [ str , Any ] | None = None , ) -> EnergyCalibration : \"\"\"Create an energy calibration curve of the specified type. Parameters ---------- curvename : Curvetypes, optional Which curve type to use, by default Curvetypes.LOGLOG approximate : bool, optional Whether to approximate the anchor point data given the uncertainties, by default False powerlaw : float, optional An approximate powerlaw guess used by LOGLOG curves, by default 1.15 extra_info : dict[str, Any] | None, optional Extra text to store in the result, by default None Returns ------- EnergyCalibration The calibration object. Raises ------ ValueError If there are too few anchor points for an approximating curve, or if `curvename` is not in `Curvetypes`. \"\"\" if approximate and self . npts < 3 : raise ValueError ( f \"approximating curves require 3 or more cal anchor points, have { self . npts } \" ) if curvename not in Curvetypes : raise ValueError ( f \" { curvename =} , must be in { Curvetypes } \" ) # Use a heuristic to repair negative uncertainties. def regularize_uncertainties ( x : NDArray [ np . float64 ]) -> np . ndarray : \"\"\"Replace negative uncertainties with the minimum non-negative uncertainty, or zero.\"\"\" if not np . any ( x < 0 ): return x target = max ( 0.0 , x . min ()) x = x . copy () x [ x < 0 ] = target return x dph = regularize_uncertainties ( self . dph ) de = regularize_uncertainties ( self . de ) if curvename == Curvetypes . LOGLOG : input_transform = EnergyCalibration . _ecal_input_log output_transform = EnergyCalibration . _ecal_output_log x = np . log ( self . ph ) y = np . log ( self . energy ) # When there's only one point, enhance it by a fake point to enforce power-law behavior if self . npts == 1 : arboffset = 1.0 x = np . hstack ([ x , x + arboffset ]) y = np . hstack ([ y , y + arboffset / powerlaw ]) dx = dph / self . ph dy = de / self . energy elif curvename == Curvetypes . GAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_gain x = self . ph y = self . ph / self . energy # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * self . energy - 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph elif curvename == Curvetypes . INVGAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_invgain x = self . ph y = self . energy / self . ph # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * self . ph / y + 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph elif curvename in { Curvetypes . LINEAR , Curvetypes . LINEAR_PLUS_ZERO }: input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_identity x = self . ph y = self . energy dx = dph dy = de if ( curvename == Curvetypes . LINEAR_PLUS_ZERO ) and ( 0.0 not in x ): # Add a \"zero\"-energy and -PH point. But to avoid numerical problems, actually just use # 1e-3 times the lowest value, giving \u00b1100% uncertainty on the values. x = np . hstack (([ x . min () * 1e-3 ], x )) y = np . hstack (([ y . min () * 1e-3 ], y )) dx = np . hstack (([ x [ 0 ] * 1e-3 ], dx )) dy = np . hstack (( y [ 0 ] * 1e-3 , dy )) elif curvename == Curvetypes . LOGGAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_loggain x = self . ph y = np . log ( self . ph / self . energy ) # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * x - 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph else : raise ValueError ( f \"curvename=' { curvename } ' not recognized\" ) if approximate : internal_spline : CubicSpline = GPRSpline ( x , y , dy , dx ) elif len ( x ) > 1 : internal_spline = CubicSpline ( x , y ) else : internal_spline = CubicSpline ( x * [ 1 , 2 ], y * [ 1 , 2 ]) ph_samplepoints = EnergyCalibrationMaker . heuristic_samplepoints ( self . ph ) E_samplepoints = output_transform ( ph_samplepoints , internal_spline ( input_transform ( ph_samplepoints ))) energy2ph = CubicSpline ( E_samplepoints , ph_samplepoints ) if approximate : dspline = internal_spline . variance ( ph_samplepoints ) ** 0.5 if curvename == Curvetypes . LOGLOG : de_samplepoints = dspline * internal_spline ( input_transform ( ph_samplepoints )) elif curvename == Curvetypes . GAIN : de_samplepoints = dspline * E_samplepoints ** 2 / ph_samplepoints elif curvename == Curvetypes . INVGAIN : de_samplepoints = dspline * ph_samplepoints elif curvename in { Curvetypes . LINEAR , Curvetypes . LINEAR_PLUS_ZERO }: de_samplepoints = dspline elif curvename == Curvetypes . LOGGAIN : abs_dfdp = np . abs ( internal_spline ( ph_samplepoints , der = 1 )) de_samplepoints = dspline * E_samplepoints * abs_dfdp else : raise ValueError ( f \"curvename=' { curvename } ' not recognized\" ) uncertainty_spline : Callable = CubicSpline ( ph_samplepoints , de_samplepoints ) else : uncertainty_spline = np . zeros_like return EnergyCalibration ( self . ph , self . energy , self . dph , self . de , self . names , curvename = curvename , approximating = approximate , spline = internal_spline , energy2ph = energy2ph , ph2uncertainty = uncertainty_spline , input_transform = input_transform , output_transform = output_transform , extra_info = extra_info , ) def drop_one_errors ( self , curvename : Curvetypes = Curvetypes . LOGLOG , approximate : bool = False , powerlaw : float = 1.15 ) -> tuple [ NDArray [ np . float64 ], NDArray [ np . float64 ]]: \"\"\"For each calibration point, calculate the difference between the 'correct' energy and the energy predicted by creating a calibration without that point and using ph2energy to calculate the predicted energy Parameters ---------- curvename : Curvetypes, optional Calibration curve type to employ, by default Curvetypes.LOGLOG approximate : bool, optional Whether to approximate the anchor point data given the uncertainties, by default False powerlaw : float, optional An approximate powerlaw guess used by LOGLOG curves, by default 1.15 Returns ------- tuple[NDArray[np.float64], NDArray[np.float64]] An array of the anchor point energies, and an array of the differences between the predicted and actual energies for each anchor point. \"\"\" # \"\"\"For each calibration point, calculate the difference between the 'correct' energy # and the energy predicted by creating a calibration without that point and using # ph2energy to calculate the predicted energy, return (energies, drop_one_energy_diff)\"\"\" drop_one_energy_diff = np . zeros ( self . npts ) for i in range ( self . npts ): dropped_pulseheight = self . ph [ i ] dropped_energy = self . energy [ i ] drop_one_maker = self . _remove_cal_point_idx ( i ) drop_one_cal = drop_one_maker . make_calibration ( curvename = curvename , approximate = approximate , powerlaw = powerlaw ) predicted_energy = drop_one_cal . ph2energy ( dropped_pulseheight ) . item ( 0 ) drop_one_energy_diff [ i ] = predicted_energy - dropped_energy return self . energy , drop_one_energy_diff","title":"EnergyCalibrationMaker"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.npts","text":"The number of calibration anchor points.","title":"npts"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.__post_init__","text":"Check for inputs of unequal length. Check for monotone anchor points. Sort the input data by energy. Source code in mass2/calibration/energy_calibration.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def __post_init__ ( self ) -> None : \"\"\"Check for inputs of unequal length. Check for monotone anchor points. Sort the input data by energy.\"\"\" N = len ( self . ph ) assert N == len ( self . energy ) assert N == len ( self . dph ) assert N == len ( self . de ) assert N == len ( self . names ) # First sort according to energy of the calibration point if not np . all ( np . diff ( self . energy ) > 0 ): sortkeys = np . argsort ( self . energy ) self . ph [:] = self . ph [ sortkeys ] self . energy [:] = self . energy [ sortkeys ] self . dph [:] = self . dph [ sortkeys ] self . de [:] = self . de [ sortkeys ] self . names [:] = [ self . names [ i ] for i in sortkeys ] # Then confirm that the pulse heights are also in order order_ph = self . ph . argsort () order_en = self . energy . argsort () if not np . all ( order_ph == order_en ): a = f \"PH: { self . ph [ order_ph ] } \" b = f \"Energy: { self . energy [ order_ph ] } \" raise ValueError ( f \"Calibration points are not monotone: \\n { a } \\n { b } \" )","title":"__post_init__"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.add_cal_point","text":"Add a single energy calibration point. Can call as .add_cal_point(ph, energy, name) or if the \"energy\" is a line name, then .add_cal_point(ph, name) will find energy as energy=mass2.STANDARD_FEATURES[name] . Thus the following are equivalent: cal = cal.add_cal_point(12345.6, 5898.801, \"Mn Ka1\") cal = cal.add_cal_point(12456.6, \"Mn Ka1\") ph must be in units of the self.ph_field and energy is in eV. ph_error is the 1-sigma uncertainty on the pulse height. If None (the default), then assign ph_error = ph /1000. e_error is the 1-sigma uncertainty on the energy itself. If None (the default), then assign e_error=0.01 eV. Careful! If you give a name that's already in the list, or you add an equivalent energy but do NOT give a name, then this value replaces the previous one. You can prevent overwriting (and instead raise an error) by setting replace =False. Source code in mass2/calibration/energy_calibration.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def add_cal_point ( self , ph : float , energy : float | str , name : str = \"\" , ph_error : float | None = None , e_error : float | None = None , replace : bool = True , ) -> EnergyCalibrationMaker : \"\"\"Add a single energy calibration point. Can call as .add_cal_point(ph, energy, name) or if the \"energy\" is a line name, then .add_cal_point(ph, name) will find energy as `energy=mass2.STANDARD_FEATURES[name]`. Thus the following are equivalent: cal = cal.add_cal_point(12345.6, 5898.801, \"Mn Ka1\") cal = cal.add_cal_point(12456.6, \"Mn Ka1\") `ph` must be in units of the self.ph_field and `energy` is in eV. `ph_error` is the 1-sigma uncertainty on the pulse height. If None (the default), then assign ph_error = `ph`/1000. `e_error` is the 1-sigma uncertainty on the energy itself. If None (the default), then assign e_error=0.01 eV. Careful! If you give a name that's already in the list, or you add an equivalent energy but do NOT give a name, then this value replaces the previous one. You can prevent overwriting (and instead raise an error) by setting `replace`=False. \"\"\" # If <energy> is a string and a known spectral feature's name, use it as the name instead # Otherwise, it needs to be a numeric type convertible to float. try : energy = float ( energy ) except ValueError : try : if type ( energy ) is str : name = energy else : name = str ( energy ) energy = STANDARD_FEATURES [ name ] except Exception : raise ValueError ( \"2nd argument must be an energy or a known name\" + \" from mass2.energy_calibration.STANDARD_FEATURES\" ) if ph_error is None : ph_error = ph * 0.001 if e_error is None : e_error = 0.01 # Assume 0.01 eV error if none given update_index : int | None = None if self . npts > 0 : if name and name in self . names : # Update an existing point by name if not replace : raise ValueError ( f \"Calibration point ' { name } ' is already known and overwrite is False\" ) update_index = self . names . index ( name ) elif np . abs ( energy - self . energy ) . min () <= e_error : # Update existing point if not replace : raise ValueError ( f \"Calibration point at energy { energy : .2f } eV is already known and overwrite is False\" ) update_index = int ( np . abs ( energy - self . energy ) . argmin ()) if update_index is None : # Add a new calibration anchor point new_ph = np . hstack (( self . ph , ph )) new_energy = np . hstack (( self . energy , energy )) new_dph = np . hstack (( self . dph , ph_error )) new_de = np . hstack (( self . de , e_error )) new_names = self . names + [ name ] else : # Replace an existing calibration anchor point. new_ph = self . ph . copy () new_energy = self . energy . copy () new_dph = self . dph . copy () new_de = self . de . copy () new_names = self . names . copy () new_ph [ update_index ] = ph new_energy [ update_index ] = energy new_dph [ update_index ] = ph_error new_de [ update_index ] = e_error new_names [ update_index ] = name return EnergyCalibrationMaker ( new_ph , new_energy , new_dph , new_de , new_names )","title":"add_cal_point"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.drop_one_errors","text":"For each calibration point, calculate the difference between the 'correct' energy and the energy predicted by creating a calibration without that point and using ph2energy to calculate the predicted energy Parameters: curvename ( Curvetypes , default: LOGLOG ) \u2013 Calibration curve type to employ, by default Curvetypes.LOGLOG approximate ( bool , default: False ) \u2013 Whether to approximate the anchor point data given the uncertainties, by default False powerlaw ( float , default: 1.15 ) \u2013 An approximate powerlaw guess used by LOGLOG curves, by default 1.15 Returns: tuple [ NDArray [ float64 ], NDArray [ float64 ]] \u2013 An array of the anchor point energies, and an array of the differences between the predicted and actual energies for each anchor point. Source code in mass2/calibration/energy_calibration.py 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def drop_one_errors ( self , curvename : Curvetypes = Curvetypes . LOGLOG , approximate : bool = False , powerlaw : float = 1.15 ) -> tuple [ NDArray [ np . float64 ], NDArray [ np . float64 ]]: \"\"\"For each calibration point, calculate the difference between the 'correct' energy and the energy predicted by creating a calibration without that point and using ph2energy to calculate the predicted energy Parameters ---------- curvename : Curvetypes, optional Calibration curve type to employ, by default Curvetypes.LOGLOG approximate : bool, optional Whether to approximate the anchor point data given the uncertainties, by default False powerlaw : float, optional An approximate powerlaw guess used by LOGLOG curves, by default 1.15 Returns ------- tuple[NDArray[np.float64], NDArray[np.float64]] An array of the anchor point energies, and an array of the differences between the predicted and actual energies for each anchor point. \"\"\" # \"\"\"For each calibration point, calculate the difference between the 'correct' energy # and the energy predicted by creating a calibration without that point and using # ph2energy to calculate the predicted energy, return (energies, drop_one_energy_diff)\"\"\" drop_one_energy_diff = np . zeros ( self . npts ) for i in range ( self . npts ): dropped_pulseheight = self . ph [ i ] dropped_energy = self . energy [ i ] drop_one_maker = self . _remove_cal_point_idx ( i ) drop_one_cal = drop_one_maker . make_calibration ( curvename = curvename , approximate = approximate , powerlaw = powerlaw ) predicted_energy = drop_one_cal . ph2energy ( dropped_pulseheight ) . item ( 0 ) drop_one_energy_diff [ i ] = predicted_energy - dropped_energy return self . energy , drop_one_energy_diff","title":"drop_one_errors"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.heuristic_samplepoints","text":"Given a set of calibration anchor points, return a few hundred sample points, reasonably spaced below, between, and above the anchor points. Parameters: anchors ( ArrayLike ) \u2013 The anchor points (in pulse height space) Returns: ndarray \u2013 description Source code in mass2/calibration/energy_calibration.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 @staticmethod def heuristic_samplepoints ( anchors : ArrayLike ) -> np . ndarray : \"\"\"Given a set of calibration anchor points, return a few hundred sample points, reasonably spaced below, between, and above the anchor points. Parameters ---------- anchors : ArrayLike The anchor points (in pulse height space) Returns ------- np.ndarray _description_ \"\"\" anchors = np . asarray ( anchors ) # Prescription is 50 points up to lowest anchor (but exclude 0): x = [ np . linspace ( 0 , anchors . min (), 51 )[ 1 :]] # Then one points, plus one extra per 1% spacing between (and at) each anchor for i in range ( len ( anchors ) - 1 ): low , high = anchors [ i : i + 2 ] n = 1 + int ( 100 * ( high / low - 1 ) + 0.5 ) x . append ( np . linspace ( low , high , n + 1 )[ 1 :]) # Finally, 100 more points between the highest anchor and 2x that. x . append ( anchors . max () * np . linspace ( 1 , 2 , 101 )[ 1 :]) return np . hstack ( x )","title":"heuristic_samplepoints"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.init","text":"Create an EnergyCalibrationMaker, filling in any missing requirements with empty arrays. Source code in mass2/calibration/energy_calibration.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 @classmethod def init ( cls , ph : ArrayLike | None = None , energy : ArrayLike | None = None , dph : ArrayLike | None = None , de : ArrayLike | None = None , names : list [ str ] | None = None , ) -> EnergyCalibrationMaker : \"\"\"Create an EnergyCalibrationMaker, filling in any missing requirements with empty arrays.\"\"\" if ph is None : ph = np . array ([], dtype = float ) else : ph = np . asarray ( ph ) if energy is None : energy = np . array ([], dtype = float ) else : energy = np . asarray ( energy ) if dph is None : dph = 1e-3 * ph else : dph = np . asarray ( dph ) if de is None : de = 1e-3 * energy else : de = np . asarray ( de ) if names is None : names = [ \"dummy\" ] * len ( dph ) return cls ( ph , energy , dph , de , names )","title":"init"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.make_calibration","text":"Create an energy calibration curve of the specified type. Parameters: curvename ( Curvetypes , default: LOGLOG ) \u2013 Which curve type to use, by default Curvetypes.LOGLOG approximate ( bool , default: False ) \u2013 Whether to approximate the anchor point data given the uncertainties, by default False powerlaw ( float , default: 1.15 ) \u2013 An approximate powerlaw guess used by LOGLOG curves, by default 1.15 extra_info ( dict [ str , Any ] | None , default: None ) \u2013 Extra text to store in the result, by default None Returns: EnergyCalibration \u2013 The calibration object. Raises: ValueError \u2013 If there are too few anchor points for an approximating curve, or if curvename is not in Curvetypes . Source code in mass2/calibration/energy_calibration.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def make_calibration ( self , curvename : Curvetypes = Curvetypes . LOGLOG , approximate : bool = False , powerlaw : float = 1.15 , extra_info : dict [ str , Any ] | None = None , ) -> EnergyCalibration : \"\"\"Create an energy calibration curve of the specified type. Parameters ---------- curvename : Curvetypes, optional Which curve type to use, by default Curvetypes.LOGLOG approximate : bool, optional Whether to approximate the anchor point data given the uncertainties, by default False powerlaw : float, optional An approximate powerlaw guess used by LOGLOG curves, by default 1.15 extra_info : dict[str, Any] | None, optional Extra text to store in the result, by default None Returns ------- EnergyCalibration The calibration object. Raises ------ ValueError If there are too few anchor points for an approximating curve, or if `curvename` is not in `Curvetypes`. \"\"\" if approximate and self . npts < 3 : raise ValueError ( f \"approximating curves require 3 or more cal anchor points, have { self . npts } \" ) if curvename not in Curvetypes : raise ValueError ( f \" { curvename =} , must be in { Curvetypes } \" ) # Use a heuristic to repair negative uncertainties. def regularize_uncertainties ( x : NDArray [ np . float64 ]) -> np . ndarray : \"\"\"Replace negative uncertainties with the minimum non-negative uncertainty, or zero.\"\"\" if not np . any ( x < 0 ): return x target = max ( 0.0 , x . min ()) x = x . copy () x [ x < 0 ] = target return x dph = regularize_uncertainties ( self . dph ) de = regularize_uncertainties ( self . de ) if curvename == Curvetypes . LOGLOG : input_transform = EnergyCalibration . _ecal_input_log output_transform = EnergyCalibration . _ecal_output_log x = np . log ( self . ph ) y = np . log ( self . energy ) # When there's only one point, enhance it by a fake point to enforce power-law behavior if self . npts == 1 : arboffset = 1.0 x = np . hstack ([ x , x + arboffset ]) y = np . hstack ([ y , y + arboffset / powerlaw ]) dx = dph / self . ph dy = de / self . energy elif curvename == Curvetypes . GAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_gain x = self . ph y = self . ph / self . energy # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * self . energy - 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph elif curvename == Curvetypes . INVGAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_invgain x = self . ph y = self . energy / self . ph # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * self . ph / y + 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph elif curvename in { Curvetypes . LINEAR , Curvetypes . LINEAR_PLUS_ZERO }: input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_identity x = self . ph y = self . energy dx = dph dy = de if ( curvename == Curvetypes . LINEAR_PLUS_ZERO ) and ( 0.0 not in x ): # Add a \"zero\"-energy and -PH point. But to avoid numerical problems, actually just use # 1e-3 times the lowest value, giving \u00b1100% uncertainty on the values. x = np . hstack (([ x . min () * 1e-3 ], x )) y = np . hstack (([ y . min () * 1e-3 ], y )) dx = np . hstack (([ x [ 0 ] * 1e-3 ], dx )) dy = np . hstack (( y [ 0 ] * 1e-3 , dy )) elif curvename == Curvetypes . LOGGAIN : input_transform = EnergyCalibration . _ecal_input_identity output_transform = EnergyCalibration . _ecal_output_loggain x = self . ph y = np . log ( self . ph / self . energy ) # Estimate spline uncertainties using slope of best-fit line slope = np . polyfit ( x , y , 1 )[ 0 ] dy = y * ((( slope * x - 1 ) * dph / x ) ** 2 + ( de / self . energy ) ** 2 ) ** 0.5 dx = dph else : raise ValueError ( f \"curvename=' { curvename } ' not recognized\" ) if approximate : internal_spline : CubicSpline = GPRSpline ( x , y , dy , dx ) elif len ( x ) > 1 : internal_spline = CubicSpline ( x , y ) else : internal_spline = CubicSpline ( x * [ 1 , 2 ], y * [ 1 , 2 ]) ph_samplepoints = EnergyCalibrationMaker . heuristic_samplepoints ( self . ph ) E_samplepoints = output_transform ( ph_samplepoints , internal_spline ( input_transform ( ph_samplepoints ))) energy2ph = CubicSpline ( E_samplepoints , ph_samplepoints ) if approximate : dspline = internal_spline . variance ( ph_samplepoints ) ** 0.5 if curvename == Curvetypes . LOGLOG : de_samplepoints = dspline * internal_spline ( input_transform ( ph_samplepoints )) elif curvename == Curvetypes . GAIN : de_samplepoints = dspline * E_samplepoints ** 2 / ph_samplepoints elif curvename == Curvetypes . INVGAIN : de_samplepoints = dspline * ph_samplepoints elif curvename in { Curvetypes . LINEAR , Curvetypes . LINEAR_PLUS_ZERO }: de_samplepoints = dspline elif curvename == Curvetypes . LOGGAIN : abs_dfdp = np . abs ( internal_spline ( ph_samplepoints , der = 1 )) de_samplepoints = dspline * E_samplepoints * abs_dfdp else : raise ValueError ( f \"curvename=' { curvename } ' not recognized\" ) uncertainty_spline : Callable = CubicSpline ( ph_samplepoints , de_samplepoints ) else : uncertainty_spline = np . zeros_like return EnergyCalibration ( self . ph , self . energy , self . dph , self . de , self . names , curvename = curvename , approximating = approximate , spline = internal_spline , energy2ph = energy2ph , ph2uncertainty = uncertainty_spline , input_transform = input_transform , output_transform = output_transform , extra_info = extra_info , )","title":"make_calibration"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.make_calibration_gain","text":"Create a calibration curve that is a spline in (pulse height/energy) vs pulse height. Source code in mass2/calibration/energy_calibration.py 263 264 265 def make_calibration_gain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in (pulse height/energy) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . GAIN , approximate = approximate , extra_info = extra_info )","title":"make_calibration_gain"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.make_calibration_invgain","text":"Create a calibration curve that is a spline in (energy/pulse height) vs pulse height. Source code in mass2/calibration/energy_calibration.py 267 268 269 def make_calibration_invgain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in (energy/pulse height) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . INVGAIN , approximate = approximate , extra_info = extra_info )","title":"make_calibration_invgain"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.make_calibration_linear","text":"Create a calibration curve that is a spline in energy vs pulse height. If addzero include a (0,0) anchor point. Source code in mass2/calibration/energy_calibration.py 275 276 277 278 279 280 def make_calibration_linear ( self , approximate : bool = False , addzero : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in energy vs pulse height. If `addzero` include a (0,0) anchor point.\"\"\" curvename = Curvetypes . LINEAR_PLUS_ZERO if addzero else Curvetypes . LINEAR return self . make_calibration ( curvename , approximate = approximate , extra_info = extra_info )","title":"make_calibration_linear"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.make_calibration_loggain","text":"Create a calibration curve that is a spline in log(pulse height/energy) vs pulse height. Source code in mass2/calibration/energy_calibration.py 271 272 273 def make_calibration_loggain ( self , approximate : bool = False , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in log(pulse height/energy) vs pulse height.\"\"\" return self . make_calibration ( Curvetypes . LOGGAIN , approximate = approximate , extra_info = extra_info )","title":"make_calibration_loggain"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.make_calibration_loglog","text":"Create a calibration curve that is a spline in log(energy) vs log(pulse height). Source code in mass2/calibration/energy_calibration.py 257 258 259 260 261 def make_calibration_loglog ( self , approximate : bool = False , powerlaw : float = 1.15 , extra_info : dict [ str , Any ] | None = None ) -> EnergyCalibration : \"\"\"Create a calibration curve that is a spline in log(energy) vs log(pulse height).\"\"\" return self . make_calibration ( Curvetypes . LOGLOG , approximate = approximate , powerlaw = powerlaw , extra_info = extra_info )","title":"make_calibration_loglog"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.remove_cal_point_energy","text":"Remove cal points at energies within \u00b1 de of energy . Return a new maker. Source code in mass2/calibration/energy_calibration.py 143 144 145 146 147 148 149 def remove_cal_point_energy ( self , energy : float , de : float ) -> EnergyCalibrationMaker : \"\"\"Remove cal points at energies within \u00b1`de` of `energy`. Return a new maker.\"\"\" idxs = np . nonzero ( np . abs ( self . energy - energy ) < de )[ 0 ] if len ( idxs ) == 0 : return self # Also recursive and less efficient. See previous method's comment. return self . _remove_cal_point_idx ( idxs [ 0 ]) . remove_cal_point_energy ( energy , de )","title":"remove_cal_point_energy"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.remove_cal_point_name","text":"Remove calibration point named name . Return a new maker. Source code in mass2/calibration/energy_calibration.py 128 129 130 131 def remove_cal_point_name ( self , name : str ) -> EnergyCalibrationMaker : \"\"\"Remove calibration point named `name`. Return a new maker.\"\"\" idx = self . names . index ( name ) return self . _remove_cal_point_idx ( idx )","title":"remove_cal_point_name"},{"location":"docstrings2/#mass2.calibration.energy_calibration.EnergyCalibrationMaker.remove_cal_point_prefix","text":"This removes all cal points whose name starts with prefix . Return a new maker. Source code in mass2/calibration/energy_calibration.py 133 134 135 136 137 138 139 140 141 def remove_cal_point_prefix ( self , prefix : str ) -> EnergyCalibrationMaker : \"\"\"This removes all cal points whose name starts with `prefix`. Return a new maker.\"\"\" # Work recursively: remove the first match and make a new Maker, and repeat until none match. # This is clearly less efficient when removing N matches, as N copies are made. So what? # This feature is likely to be rarely used, and we favor clarity over performance here. for name in tuple ( self . names ): if name . startswith ( prefix ): return self . remove_cal_point_name ( name ) . remove_cal_point_prefix ( prefix ) return self","title":"remove_cal_point_prefix"},{"location":"docstrings2/#fluorescence-line-shapes","text":"fluorescence_lines.py Tools for fitting and simulating X-ray fluorescence lines.","title":"Fluorescence line shapes"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.AmplitudeType","text":"Bases: Enum AmplitudeType: which form of amplitude is used in the reference data. Source code in mass2/calibration/fluorescence_lines.py 76 77 78 79 80 81 class AmplitudeType ( Enum ): \"\"\"AmplitudeType: which form of amplitude is used in the reference data.\"\"\" LORENTZIAN_PEAK_HEIGHT = \"Peak height of Lorentzians\" LORENTZIAN_INTEGRAL_INTENSITY = \"Integrated intensity of Lorentzians\" VOIGT_PEAK_HEIGHT = \"Peak height of Voigts\"","title":"AmplitudeType"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.LineshapeReference","text":"Description of our source of information on a line shape. Might be a reference to the literature, or notes on conversations. They are stored in a YAML file mass2/data/fluorescence_line_references.yaml Source code in mass2/calibration/fluorescence_lines.py 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 @dataclass ( frozen = True ) class LineshapeReference : \"\"\"Description of our source of information on a line shape. Might be a reference to the literature, or notes on conversations. They are stored in a YAML file mass2/data/fluorescence_line_references.yaml \"\"\" tag : str description : str url : str @classmethod def load ( cls , filename : pathlib . Path | str | None = None ) -> dict : \"\"\"Load the reference comments from a YAML file. If filename is None, load the default file in mass2/data. Parameters ---------- filename : pathlib.Path | str | None, optional The file to read containing reference comments, by default None Returns ------- dict A dictionary of LineshapeReference objects, keyed by their tag. \"\"\" references = { \"unknown\" : LineshapeReference ( \"unknown\" , \"unknown\" , \"\" )} if filename is None : filename = str ( pkg_resources . files ( \"mass2\" ) / \"data\" / \"fluorescence_line_references.yaml\" ) with open ( filename , \"r\" , encoding = \"utf-8\" ) as file : d = yaml . safe_load ( file ) for item in d : url = item . get ( \"URL\" , \"\" ) references [ item [ \"tag\" ]] = LineshapeReference ( item [ \"tag\" ], item [ \"description\" ], url ) return references def __str__ ( self ) -> str : \"\"\"The citation string for this reference.\"\"\" lines = [ f 'lineshape_references[\" { self . tag } \"]:' ] lines . append ( self . description . rstrip ( \" \\n \" )) if len ( self . url ) > 1 : lines . append ( f \"url: { self . url } \" ) return \" \\n \" . join ( lines )","title":"LineshapeReference"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.LineshapeReference.__str__","text":"The citation string for this reference. Source code in mass2/calibration/fluorescence_lines.py 427 428 429 430 431 432 433 def __str__ ( self ) -> str : \"\"\"The citation string for this reference.\"\"\" lines = [ f 'lineshape_references[\" { self . tag } \"]:' ] lines . append ( self . description . rstrip ( \" \\n \" )) if len ( self . url ) > 1 : lines . append ( f \"url: { self . url } \" ) return \" \\n \" . join ( lines )","title":"__str__"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.LineshapeReference.load","text":"Load the reference comments from a YAML file. If filename is None, load the default file in mass2/data. Parameters: filename ( Path | str | None , default: None ) \u2013 The file to read containing reference comments, by default None Returns: dict \u2013 A dictionary of LineshapeReference objects, keyed by their tag. Source code in mass2/calibration/fluorescence_lines.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 @classmethod def load ( cls , filename : pathlib . Path | str | None = None ) -> dict : \"\"\"Load the reference comments from a YAML file. If filename is None, load the default file in mass2/data. Parameters ---------- filename : pathlib.Path | str | None, optional The file to read containing reference comments, by default None Returns ------- dict A dictionary of LineshapeReference objects, keyed by their tag. \"\"\" references = { \"unknown\" : LineshapeReference ( \"unknown\" , \"unknown\" , \"\" )} if filename is None : filename = str ( pkg_resources . files ( \"mass2\" ) / \"data\" / \"fluorescence_line_references.yaml\" ) with open ( filename , \"r\" , encoding = \"utf-8\" ) as file : d = yaml . safe_load ( file ) for item in d : url = item . get ( \"URL\" , \"\" ) references [ item [ \"tag\" ]] = LineshapeReference ( item [ \"tag\" ], item [ \"description\" ], url ) return references","title":"load"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine","text":"An abstract base class for modeling spectral lines as a sum of Voigt profiles (i.e., Gaussian-convolved Lorentzians). Call SpectralLine.addline(...) to create a new instance. The API follows scipy.stats.stats.rv_continuous and is kind of like rv_frozen. Calling this object with an argument evalutes the pdf at the argument, it does not return an rv_frozen. So far, we ony define rvs and pdf . Source code in mass2/calibration/fluorescence_lines.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 @dataclass ( frozen = True ) class SpectralLine : \"\"\"An abstract base class for modeling spectral lines as a sum of Voigt profiles (i.e., Gaussian-convolved Lorentzians). Call `SpectralLine.addline(...)` to create a new instance. The API follows scipy.stats.stats.rv_continuous and is kind of like rv_frozen. Calling this object with an argument evalutes the pdf at the argument, it does not return an rv_frozen. So far, we ony define `rvs` and `pdf`. \"\"\" element : str material : str linetype : str nominal_peak_energy : float energies : NDArray [ np . float64 ] lorentzian_fwhm : NDArray [ np . float64 ] reference_amplitude : NDArray [ np . float64 ] reference_amplitude_type : AmplitudeType = AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY reference_measurement_type : str | None = \"unknown\" intrinsic_sigma : float = 0.0 reference_plot_instrument_gaussian_fwhm : float | None = 0.0 reference_short : str = \"unknown\" position_uncertainty : float = 0.0 is_default_material : bool = True @cached_property def peak_energy ( self ) -> float : \"\"\"Find the peak energy of the line shape assuming ideal instrument resolution.\"\"\" try : peak_energy = sp . optimize . brent ( lambda x : - self . pdf ( x , instrument_gaussian_fwhm = 0 ), brack = np . array (( 0.5 , 1 , 1.5 )) * self . nominal_peak_energy ) except ValueError : peak_energy = self . nominal_peak_energy return peak_energy @property def cumulative_amplitudes ( self ) -> NDArray : \"\"\"Cumulative sum of the Lorentzian integral intensities.\"\"\" return self . lorentzian_integral_intensity . cumsum () @cached_property def lorentzian_integral_intensity ( self ) -> NDArray : \"\"\"Return (and cache) computed integrated intensities of the Lorentzian components.\"\"\" if self . reference_amplitude_type == AmplitudeType . VOIGT_PEAK_HEIGHT : sigma = self . reference_plot_instrument_gaussian_fwhm / FWHM_OVER_SIGMA return np . array ([ ph / voigt ( 0 , 0 , fwhm / 2.0 , sigma ) for ( ph , fwhm ) in zip ( self . reference_amplitude , self . lorentzian_fwhm ) ]) if self . reference_amplitude_type == AmplitudeType . LORENTZIAN_PEAK_HEIGHT : return self . reference_amplitude * ( 0.5 * np . pi * self . lorentzian_fwhm ) if self . reference_amplitude_type == AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY : return self . reference_amplitude @cached_property def normalized_lorentzian_integral_intensity ( self ) -> NDArray : \"\"\"Return (and cache) computed integrated intensities of the Lorentzian components, normalized so sum=1.\"\"\" x = self . lorentzian_integral_intensity return x / np . sum ( x ) @cached_property def lorentz_amplitude ( self ) -> NDArray : \"\"\"Return (and cache) computed Lorentzian peak heights of the components.\"\"\" return self . lorentzian_integral_intensity / self . lorentzian_fwhm def __call__ ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> NDArray : \"\"\"Make the class callable, returning the same value as the self.pdf method.\"\"\" return self . pdf ( x , instrument_gaussian_fwhm ) def pdf ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> NDArray : \"\"\"Spectrum (units of fraction per eV) as a function of <x>, the energy in eV\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) x = np . asarray ( x , dtype = float ) result = np . zeros_like ( x ) for energy , fwhm , ampl in zip ( self . energies , self . lorentzian_fwhm , self . normalized_lorentzian_integral_intensity ): result += ampl * voigt ( x , energy , hwhm = fwhm * 0.5 , sigma = gaussian_sigma ) # mass2.voigt() is normalized to have unit integrated intensity return result def components ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> list [ NDArray ]: \"\"\"List of spectrum components as a function of <x>, the energy in eV\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) x = np . asarray ( x , dtype = float ) components = [] for energy , fwhm , ampl in zip ( self . energies , self . lorentzian_fwhm , self . lorentzian_integral_intensity ): components . append ( ampl * voigt ( x , energy , hwhm = fwhm * 0.5 , sigma = gaussian_sigma )) return components def plot ( self , x : ArrayLike | None = None , instrument_gaussian_fwhm : float = 0 , axis : plt . Axes | None = None , components : bool = True , label : str | None = None , setylim : bool = True , color : str | None = None , ) -> plt . Axes : \"\"\"Plot the spectrum. x - np array of energy in eV to plot at (sensible default) axis - axis to plot on (default creates new figure) components - True plots each voigt component in addition to the spectrum label - a string to label the plot with (optional)\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) if x is None : width = max ( 2 * gaussian_sigma , 3 * float ( np . amax ( self . lorentzian_fwhm ))) lo = np . amin ( self . energies ) - width hi = np . amax ( self . energies ) + width x = np . linspace ( lo , hi , 500 ) x = np . asarray ( x ) if axis is None : plt . figure () axis = plt . gca () if components : for component in self . components ( x , instrument_gaussian_fwhm ): axis . plot ( x , component , \"--\" ) pdf = self . pdf ( x , instrument_gaussian_fwhm ) axis . plot ( x , pdf , lw = 2 , label = label , color = color ) axis . set_xlabel ( \"Energy (eV)\" ) axis . set_ylabel ( f \"Counts per { float ( x [ 1 ] - x [ 0 ]) : .2 } eV bin\" ) axis . set_xlim ( x [ 0 ], x [ - 1 ]) if setylim : axis . set_ylim ( 0 , np . amax ( pdf ) * 1.05 ) axis . set_title ( f \" { self . shortname } with resolution { instrument_gaussian_fwhm : .2f } eV FWHM\" ) return axis def plot_like_reference ( self , axis : plt . Axes | None = None ) -> plt . Axes : \"\"\"Plot the spectrum to match the instrument resolution used in the reference data publication, if known.\"\"\" if self . reference_plot_instrument_gaussian_fwhm is None : fwhm = 0.001 else : fwhm = self . reference_plot_instrument_gaussian_fwhm axis = self . plot ( axis = axis , instrument_gaussian_fwhm = fwhm ) return axis def rvs ( self , size : int | tuple [ int ] | None , instrument_gaussian_fwhm : float , rng : np . random . Generator | None = None ) -> NDArray : \"\"\"The CDF and PPF (cumulative distribution and percentile point functions) are hard to compute. But it's easy enough to generate the random variates themselves, so we override that method.\"\"\" if rng is None : rng = _rng gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) # Choose from among the N Lorentzian lines in proportion to the line amplitudes iline = self . cumulative_amplitudes . searchsorted ( rng . uniform ( 0 , self . cumulative_amplitudes [ - 1 ], size = size )) # Choose Lorentzian variates of the appropriate width (but centered on 0) lor = rng . standard_cauchy ( size = size ) * self . lorentzian_fwhm [ iline ] * 0.5 # If necessary, add a Gaussian variate to mimic finite resolution if gaussian_sigma > 0.0 : lor += rng . standard_normal ( size = size ) * gaussian_sigma # Finally, add the line centers. results = lor + self . energies [ iline ] # We must check for non-positive results and replace them by recursive call # to self.rvs(). not_positive = results <= 0.0 if np . any ( not_positive ): Nbad = not_positive . sum () results [ not_positive ] = self . rvs ( size = Nbad , instrument_gaussian_fwhm = instrument_gaussian_fwhm ) return results @property def shortname ( self ) -> str : \"\"\"A short name for the line, suitable for use as a dictionary key.\"\"\" if self . is_default_material : return f \" { self . element }{ self . linetype } \" else : return f \" { self . element }{ self . linetype } _ { self . material } \" @property def reference ( self ) -> str : \"\"\"The full comment and/or citation for the reference data.\"\"\" return lineshape_references [ self . reference_short ] def _gaussian_sigma ( self , instrument_gaussian_fwhm : float ) -> float : \"\"\"combined intrinstic_sigma and insturment_gaussian_fwhm in quadrature and return the result\"\"\" assert instrument_gaussian_fwhm >= 0 return (( instrument_gaussian_fwhm / FWHM_OVER_SIGMA ) ** 2 + self . intrinsic_sigma ** 2 ) ** 0.5 def __repr__ ( self ) -> str : \"\"\"String representation of the SpectralLine.\"\"\" return f \"SpectralLine: { self . shortname } \" def model ( self , has_linear_background : bool = True , has_tails : bool = False , prefix : str = \"\" , qemodel : Callable | None = None ) -> GenericLineModel : \"\"\"Generate a LineModel instance from a SpectralLine\"\"\" model_class = GenericLineModel name = f \" { self . element }{ self . linetype } \" m = model_class ( name = name , spect = self , has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix , qemodel = qemodel ) return m def fitter ( self ) -> GenericLineModel : \"\"\"Generate a GenericLineModel instance for fitting this SpectralLine.\"\"\" fitter_class = GenericLineModel f = fitter_class ( self ) f . name = f \" { self . element }{ self . linetype } \" return f def minimum_fwhm ( self , instrument_gaussian_fwhm : float ) -> float : \"\"\"for the narrowest lorentzian in the line model, calculate the combined fwhm including the lorentzian, intrinstic_sigma, and instrument_gaussian_fwhm\"\"\" fwhm2 = np . amin ( self . lorentzian_fwhm ) ** 2 + instrument_gaussian_fwhm ** 2 + ( self . intrinsic_sigma * FWHM_OVER_SIGMA ) ** 2 return np . sqrt ( fwhm2 ) @classmethod def quick_monochromatic_line ( cls , name : str , energy : float , lorentzian_fwhm : float , intrinsic_sigma : float = 0.0 ) -> \"SpectralLine\" : \"\"\" Create a quick monochromatic line. Intended for use in calibration when we know a line energy, but not a lineshape model. Returns and instrance of SpectralLine with most fields having contents like \"unknown: quick_line\". The line will have a single lorentzian element with the given energy, fwhm, and intrinsic_sigma values. \"\"\" energy = float ( energy ) element = name material = \"unknown: quick_line\" if lorentzian_fwhm <= 0 and intrinsic_sigma <= 1e-6 : intrinsic_sigma = 1e-6 linetype = \"Gaussian\" reference_short = \"unknown: quick_line\" reference_amplitude = np . array ([ 1.0 ]) reference_amplitude_type = AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY nominal_peak_energy = energy position_uncertainty = 0.0 reference_measurement_type = \"unkown: quick_line\" return cls ( element = element , material = material , linetype = linetype , energies = np . array ([ energy ]), lorentzian_fwhm = np . array ([ lorentzian_fwhm ]), intrinsic_sigma = intrinsic_sigma , reference_short = reference_short , reference_amplitude = reference_amplitude , reference_amplitude_type = reference_amplitude_type , nominal_peak_energy = nominal_peak_energy , position_uncertainty = position_uncertainty , reference_measurement_type = reference_measurement_type , ) @classmethod def addline ( # noqa: PLR0917 cls , element : str , linetype : str , material : str , reference_short : str , reference_plot_instrument_gaussian_fwhm : float | None , nominal_peak_energy : float , energies : ArrayLike , lorentzian_fwhm : ArrayLike , reference_amplitude : ArrayLike , reference_amplitude_type : AmplitudeType , ka12_energy_diff : float | None = None , position_uncertainty : float = np . nan , intrinsic_sigma : float = 0 , reference_measurement_type : str | None = None , is_default_material : bool = True , allow_replacement : bool = True , ) -> \"SpectralLine\" : \"\"\"Add a new SpectralLine to the `mass2.fluorescence_lines.spectra` dictionary, and as a variable in this module.\"\"\" # require exactly one method of specifying the amplitude of each component assert reference_amplitude_type in { AmplitudeType . LORENTZIAN_PEAK_HEIGHT , AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY , AmplitudeType . VOIGT_PEAK_HEIGHT , } # require the reference exists in lineshape_references assert reference_short in lineshape_references # require kalpha lines to have ka12_energy_diff if linetype . startswith ( \"KAlpha\" ) and ka12_energy_diff is not None : ka12_energy_diff = float ( ka12_energy_diff ) # require reference_plot_instrument_gaussian_fwhm to be a float or None assert reference_plot_instrument_gaussian_fwhm is None or isinstance ( reference_plot_instrument_gaussian_fwhm , float ) line = cls ( element = element , material = material , linetype = linetype , nominal_peak_energy = float ( nominal_peak_energy ), energies = np . array ( energies ), lorentzian_fwhm = np . array ( lorentzian_fwhm ), reference_amplitude = np . array ( reference_amplitude ), reference_amplitude_type = reference_amplitude_type , reference_measurement_type = reference_measurement_type , intrinsic_sigma = intrinsic_sigma , reference_plot_instrument_gaussian_fwhm = reference_plot_instrument_gaussian_fwhm , reference_short = reference_short , position_uncertainty = float ( position_uncertainty ), is_default_material = is_default_material , ) name = line . shortname if name in spectra . keys () and ( not allow_replacement ): raise ValueError ( f \"spectrum { name } already exists\" ) # Add this SpectralLine to spectra dict AND make it be a variable in the module spectra [ name ] = line globals ()[ name ] = line return line","title":"SpectralLine"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.cumulative_amplitudes","text":"Cumulative sum of the Lorentzian integral intensities.","title":"cumulative_amplitudes"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.lorentz_amplitude","text":"Return (and cache) computed Lorentzian peak heights of the components.","title":"lorentz_amplitude"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.lorentzian_integral_intensity","text":"Return (and cache) computed integrated intensities of the Lorentzian components.","title":"lorentzian_integral_intensity"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.normalized_lorentzian_integral_intensity","text":"Return (and cache) computed integrated intensities of the Lorentzian components, normalized so sum=1.","title":"normalized_lorentzian_integral_intensity"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.peak_energy","text":"Find the peak energy of the line shape assuming ideal instrument resolution.","title":"peak_energy"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.reference","text":"The full comment and/or citation for the reference data.","title":"reference"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.shortname","text":"A short name for the line, suitable for use as a dictionary key.","title":"shortname"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.__call__","text":"Make the class callable, returning the same value as the self.pdf method. Source code in mass2/calibration/fluorescence_lines.py 152 153 154 def __call__ ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> NDArray : \"\"\"Make the class callable, returning the same value as the self.pdf method.\"\"\" return self . pdf ( x , instrument_gaussian_fwhm )","title":"__call__"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.__repr__","text":"String representation of the SpectralLine. Source code in mass2/calibration/fluorescence_lines.py 264 265 266 def __repr__ ( self ) -> str : \"\"\"String representation of the SpectralLine.\"\"\" return f \"SpectralLine: { self . shortname } \"","title":"__repr__"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.addline","text":"Add a new SpectralLine to the mass2.fluorescence_lines.spectra dictionary, and as a variable in this module. Source code in mass2/calibration/fluorescence_lines.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 @classmethod def addline ( # noqa: PLR0917 cls , element : str , linetype : str , material : str , reference_short : str , reference_plot_instrument_gaussian_fwhm : float | None , nominal_peak_energy : float , energies : ArrayLike , lorentzian_fwhm : ArrayLike , reference_amplitude : ArrayLike , reference_amplitude_type : AmplitudeType , ka12_energy_diff : float | None = None , position_uncertainty : float = np . nan , intrinsic_sigma : float = 0 , reference_measurement_type : str | None = None , is_default_material : bool = True , allow_replacement : bool = True , ) -> \"SpectralLine\" : \"\"\"Add a new SpectralLine to the `mass2.fluorescence_lines.spectra` dictionary, and as a variable in this module.\"\"\" # require exactly one method of specifying the amplitude of each component assert reference_amplitude_type in { AmplitudeType . LORENTZIAN_PEAK_HEIGHT , AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY , AmplitudeType . VOIGT_PEAK_HEIGHT , } # require the reference exists in lineshape_references assert reference_short in lineshape_references # require kalpha lines to have ka12_energy_diff if linetype . startswith ( \"KAlpha\" ) and ka12_energy_diff is not None : ka12_energy_diff = float ( ka12_energy_diff ) # require reference_plot_instrument_gaussian_fwhm to be a float or None assert reference_plot_instrument_gaussian_fwhm is None or isinstance ( reference_plot_instrument_gaussian_fwhm , float ) line = cls ( element = element , material = material , linetype = linetype , nominal_peak_energy = float ( nominal_peak_energy ), energies = np . array ( energies ), lorentzian_fwhm = np . array ( lorentzian_fwhm ), reference_amplitude = np . array ( reference_amplitude ), reference_amplitude_type = reference_amplitude_type , reference_measurement_type = reference_measurement_type , intrinsic_sigma = intrinsic_sigma , reference_plot_instrument_gaussian_fwhm = reference_plot_instrument_gaussian_fwhm , reference_short = reference_short , position_uncertainty = float ( position_uncertainty ), is_default_material = is_default_material , ) name = line . shortname if name in spectra . keys () and ( not allow_replacement ): raise ValueError ( f \"spectrum { name } already exists\" ) # Add this SpectralLine to spectra dict AND make it be a variable in the module spectra [ name ] = line globals ()[ name ] = line return line","title":"addline"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.components","text":"List of spectrum components as a function of , the energy in eV Source code in mass2/calibration/fluorescence_lines.py 166 167 168 169 170 171 172 173 def components ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> list [ NDArray ]: \"\"\"List of spectrum components as a function of <x>, the energy in eV\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) x = np . asarray ( x , dtype = float ) components = [] for energy , fwhm , ampl in zip ( self . energies , self . lorentzian_fwhm , self . lorentzian_integral_intensity ): components . append ( ampl * voigt ( x , energy , hwhm = fwhm * 0.5 , sigma = gaussian_sigma )) return components","title":"components"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.fitter","text":"Generate a GenericLineModel instance for fitting this SpectralLine. Source code in mass2/calibration/fluorescence_lines.py 279 280 281 282 283 284 def fitter ( self ) -> GenericLineModel : \"\"\"Generate a GenericLineModel instance for fitting this SpectralLine.\"\"\" fitter_class = GenericLineModel f = fitter_class ( self ) f . name = f \" { self . element }{ self . linetype } \" return f","title":"fitter"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.minimum_fwhm","text":"for the narrowest lorentzian in the line model, calculate the combined fwhm including the lorentzian, intrinstic_sigma, and instrument_gaussian_fwhm Source code in mass2/calibration/fluorescence_lines.py 286 287 288 289 290 def minimum_fwhm ( self , instrument_gaussian_fwhm : float ) -> float : \"\"\"for the narrowest lorentzian in the line model, calculate the combined fwhm including the lorentzian, intrinstic_sigma, and instrument_gaussian_fwhm\"\"\" fwhm2 = np . amin ( self . lorentzian_fwhm ) ** 2 + instrument_gaussian_fwhm ** 2 + ( self . intrinsic_sigma * FWHM_OVER_SIGMA ) ** 2 return np . sqrt ( fwhm2 )","title":"minimum_fwhm"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.model","text":"Generate a LineModel instance from a SpectralLine Source code in mass2/calibration/fluorescence_lines.py 268 269 270 271 272 273 274 275 276 277 def model ( self , has_linear_background : bool = True , has_tails : bool = False , prefix : str = \"\" , qemodel : Callable | None = None ) -> GenericLineModel : \"\"\"Generate a LineModel instance from a SpectralLine\"\"\" model_class = GenericLineModel name = f \" { self . element }{ self . linetype } \" m = model_class ( name = name , spect = self , has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix , qemodel = qemodel ) return m","title":"model"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.pdf","text":"Spectrum (units of fraction per eV) as a function of , the energy in eV Source code in mass2/calibration/fluorescence_lines.py 156 157 158 159 160 161 162 163 164 def pdf ( self , x : ArrayLike , instrument_gaussian_fwhm : float ) -> NDArray : \"\"\"Spectrum (units of fraction per eV) as a function of <x>, the energy in eV\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) x = np . asarray ( x , dtype = float ) result = np . zeros_like ( x ) for energy , fwhm , ampl in zip ( self . energies , self . lorentzian_fwhm , self . normalized_lorentzian_integral_intensity ): result += ampl * voigt ( x , energy , hwhm = fwhm * 0.5 , sigma = gaussian_sigma ) # mass2.voigt() is normalized to have unit integrated intensity return result","title":"pdf"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.plot","text":"Plot the spectrum. x - np array of energy in eV to plot at (sensible default) axis - axis to plot on (default creates new figure) components - True plots each voigt component in addition to the spectrum label - a string to label the plot with (optional) Source code in mass2/calibration/fluorescence_lines.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def plot ( self , x : ArrayLike | None = None , instrument_gaussian_fwhm : float = 0 , axis : plt . Axes | None = None , components : bool = True , label : str | None = None , setylim : bool = True , color : str | None = None , ) -> plt . Axes : \"\"\"Plot the spectrum. x - np array of energy in eV to plot at (sensible default) axis - axis to plot on (default creates new figure) components - True plots each voigt component in addition to the spectrum label - a string to label the plot with (optional)\"\"\" gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) if x is None : width = max ( 2 * gaussian_sigma , 3 * float ( np . amax ( self . lorentzian_fwhm ))) lo = np . amin ( self . energies ) - width hi = np . amax ( self . energies ) + width x = np . linspace ( lo , hi , 500 ) x = np . asarray ( x ) if axis is None : plt . figure () axis = plt . gca () if components : for component in self . components ( x , instrument_gaussian_fwhm ): axis . plot ( x , component , \"--\" ) pdf = self . pdf ( x , instrument_gaussian_fwhm ) axis . plot ( x , pdf , lw = 2 , label = label , color = color ) axis . set_xlabel ( \"Energy (eV)\" ) axis . set_ylabel ( f \"Counts per { float ( x [ 1 ] - x [ 0 ]) : .2 } eV bin\" ) axis . set_xlim ( x [ 0 ], x [ - 1 ]) if setylim : axis . set_ylim ( 0 , np . amax ( pdf ) * 1.05 ) axis . set_title ( f \" { self . shortname } with resolution { instrument_gaussian_fwhm : .2f } eV FWHM\" ) return axis","title":"plot"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.plot_like_reference","text":"Plot the spectrum to match the instrument resolution used in the reference data publication, if known. Source code in mass2/calibration/fluorescence_lines.py 213 214 215 216 217 218 219 220 def plot_like_reference ( self , axis : plt . Axes | None = None ) -> plt . Axes : \"\"\"Plot the spectrum to match the instrument resolution used in the reference data publication, if known.\"\"\" if self . reference_plot_instrument_gaussian_fwhm is None : fwhm = 0.001 else : fwhm = self . reference_plot_instrument_gaussian_fwhm axis = self . plot ( axis = axis , instrument_gaussian_fwhm = fwhm ) return axis","title":"plot_like_reference"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.quick_monochromatic_line","text":"Create a quick monochromatic line. Intended for use in calibration when we know a line energy, but not a lineshape model. Returns and instrance of SpectralLine with most fields having contents like \"unknown: quick_line\". The line will have a single lorentzian element with the given energy, fwhm, and intrinsic_sigma values. Source code in mass2/calibration/fluorescence_lines.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 @classmethod def quick_monochromatic_line ( cls , name : str , energy : float , lorentzian_fwhm : float , intrinsic_sigma : float = 0.0 ) -> \"SpectralLine\" : \"\"\" Create a quick monochromatic line. Intended for use in calibration when we know a line energy, but not a lineshape model. Returns and instrance of SpectralLine with most fields having contents like \"unknown: quick_line\". The line will have a single lorentzian element with the given energy, fwhm, and intrinsic_sigma values. \"\"\" energy = float ( energy ) element = name material = \"unknown: quick_line\" if lorentzian_fwhm <= 0 and intrinsic_sigma <= 1e-6 : intrinsic_sigma = 1e-6 linetype = \"Gaussian\" reference_short = \"unknown: quick_line\" reference_amplitude = np . array ([ 1.0 ]) reference_amplitude_type = AmplitudeType . LORENTZIAN_INTEGRAL_INTENSITY nominal_peak_energy = energy position_uncertainty = 0.0 reference_measurement_type = \"unkown: quick_line\" return cls ( element = element , material = material , linetype = linetype , energies = np . array ([ energy ]), lorentzian_fwhm = np . array ([ lorentzian_fwhm ]), intrinsic_sigma = intrinsic_sigma , reference_short = reference_short , reference_amplitude = reference_amplitude , reference_amplitude_type = reference_amplitude_type , nominal_peak_energy = nominal_peak_energy , position_uncertainty = position_uncertainty , reference_measurement_type = reference_measurement_type , )","title":"quick_monochromatic_line"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.SpectralLine.rvs","text":"The CDF and PPF (cumulative distribution and percentile point functions) are hard to compute. But it's easy enough to generate the random variates themselves, so we override that method. Source code in mass2/calibration/fluorescence_lines.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def rvs ( self , size : int | tuple [ int ] | None , instrument_gaussian_fwhm : float , rng : np . random . Generator | None = None ) -> NDArray : \"\"\"The CDF and PPF (cumulative distribution and percentile point functions) are hard to compute. But it's easy enough to generate the random variates themselves, so we override that method.\"\"\" if rng is None : rng = _rng gaussian_sigma = self . _gaussian_sigma ( instrument_gaussian_fwhm ) # Choose from among the N Lorentzian lines in proportion to the line amplitudes iline = self . cumulative_amplitudes . searchsorted ( rng . uniform ( 0 , self . cumulative_amplitudes [ - 1 ], size = size )) # Choose Lorentzian variates of the appropriate width (but centered on 0) lor = rng . standard_cauchy ( size = size ) * self . lorentzian_fwhm [ iline ] * 0.5 # If necessary, add a Gaussian variate to mimic finite resolution if gaussian_sigma > 0.0 : lor += rng . standard_normal ( size = size ) * gaussian_sigma # Finally, add the line centers. results = lor + self . energies [ iline ] # We must check for non-positive results and replace them by recursive call # to self.rvs(). not_positive = results <= 0.0 if np . any ( not_positive ): Nbad = not_positive . sum () results [ not_positive ] = self . rvs ( size = Nbad , instrument_gaussian_fwhm = instrument_gaussian_fwhm ) return results","title":"rvs"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.LineEnergies","text":"A dictionary to know a lot of x-ray fluorescence line energies, based on Deslattes' database. It is built on facts from mass2.calibration.nist_xray_database module. It is a dictionary from peak name to energy, with several alternate names for the lines: E = Energies() print E[\"MnKAlpha\"] print E[\"MnKAlpha\"], E[\"MnKA\"], E[\"MnKA1\"], E[\"MnKL3\"] Source code in mass2/calibration/fluorescence_lines.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def LineEnergies () -> dict [ str , float ]: \"\"\" A dictionary to know a lot of x-ray fluorescence line energies, based on Deslattes' database. It is built on facts from mass2.calibration.nist_xray_database module. It is a dictionary from peak name to energy, with several alternate names for the lines: E = Energies() print E[\"MnKAlpha\"] print E[\"MnKAlpha\"], E[\"MnKA\"], E[\"MnKA1\"], E[\"MnKL3\"] \"\"\" db = NISTXrayDBFile () alternate_line_names = { v : k for ( k , v ) in db . LINE_NICKNAMES . items ()} data = {} for fullname , L in db . lines . items (): element , linename = fullname . split ( \" \" , 1 ) allnames = [ linename ] if linename in alternate_line_names : siegbahn_linename = alternate_line_names [ linename ] long_linename = siegbahn_linename . replace ( \"A\" , \"Alpha\" ) . replace ( \"B\" , \"Beta\" ) . replace ( \"G\" , \"Gamma\" ) allnames . append ( siegbahn_linename ) allnames . append ( long_linename ) if siegbahn_linename . endswith ( \"1\" ): allnames . append ( siegbahn_linename [: - 1 ]) allnames . append ( long_linename [: - 1 ]) for name in allnames : key = \"\" . join (( element , name )) data [ key ] = L . peak return data","title":"LineEnergies"},{"location":"docstrings2/#mass2.calibration.fluorescence_lines.plot_all_spectra","text":"Makes plots showing the line shape and component parts for some lines. Intended to replicate plots in the literature giving spectral lineshapes. Source code in mass2/calibration/fluorescence_lines.py 1632 1633 1634 1635 1636 1637 1638 def plot_all_spectra ( maxplots : int = 10 ) -> None : \"\"\"Makes plots showing the line shape and component parts for some lines. Intended to replicate plots in the literature giving spectral lineshapes.\"\"\" keys = list ( spectra . keys ())[: maxplots ] for name in keys : spectrum = spectra [ name ] spectrum . plot_like_reference () Implements MLEModel, CompositeMLEModel, GenericLineModel","title":"plot_all_spectra"},{"location":"docstrings2/#mass2.calibration.line_models.CompositeMLEModel","text":"Bases: MLEModel , CompositeModel A version of lmfit.CompositeModel that uses Maximum Likelihood weights in place of chisq, as described in: doi:10.1007/s10909-014-1098-4 \"Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation\" Source code in mass2/calibration/line_models.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 class CompositeMLEModel ( MLEModel , lmfit . CompositeModel ): \"\"\"A version of lmfit.CompositeModel that uses Maximum Likelihood weights in place of chisq, as described in: doi:10.1007/s10909-014-1098-4 \"Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation\" \"\"\" def _residual ( self , params : lmfit . Parameters , data : NDArray | None , weights : NDArray | None , ** kwargs : Any ) -> NDArray : \"\"\"Calculate the chi_MLE^2 value from Joe Fowler's Paper doi:10.1007/s10909-014-1098-4 Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation \"\"\" y = self . eval ( params , ** kwargs ) if data is None : return y r2 = y - data nonzero = data > 0 r2 [ nonzero ] += data [ nonzero ] * np . log (( data / y )[ nonzero ]) vals = ( 2 * r2 ) ** 0.5 vals [ y < data ] *= - 1 return vals def __add__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Sum of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . add ) def __sub__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Difference of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . sub ) def __mul__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Product of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . mul ) def __truediv__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Ratio of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . truediv )","title":"CompositeMLEModel"},{"location":"docstrings2/#mass2.calibration.line_models.CompositeMLEModel.__add__","text":"Sum of two models Source code in mass2/calibration/line_models.py 236 237 238 def __add__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Sum of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . add )","title":"__add__"},{"location":"docstrings2/#mass2.calibration.line_models.CompositeMLEModel.__mul__","text":"Product of two models Source code in mass2/calibration/line_models.py 244 245 246 def __mul__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Product of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . mul )","title":"__mul__"},{"location":"docstrings2/#mass2.calibration.line_models.CompositeMLEModel.__sub__","text":"Difference of two models Source code in mass2/calibration/line_models.py 240 241 242 def __sub__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Difference of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . sub )","title":"__sub__"},{"location":"docstrings2/#mass2.calibration.line_models.CompositeMLEModel.__truediv__","text":"Ratio of two models Source code in mass2/calibration/line_models.py 248 249 250 def __truediv__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Ratio of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . truediv )","title":"__truediv__"},{"location":"docstrings2/#mass2.calibration.line_models.GenericLineModel","text":"Bases: MLEModel A generic line model for fitting spectral lines. Source code in mass2/calibration/line_models.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 class GenericLineModel ( MLEModel ): \"\"\"A generic line model for fitting spectral lines.\"\"\" def __init__ ( self , spect : \"SpectralLine\" , independent_vars : list [ str ] = [ \"bin_centers\" ], prefix : str = \"\" , nan_policy : str = \"raise\" , has_linear_background : bool = True , has_tails : bool = False , qemodel : Callable | None = None , ** kwargs : Any , ): \"\"\"Initialize a GenericLineModel Parameters ---------- spect : SpectralLine The line or feature to be modeled independent_vars : list[str], optional List of independent variable names, by default [\"bin_centers\"] prefix : str, optional Model, by default \"\" nan_policy : str, optional How to handle NaN results in the computed spectrum, by default \"raise\" has_linear_background : bool, optional Whether the background model can have a nonzero slope, by default True has_tails : bool, optional Whether exponential tails are included in the model, by default False qemodel : Callable | None, optional A model for the quantum efficiency (which changes the expected line shape), by default None Returns ------- GenericLineModel The initialized model Raises ------ ValueError If the spectral model produces negative or NaN values \"\"\" self . spect = spect self . _has_tails = has_tails self . _has_linear_background = has_linear_background param_names = [ \"fwhm\" , \"peak_ph\" , \"dph_de\" , \"integral\" ] if self . _has_linear_background : param_names += [ \"background\" , \"bg_slope\" ] if self . _has_tails : param_names += [ \"tail_frac\" , \"tail_tau\" , \"tail_share_hi\" , \"tail_tau_hi\" ] kwargs . update ({ \"prefix\" : prefix , \"nan_policy\" : nan_policy , \"independent_vars\" : independent_vars , \"param_names\" : param_names }) if has_tails : def modelfunctails ( # noqa: PLR0917 bin_centers : ArrayLike , fwhm : float , peak_ph : float , dph_de : float , integral : float , background : float = 0 , bg_slope : float = 0 , tail_frac : float = 0 , tail_tau : float = 8 , tail_share_hi : float = 0 , tail_tau_hi : float = 8 , ) -> NDArray : bin_centers = np . asarray ( bin_centers , dtype = float ) bin_width = bin_centers [ 1 ] - bin_centers [ 0 ] energy = ( bin_centers - peak_ph ) / dph_de + self . spect . peak_energy def cleanspectrum_fn ( x : ArrayLike ) -> NDArray : return self . spect . pdf ( x , instrument_gaussian_fwhm = fwhm ) # tail_tau* is in energy units but has to be converted to the same units as `bin_centers` tail_arbs_lo = tail_tau * dph_de tail_arbs_hi = tail_tau_hi * dph_de spectrum = _smear_exponential_tail ( cleanspectrum_fn , energy , fwhm , tail_frac , tail_arbs_lo , tail_share_hi , tail_arbs_hi ) scale_factor = integral * bin_width * dph_de r = _scale_add_bg ( spectrum , scale_factor , background , bg_slope ) if any ( np . isnan ( r )) or any ( r < 0 ): raise ValueError ( \"some entry in r is nan or negative\" ) if qemodel is None : return r return r * qemodel ( energy ) super () . __init__ ( modelfunctails , ** kwargs ) else : def modelfunc ( bin_centers : ArrayLike , fwhm : float , peak_ph : float , dph_de : float , integral : float , background : float = 0 , bg_slope : float = 0 , ) -> NDArray : bin_centers = np . asarray ( bin_centers , dtype = float ) bin_width = bin_centers [ 1 ] - bin_centers [ 0 ] energy = ( bin_centers - peak_ph ) / dph_de + self . spect . peak_energy spectrum = self . spect . pdf ( energy , fwhm ) scale_factor = integral * bin_width / dph_de r = _scale_add_bg ( spectrum , scale_factor , background , bg_slope ) if any ( np . isnan ( r )) or any ( r < 0 ): raise ValueError ( \"some entry in r is nan or negative\" ) if qemodel is None : return r return r * qemodel ( energy ) super () . __init__ ( modelfunc , ** kwargs ) self . _set_paramhints_prefix () def _set_paramhints_prefix ( self ) -> None : \"\"\"Set parameter hints with reasonable initial values and bounds.\"\"\" nominal_peak_energy = self . spect . nominal_peak_energy self . set_param_hint ( \"fwhm\" , value = nominal_peak_energy / 1000 , min = nominal_peak_energy / 10000 , max = nominal_peak_energy ) self . set_param_hint ( \"peak_ph\" , value = nominal_peak_energy , min = 0 ) self . set_param_hint ( \"dph_de\" , value = 1 , min = 0.01 , max = 100 ) self . set_param_hint ( \"integral\" , value = 100 , min = 0 ) if self . _has_linear_background : self . set_param_hint ( \"background\" , value = 1 , min = 0 ) self . set_param_hint ( \"bg_slope\" , value = 0 , vary = False ) if self . _has_tails : self . set_param_hint ( \"tail_frac\" , value = 0.05 , min = 0 , max = 1 , vary = True ) self . set_param_hint ( \"tail_tau\" , value = nominal_peak_energy / 200 , min = 0 , max = nominal_peak_energy / 10 , vary = True ) self . set_param_hint ( \"tail_share_hi\" , value = 0 , min = 0 , max = 1 , vary = False ) self . set_param_hint ( \"tail_tau_hi\" , value = nominal_peak_energy / 200 , min = 0 , max = nominal_peak_energy / 10 , vary = False ) def guess ( self , data : ArrayLike , bin_centers : ArrayLike , dph_de : float , ** kwargs : Any ) -> lmfit . Parameters : \"Guess values for the peak_ph, integral, and background.\" data = np . asarray ( data ) bin_centers = np . asarray ( bin_centers ) order_stat = np . array ( data . cumsum (), dtype = float ) / data . sum () def percentiles ( p : float ) -> NDArray : \"\"\"Find the p-th percentile of the data using histograms.\"\"\" return bin_centers [( order_stat > p ) . argmax ()] fwhm_arb = 0.7 * ( percentiles ( 0.75 ) - percentiles ( 0.25 )) peak_ph = bin_centers [ data . argmax ()] if len ( data ) > 20 : # Ensure baseline guess > 0 (see Issue #152). Guess at least 1 background across all bins baseline = max ( data [ 0 : 10 ] . mean (), 1.0 / len ( data )) else : baseline = 0.1 tcounts_above_bg = data . sum () - baseline * len ( data ) if tcounts_above_bg < 0 : tcounts_above_bg = data . sum () # lets avoid negative estimates for the integral pars = self . make_params ( peak_ph = peak_ph , background = baseline , integral = tcounts_above_bg , fwhm = fwhm_arb / dph_de , dph_de = dph_de ) return lmfit . models . update_param_vals ( pars , self . prefix , ** kwargs )","title":"GenericLineModel"},{"location":"docstrings2/#mass2.calibration.line_models.GenericLineModel.__init__","text":"Initialize a GenericLineModel Parameters: spect ( SpectralLine ) \u2013 The line or feature to be modeled independent_vars ( list [ str ] , default: ['bin_centers'] ) \u2013 List of independent variable names, by default [\"bin_centers\"] prefix ( str , default: '' ) \u2013 Model, by default \"\" nan_policy ( str , default: 'raise' ) \u2013 How to handle NaN results in the computed spectrum, by default \"raise\" has_linear_background ( bool , default: True ) \u2013 Whether the background model can have a nonzero slope, by default True has_tails ( bool , default: False ) \u2013 Whether exponential tails are included in the model, by default False qemodel ( Callable | None , default: None ) \u2013 A model for the quantum efficiency (which changes the expected line shape), by default None Returns: GenericLineModel \u2013 The initialized model Raises: ValueError \u2013 If the spectral model produces negative or NaN values Source code in mass2/calibration/line_models.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def __init__ ( self , spect : \"SpectralLine\" , independent_vars : list [ str ] = [ \"bin_centers\" ], prefix : str = \"\" , nan_policy : str = \"raise\" , has_linear_background : bool = True , has_tails : bool = False , qemodel : Callable | None = None , ** kwargs : Any , ): \"\"\"Initialize a GenericLineModel Parameters ---------- spect : SpectralLine The line or feature to be modeled independent_vars : list[str], optional List of independent variable names, by default [\"bin_centers\"] prefix : str, optional Model, by default \"\" nan_policy : str, optional How to handle NaN results in the computed spectrum, by default \"raise\" has_linear_background : bool, optional Whether the background model can have a nonzero slope, by default True has_tails : bool, optional Whether exponential tails are included in the model, by default False qemodel : Callable | None, optional A model for the quantum efficiency (which changes the expected line shape), by default None Returns ------- GenericLineModel The initialized model Raises ------ ValueError If the spectral model produces negative or NaN values \"\"\" self . spect = spect self . _has_tails = has_tails self . _has_linear_background = has_linear_background param_names = [ \"fwhm\" , \"peak_ph\" , \"dph_de\" , \"integral\" ] if self . _has_linear_background : param_names += [ \"background\" , \"bg_slope\" ] if self . _has_tails : param_names += [ \"tail_frac\" , \"tail_tau\" , \"tail_share_hi\" , \"tail_tau_hi\" ] kwargs . update ({ \"prefix\" : prefix , \"nan_policy\" : nan_policy , \"independent_vars\" : independent_vars , \"param_names\" : param_names }) if has_tails : def modelfunctails ( # noqa: PLR0917 bin_centers : ArrayLike , fwhm : float , peak_ph : float , dph_de : float , integral : float , background : float = 0 , bg_slope : float = 0 , tail_frac : float = 0 , tail_tau : float = 8 , tail_share_hi : float = 0 , tail_tau_hi : float = 8 , ) -> NDArray : bin_centers = np . asarray ( bin_centers , dtype = float ) bin_width = bin_centers [ 1 ] - bin_centers [ 0 ] energy = ( bin_centers - peak_ph ) / dph_de + self . spect . peak_energy def cleanspectrum_fn ( x : ArrayLike ) -> NDArray : return self . spect . pdf ( x , instrument_gaussian_fwhm = fwhm ) # tail_tau* is in energy units but has to be converted to the same units as `bin_centers` tail_arbs_lo = tail_tau * dph_de tail_arbs_hi = tail_tau_hi * dph_de spectrum = _smear_exponential_tail ( cleanspectrum_fn , energy , fwhm , tail_frac , tail_arbs_lo , tail_share_hi , tail_arbs_hi ) scale_factor = integral * bin_width * dph_de r = _scale_add_bg ( spectrum , scale_factor , background , bg_slope ) if any ( np . isnan ( r )) or any ( r < 0 ): raise ValueError ( \"some entry in r is nan or negative\" ) if qemodel is None : return r return r * qemodel ( energy ) super () . __init__ ( modelfunctails , ** kwargs ) else : def modelfunc ( bin_centers : ArrayLike , fwhm : float , peak_ph : float , dph_de : float , integral : float , background : float = 0 , bg_slope : float = 0 , ) -> NDArray : bin_centers = np . asarray ( bin_centers , dtype = float ) bin_width = bin_centers [ 1 ] - bin_centers [ 0 ] energy = ( bin_centers - peak_ph ) / dph_de + self . spect . peak_energy spectrum = self . spect . pdf ( energy , fwhm ) scale_factor = integral * bin_width / dph_de r = _scale_add_bg ( spectrum , scale_factor , background , bg_slope ) if any ( np . isnan ( r )) or any ( r < 0 ): raise ValueError ( \"some entry in r is nan or negative\" ) if qemodel is None : return r return r * qemodel ( energy ) super () . __init__ ( modelfunc , ** kwargs ) self . _set_paramhints_prefix ()","title":"__init__"},{"location":"docstrings2/#mass2.calibration.line_models.GenericLineModel.guess","text":"Guess values for the peak_ph, integral, and background. Source code in mass2/calibration/line_models.py 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 def guess ( self , data : ArrayLike , bin_centers : ArrayLike , dph_de : float , ** kwargs : Any ) -> lmfit . Parameters : \"Guess values for the peak_ph, integral, and background.\" data = np . asarray ( data ) bin_centers = np . asarray ( bin_centers ) order_stat = np . array ( data . cumsum (), dtype = float ) / data . sum () def percentiles ( p : float ) -> NDArray : \"\"\"Find the p-th percentile of the data using histograms.\"\"\" return bin_centers [( order_stat > p ) . argmax ()] fwhm_arb = 0.7 * ( percentiles ( 0.75 ) - percentiles ( 0.25 )) peak_ph = bin_centers [ data . argmax ()] if len ( data ) > 20 : # Ensure baseline guess > 0 (see Issue #152). Guess at least 1 background across all bins baseline = max ( data [ 0 : 10 ] . mean (), 1.0 / len ( data )) else : baseline = 0.1 tcounts_above_bg = data . sum () - baseline * len ( data ) if tcounts_above_bg < 0 : tcounts_above_bg = data . sum () # lets avoid negative estimates for the integral pars = self . make_params ( peak_ph = peak_ph , background = baseline , integral = tcounts_above_bg , fwhm = fwhm_arb / dph_de , dph_de = dph_de ) return lmfit . models . update_param_vals ( pars , self . prefix , ** kwargs )","title":"guess"},{"location":"docstrings2/#mass2.calibration.line_models.LineModelResult","text":"Bases: ModelResult like lmfit.model.Model result, but with some convenient plotting functions for line spectra fits Source code in mass2/calibration/line_models.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 class LineModelResult ( lmfit . model . ModelResult ): \"\"\"like lmfit.model.Model result, but with some convenient plotting functions for line spectra fits\"\"\" def _compact_fit_report ( self ) -> str : \"\"\"A compact fit report suitable for annotating a plot\"\"\" s = \"\" sn = { \"background\" : \"bg\" , \"integral\" : \"intgrl\" , \"bg_slope\" : \"bg_slp\" } for k in sorted ( self . params . keys ()): v = self . params [ k ] if v . vary : if v . stderr is None : sig_figs = 2 s += f \" { sn . get ( k , k ) : 7 } { v . value : . { sig_figs } g } \u00b1None \\n \" else : sig_figs = int ( np . ceil ( np . log10 ( np . abs ( v . value / v . stderr ))) + 1 ) sig_figs = max ( 1 , sig_figs ) s += f \" { sn . get ( k , k ) : 7 } { v . value : . { sig_figs } g } \u00b1 { v . stderr : .2g } \\n \" else : sig_figs = 2 s += f \" { sn . get ( k , k ) : 7 } { v . value : . { sig_figs } g } HELD \\n \" s += f \"redchi { self . redchi : .2g } \" return s def plotm ( self , ax : plt . Axes | None = None , title : str | None = None , xlabel : str | None = None , ylabel : str | None = None ) -> None : \"\"\"plot the data, the fit, and annotate the plot with the parameters\"\"\" title , xlabel , ylabel = self . _handle_default_labels ( title , xlabel , ylabel ) if ax is None : plt . figure () ax = plt . gca () ax = lmfit . model . ModelResult . plot_fit ( self , ax = ax , xlabel = xlabel , ylabel = ylabel ) if title is not None : plt . title ( title ) ax . text ( 0.05 , 0.95 , self . _compact_fit_report (), transform = ax . transAxes , verticalalignment = \"top\" , bbox = dict ( facecolor = \"w\" , alpha = 0.5 ), family = \"monospace\" , ) # ax.legend([\"data\", self._compact_fit_report()],loc='best', frameon=True, framealpha = 0.5) ax . legend ( loc = \"upper right\" ) def set_label_hints ( self , binsize : float , ds_shortname : str , attr_str : str , unit_str : str , cut_hint : str , states_hint : str = \"\" ) -> None : \"\"\"Set hints for axis labels and title for plotm().\"\"\" self . _binsize = binsize self . _ds_shortname = ds_shortname self . _attr_str = attr_str self . _unit_str = unit_str self . _cut_hint = cut_hint self . _states_hint = states_hint self . _has_label_hints = True def _handle_default_labels ( self , title : str | None , xlabel : str | None , ylabel : str | None ) -> tuple [ str , str , str ]: \"\"\"Handle default labels for plotm().\"\"\" if hasattr ( self , \"_has_label_hints\" ): if title is None : title = f \" { self . _ds_shortname } : { self . model . spect . shortname } \" if ylabel is None : ylabel = f \"counts per { self . _binsize : g } { self . _unit_str } bin\" if len ( self . _states_hint ) > 0 : ylabel += f \" \\n states= { self . _states_hint } : { self . _cut_hint } \" if xlabel is None : xlabel = f \" { self . _attr_str } ( { self . _unit_str } )\" elif ylabel is None and \"bin_centers\" in self . userkws : binsize = self . userkws [ \"bin_centers\" ][ 1 ] - self . userkws [ \"bin_centers\" ][ 0 ] ylabel = f \"counts per { binsize : g } unit bin\" if title is None : title = \"\" if xlabel is None : xlabel = \"\" if ylabel is None : ylabel = \"\" return title , xlabel , ylabel def _validate_bins_per_fwhm ( self , minimum_bins_per_fwhm : float ) -> None : \"\"\"Validate that the bin size is small enough compared to the fitted FWHM to prevent approximation problems.\"\"\" if \"bin_centers\" not in self . userkws : return # i guess someone used this for a non histogram fit if not VALIDATE_BIN_SIZE : return bin_centers = self . userkws [ \"bin_centers\" ] bin_size = bin_centers [ 1 ] - bin_centers [ 0 ] for iComp in self . components : prefix = iComp . prefix dphde = f \" { prefix } dph_de\" fwhm = f \" { prefix } fwhm\" if ( dphde in self . params ) and ( fwhm in self . params ): bin_size_energy = bin_size / self . params [ dphde ] instrument_gaussian_fwhm = self . params [ fwhm ] . value minimum_fwhm_energy = iComp . spect . minimum_fwhm ( instrument_gaussian_fwhm ) bins_per_fwhm = minimum_fwhm_energy / bin_size_energy if bins_per_fwhm < minimum_bins_per_fwhm : msg = f \"\"\"bins are too large. Bin size (energy units) = { bin_size_energy : .3g } , fit FWHM (energy units) = { instrument_gaussian_fwhm : .3g } Minimum FWHM accounting for narrowest Lorentzian in spectrum (energy units) = { minimum_fwhm_energy : .3g } Bins per FWHM = { bins_per_fwhm : .3g } , Minimum Bins per FWHM = { minimum_bins_per_fwhm : .3g } To avoid this error: 1. use smaller bins, or 2. pass a smaller value of `minimum_bins_per_fwhm` to .fit, or 3. set `mass2.calibration.line_models.VALIDATE_BIN_SIZE = False`. See https://github.com/usnistgov/mass/issues/162 for discussion on this issue\"\"\" raise ValueError ( msg )","title":"LineModelResult"},{"location":"docstrings2/#mass2.calibration.line_models.LineModelResult.plotm","text":"plot the data, the fit, and annotate the plot with the parameters Source code in mass2/calibration/line_models.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 def plotm ( self , ax : plt . Axes | None = None , title : str | None = None , xlabel : str | None = None , ylabel : str | None = None ) -> None : \"\"\"plot the data, the fit, and annotate the plot with the parameters\"\"\" title , xlabel , ylabel = self . _handle_default_labels ( title , xlabel , ylabel ) if ax is None : plt . figure () ax = plt . gca () ax = lmfit . model . ModelResult . plot_fit ( self , ax = ax , xlabel = xlabel , ylabel = ylabel ) if title is not None : plt . title ( title ) ax . text ( 0.05 , 0.95 , self . _compact_fit_report (), transform = ax . transAxes , verticalalignment = \"top\" , bbox = dict ( facecolor = \"w\" , alpha = 0.5 ), family = \"monospace\" , ) # ax.legend([\"data\", self._compact_fit_report()],loc='best', frameon=True, framealpha = 0.5) ax . legend ( loc = \"upper right\" )","title":"plotm"},{"location":"docstrings2/#mass2.calibration.line_models.LineModelResult.set_label_hints","text":"Set hints for axis labels and title for plotm(). Source code in mass2/calibration/line_models.py 458 459 460 461 462 463 464 465 466 467 468 def set_label_hints ( self , binsize : float , ds_shortname : str , attr_str : str , unit_str : str , cut_hint : str , states_hint : str = \"\" ) -> None : \"\"\"Set hints for axis labels and title for plotm().\"\"\" self . _binsize = binsize self . _ds_shortname = ds_shortname self . _attr_str = attr_str self . _unit_str = unit_str self . _cut_hint = cut_hint self . _states_hint = states_hint self . _has_label_hints = True","title":"set_label_hints"},{"location":"docstrings2/#mass2.calibration.line_models.MLEModel","text":"Bases: Model A version of lmfit.Model that uses Maximum Likelihood weights in place of chisq, as described in: doi:10.1007/s10909-014-1098-4 \"Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation\" Source code in mass2/calibration/line_models.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class MLEModel ( lmfit . Model ): \"\"\"A version of lmfit.Model that uses Maximum Likelihood weights in place of chisq, as described in: doi:10.1007/s10909-014-1098-4 \"Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation\" \"\"\" def _residual ( self , params : lmfit . Parameters , data : NDArray | None , weights : NDArray | None , ** kwargs : Any ) -> NDArray : \"\"\"Calculate the chi_MLE^2 value from Joe Fowler's Paper doi:10.1007/s10909-014-1098-4 \"Maximum-Likelihood Fits to Histograms for Improved Parameter Estimation\" \"\"\" y = self . eval ( params , ** kwargs ) if data is None : return y r2 = y - data nonzero = data > 0 r2 [ nonzero ] += data [ nonzero ] * np . log (( data / y )[ nonzero ]) # points that are zero do not effect the chisq value, so should not # be inlcuded in the calculate on ndegrees of freedome, and therefore reduced chisq # GCO tried setting self.ndata here, but it doesn't persist # not clear how to calculate reduced chisq correctly # Calculate the sqrt(2*r2) in place into vals. # The mask for r2>0 avoids the problem found in MASS issue #217. vals = np . zeros_like ( r2 ) nonneg = r2 > 0 vals [ nonneg ] = np . sqrt ( 2 * r2 [ nonneg ]) vals [ y < data ] *= - 1 return vals def __repr__ ( self ) -> str : \"\"\"Return representation of Model.\"\"\" return f \"< { type ( self ) . __name__ } : { self . name } >\" def _reprstring ( self , long : bool = False ) -> str : \"\"\"Return a longer string representation of Model, with its options.\"\"\" out = self . _name opts = [] if len ( self . _prefix ) > 0 : opts . append ( f \"prefix=' { self . _prefix } '\" ) if long : for k , v in self . opts . items (): opts . append ( f \" { k } =' { v } '\" ) if len ( opts ) > 0 : out = \" {} , {} \" . format ( out , \", \" . join ( opts )) return f \" { type ( self ) . __name__ } ( { out } )\" def __add__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Sum of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . add ) def __sub__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Difference of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . sub ) def __mul__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Product of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . mul ) def __truediv__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Ratio of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . truediv ) def fit ( self , * args : Any , minimum_bins_per_fwhm : float | None = 3 , ** kwargs : Any ) -> \"LineModelResult\" : \"\"\"as lmfit.Model.fit except 1. the default method is \"least_squares because it gives error bars more often at 1.5-2.0X speed penalty 2. supports \"leastsq_refit\" which uses \"leastsq\" to fit, but if there are no error bars, refits with \"least_squares\" call result.set_label_hints(...) then result.plotm() for a nice plot. \"\"\" if \"method\" not in kwargs : # change default method kwargs [ \"method\" ] = \"least_squares\" # least_squares always gives uncertainties, while the normal default leastsq often does not # leastsq fails to give uncertaities if parameters are near bounds or at their initial value # least_squares is about 1.5X to 2.0X slower based on two test case if minimum_bins_per_fwhm is None : minimum_bins_per_fwhm = 3 # provide default value if \"weights\" in kwargs and kwargs [ \"weights\" ] is not None : msg = \"MLEModel assumes Poisson-distributed data; cannot use weights other than None\" raise Exception ( msg ) result = self . _fit ( * args , ** kwargs ) result . __class__ = LineModelResult result . _validate_bins_per_fwhm ( minimum_bins_per_fwhm ) return result def _fit ( self , * args : Any , ** kwargs : Any ) -> \"LineModelResult\" : \"\"\"internal implementation of fit to add support for \"leastsq_refit\" method\"\"\" if kwargs [ \"method\" ] == \"leastsq_refit\" : # First fit with leastsq (the fastest method) kwargs [ \"method\" ] = \"leastsq\" result0 = lmfit . Model . fit ( self , * args , ** kwargs ) if result0 . success and result0 . errorbars : return result0 # If we didn't get uncertainties, fit again with least_squares kwargs [ \"method\" ] = \"least_squares\" if \"params\" in kwargs : kwargs [ \"params\" ] = result0 . params elif len ( args ) > 1 : args = tuple ([ result0 . params if i == 1 else arg for ( i , arg ) in enumerate ( args )]) result = lmfit . Model . fit ( self , * args , ** kwargs ) else : result = lmfit . Model . fit ( self , * args , ** kwargs ) return result","title":"MLEModel"},{"location":"docstrings2/#mass2.calibration.line_models.MLEModel.__add__","text":"Sum of two models Source code in mass2/calibration/line_models.py 156 157 158 def __add__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Sum of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . add )","title":"__add__"},{"location":"docstrings2/#mass2.calibration.line_models.MLEModel.__mul__","text":"Product of two models Source code in mass2/calibration/line_models.py 164 165 166 def __mul__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Product of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . mul )","title":"__mul__"},{"location":"docstrings2/#mass2.calibration.line_models.MLEModel.__repr__","text":"Return representation of Model. Source code in mass2/calibration/line_models.py 139 140 141 def __repr__ ( self ) -> str : \"\"\"Return representation of Model.\"\"\" return f \"< { type ( self ) . __name__ } : { self . name } >\"","title":"__repr__"},{"location":"docstrings2/#mass2.calibration.line_models.MLEModel.__sub__","text":"Difference of two models Source code in mass2/calibration/line_models.py 160 161 162 def __sub__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Difference of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . sub )","title":"__sub__"},{"location":"docstrings2/#mass2.calibration.line_models.MLEModel.__truediv__","text":"Ratio of two models Source code in mass2/calibration/line_models.py 168 169 170 def __truediv__ ( self , other : lmfit . Model ) -> \"CompositeMLEModel\" : \"\"\"Ratio of two models\"\"\" return CompositeMLEModel ( self , other , lmfit . model . operator . truediv )","title":"__truediv__"},{"location":"docstrings2/#mass2.calibration.line_models.MLEModel.fit","text":"as lmfit.Model.fit except 1. the default method is \"least_squares because it gives error bars more often at 1.5-2.0X speed penalty 2. supports \"leastsq_refit\" which uses \"leastsq\" to fit, but if there are no error bars, refits with \"least_squares\" call result.set_label_hints(...) then result.plotm() for a nice plot. Source code in mass2/calibration/line_models.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def fit ( self , * args : Any , minimum_bins_per_fwhm : float | None = 3 , ** kwargs : Any ) -> \"LineModelResult\" : \"\"\"as lmfit.Model.fit except 1. the default method is \"least_squares because it gives error bars more often at 1.5-2.0X speed penalty 2. supports \"leastsq_refit\" which uses \"leastsq\" to fit, but if there are no error bars, refits with \"least_squares\" call result.set_label_hints(...) then result.plotm() for a nice plot. \"\"\" if \"method\" not in kwargs : # change default method kwargs [ \"method\" ] = \"least_squares\" # least_squares always gives uncertainties, while the normal default leastsq often does not # leastsq fails to give uncertaities if parameters are near bounds or at their initial value # least_squares is about 1.5X to 2.0X slower based on two test case if minimum_bins_per_fwhm is None : minimum_bins_per_fwhm = 3 # provide default value if \"weights\" in kwargs and kwargs [ \"weights\" ] is not None : msg = \"MLEModel assumes Poisson-distributed data; cannot use weights other than None\" raise Exception ( msg ) result = self . _fit ( * args , ** kwargs ) result . __class__ = LineModelResult result . _validate_bins_per_fwhm ( minimum_bins_per_fwhm ) return result","title":"fit"},{"location":"docstrings2/#calibration-algorithms","text":"This file is intended to include algorithms that could be generally useful for calibration. Mostly they are pulled out of the former mass.calibration.young module.","title":"Calibration algorithms"},{"location":"docstrings2/#mass2.calibration.algorithms.FailedToGetModelException","text":"Bases: Exception Exception raised when get_model() fails to find a model for a line Source code in mass2/calibration/algorithms.py 176 177 178 179 class FailedToGetModelException ( Exception ): \"\"\"Exception raised when get_model() fails to find a model for a line\"\"\" pass","title":"FailedToGetModelException"},{"location":"docstrings2/#mass2.calibration.algorithms.build_fit_ranges","text":"Returns a list of (lo,hi) where lo and hi have units of energy of ranges to fit in for each energy in line_names. Args: line_names (list[str or float]): list or line names or energies excluded_line_names (list[str or float]): list of line_names or energies to avoid when making fit ranges approx_ecal: an EnergyCalibration object containing an approximate calibration fit_width_ev (float): full size in eV of fit ranges Source code in mass2/calibration/algorithms.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def build_fit_ranges ( line_names : Iterable [ str | float ], excluded_line_names : Iterable [ str | float ], approx_ecal : EnergyCalibration , fit_width_ev : float ) -> tuple [ list [ float ], list [ tuple [ float , float ]], list [ float ]]: \"\"\"Returns a list of (lo,hi) where lo and hi have units of energy of ranges to fit in for each energy in line_names. Args: line_names (list[str or float]): list or line names or energies excluded_line_names (list[str or float]): list of line_names or energies to avoid when making fit ranges approx_ecal: an EnergyCalibration object containing an approximate calibration fit_width_ev (float): full size in eV of fit ranges \"\"\" _names , e_e = line_names_and_energies ( line_names ) _excl_names , excl_e_e = line_names_and_energies ( excluded_line_names ) half_width_ev = fit_width_ev / 2.0 all_e = np . sort ( np . hstack (( e_e , excl_e_e ))) assert len ( all_e ) == len ( np . unique ( all_e )) fit_lo_hi_energy = [] slopes_de_dph = [] for e in e_e : slope_de_dph = cast ( float , approx_ecal . energy2dedph ( e )) if any ( all_e < e ): nearest_below = all_e [ all_e < e ][ - 1 ] else : nearest_below = - np . inf if any ( all_e > e ): nearest_above = all_e [ all_e > e ][ 0 ] else : nearest_above = np . inf lo = max ( e - half_width_ev , ( e + nearest_below ) / 2.0 ) hi = min ( e + half_width_ev , ( e + nearest_above ) / 2.0 ) fit_lo_hi_energy . append (( lo , hi )) slopes_de_dph . append ( slope_de_dph ) return e_e , fit_lo_hi_energy , slopes_de_dph","title":"build_fit_ranges"},{"location":"docstrings2/#mass2.calibration.algorithms.build_fit_ranges_ph","text":"Call build_fit_ranges() to get (lo,hi) for fitranges in energy units, then convert to ph using approx_ecal Source code in mass2/calibration/algorithms.py 122 123 124 125 126 127 128 129 130 131 132 133 134 def build_fit_ranges_ph ( line_names : Iterable [ str | float ], excluded_line_names : Iterable [ str | float ], approx_ecal : EnergyCalibration , fit_width_ev : float ) -> tuple [ list [ float ], list [ tuple [ float , float ]], list [ float ]]: \"\"\"Call build_fit_ranges() to get (lo,hi) for fitranges in energy units, then convert to ph using approx_ecal\"\"\" e_e , fit_lo_hi_energy , slopes_de_dph = build_fit_ranges ( line_names , excluded_line_names , approx_ecal , fit_width_ev ) fit_lo_hi_ph = [] for lo , hi in fit_lo_hi_energy : lo_ph = cast ( float , approx_ecal . energy2ph ( lo )) hi_ph = cast ( float , approx_ecal . energy2ph ( hi )) fit_lo_hi_ph . append (( lo_ph , hi_ph )) return e_e , fit_lo_hi_ph , slopes_de_dph","title":"build_fit_ranges_ph"},{"location":"docstrings2/#mass2.calibration.algorithms.find_local_maxima","text":"Smears each pulse by a gaussian of gaussian_fhwm and finds local maxima, returns a list of their locations in pulse_height units (sorted by number of pulses in peak) AND their peak values as: (peak_locations, peak_intensities) Args: pulse_heights (np.array(dtype=float)): a list of pulse heights (eg p_filt_value) gaussian_fwhm = fwhm of a gaussian that each pulse is smeared with, in same units as pulse heights Source code in mass2/calibration/algorithms.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def find_local_maxima ( pulse_heights : ArrayLike , gaussian_fwhm : float ) -> tuple [ NDArray , NDArray ]: \"\"\"Smears each pulse by a gaussian of gaussian_fhwm and finds local maxima, returns a list of their locations in pulse_height units (sorted by number of pulses in peak) AND their peak values as: (peak_locations, peak_intensities) Args: pulse_heights (np.array(dtype=float)): a list of pulse heights (eg p_filt_value) gaussian_fwhm = fwhm of a gaussian that each pulse is smeared with, in same units as pulse heights \"\"\" # kernel density estimation (with a gaussian kernel) n = 128 * 1024 gaussian_fwhm = float ( gaussian_fwhm ) # The above ensures that lo & hi are floats, so that (lo-hi)/n is always a float in python2 sigma = gaussian_fwhm / ( np . sqrt ( np . log ( 2 ) * 2 ) * 2 ) tbw = 1.0 / sigma / ( np . pi * 2 ) lo = np . min ( pulse_heights ) - 3 * gaussian_fwhm hi = np . max ( pulse_heights ) + 3 * gaussian_fwhm hist , bins = np . histogram ( pulse_heights , np . linspace ( lo , hi , n + 1 )) tx = np . fft . rfftfreq ( n , ( lo - hi ) / n ) ty = np . exp ( - ( tx ** 2 ) / 2 / tbw ** 2 ) x = ( bins [ 1 :] + bins [: - 1 ]) / 2 y = np . fft . irfft ( np . fft . rfft ( hist ) * ty ) flag = ( y [ 1 : - 1 ] > y [: - 2 ]) & ( y [ 1 : - 1 ] > y [ 2 :]) lm = np . arange ( 1 , n - 1 )[ flag ] lm = lm [ np . argsort ( - y [ lm ])] return np . array ( x [ lm ]), np . array ( y [ lm ])","title":"find_local_maxima"},{"location":"docstrings2/#mass2.calibration.algorithms.find_opt_assignment","text":"Tries to find an assignment of peaks to line names that is reasonably self consistent and smooth Args: peak_positions (np.array(dtype=float)): a list of peak locations in arb units, e.g. p_filt_value units line_names (list[str or float)]): a list of calibration lines either as number (which is energies in eV), or name to be looked up in STANDARD_FEATURES nextra (int): the algorithm starts with the first len(line_names) + nextra peak_positions nincrement (int): each the algorithm fails to find a satisfactory peak assignment, it uses nincrement more lines nextramax (int): the algorithm stops incrementint nextra past this value, instead failing with a ValueError saying \"no peak assignment succeeded\" maxacc (float): an empirical number that determines if an assignment is good enough. The default number works reasonably well for tupac data Source code in mass2/calibration/algorithms.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def find_opt_assignment ( peak_positions : ArrayLike , line_names : list [ str | float ], nextra : int = 2 , nincrement : int = 3 , nextramax : int = 8 , maxacc : float = 0.015 , ) -> tuple [ list [ str ], NDArray , list [ int ]]: \"\"\"Tries to find an assignment of peaks to line names that is reasonably self consistent and smooth Args: peak_positions (np.array(dtype=float)): a list of peak locations in arb units, e.g. p_filt_value units line_names (list[str or float)]): a list of calibration lines either as number (which is energies in eV), or name to be looked up in STANDARD_FEATURES nextra (int): the algorithm starts with the first len(line_names) + nextra peak_positions nincrement (int): each the algorithm fails to find a satisfactory peak assignment, it uses nincrement more lines nextramax (int): the algorithm stops incrementint nextra past this value, instead failing with a ValueError saying \"no peak assignment succeeded\" maxacc (float): an empirical number that determines if an assignment is good enough. The default number works reasonably well for tupac data \"\"\" name_e , e_e = line_names_and_energies ( line_names ) n_sel_pp = len ( line_names ) + nextra # number of peak_positions to use to line up to line_names nmax = len ( line_names ) + nextramax peak_positions = np . asarray ( peak_positions ) while True : sel_positions = np . asarray ( peak_positions [: n_sel_pp ], dtype = \"float\" ) energies = np . asarray ( e_e , dtype = \"float\" ) assign = np . array ( list ( itertools . combinations ( sel_positions , len ( line_names )))) assign . sort ( axis = 1 ) fracs = np . divide ( energies [ 1 : - 1 ] - energies [: - 2 ], energies [ 2 :] - energies [: - 2 ]) est_pos = assign [:, : - 2 ] * ( 1 - fracs ) + assign [:, 2 :] * fracs acc_est = np . linalg . norm ( np . divide ( est_pos - assign [:, 1 : - 1 ], assign [:, 2 :] - assign [:, : - 2 ]), axis = 1 ) opt_assign_i = np . argmin ( acc_est ) acc = acc_est [ opt_assign_i ] opt_assign = assign [ opt_assign_i ] if acc > maxacc * np . sqrt ( len ( energies )): n_sel_pp += nincrement if n_sel_pp > nmax : msg = f \"no peak assignment succeeded: acc { acc : g } , maxacc*sqrt(len(energies)) { maxacc * np . sqrt ( len ( energies )) : g } \" raise ValueError ( msg ) else : continue else : return name_e , energies , list ( opt_assign )","title":"find_opt_assignment"},{"location":"docstrings2/#mass2.calibration.algorithms.get_model","text":"Get a GenericLineModel for a line, given either a line name or energy in eV Parameters: lineNameOrEnergy ( GenericLineModel | SpectralLine | str | float ) \u2013 A line name, or energy, or a SpectralLine, or a GenericLineModel has_linear_background ( bool , default: True ) \u2013 Whether to allow a background slope, by default True has_tails ( bool , default: False ) \u2013 Whether to allow exponential tails, by default False prefix ( str , default: '' ) \u2013 Line nae prefix, by default \"\" Returns: GenericLineModel \u2013 An appropriate line model Raises: FailedToGetModelException \u2013 When a matching line cannot be found Source code in mass2/calibration/algorithms.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def get_model ( lineNameOrEnergy : GenericLineModel | SpectralLine | str | float , has_linear_background : bool = True , has_tails : bool = False , prefix : str = \"\" , ) -> GenericLineModel : \"\"\"Get a GenericLineModel for a line, given either a line name or energy in eV Parameters ---------- lineNameOrEnergy : GenericLineModel | SpectralLine | str | float A line name, or energy, or a SpectralLine, or a GenericLineModel has_linear_background : bool, optional Whether to allow a background slope, by default True has_tails : bool, optional Whether to allow exponential tails, by default False prefix : str, optional Line nae prefix, by default \"\" Returns ------- GenericLineModel An appropriate line model Raises ------ FailedToGetModelException When a matching line cannot be found \"\"\" if isinstance ( lineNameOrEnergy , GenericLineModel ): line = lineNameOrEnergy . spect elif isinstance ( lineNameOrEnergy , SpectralLine ): line = lineNameOrEnergy elif isinstance ( lineNameOrEnergy , str ): if lineNameOrEnergy in mass2 . spectra : line = mass2 . spectra [ lineNameOrEnergy ] elif lineNameOrEnergy in mass2 . STANDARD_FEATURES : energy = mass2 . STANDARD_FEATURES [ lineNameOrEnergy ] line = SpectralLine . quick_monochromatic_line ( lineNameOrEnergy , energy , 0.001 , 0 ) else : raise FailedToGetModelException ( f \"failed to get line from lineNameOrEnergy= { lineNameOrEnergy } \" ) else : try : energy = float ( lineNameOrEnergy ) except Exception : raise FailedToGetModelException ( f \"lineNameOrEnergy = { lineNameOrEnergy } is not convertable\" \" to float or a str in mass2.spectra or mass2.STANDARD_FEATURES\" ) line = SpectralLine . quick_monochromatic_line ( f \" { lineNameOrEnergy } eV\" , float ( lineNameOrEnergy ), 0.001 , 0 ) return line . model ( has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix )","title":"get_model"},{"location":"docstrings2/#mass2.calibration.algorithms.line_names_and_energies","text":"Given a list of line_names, return (names, energies) in eV. Can also accept energies in eV directly and return (names, energies). Source code in mass2/calibration/algorithms.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def line_names_and_energies ( line_names : Iterable [ str | float ]) -> tuple [ list [ str ], list [ float ]]: \"\"\"Given a list of line_names, return (names, energies) in eV. Can also accept energies in eV directly and return (names, energies). \"\"\" energies : list [ float ] = [] for name_or_energy in line_names : if isinstance ( name_or_energy , str ): energies . append ( STANDARD_FEATURES [ name_or_energy ]) else : energies . append ( float ( name_or_energy )) order : NDArray = np . argsort ( energies ) names = list ( line_names ) sorted_names = [ str ( names [ i ]) for i in order ] energies . sort () return sorted_names , energies","title":"line_names_and_energies"},{"location":"docstrings2/#mass2.calibration.algorithms.multifit","text":"Args: ph (np.array(dtype=float)): list of pulse heights line_names: names of calibration lines fit_lo_hi (list[list[float]]): a list of (lo,hi) with units of ph, used as edges of histograms for fitting binsize_ev (list[float]): list of binsizes in eV for calibration lines slopes_de_dph (list[float]): - list of slopes de_dph (e in eV) hide_deprecation: whether to suppress deprecation warnings Source code in mass2/calibration/algorithms.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def multifit ( ph : ArrayLike , line_names : Iterable [ str ], fit_lo_hi : list [ list [ float ]], binsize_ev : list [ float ], slopes_de_dph : list [ float ], hide_deprecation : bool = False , ) -> dict [ str , Any ]: \"\"\" Args: ph (np.array(dtype=float)): list of pulse heights line_names: names of calibration lines fit_lo_hi (list[list[float]]): a list of (lo,hi) with units of ph, used as edges of histograms for fitting binsize_ev (list[float]): list of binsizes in eV for calibration lines slopes_de_dph (list[float]): - list of slopes de_dph (e in eV) hide_deprecation: whether to suppress deprecation warnings \"\"\" name_e , e_e = line_names_and_energies ( line_names ) results = [] peak_ph = [] eres = [] for i , name in enumerate ( name_e ): lo , hi = fit_lo_hi [ i ] dP_dE = 1 / slopes_de_dph [ i ] binsize_ph = binsize_ev [ i ] * dP_dE result = singlefit ( ph , name , lo , hi , binsize_ph , dP_dE ) results . append ( result ) peak_ph . append ( result . best_values [ \"peak_ph\" ]) eres . append ( result . best_values [ \"fwhm\" ]) return { \"results\" : results , \"peak_ph\" : peak_ph , \"eres\" : eres , \"line_names\" : name_e , \"energies\" : e_e }","title":"multifit"},{"location":"docstrings2/#mass2.calibration.algorithms.singlefit","text":"Performs a fit to a single line in pulse height units Parameters: ph ( ArrayLike ) \u2013 Measured pulse heights name ( GenericLineModel | SpectralLine | str | float ) \u2013 Spectral line to fit, either as a name, energy in eV, SpectralLine, or GenericLineModel lo ( float ) \u2013 minimum pulse height to include in fit hi ( float ) \u2013 maximum pulse height to include in fit binsize_ph ( float ) \u2013 bin size in pulse height units approx_dP_dE ( float ) \u2013 Estimate of the dph/dE at the line energy, used to constrain the fit Returns: LineModelResult \u2013 The best-fit result Raises: ValueError \u2013 When too many bins would be used in the fit Source code in mass2/calibration/algorithms.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def singlefit ( ph : ArrayLike , name : GenericLineModel | SpectralLine | str | float , lo : float , hi : float , binsize_ph : float , approx_dP_dE : float ) -> LineModelResult : \"\"\"Performs a fit to a single line in pulse height units Parameters ---------- ph : ArrayLike Measured pulse heights name : GenericLineModel | SpectralLine | str | float Spectral line to fit, either as a name, energy in eV, SpectralLine, or GenericLineModel lo : float minimum pulse height to include in fit hi : float maximum pulse height to include in fit binsize_ph : float bin size in pulse height units approx_dP_dE : float Estimate of the dph/dE at the line energy, used to constrain the fit Returns ------- LineModelResult The best-fit result Raises ------ ValueError When too many bins would be used in the fit \"\"\" nbins = ( hi - lo ) / binsize_ph if nbins > 5000 : raise ValueError ( \"too damn many bins, dont like running out of memory\" ) counts , bin_edges = np . histogram ( ph , np . arange ( lo , hi , binsize_ph )) e = bin_edges [: - 1 ] + 0.5 * ( bin_edges [ 1 ] - bin_edges [ 0 ]) model = get_model ( name ) guess_params = model . guess ( counts , bin_centers = e , dph_de = approx_dP_dE ) if \"Gaussian\" not in model . name : guess_params [ \"dph_de\" ] . set ( approx_dP_dE , vary = False ) result = model . fit ( counts , guess_params , bin_centers = e , minimum_bins_per_fwhm = 1.5 ) result . energies = e return result","title":"singlefit"},{"location":"docstrings2/#highly-charged-ions","text":"hci_lines.py Uses pickle file containing NIST ASD levels data to generate some commonly used HCI lines in mass. Meant to be a replacement for _highly_charged_ion_lines.py, which hard codes in line parameters. The pickle file can be gzip-compressed, provided the compressed filename ends with \".gz\". February 2020 Paul Szypryt","title":"Highly charged ions"},{"location":"docstrings2/#mass2.calibration.hci_lines.NIST_ASD","text":"Class for working with a pickled atomic spectra database Source code in mass2/calibration/hci_lines.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class NIST_ASD : \"\"\"Class for working with a pickled atomic spectra database\"\"\" def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy","title":"NIST_ASD"},{"location":"docstrings2/#mass2.calibration.hci_lines.NIST_ASD.__init__","text":"Loads ASD pickle file (optionally gzipped) Parameters: pickleFilename ( str | None , default: None ) \u2013 ASD pickle file name, as str, or if none then mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH (default None) Source code in mass2/calibration/hci_lines.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle )","title":"__init__"},{"location":"docstrings2/#mass2.calibration.hci_lines.NIST_ASD.getAvailableElements","text":"Returns a list of all available elements from the ASD pickle file Source code in mass2/calibration/hci_lines.py 52 53 54 55 def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ())","title":"getAvailableElements"},{"location":"docstrings2/#mass2.calibration.hci_lines.NIST_ASD.getAvailableLevels","text":"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters: element ( str ) \u2013 Elemental atomic symbol, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf ( str | None , default: None ) \u2013 if not None, limits results to those with conf == requiredConf , by default None requiredTerm ( str | None , default: None ) \u2013 if not None, limits results to those with term == requiredTerm , by default None requiredJVal ( str | None , default: None ) \u2013 if not None, limits results to those with a == requiredJVal , by default None maxLevels ( int | None , default: None ) \u2013 the maximum number of levels (sorted by energy) to return, by default None units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 whether to return uncertain values, by default True Returns: dict \u2013 A dictionary of energy level strings to energy levels. Source code in mass2/calibration/hci_lines.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict","title":"getAvailableLevels"},{"location":"docstrings2/#mass2.calibration.hci_lines.NIST_ASD.getAvailableSpectralCharges","text":"For a given element, returns a list of all available charge states from the ASD pickle file Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' Returns: list [ int ] \u2013 Available charge states Source code in mass2/calibration/hci_lines.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ())","title":"getAvailableSpectralCharges"},{"location":"docstrings2/#mass2.calibration.hci_lines.NIST_ASD.getSingleLevel","text":"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf ( str ) \u2013 nuclear configuration, e.g. '2p' term ( str ) \u2013 nuclear term, e.g. '2P*' JVal ( str ) \u2013 total angular momentum J, e.g. '3/2' units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 includes uncertainties in list of levels, by default True Returns: float \u2013 description Source code in mass2/calibration/hci_lines.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy","title":"getSingleLevel"},{"location":"docstrings2/#mass2.calibration.hci_lines.add_H_like_lines_from_asd","text":"Add all known H-like lines for a given element from the ASD database Source code in mass2/calibration/hci_lines.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def add_H_like_lines_from_asd ( asd : NIST_ASD , element : str , maxLevels : int | None = None ) -> list [ SpectralLine ]: \"\"\"Add all known H-like lines for a given element from the ASD database\"\"\" spectr_ch = xraydb . atomic_number ( element ) added_lines = [] if maxLevels is not None : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch , maxLevels = maxLevels + 1 ) else : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch ) for iLevel in list ( levelsDict . keys ()): lineEnergy = levelsDict [ iLevel ][ 0 ] if lineEnergy != 0.0 : iLine = add_hci_line ( element = element , spectr_ch = spectr_ch , line_identifier = iLevel , energies = [ lineEnergy ], widths = [ 0.1 ], ratios = [ 1.0 ] ) added_lines . append ( iLine ) return added_lines","title":"add_H_like_lines_from_asd"},{"location":"docstrings2/#mass2.calibration.hci_lines.add_He_like_lines_from_asd","text":"Add all known He-like lines for a given element from the ASD database Source code in mass2/calibration/hci_lines.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def add_He_like_lines_from_asd ( asd : NIST_ASD , element : str , maxLevels : int | None = None ) -> list [ SpectralLine ]: \"\"\"Add all known He-like lines for a given element from the ASD database\"\"\" spectr_ch = xraydb . atomic_number ( element ) - 1 added_lines = [] if maxLevels is not None : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch , maxLevels = maxLevels + 1 ) else : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch ) for iLevel in list ( levelsDict . keys ()): lineEnergy = levelsDict [ iLevel ][ 0 ] if lineEnergy != 0.0 : iLine = add_hci_line ( element = element , spectr_ch = spectr_ch , line_identifier = iLevel , energies = [ lineEnergy ], widths = [ 0.1 ], ratios = [ 1.0 ] ) added_lines . append ( iLine ) return added_lines","title":"add_He_like_lines_from_asd"},{"location":"docstrings2/#mass2.calibration.hci_lines.add_hci_line","text":"Add a single HCI line to the fluorescence_lines database Parameters: element ( str ) \u2013 The element whose line is being added, e.g. 'Ne' spectr_ch ( int ) \u2013 The charge state of the ion whose line is being added, e.g. 9 for H-like Ne line_identifier ( str ) \u2013 The line identifier, e.g. '1s2S1/2 - 2p2P3/2' energies ( ArrayLike ) \u2013 The energies of the components of the line, in eV widths ( ArrayLike ) \u2013 The Lorentzian FWHM widths of the components of the line, in eV ratios ( ArrayLike ) \u2013 The relative intensities of the components of the line nominal_peak_energy ( float | None , default: None ) \u2013 The nominal spectral peak in eV, by default None Returns: SpectralLine \u2013 The newly added SpectralLine object Source code in mass2/calibration/hci_lines.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def add_hci_line ( element : str , spectr_ch : int , line_identifier : str , energies : ArrayLike , widths : ArrayLike , ratios : ArrayLike , nominal_peak_energy : float | None = None , ) -> SpectralLine : \"\"\"Add a single HCI line to the fluorescence_lines database Parameters ---------- element : str The element whose line is being added, e.g. 'Ne' spectr_ch : int The charge state of the ion whose line is being added, e.g. 9 for H-like Ne line_identifier : str The line identifier, e.g. '1s2S1/2 - 2p2P3/2' energies : ArrayLike The energies of the components of the line, in eV widths : ArrayLike The Lorentzian FWHM widths of the components of the line, in eV ratios : ArrayLike The relative intensities of the components of the line nominal_peak_energy : float | None, optional The nominal spectral peak in eV, by default None Returns ------- SpectralLine The newly added SpectralLine object \"\"\" energies = np . asarray ( energies ) widths = np . asarray ( widths ) ratios = np . asarray ( ratios ) if nominal_peak_energy is None : nominal_peak_energy = np . dot ( energies , ratios ) / np . sum ( ratios ) linetype = f \" { int ( spectr_ch ) } { line_identifier } \" spectrum_class = fluorescence_lines . addline ( element = element , material = \"Highly Charged Ion\" , linetype = linetype , reference_short = \"NIST ASD\" , reference_plot_instrument_gaussian_fwhm = 0.5 , nominal_peak_energy = nominal_peak_energy , energies = energies , lorentzian_fwhm = widths , reference_amplitude = ratios , reference_amplitude_type = AmplitudeType . LORENTZIAN_PEAK_HEIGHT , ka12_energy_diff = None , ) return spectrum_class hci_models.py Some useful methods for initializing GenericLineModel and CompositeMLEModel objects applied to HCI lines. June 2020 Paul Szypryt","title":"add_hci_line"},{"location":"docstrings2/#mass2.calibration.hci_models.add_bg_model","text":"Adds a LinearBackgroundModel to a generic lmfit model Parameters: generic_model ( GenericLineModel ) \u2013 object to which to add a linear background model vary_slope ( bool , default: False ) \u2013 allows a varying linear slope rather than just constant value, by default False Returns: GenericLineModel \u2013 The input model, with background componets added Source code in mass2/calibration/hci_models.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def add_bg_model ( generic_model : GenericLineModel , vary_slope : bool = False ) -> GenericLineModel : \"\"\"Adds a LinearBackgroundModel to a generic lmfit model Parameters ---------- generic_model : GenericLineModel object to which to add a linear background model vary_slope : bool, optional allows a varying linear slope rather than just constant value, by default False Returns ------- GenericLineModel The input model, with background componets added \"\"\" # composite_name = generic_model._name # bg_prefix = f\"{composite_name}_\".replace(\" \", \"_\").replace(\"J=\", \"\").replace(\"/\", \"_\").replace(\"*\", \"\").replace(\".\", \"\") raise NotImplementedError ( \"No LinearBackgroundModel still exists in mass2\" )","title":"add_bg_model"},{"location":"docstrings2/#mass2.calibration.hci_models.initialize_HLike_2P_model","text":"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' conf ( str ) \u2013 nuclear configuration as str, e.g. '2p' or '3p' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False vary_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def initialize_HLike_2P_model ( element : str , conf : str , has_linear_background : bool = False , has_tails : bool = False , vary_amp_ratio : bool = False ) -> GenericLineModel : \"\"\"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' conf : str nuclear configuration as str, e.g. '2p' or '3p' has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional include low energy tail in the model, by default False vary_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns ------- GenericLineModel The new composite line \"\"\" # Set up line names and lmfit prefixes charge = int ( xraydb . atomic_number ( element )) line_name_1_2 = f \" { element }{ charge } { conf } 2P* J=1/2\" line_name_3_2 = f \" { element }{ charge } { conf } 2P* J=3/2\" prefix_1_2 = f \" { line_name_1_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) prefix_3_2 = f \" { line_name_3_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) # Initialize individual lines and models line_1_2 = spectra [ line_name_1_2 ] line_3_2 = spectra [ line_name_3_2 ] model_1_2 = line_1_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_1_2 ) model_3_2 = line_3_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_3_2 ) # Initialize composite model and set addition H-like constraints composite_name = f \" { element }{ charge } { conf } \" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = [ model_1_2 , model_3_2 ], has_linear_background = has_linear_background , peak_component_name = line_name_3_2 , ) amp_ratio_param_name = f \" { element }{ charge } _ { conf } _amp_ratio\" composite_model . set_param_hint ( name = amp_ratio_param_name , value = 0.5 , min = 0.0 , vary = vary_amp_ratio ) composite_model . set_param_hint ( f \" { prefix_1_2 } integral\" , expr = f \" { prefix_3_2 } integral * { amp_ratio_param_name } \" ) return composite_model","title":"initialize_HLike_2P_model"},{"location":"docstrings2/#mass2.calibration.hci_models.initialize_HeLike_complex_model","text":"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P J=1, and 1s.2p 1P J=1 lines. Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the Lorentzian models, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False additional_line_names ( list , default: [] ) \u2013 additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns: GenericLineModel \u2013 A model of the given HCI complex. Source code in mass2/calibration/hci_models.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def initialize_HeLike_complex_model ( element : str , has_linear_background : bool = False , has_tails : bool = False , additional_line_names : list = [] ) -> GenericLineModel : \"\"\"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P* J=1, and 1s.2p 1P* J=1 lines. Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background : bool, optional include a single linear background on top of the Lorentzian models, by default False has_tails : bool, optional include low energy tail in the model, by default False additional_line_names : list, optional additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns ------- GenericLineModel A model of the given HCI complex. \"\"\" # Set up line names charge = int ( xraydb . atomic_number ( element ) - 1 ) line_name_1s2s_3S = f \" { element }{ charge } 1s.2s 3S J=1\" line_name_1s2p_3P = f \" { element }{ charge } 1s.2p 3P* J=1\" line_name_1s2p_1P = f \" { element }{ charge } 1s.2p 1P* J=1\" line_names = np . hstack ([[ line_name_1s2s_3S , line_name_1s2p_3P , line_name_1s2p_1P ], additional_line_names ]) # Set up lines and models based on line_names # individual_lines = [spectra[i_line_name]() for i_line_name in line_names] individual_models = [ initialize_hci_line_model ( i_line_name , has_linear_background = False , has_tails = has_tails ) for i_line_name in line_names ] # Set up composite model composite_name = f \" { element }{ charge } 1s2s_2p Complex\" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = individual_models , has_linear_background = has_linear_background , peak_component_name = line_name_1s2p_1P , ) return composite_model","title":"initialize_HeLike_complex_model"},{"location":"docstrings2/#mass2.calibration.hci_models.initialize_hci_composite_model","text":"Initializes composite lmfit model from the sum of input models Parameters: composite_name ( str ) \u2013 name given to composite line model individual_models ( list [ GenericLineModel ] ) \u2013 Models to sum into a composite has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of group of lorentzians, by default False peak_component_name ( str | None , default: None ) \u2013 designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def initialize_hci_composite_model ( composite_name : str , individual_models : list [ GenericLineModel ], has_linear_background : bool = False , peak_component_name : str | None = None , ) -> GenericLineModel : \"\"\"Initializes composite lmfit model from the sum of input models Parameters ---------- composite_name : str name given to composite line model individual_models : list[GenericLineModel] Models to sum into a composite has_linear_background : bool, optional include a single linear background on top of group of lorentzians, by default False peak_component_name : str | None, optional designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns ------- GenericLineModel The new composite line \"\"\" composite_model : GenericLineModel = np . sum ( individual_models ) composite_model . name = composite_name if has_linear_background : composite_model = add_bg_model ( composite_model ) # Workaround for energy calibration using composite models, pick 1st GenericLineModel component line_model_components = [ i_comp for i_comp in composite_model . components if isinstance ( i_comp , mass2 . calibration . line_models . GenericLineModel ) ] if peak_component_name is None : peak_component_name = line_model_components [ 0 ] . _name peak_component_index = [ i_comp . _name for i_comp in line_model_components ] . index ( peak_component_name ) peak_component = line_model_components [ peak_component_index ] composite_model . peak_prefix = peak_component . prefix composite_model . peak_energy = peak_component . spect . peak_energy # Set up some constraints relative to peak_component num_line_components = len ( line_model_components ) line_component_prefixes = [ iComp . prefix for iComp in line_model_components ] line_component_energies = [ iComp . spect . peak_energy for iComp in line_model_components ] for i in np . arange ( num_line_components ): if i != peak_component_index : # Single fwhm across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } fwhm\" , expr = f \" { composite_model . peak_prefix } fwhm\" ) # Single dph_de across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } dph_de\" , expr = f \" { composite_model . peak_prefix } dph_de\" ) # Fixed energy separation based on database values separation = line_component_energies [ i ] - composite_model . peak_energy hint = f \"( { separation } * { composite_model . peak_prefix } dph_de) + { composite_model . peak_prefix } peak_ph\" composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } peak_ph\" , expr = hint ) composite_model . shortname = composite_name return composite_model","title":"initialize_hci_composite_model"},{"location":"docstrings2/#mass2.calibration.hci_models.initialize_hci_line_model","text":"Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters: line_name ( str ) \u2013 name of line to use in mass2.spectra has_linear_background ( bool , default: False ) \u2013 include linear background in the model, by default False has_tails ( bool , default: False ) \u2013 include low-energy tail in the model, by default False Returns: GenericLineModel \u2013 New HCI line. Source code in mass2/calibration/hci_models.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def initialize_hci_line_model ( line_name : str , has_linear_background : bool = False , has_tails : bool = False ) -> GenericLineModel : \"\"\"Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters ---------- line_name : str name of line to use in mass2.spectra has_linear_background : bool, optional include linear background in the model, by default False has_tails : bool, optional include low-energy tail in the model, by default False Returns ------- GenericLineModel New HCI line. \"\"\" line = spectra [ line_name ] prefix = f \" { line_name } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) line_model = line . model ( has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix ) line_model . shortname = line_name return line_model","title":"initialize_hci_line_model"},{"location":"docstrings2/#mass2.calibration.hci_models.models","text":"Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters: has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines ( list , default: [] ) \u2013 additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns: _type_ \u2013 Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. Source code in mass2/calibration/hci_models.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def models ( has_linear_background : bool = False , has_tails : bool = False , vary_Hlike_amp_ratio : bool = False , additional_Helike_complex_lines : list = [], ) -> dict : \"\"\"Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters ---------- has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines : list, optional additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns ------- _type_ Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. \"\"\" models_dict = {} # Make some common H-like 2P* models conf_Hlike_2P_dict = {} conf_Hlike_2P_dict [ \"N\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"O\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ne\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ar\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] for i_element in list ( conf_Hlike_2P_dict . keys ()): for i_conf in conf_Hlike_2P_dict [ i_element ]: Hlike_model = initialize_HLike_2P_model ( i_element , i_conf , has_linear_background = has_linear_background , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio , ) models_dict [ Hlike_model . _name ] = Hlike_model # Make some common He-like 1s2s,2p complex and higher order 1p* models # He-like lines Helike_complex_elements = [ \"N\" , \"O\" , \"Ne\" , \"Ar\" ] for i_element in Helike_complex_elements : Helike_model = initialize_HeLike_complex_model ( i_element , has_linear_background = has_linear_background , has_tails = has_tails , additional_line_names = additional_Helike_complex_lines , ) models_dict [ Helike_model . _name ] = Helike_model # 1s.np 1P* lines for n>=3 conf_Helike_1P_dict = {} conf_Helike_1P_dict [ \"N\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"O\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ne\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ar\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] for i_element in list ( conf_Helike_1P_dict . keys ()): i_charge = int ( xraydb . atomic_number ( i_element ) - 1 ) for i_conf in conf_Helike_1P_dict [ i_element ]: Helike_line_name = f \" { i_element }{ i_charge } { i_conf } 1P* J=1\" Helike_model = initialize_hci_line_model ( Helike_line_name , has_linear_background = has_linear_background , has_tails = has_tails ) models_dict [ Helike_model . _name ] = Helike_model # Some more complicated cases # 500 eV region of H-/He-like N N6_1s3p_model = initialize_hci_line_model ( \"N6 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) N7_2p_model = initialize_HLike_2P_model ( \"N\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) N_500eV_model = initialize_hci_composite_model ( \"N 500eV Region\" , [ N6_1s3p_model , N7_2p_model ], has_linear_background = has_linear_background , peak_component_name = \"N7 2p 2P* J=3/2\" , ) models_dict [ N_500eV_model . _name ] = N_500eV_model # 660 eV region of H-/He-like O O8_2p_model = initialize_HLike_2P_model ( \"O\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) O7_1s3p_model = initialize_hci_line_model ( \"O7 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) O_660eV_model = initialize_hci_composite_model ( \"O 660eV Region\" , [ O8_2p_model , O7_1s3p_model ], has_linear_background = has_linear_background , peak_component_name = \"O8 2p 2P* J=3/2\" , ) models_dict [ O_660eV_model . _name ] = O_660eV_model return models_dict","title":"models"},{"location":"docstrings2/#bookkeeping","text":"import_asd.py Tool for converting a NIST ASD levels sql dump into a pickle file February 2020 Paul Szypryt","title":"Bookkeeping"},{"location":"docstrings2/#mass2.calibration.import_asd.parseLine","text":"Parse a line from the ASD sql dump and add it to the energyLevelsDict Parameters: energyLevelsDict ( dict [ str , dict [ int , dict [ str , list [ float ]]]] ) \u2013 description fieldNamesDict ( dict [ str , Any ] ) \u2013 description formattedLine ( str ) \u2013 description Source code in mass2/calibration/import_asd.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def parseLine ( energyLevelsDict : dict [ str , dict [ int , dict [ str , list [ float ]]]], fieldNamesDict : dict [ str , Any ], formattedLine : str ) -> None : \"\"\"Parse a line from the ASD sql dump and add it to the energyLevelsDict Parameters ---------- energyLevelsDict : dict[str, dict[int, dict[str, list[float]]]] _description_ fieldNamesDict : dict[str, Any] _description_ formattedLine : str _description_ \"\"\" lineAsArray = np . array ( ast . literal_eval ( formattedLine )) for iEntry in lineAsArray : element = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"element\" )] spectr_charge = int ( iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"spectr_charge\" )]) # Pull information that will be used to name dictionary keys conf = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"conf\" )] term = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"term\" )] j_val = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"j_val\" )] # Pull energy and uncertainty energy = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"energy\" )] # cm^-1, str unc = iEntry [ fieldNamesDict [ \"ASD_Levels\" ] . index ( \"unc\" )] # cm^-1, str try : energy_inv_cm = float ( energy ) # cm^-1 except ValueError : energy_inv_cm = np . nan try : unc_inv_cm = float ( unc ) # cm^-1 except ValueError : unc_inv_cm = np . nan if conf and term and term != \"*\" : # Set up upper level dictionary if element not in energyLevelsDict . keys (): energyLevelsDict [ element ] = {} if spectr_charge not in energyLevelsDict [ element ] . keys (): energyLevelsDict [ element ][ spectr_charge ] = {} levelName = f \" { conf } { term } J= { j_val } \" energyLevelsDict [ element ][ spectr_charge ][ levelName ] = [ energy_inv_cm , unc_inv_cm ]","title":"parseLine"},{"location":"docstrings2/#mass2.calibration.import_asd.write_asd_pickle","text":"Write the levels from a NIST Atomic Spectra Database SQL dump to a pickle file Parameters: inputFilename ( str ) \u2013 The ASD's sql dump file name outputFilename ( str ) \u2013 The pickle file name to write the output dictionary to Source code in mass2/calibration/import_asd.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def write_asd_pickle ( inputFilename : str , outputFilename : str ) -> None : \"\"\"Write the levels from a NIST Atomic Spectra Database SQL dump to a pickle file Parameters ---------- inputFilename : str The ASD's sql dump file name outputFilename : str The pickle file name to write the output dictionary to \"\"\" createTableString = \"CREATE TABLE\" valueSearchString = r \"\\`([^\\`]*)\\`\" tableName = \"\" fieldNamesDict : dict [ str , Any ] = {} energyLevelsDict : dict [ str , dict [ int , dict [ str , list [ float ]]]] = {} with open ( inputFilename , \"r\" , encoding = \"utf-8\" ) as ASD_file : for line in ASD_file : # Create dictionary of field names for various tables if line . startswith ( createTableString ): match = re . search ( valueSearchString , line ) if match is not None : fieldNamesDict [ match . groups ()[ 0 ]] = [] elif tableName and line . strip () . startswith ( \"`\" ): match = re . search ( valueSearchString , line ) if match is not None : fieldNamesDict [ tableName ] . append ( match . groups ()[ 0 ]) # Parse Levels portion elif line . startswith ( \"INSERT INTO `ASD_Levels` VALUES\" ): partitionedLine = line . partition ( \" VALUES \" )[ - 1 ] . strip () nullReplacedLine = partitionedLine . replace ( \"NULL\" , \"''\" ) formattedLine = nullReplacedLine if nullReplacedLine [ - 1 ] == \";\" : formattedLine = nullReplacedLine [: - 1 ] parseLine ( energyLevelsDict , fieldNamesDict , formattedLine ) # Sort levels within an element/charge state by energy outputDict : dict [ str , dict [ int , dict [ str , list [ float ]]]] = {} for iElement , element in energyLevelsDict . items (): for iCharge , chargestate in element . items (): energyOrder = np . argsort ( np . array ( list ( chargestate . values ()))[:, 0 ]) orderedKeys = np . array ( list ( chargestate . keys ()))[ energyOrder ] orderedValues = np . array ( list ( chargestate . values ()))[ energyOrder ] for i , iKey in enumerate ( list ( orderedKeys )): if iElement not in outputDict . keys (): outputDict [ iElement ] = {} if iCharge not in outputDict [ iElement ] . keys (): outputDict [ iElement ][ iCharge ] = {} outputDict [ iElement ][ iCharge ][ str ( iKey )] = orderedValues [ i ] . tolist () # Write dict to pickle file with open ( outputFilename , \"wb\" ) as handle : pickle . dump ( outputDict , handle , protocol = 2 ) nist_xray_database Download the NIST x-ray line database from the website, and parse the downloaded data into useable form. For loading a file (locally, from disk) and plotting some information: * NISTXrayDBFile * plot_line_uncertainties For updating the data files: * NISTXrayDBRetrieve * GetAllLines Basic usage (assuming you put the x-ray files in ${MASS_HOME}/mass2/calibration/nist_xray_data.dat): J. Fowler, NIST February 2014","title":"write_asd_pickle"},{"location":"docstrings2/#mass2.calibration.nist_xray_database.NISTXrayDBFile","text":"A NIST X-ray database file, loaded from disk. Source code in mass2/calibration/nist_xray_database.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class NISTXrayDBFile : \"\"\"A NIST X-ray database file, loaded from disk.\"\"\" DEFAULT_FILENAMES = \"nist_xray_data.dat\" , \"low_z_xray_data.dat\" def __init__ ( self , * filenames : str ): \"\"\"Initialize the database from 1 or more <filenames>, which point to files downloaded using NISTXrayDBRetrieve. If the list is empty (the default), then the file named by self.DEFAULT_FILENAME will be used.\"\"\" self . lines = {} self . alllines = set () if not filenames : path = os . path . split ( __file__ )[ 0 ] filenames = tuple ([ os . path . join ( path , df ) for df in self . DEFAULT_FILENAMES ]) self . loaded_filenames = [] for filename in filenames : try : fp = open ( filename , \"r\" , encoding = \"utf-8\" ) except OSError : print ( f \"' { filename } ' is not a readable file with X-ray database info! Continuing...\" ) continue while True : line = fp . readline () if \"Theory\" in line and \"Blend\" in line and \"Ref.\" in line : break for textline in fp . readlines (): try : xrayline = NISTXrayLine ( textline ) self . lines [ xrayline . name ] = xrayline self . alllines . add ( xrayline ) except Exception : continue self . loaded_filenames . append ( filename ) fp . close () LINE_NICKNAMES = { \"KA1\" : \"KL3\" , \"KA2\" : \"KL2\" , \"KB1\" : \"KM3\" , \"KB3\" : \"KM2\" , \"KB5\" : \"KM5\" , \"LA1\" : \"L3M5\" , \"LA2\" : \"L3M4\" , \"Ll\" : \"L3M1\" , \"LB3\" : \"L1M3\" , \"LB1\" : \"L2M4\" , \"LB2\" : \"L3N5\" , \"LG1\" : \"L2N4\" , } def get_lines_by_type ( self , linetype : str ) -> tuple [ \"NISTXrayLine\" ]: \"\"\"Return a tuple containing all lines of a certain type, e.g., \"KL3\". See self.LINE_NICKNAMES for some known line \"nicknames\".\"\"\" linetype = linetype . upper () if \"ALPHA\" in linetype : linetype = linetype . replace ( \"ALPHA\" , \"A\" ) elif \"BETA\" in linetype : linetype = linetype . replace ( \"BETA\" , \"B\" ) elif \"GAMMA\" in linetype : linetype = linetype . replace ( \"GAMMA\" , \"G\" ) linetype = self . LINE_NICKNAMES . get ( linetype , linetype ) lines = [] for element in ELEMENTS : linename = f \" { element } { linetype } \" if linename in self . lines : lines . append ( self . lines [ linename ]) return tuple ( lines ) def __getitem__ ( self , key : str ) -> \"NISTXrayLine\" : \"\"\"Get a line by its full name, e.g., \"Fe KL3\", or by a nickname, e.g., \"Fe Kalpha1\". Parameters ---------- key : str The line name or nickname Returns ------- NISTXrayLine The matching NISTXrayLine object Raises ------ KeyError If not found \"\"\" element , line = key . split ()[: 2 ] element = element . capitalize () line = line . upper () key = f \" { element } { line } \" if key in self . lines : return self . lines [ key ] lcline = line . lower () lcline = lcline . replace ( \"alpha\" , \"a\" ) lcline = lcline . replace ( \"beta\" , \"b\" ) lcline = lcline . replace ( \"gamma\" , \"g\" ) if lcline in self . LINE_NICKNAMES : key = f \" { element } { self . LINE_NICKNAMES [ lcline ] } \" return self . lines [ key ] raise KeyError ( f \" { key } is not a known line or line nickname\" )","title":"NISTXrayDBFile"},{"location":"docstrings2/#mass2.calibration.nist_xray_database.NISTXrayDBFile.__getitem__","text":"Get a line by its full name, e.g., \"Fe KL3\", or by a nickname, e.g., \"Fe Kalpha1\". Parameters: key ( str ) \u2013 The line name or nickname Returns: NISTXrayLine \u2013 The matching NISTXrayLine object Raises: KeyError \u2013 If not found Source code in mass2/calibration/nist_xray_database.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def __getitem__ ( self , key : str ) -> \"NISTXrayLine\" : \"\"\"Get a line by its full name, e.g., \"Fe KL3\", or by a nickname, e.g., \"Fe Kalpha1\". Parameters ---------- key : str The line name or nickname Returns ------- NISTXrayLine The matching NISTXrayLine object Raises ------ KeyError If not found \"\"\" element , line = key . split ()[: 2 ] element = element . capitalize () line = line . upper () key = f \" { element } { line } \" if key in self . lines : return self . lines [ key ] lcline = line . lower () lcline = lcline . replace ( \"alpha\" , \"a\" ) lcline = lcline . replace ( \"beta\" , \"b\" ) lcline = lcline . replace ( \"gamma\" , \"g\" ) if lcline in self . LINE_NICKNAMES : key = f \" { element } { self . LINE_NICKNAMES [ lcline ] } \" return self . lines [ key ] raise KeyError ( f \" { key } is not a known line or line nickname\" )","title":"__getitem__"},{"location":"docstrings2/#mass2.calibration.nist_xray_database.NISTXrayDBFile.__init__","text":"Initialize the database from 1 or more , which point to files downloaded using NISTXrayDBRetrieve. If the list is empty (the default), then the file named by self.DEFAULT_FILENAME will be used. Source code in mass2/calibration/nist_xray_database.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def __init__ ( self , * filenames : str ): \"\"\"Initialize the database from 1 or more <filenames>, which point to files downloaded using NISTXrayDBRetrieve. If the list is empty (the default), then the file named by self.DEFAULT_FILENAME will be used.\"\"\" self . lines = {} self . alllines = set () if not filenames : path = os . path . split ( __file__ )[ 0 ] filenames = tuple ([ os . path . join ( path , df ) for df in self . DEFAULT_FILENAMES ]) self . loaded_filenames = [] for filename in filenames : try : fp = open ( filename , \"r\" , encoding = \"utf-8\" ) except OSError : print ( f \"' { filename } ' is not a readable file with X-ray database info! Continuing...\" ) continue while True : line = fp . readline () if \"Theory\" in line and \"Blend\" in line and \"Ref.\" in line : break for textline in fp . readlines (): try : xrayline = NISTXrayLine ( textline ) self . lines [ xrayline . name ] = xrayline self . alllines . add ( xrayline ) except Exception : continue self . loaded_filenames . append ( filename ) fp . close ()","title":"__init__"},{"location":"docstrings2/#mass2.calibration.nist_xray_database.NISTXrayDBFile.get_lines_by_type","text":"Return a tuple containing all lines of a certain type, e.g., \"KL3\". See self.LINE_NICKNAMES for some known line \"nicknames\". Source code in mass2/calibration/nist_xray_database.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def get_lines_by_type ( self , linetype : str ) -> tuple [ \"NISTXrayLine\" ]: \"\"\"Return a tuple containing all lines of a certain type, e.g., \"KL3\". See self.LINE_NICKNAMES for some known line \"nicknames\".\"\"\" linetype = linetype . upper () if \"ALPHA\" in linetype : linetype = linetype . replace ( \"ALPHA\" , \"A\" ) elif \"BETA\" in linetype : linetype = linetype . replace ( \"BETA\" , \"B\" ) elif \"GAMMA\" in linetype : linetype = linetype . replace ( \"GAMMA\" , \"G\" ) linetype = self . LINE_NICKNAMES . get ( linetype , linetype ) lines = [] for element in ELEMENTS : linename = f \" { element } { linetype } \" if linename in self . lines : lines . append ( self . lines [ linename ]) return tuple ( lines )","title":"get_lines_by_type"},{"location":"docstrings2/#mass2.calibration.nist_xray_database.NISTXrayLine","text":"A single line from the NIST X-ray database. Source code in mass2/calibration/nist_xray_database.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class NISTXrayLine : \"\"\"A single line from the NIST X-ray database.\"\"\" DEFAULT_COLUMN_DEFS = { \"element\" : ( 1 , 4 ), \"transition\" : ( 10 , 16 ), \"peak\" : ( 45 , 59 ), \"peak_unc\" : ( 61 , 72 ), \"blend\" : ( 74 , 79 ), \"ref\" : ( 81 , 91 ), } def __init__ ( self , textline : str , column_defs : dict [ str , tuple [ int , int ]] | None = None ): \"\"\"Initialize a NISTXrayLine from a line of text found in the NIST x-ray database file. Parameters ---------- textline : str The text line from the database file column_defs : dict[str, tuple[int, int]] | None, optional The column boundaries of the relevant data, by default None \"\"\" self . element = \"\" self . transition = \"\" self . peak = 0.0 self . peak_unc = 0.0 self . blend = \"\" self . ref = \"\" if column_defs is None : column_defs = self . DEFAULT_COLUMN_DEFS for name , colrange in column_defs . items (): a = colrange [ 0 ] - 1 b = colrange [ 1 ] self . __dict__ [ name ] = textline [ a : b ] . rstrip () self . peak = float ( self . peak ) self . peak_unc = float ( self . peak_unc ) self . name = f \" { self . element } { self . transition } \" self . raw = textline . rstrip () def __str__ ( self ) -> str : \"\"\"The user-friendly string representation of the line\"\"\" return f \" { self . element } { self . transition } line: { self . peak : .3f } +- { self . peak_unc : .3f } eV\" def __repr__ ( self ) -> str : \"The code representation of the line\" return self . raw","title":"NISTXrayLine"},{"location":"docstrings2/#mass2.calibration.nist_xray_database.NISTXrayLine.__init__","text":"Initialize a NISTXrayLine from a line of text found in the NIST x-ray database file. Parameters: textline ( str ) \u2013 The text line from the database file column_defs ( dict [ str , tuple [ int , int ]] | None , default: None ) \u2013 The column boundaries of the relevant data, by default None Source code in mass2/calibration/nist_xray_database.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def __init__ ( self , textline : str , column_defs : dict [ str , tuple [ int , int ]] | None = None ): \"\"\"Initialize a NISTXrayLine from a line of text found in the NIST x-ray database file. Parameters ---------- textline : str The text line from the database file column_defs : dict[str, tuple[int, int]] | None, optional The column boundaries of the relevant data, by default None \"\"\" self . element = \"\" self . transition = \"\" self . peak = 0.0 self . peak_unc = 0.0 self . blend = \"\" self . ref = \"\" if column_defs is None : column_defs = self . DEFAULT_COLUMN_DEFS for name , colrange in column_defs . items (): a = colrange [ 0 ] - 1 b = colrange [ 1 ] self . __dict__ [ name ] = textline [ a : b ] . rstrip () self . peak = float ( self . peak ) self . peak_unc = float ( self . peak_unc ) self . name = f \" { self . element } { self . transition } \" self . raw = textline . rstrip ()","title":"__init__"},{"location":"docstrings2/#mass2.calibration.nist_xray_database.NISTXrayLine.__repr__","text":"The code representation of the line Source code in mass2/calibration/nist_xray_database.py 286 287 288 def __repr__ ( self ) -> str : \"The code representation of the line\" return self . raw","title":"__repr__"},{"location":"docstrings2/#mass2.calibration.nist_xray_database.NISTXrayLine.__str__","text":"The user-friendly string representation of the line Source code in mass2/calibration/nist_xray_database.py 282 283 284 def __str__ ( self ) -> str : \"\"\"The user-friendly string representation of the line\"\"\" return f \" { self . element } { self . transition } line: { self . peak : .3f } +- { self . peak_unc : .3f } eV\"","title":"__str__"},{"location":"docstrings2/#mass2.calibration.nist_xray_database.plot_line_energies","text":"Plot the energies of some common families of lines from the NIST X-ray database. Source code in mass2/calibration/nist_xray_database.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def plot_line_energies () -> None : \"\"\"Plot the energies of some common families of lines from the NIST X-ray database.\"\"\" db = NISTXrayDBFile () plt . clf () cm = plt . cm . nipy_spectral transitions = ( \"KL2\" , \"KL3\" , \"KM5\" , \"KM3\" , \"KM2\" , \"L3M5\" , \"L3M4\" , \"L3M1\" , \"L2M4\" , \"L2N4\" , \"L3N5\" , \"L1M3\" , \"L3N7\" , \"M5N7\" , \"M5N6\" , \"M4N6\" , \"M3N5\" , \"M3N4\" , ) for i , linetype in enumerate ( transitions ): lines = db . get_lines_by_type ( linetype ) z = [ ATOMIC_NUMBERS [ line . element ] for line in lines ] e = [ line . peak for line in lines ] plt . loglog ( z , e , \"o-\" , color = cm ( float ( i ) / len ( transitions )), label = linetype ) plt . legend ( loc = \"upper left\" ) plt . xlim ([ 6 , 100 ]) plt . grid () r = list ( range ( 6 , 22 )) + list ( range ( 22 , 43 , 2 )) + list ( range ( 45 , 75 , 3 )) + list ( range ( 75 , 100 , 5 )) plt . xticks ( r , [ \" \\n \" . join ([ ELEMENTS [ i ], str ( i )]) for i in r ])","title":"plot_line_energies"},{"location":"docstrings2/#mass2.calibration.nist_xray_database.plot_line_uncertainties","text":"Plot the uncertainties of some common families of lines from the NIST X-ray database. Source code in mass2/calibration/nist_xray_database.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def plot_line_uncertainties () -> None : \"\"\"Plot the uncertainties of some common families of lines from the NIST X-ray database.\"\"\" db = NISTXrayDBFile () transitions = ( \"KL3\" , \"KL2\" , \"KM3\" , \"KM5\" , \"L3M5\" , \"L3M4\" , \"L2M4\" , \"L3N5\" , \"L2N4\" , \"L1M3\" , \"L3N7\" , \"L3M1\" ) titles = { \"KL3\" : \"K$ \\\\ alpha_1$: Intense\" , \"KL2\" : \"K$ \\\\ alpha_2$: Intense, but not easily resolved\" , \"KM3\" : \"K$ \\\\ beta_1$: Intense\" , \"KM2\" : \"K$ \\\\ beta_3$: Intense, usually unresolvable\" , \"KM5\" : \"K$ \\\\ beta_5$: Weak line on high-E tail of K$ \\\\ beta_1$\" , \"L3M5\" : \"L$ \\\\ alpha_1$: Prominent\" , \"L3M4\" : \"L$ \\\\ alpha_2$: Small satellite\" , \"L2M4\" : \"L$ \\\\ beta_1$: Prominent\" , \"L3N5\" : \"L$ \\\\ beta_2$: Prominent\" , \"L2N4\" : \"K$ \\\\ gamma_1$: Weaker\" , \"L1M3\" : \"L$ \\\\ beta_3$: Weaker\" , \"L3N7\" : \"Lu: barely visible\" , \"L3M1\" : \"L$ \\\\ ell$: very weak\" , } axes = {} NX , NY = 3 , 4 plt . clf () for i , tr in enumerate ( transitions ): axes [ i ] = plt . subplot ( NY , NX , i + 1 ) plt . loglog () plt . grid ( True ) plt . title ( titles [ tr ]) if i >= NX * ( NY - 1 ): plt . xlabel ( \"Line energy (eV)\" ) if i % NX == 0 : plt . ylabel ( \"Line uncertainty (eV)\" ) plt . ylim ([ 1e-3 , 10 ]) plt . xlim ([ 100 , 3e4 ]) for line in db . lines . values (): if line . transition not in transitions : continue i = transitions . index ( line . transition ) plt . sca ( axes [ i ]) plt . plot ( line . peak , line . peak_unc , \"or\" ) plt . text ( line . peak , line . peak_unc , line . name )","title":"plot_line_uncertainties"},{"location":"docstrings2/#materials-transmission","text":"Models for X-ray filter and detector efficiency.","title":"Materials transmission"},{"location":"docstrings2/#mass2.materials.efficiency_models.Filter","text":"Represent a single material layer in a FilterStack Source code in mass2/materials/efficiency_models.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 @dataclass ( frozen = True ) class Filter : \"\"\"Represent a single material layer in a FilterStack\"\"\" name : str material : NDArray atomic_number : NDArray density_g_per_cm3 : NDArray [ np . float64 ] thickness_cm : NDArray [ np . float64 ] fill_fraction : Variable = ufloat ( 1.0 , 1e-8 ) absorber : bool = False def get_efficiency ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Return the efficiency of this Filter at the given x-ray energies.\"\"\" optical_depth = np . vstack ([ xraydb . material_mu ( m , xray_energies_eV , density = d ) * t for ( m , d , t ) in zip ( self . material , self . density_g_per_cm3 , self . thickness_cm ) ]) individual_transmittance = unp . exp ( - optical_depth ) transmittance = np . prod ( individual_transmittance , axis = 0 ) if self . absorber : efficiency = ( 1.0 - transmittance ) * self . fill_fraction else : efficiency = ( transmittance * self . fill_fraction ) + ( 1.0 - self . fill_fraction ) if uncertain : return efficiency else : return unp . nominal_values ( efficiency ) def __repr__ ( self ) -> str : \"\"\"Return a string representation of the Filter object.\"\"\" s = f \" { type ( self ) } (\" for material , density , thick in zip ( self . material , self . density_g_per_cm3 , self . thickness_cm ): area_density = density * thick s += f \" { material } { area_density : .3g } g/cm^2, \" s += f \"fill_fraction= { self . fill_fraction : .3f } , absorber= { self . absorber } )\" return s @classmethod def newfilter ( cls , name : str , material : ArrayLike , area_density_g_per_cm2 : ArrayLike | None = None , thickness_nm : ArrayLike | None = None , density_g_per_cm3 : ArrayLike | None = None , fill_fraction : Variable = ufloat ( 1 , 1e-8 ), absorber : bool = False , ) -> \"Filter\" : \"\"\"Create a Filter from the given parameters, filling in defaults as needed.\"\"\" material = np . array ( material , ndmin = 1 ) atomic_number = np . array ([ xraydb . atomic_number ( iMaterial ) for iMaterial in material ], ndmin = 1 ) fill_fraction = ensure_uncertain ( fill_fraction ) # Save density, either default values for that element, or the given density. if density_g_per_cm3 is None : density_g_per_cm3 = np . array ([ xraydb . atomic_density ( int ( iAtomicNumber )) for iAtomicNumber in atomic_number ], ndmin = 1 ) else : density_g_per_cm3 = np . array ( density_g_per_cm3 , ndmin = 1 ) assert len ( material ) == len ( density_g_per_cm3 ) # Handle input value of areal density or thickness, but not both. assert np . logical_xor ( area_density_g_per_cm2 is None , thickness_nm is None ), ( \"must specify either areal density or thickness, not both\" ) if thickness_nm is not None : thickness_cm = np . array ( thickness_nm , ndmin = 1 ) * 1e-7 elif area_density_g_per_cm2 is not None : area_density_g_per_cm2 = np . array ( area_density_g_per_cm2 , ndmin = 1 ) thickness_cm = area_density_g_per_cm2 / density_g_per_cm3 if np . ndim == 0 : thickness_cm = np . array ( thickness_cm , ndmin = 1 ) else : raise ValueError ( \"must specify either areal density or thickness, not both\" ) thickness_cm = ensure_uncertain ( thickness_cm ) assert len ( thickness_cm ) >= 1 return cls ( name , material , atomic_number , density_g_per_cm3 , thickness_cm , fill_fraction , absorber )","title":"Filter"},{"location":"docstrings2/#mass2.materials.efficiency_models.Filter.__repr__","text":"Return a string representation of the Filter object. Source code in mass2/materials/efficiency_models.py 133 134 135 136 137 138 139 140 def __repr__ ( self ) -> str : \"\"\"Return a string representation of the Filter object.\"\"\" s = f \" { type ( self ) } (\" for material , density , thick in zip ( self . material , self . density_g_per_cm3 , self . thickness_cm ): area_density = density * thick s += f \" { material } { area_density : .3g } g/cm^2, \" s += f \"fill_fraction= { self . fill_fraction : .3f } , absorber= { self . absorber } )\" return s","title":"__repr__"},{"location":"docstrings2/#mass2.materials.efficiency_models.Filter.get_efficiency","text":"Return the efficiency of this Filter at the given x-ray energies. Source code in mass2/materials/efficiency_models.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def get_efficiency ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Return the efficiency of this Filter at the given x-ray energies.\"\"\" optical_depth = np . vstack ([ xraydb . material_mu ( m , xray_energies_eV , density = d ) * t for ( m , d , t ) in zip ( self . material , self . density_g_per_cm3 , self . thickness_cm ) ]) individual_transmittance = unp . exp ( - optical_depth ) transmittance = np . prod ( individual_transmittance , axis = 0 ) if self . absorber : efficiency = ( 1.0 - transmittance ) * self . fill_fraction else : efficiency = ( transmittance * self . fill_fraction ) + ( 1.0 - self . fill_fraction ) if uncertain : return efficiency else : return unp . nominal_values ( efficiency )","title":"get_efficiency"},{"location":"docstrings2/#mass2.materials.efficiency_models.Filter.newfilter","text":"Create a Filter from the given parameters, filling in defaults as needed. Source code in mass2/materials/efficiency_models.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 @classmethod def newfilter ( cls , name : str , material : ArrayLike , area_density_g_per_cm2 : ArrayLike | None = None , thickness_nm : ArrayLike | None = None , density_g_per_cm3 : ArrayLike | None = None , fill_fraction : Variable = ufloat ( 1 , 1e-8 ), absorber : bool = False , ) -> \"Filter\" : \"\"\"Create a Filter from the given parameters, filling in defaults as needed.\"\"\" material = np . array ( material , ndmin = 1 ) atomic_number = np . array ([ xraydb . atomic_number ( iMaterial ) for iMaterial in material ], ndmin = 1 ) fill_fraction = ensure_uncertain ( fill_fraction ) # Save density, either default values for that element, or the given density. if density_g_per_cm3 is None : density_g_per_cm3 = np . array ([ xraydb . atomic_density ( int ( iAtomicNumber )) for iAtomicNumber in atomic_number ], ndmin = 1 ) else : density_g_per_cm3 = np . array ( density_g_per_cm3 , ndmin = 1 ) assert len ( material ) == len ( density_g_per_cm3 ) # Handle input value of areal density or thickness, but not both. assert np . logical_xor ( area_density_g_per_cm2 is None , thickness_nm is None ), ( \"must specify either areal density or thickness, not both\" ) if thickness_nm is not None : thickness_cm = np . array ( thickness_nm , ndmin = 1 ) * 1e-7 elif area_density_g_per_cm2 is not None : area_density_g_per_cm2 = np . array ( area_density_g_per_cm2 , ndmin = 1 ) thickness_cm = area_density_g_per_cm2 / density_g_per_cm3 if np . ndim == 0 : thickness_cm = np . array ( thickness_cm , ndmin = 1 ) else : raise ValueError ( \"must specify either areal density or thickness, not both\" ) thickness_cm = ensure_uncertain ( thickness_cm ) assert len ( thickness_cm ) >= 1 return cls ( name , material , atomic_number , density_g_per_cm3 , thickness_cm , fill_fraction , absorber )","title":"newfilter"},{"location":"docstrings2/#mass2.materials.efficiency_models.FilterStack","text":"Represent a sequence of named materials Source code in mass2/materials/efficiency_models.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @dataclass () class FilterStack : \"\"\"Represent a sequence of named materials\"\"\" name : str components : list [ \"Filter | FilterStack\" ] = field ( default_factory = list ) def add ( self , film : \"Filter | FilterStack\" ) -> None : \"\"\"Add a Filter or FilterStack to this FilterStack.\"\"\" self . components . append ( film ) def add_filter ( self , name : str , material : str , area_density_g_per_cm2 : float | None = None , thickness_nm : float | None = None , density_g_per_cm3 : float | None = None , fill_fraction : Variable = ufloat ( 1 , 1e-8 ), absorber : bool = False , ) -> None : \"\"\"Create and add a Filter layer to this FilterStack.\"\"\" self . add ( Filter . newfilter ( name , material , area_density_g_per_cm2 = area_density_g_per_cm2 , thickness_nm = thickness_nm , density_g_per_cm3 = density_g_per_cm3 , fill_fraction = fill_fraction , absorber = absorber , ) ) def get_efficiency ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Return the overall efficiency of this FilterStack at the given x-ray energies.\"\"\" assert len ( self . components ) > 0 , f \" { self . name } has no components of which to calculate efficiency\" individual_efficiency = np . array ([ iComponent . get_efficiency ( xray_energies_eV , uncertain = uncertain ) for iComponent in self . components ]) efficiency = np . prod ( individual_efficiency , axis = 0 ) if uncertain : return efficiency else : return unp . nominal_values ( efficiency ) def __call__ ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Equivalent to get_efficiency.\"\"\" return self . get_efficiency ( xray_energies_eV , uncertain = uncertain ) def plot_efficiency ( self , xray_energies_eV : ArrayLike , ax : plt . Axes | None = None ) -> None : \"\"\"Plot the efficiency of this FilterStack and its components.\"\"\" efficiency = unp . nominal_values ( self . get_efficiency ( xray_energies_eV )) if ax is None : fig = plt . figure () ax = fig . add_subplot ( 111 ) ax . plot ( xray_energies_eV , efficiency * 100.0 , label = \"total\" , lw = 2 ) ax . set_xlabel ( \"Energy (keV)\" ) ax . set_ylabel ( \"Efficiency (%)\" ) ax . set_title ( self . name ) ax . set_title ( f \" { self . name } Efficiency\" ) for v in self . components : efficiency = v . get_efficiency ( xray_energies_eV ) ax . plot ( xray_energies_eV , efficiency * 100.0 , \"--\" , label = v . name ) ax . legend () def __repr__ ( self ) -> str : \"\"\"Return a string representation of the FilterStack object.\"\"\" s = f \" { type ( self ) } ( \\n \" for v in self . components : s += f \" { v . name } : { v } \\n \" s += \")\" return s","title":"FilterStack"},{"location":"docstrings2/#mass2.materials.efficiency_models.FilterStack.__call__","text":"Equivalent to get_efficiency. Source code in mass2/materials/efficiency_models.py 72 73 74 def __call__ ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Equivalent to get_efficiency.\"\"\" return self . get_efficiency ( xray_energies_eV , uncertain = uncertain )","title":"__call__"},{"location":"docstrings2/#mass2.materials.efficiency_models.FilterStack.__repr__","text":"Return a string representation of the FilterStack object. Source code in mass2/materials/efficiency_models.py 95 96 97 98 99 100 101 def __repr__ ( self ) -> str : \"\"\"Return a string representation of the FilterStack object.\"\"\" s = f \" { type ( self ) } ( \\n \" for v in self . components : s += f \" { v . name } : { v } \\n \" s += \")\" return s","title":"__repr__"},{"location":"docstrings2/#mass2.materials.efficiency_models.FilterStack.add","text":"Add a Filter or FilterStack to this FilterStack. Source code in mass2/materials/efficiency_models.py 33 34 35 def add ( self , film : \"Filter | FilterStack\" ) -> None : \"\"\"Add a Filter or FilterStack to this FilterStack.\"\"\" self . components . append ( film )","title":"add"},{"location":"docstrings2/#mass2.materials.efficiency_models.FilterStack.add_filter","text":"Create and add a Filter layer to this FilterStack. Source code in mass2/materials/efficiency_models.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def add_filter ( self , name : str , material : str , area_density_g_per_cm2 : float | None = None , thickness_nm : float | None = None , density_g_per_cm3 : float | None = None , fill_fraction : Variable = ufloat ( 1 , 1e-8 ), absorber : bool = False , ) -> None : \"\"\"Create and add a Filter layer to this FilterStack.\"\"\" self . add ( Filter . newfilter ( name , material , area_density_g_per_cm2 = area_density_g_per_cm2 , thickness_nm = thickness_nm , density_g_per_cm3 = density_g_per_cm3 , fill_fraction = fill_fraction , absorber = absorber , ) )","title":"add_filter"},{"location":"docstrings2/#mass2.materials.efficiency_models.FilterStack.get_efficiency","text":"Return the overall efficiency of this FilterStack at the given x-ray energies. Source code in mass2/materials/efficiency_models.py 60 61 62 63 64 65 66 67 68 69 70 def get_efficiency ( self , xray_energies_eV : ArrayLike , uncertain : bool = False ) -> NDArray : \"\"\"Return the overall efficiency of this FilterStack at the given x-ray energies.\"\"\" assert len ( self . components ) > 0 , f \" { self . name } has no components of which to calculate efficiency\" individual_efficiency = np . array ([ iComponent . get_efficiency ( xray_energies_eV , uncertain = uncertain ) for iComponent in self . components ]) efficiency = np . prod ( individual_efficiency , axis = 0 ) if uncertain : return efficiency else : return unp . nominal_values ( efficiency )","title":"get_efficiency"},{"location":"docstrings2/#mass2.materials.efficiency_models.FilterStack.plot_efficiency","text":"Plot the efficiency of this FilterStack and its components. Source code in mass2/materials/efficiency_models.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def plot_efficiency ( self , xray_energies_eV : ArrayLike , ax : plt . Axes | None = None ) -> None : \"\"\"Plot the efficiency of this FilterStack and its components.\"\"\" efficiency = unp . nominal_values ( self . get_efficiency ( xray_energies_eV )) if ax is None : fig = plt . figure () ax = fig . add_subplot ( 111 ) ax . plot ( xray_energies_eV , efficiency * 100.0 , label = \"total\" , lw = 2 ) ax . set_xlabel ( \"Energy (keV)\" ) ax . set_ylabel ( \"Efficiency (%)\" ) ax . set_title ( self . name ) ax . set_title ( f \" { self . name } Efficiency\" ) for v in self . components : efficiency = v . get_efficiency ( xray_energies_eV ) ax . plot ( xray_energies_eV , efficiency * 100.0 , \"--\" , label = v . name ) ax . legend ()","title":"plot_efficiency"},{"location":"docstrings2/#mass2.materials.efficiency_models.AlFilmWithOxide","text":"Create a Filter made of an alumninum film with oxides on one or both surfaces Args: name: name given to filter object, e.g. '50K Filter'. Al_thickness_nm: thickness, in nm, of Al film Al_density_g_per_cm3: Al film density, in g/cm3, defaults to xraydb value num_oxidized_surfaces: Number of film surfaces that contain a native oxide, default 2 oxide_density_g_per_cm3: Al2O3 oxide density, in g/cm3, defaults to bulk xraydb value Source code in mass2/materials/efficiency_models.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def AlFilmWithOxide ( name : str , Al_thickness_nm : float , Al_density_g_per_cm3 : float | None = None , num_oxidized_surfaces : int = 2 , oxide_density_g_per_cm3 : ArrayLike | None = None , ) -> Filter : \"\"\"Create a Filter made of an alumninum film with oxides on one or both surfaces Args: name: name given to filter object, e.g. '50K Filter'. Al_thickness_nm: thickness, in nm, of Al film Al_density_g_per_cm3: Al film density, in g/cm3, defaults to xraydb value num_oxidized_surfaces: Number of film surfaces that contain a native oxide, default 2 oxide_density_g_per_cm3: Al2O3 oxide density, in g/cm3, defaults to bulk xraydb value \"\"\" assert num_oxidized_surfaces in { 1 , 2 }, \"only 1 or 2 oxidzed surfaces allowed\" if Al_density_g_per_cm3 is None : Al_density_g_per_cm3 = float ( xraydb . atomic_density ( \"Al\" )) arbE = 5000.0 # an arbitrary energy (5 keV) is used to get answers from material_mu_components() oxide_dict = xraydb . material_mu_components ( \"sapphire\" , arbE ) oxide_material = oxide_dict [ \"elements\" ] oxide_mass_fractions = [ oxide_dict [ x ][ 0 ] * oxide_dict [ x ][ 1 ] / oxide_dict [ \"mass\" ] for x in oxide_material ] # Assume oxidized surfaces are each 3 nm thick. num_oxide_elements = len ( oxide_material ) oxide_thickness_nm = np . repeat ( num_oxidized_surfaces * 3.0 , num_oxide_elements ) if oxide_density_g_per_cm3 is None : oxide_density_g_per_cm3 = np . repeat ( oxide_dict [ \"density\" ], num_oxide_elements ) else : oxide_density_g_per_cm3 = np . asarray ( oxide_density_g_per_cm3 ) material = np . hstack ([ \"Al\" , oxide_material ]) density_g_per_cm3 = np . hstack ([ Al_density_g_per_cm3 , oxide_density_g_per_cm3 * oxide_mass_fractions ]) thickness_nm = np . hstack ([ Al_thickness_nm , oxide_thickness_nm ]) return Filter . newfilter ( name , material , thickness_nm = thickness_nm , density_g_per_cm3 = density_g_per_cm3 )","title":"AlFilmWithOxide"},{"location":"docstrings2/#mass2.materials.efficiency_models.AlFilmWithPolymer","text":"Create a Filter made of an alumninum film with polymer backing Args: name: name given to filter object, e.g. '50K Filter'. Al_thickness_nm: thickness, in nm, of Al film polymer_thickness_nm: thickness, in nm, of filter backside polymer Al_density_g_per_cm3: Al film density, in g/cm3, defaults to xraydb value num_oxidized_surfaces: Number of film surfaces that contain a native oxide, default 2 oxide_density_g_per_cm3: Al2O3 oxide density, in g/cm3, defaults to bulk xraydb value polymer_density_g_per_cm3: Polymer density, in g/cm3, defaults to Kapton Source code in mass2/materials/efficiency_models.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def AlFilmWithPolymer ( name : str , Al_thickness_nm : float , polymer_thickness_nm : float , Al_density_g_per_cm3 : float | None = None , num_oxidized_surfaces : int = 1 , oxide_density_g_per_cm3 : float | None = None , polymer_density_g_per_cm3 : float | None = None , ) -> Filter : \"\"\"Create a Filter made of an alumninum film with polymer backing Args: name: name given to filter object, e.g. '50K Filter'. Al_thickness_nm: thickness, in nm, of Al film polymer_thickness_nm: thickness, in nm, of filter backside polymer Al_density_g_per_cm3: Al film density, in g/cm3, defaults to xraydb value num_oxidized_surfaces: Number of film surfaces that contain a native oxide, default 2 oxide_density_g_per_cm3: Al2O3 oxide density, in g/cm3, defaults to bulk xraydb value polymer_density_g_per_cm3: Polymer density, in g/cm3, defaults to Kapton \"\"\" assert num_oxidized_surfaces in { 1 , 2 }, \"only 1 or 2 oxidzed surfaces allowed\" if Al_density_g_per_cm3 is None : Al_density_g_per_cm3 = xraydb . atomic_density ( \"Al\" ) arbE = 5000.0 # an arbitrary energy (5 keV) is used to get answers from material_mu_components() oxide_dict = xraydb . material_mu_components ( \"sapphire\" , arbE ) oxide_thickness_nm = num_oxidized_surfaces * 3.0 # assume 3 nm per oxidized surface oxide_material = oxide_dict [ \"elements\" ] oxide_mass_fractions = np . array ([ oxide_dict [ x ][ 0 ] * oxide_dict [ x ][ 1 ] / oxide_dict [ \"mass\" ] for x in oxide_material ]) if oxide_density_g_per_cm3 is None : oxide_density_g_per_cm3 = oxide_dict [ \"density\" ] * np . ones ( len ( oxide_material )) polymer_dict = xraydb . material_mu_components ( \"kapton\" , arbE ) polymer_material = polymer_dict [ \"elements\" ] polymer_thickness_nm_array = np . ones ( len ( polymer_material )) * polymer_thickness_nm polymer_mass_fractions = np . array ([ polymer_dict [ x ][ 0 ] * polymer_dict [ x ][ 1 ] / polymer_dict [ \"mass\" ] for x in polymer_material ]) if polymer_density_g_per_cm3 is None : polymer_density_g_per_cm3 = polymer_dict [ \"density\" ] * np . ones ( len ( polymer_material )) material = np . hstack ([ \"Al\" , oxide_material , polymer_material ]) density_g_per_cm3 = np . hstack ([ [ Al_density_g_per_cm3 ], oxide_density_g_per_cm3 * oxide_mass_fractions , polymer_density_g_per_cm3 * polymer_mass_fractions , ]) thickness_nm = np . hstack ([ Al_thickness_nm , oxide_thickness_nm , polymer_thickness_nm_array ]) return Filter . newfilter ( name = name , material = material , thickness_nm = thickness_nm , density_g_per_cm3 = density_g_per_cm3 )","title":"AlFilmWithPolymer"},{"location":"docstrings2/#mass2.materials.efficiency_models.LEX_HT","text":"Create an Al film with polymer and stainless steel backing. Models the LEX-HT vacuum window. Args: name: name given to filter object, e.g. '50K Filter'. Source code in mass2/materials/efficiency_models.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def LEX_HT ( name : str ) -> FilterStack : \"\"\"Create an Al film with polymer and stainless steel backing. Models the LEX-HT vacuum window. Args: name: name given to filter object, e.g. '50K Filter'. \"\"\" # Set up Al + polyimide film film_material = [ \"C\" , \"H\" , \"N\" , \"O\" , \"Al\" ] film_area_density_g_per_cm2_given = np . array ([ 6.7e-5 , 2.6e-6 , 7.2e-6 , 1.7e-5 , 1.7e-5 ]) film_area_density_g_per_cm2 = with_fractional_uncertainty ( film_area_density_g_per_cm2_given , 0.03 ) film1 = Filter . newfilter ( name = \"LEX_HT Film\" , material = film_material , area_density_g_per_cm2 = film_area_density_g_per_cm2 ) # Set up mesh mesh_material = [ \"Fe\" , \"Cr\" , \"Ni\" , \"Mn\" , \"Si\" ] mesh_thickness = 100.0e-4 # cm mesh_density = 8.0 # g/cm^3 mesh_material_fractions = np . array ([ 0.705 , 0.19 , 0.09 , 0.01 , 0.005 ]) # fraction by weight mesh_area_density_g_per_cm2_scalar = mesh_material_fractions * mesh_density * mesh_thickness # g/cm^2 mesh_area_density_g_per_cm2 = with_fractional_uncertainty ( mesh_area_density_g_per_cm2_scalar , 0.02 ) mesh_fill_fraction = ufloat ( 0.19 , 0.01 ) film2 = Filter . newfilter ( name = \"LEX_HT Mesh\" , material = mesh_material , area_density_g_per_cm2 = mesh_area_density_g_per_cm2 , fill_fraction = mesh_fill_fraction , ) stack = FilterStack ( name ) stack . add ( film1 ) stack . add ( film2 ) return stack","title":"LEX_HT"},{"location":"docstrings2/#mass2.materials.efficiency_models.get_filter_stacks_dict","text":"Create a dictionary with a few examples of FilterStack objects Returns: dict \u2013 A dictionary of named FilterStacks Source code in mass2/materials/efficiency_models.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 def get_filter_stacks_dict () -> dict [ str , FilterStack ]: \"\"\"Create a dictionary with a few examples of FilterStack objects Returns ------- dict A dictionary of named FilterStacks \"\"\" fs_dict : dict [ str , FilterStack ] = {} # EBIT Instrument EBIT_filter_stack = FilterStack ( name = \"EBIT 2018\" ) EBIT_filter_stack . add_filter ( name = \"Electroplated Au Absorber\" , material = \"Au\" , thickness_nm = with_fractional_uncertainty ( 965.5 , 0.03 ), absorber = True ) EBIT_filter_stack . add ( AlFilmWithOxide ( name = \"50mK Filter\" , Al_thickness_nm = with_fractional_uncertainty ( 112.5 , 0.02 ))) EBIT_filter_stack . add ( AlFilmWithOxide ( name = \"3K Filter\" , Al_thickness_nm = with_fractional_uncertainty ( 108.5 , 0.02 ))) filter_50K = FilterStack ( name = \"50K Filter\" ) filter_50K . add ( AlFilmWithOxide ( name = \"Al Film\" , Al_thickness_nm = with_fractional_uncertainty ( 102.6 , 0.02 ))) nickel = Filter . newfilter ( name = \"Ni Mesh\" , material = \"Ni\" , thickness_nm = ufloat ( 15.0e3 , 2e3 ), fill_fraction = ufloat ( 0.17 , 0.01 )) filter_50K . add ( nickel ) EBIT_filter_stack . add ( filter_50K ) luxel1 = LEX_HT ( \"Luxel Window TES\" ) luxel2 = LEX_HT ( \"Luxel Window EBIT\" ) EBIT_filter_stack . add ( luxel1 ) EBIT_filter_stack . add ( luxel2 ) fs_dict [ EBIT_filter_stack . name ] = EBIT_filter_stack # RAVEN Instrument RAVEN1_fs = FilterStack ( name = \"RAVEN1 2019\" ) RAVEN1_fs . add_filter ( name = \"Evaporated Bi Absorber\" , material = \"Bi\" , thickness_nm = 4.4e3 , absorber = True ) RAVEN1_fs . add ( AlFilmWithPolymer ( name = \"50mK Filter\" , Al_thickness_nm = 108.4 , polymer_thickness_nm = 206.4 )) RAVEN1_fs . add ( AlFilmWithPolymer ( name = \"3K Filter\" , Al_thickness_nm = 108.4 , polymer_thickness_nm = 206.4 )) RAVEN1_fs . add ( AlFilmWithOxide ( name = \"50K Filter\" , Al_thickness_nm = 1.0e3 )) RAVEN1_fs . add_filter ( name = \"Be TES Vacuum Window\" , material = \"Be\" , thickness_nm = 200.0e3 ) RAVEN1_fs . add ( AlFilmWithOxide ( name = \"e- Filter\" , Al_thickness_nm = 5.0e3 )) RAVEN1_fs . add_filter ( name = \"Be SEM Vacuum Window\" , material = \"Be\" , thickness_nm = 200.0e3 ) fs_dict [ RAVEN1_fs . name ] = RAVEN1_fs # Horton spring 2018, for metrology campaign. Horton_filter_stack = FilterStack ( name = \"Horton 2018\" ) Horton_filter_stack . add_filter ( name = \"Electroplated Au Absorber\" , material = \"Au\" , thickness_nm = 965.5 , absorber = True ) Horton_filter_stack . add ( AlFilmWithOxide ( name = \"50mK Filter\" , Al_thickness_nm = 5000 )) Horton_filter_stack . add ( AlFilmWithOxide ( name = \"3K Filter\" , Al_thickness_nm = 5000 )) Horton_filter_stack . add ( AlFilmWithOxide ( name = \"50K Filter\" , Al_thickness_nm = 12700 )) Horton_filter_stack . add ( LEX_HT ( \"Luxel Window TES\" )) fs_dict [ Horton_filter_stack . name ] = Horton_filter_stack return fs_dict","title":"get_filter_stacks_dict"},{"location":"docstrings2/#math-and-statistics-functions","text":"These live in mass2.mathstat .","title":"Math and statistics functions"},{"location":"docstrings2/#entropy","text":"entropy.py Estimates of the distribution entropy computed using kernel-density estimates of the distribution. laplace_entropy(x, w=1.0) - Compute the entropy H(p) of data set x where the kernel used to estimate p from x is the Laplace kernel k(x) \\propto exp(-abs(x-x0)/w). laplace_cross_entropy(x, y, w=1.0) - Compute the cross entropy of q from p, where q and p are the kernel-density estimates taken from data set y and data set x , and where the kernel is the Laplace kernel. KL_divergence(x, y, w=1.0) - Compute the Kullback-Leibler Divergence of data set y from x , where the kernel is the Laplace kernel. The K-L divergence of Q(x) from P(x) is defined as the integral over the full x domain of P(x) log[P(x)/Q(x)]. This equals the cross-entropy H(P,Q) - H(P). Note that cross-entropy and K-L divergence are not symmetric with respect to reversal of x and y .","title":"Entropy"},{"location":"docstrings2/#mass2.mathstat.entropy.laplace_KL_divergence","text":"Compute the Kullback-Leibler divergence of data set y from data set x . Use kernel-density estimation, where the kernel is the Laplace kernel k(x) \\propto exp(-abs(x-x0)/w). The approx_mode can be one of: exact The exact integral is computed (can take ~0.25 sec per 10^6 values). approx The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. size Uses \"approx\" if len(x)+len(y)>200000, or \"exact\" otherwise. Source code in mass2/mathstat/entropy.py 254 255 256 257 258 259 260 261 262 263 264 265 266 def laplace_KL_divergence ( x : ArrayLike , y : ArrayLike , w : float = 1.0 , approx_mode : str = \"size\" ) -> float : r \"\"\"Compute the Kullback-Leibler divergence of data set `y` from data set `x`. Use kernel-density estimation, where the kernel is the Laplace kernel k(x) \\propto exp(-abs(x-x0)/w). The `approx_mode` can be one of: ``exact`` The exact integral is computed (can take ~0.25 sec per 10^6 values). ``approx`` The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. ``size`` Uses \"approx\" if len(x)+len(y)>200000, or \"exact\" otherwise. \"\"\" return laplace_cross_entropy ( x , y , w , approx_mode = approx_mode ) - laplace_entropy ( x , w , approx_mode = approx_mode )","title":"laplace_KL_divergence"},{"location":"docstrings2/#mass2.mathstat.entropy.laplace_cross_entropy","text":"laplace_cross_entropy(x, y, w: float = 1.0, approx_mode=\"size\") Compute the cross-entropy of data set x from data set y , where the kernel for x is the Laplace kernel k(x) \\propto exp(-abs(x-x0)/w). The kernel for the y data is the piecewise-constant (top-hat) kernel. We choose this because a Laplace kernel for y led to possible divergences when the y-distribtion q is exceedingly small, but the x-distribution p nevertheless is non-zero because of a random x-value lying far from any random y-values. The constant kernel is given a non-zero floor value, so that q is never so small as to make any x-value impossible. Args: x (array): One vector of data. y (array): The other vector of data. w (double): The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation. approx_mode (string): How to balance execution speed and accuracy (default \"size\"). The approx_mode can be one of: exact The exact integral is computed (can take ~0.25 sec per 10^6 values). approx The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. size Uses \"approx\" if len(x)+len(y)>200000, or \"exact\" otherwise. Source code in mass2/mathstat/entropy.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def laplace_cross_entropy ( x : ArrayLike , y : ArrayLike , w : float = 1.0 , approx_mode : str = \"size\" ) -> float : r \"\"\"`laplace_cross_entropy(x, y, w: float = 1.0, approx_mode=\"size\")` Compute the cross-entropy of data set `x` from data set `y`, where the kernel for x is the Laplace kernel k(x) \\propto exp(-abs(x-x0)/w). The kernel for the y data is the piecewise-constant (top-hat) kernel. We choose this because a Laplace kernel for y led to possible divergences when the y-distribtion q is exceedingly small, but the x-distribution p nevertheless is non-zero because of a random x-value lying far from any random y-values. The constant kernel is given a non-zero floor value, so that q is never so small as to make any x-value impossible. Args: x (array): One vector of data. y (array): The other vector of data. w (double): The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation. approx_mode (string): How to balance execution speed and accuracy (default \"size\"). The `approx_mode` can be one of: ``exact`` The exact integral is computed (can take ~0.25 sec per 10^6 values). ``approx`` The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. ``size`` Uses \"approx\" if len(x)+len(y)>200000, or \"exact\" otherwise. \"\"\" if w <= 0.0 : raise ValueError ( \"laplace_cross_entropy(x, y, w) needs `w>0`.\" ) x = np . asarray ( x ) y = np . asarray ( y ) Nx = len ( x ) Ny = len ( y ) if Nx == 0 or Ny == 0 : raise ValueError ( \"laplace_cross_entropy(x, y) needs at least 1 element apiece in `x` and `y`.\" ) if approx_mode == \"size\" : if Nx + Ny <= 200000 : approx_mode = \"exact\" else : approx_mode = \"approx\" if approx_mode . startswith ( \"exact\" ): xsorted = np . asarray ( np . sort ( x ) / w , dtype = DTYPE ) ysorted = np . asarray ( np . sort ( y ) / w , dtype = DTYPE ) return laplace_cross_entropy_arrays ( xsorted , ysorted ) + np . log ( w ) else : return laplace_cross_entropy_approx ( np . asarray ( x , dtype = DTYPE ), np . asarray ( y , dtype = DTYPE ), w )","title":"laplace_cross_entropy"},{"location":"docstrings2/#mass2.mathstat.entropy.laplace_cross_entropy_approx","text":"Approximate the cross-entropy H(P, Q) between two empirical distributions P and Q, where P is estimated from data x and Q from data y using Laplace kernel-density estimation and binned histograms. This method uses histograms and convolution with a Laplace kernel to estimate the probability distributions, then computes the cross-entropy using numerical integration. Parameters: x ( ArrayLike ) \u2013 Data points for the P distribution. y ( ArrayLike ) \u2013 Data points for the Q distribution. w ( float , default: 1.0 ) \u2013 The width (exponential scale length) of the Laplace kernel, by default 1.0. Returns: float \u2013 The approximate cross-entropy H(P, Q) between the two distributions. Notes This is an approximate method, suitable for large data sets. The Laplace kernel is defined as k(x) \u221d exp(-abs(x-x0)/w). Uses Simpson's rule for numerical integration. Source code in mass2/mathstat/entropy.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def laplace_cross_entropy_approx ( x : ArrayLike , y : ArrayLike , w : float = 1.0 ) -> float : \"\"\" Approximate the cross-entropy H(P, Q) between two empirical distributions P and Q, where P is estimated from data `x` and Q from data `y` using Laplace kernel-density estimation and binned histograms. This method uses histograms and convolution with a Laplace kernel to estimate the probability distributions, then computes the cross-entropy using numerical integration. Parameters ---------- x : ArrayLike Data points for the P distribution. y : ArrayLike Data points for the Q distribution. w : float, optional The width (exponential scale length) of the Laplace kernel, by default 1.0. Returns ------- float The approximate cross-entropy H(P, Q) between the two distributions. Notes ----- - This is an approximate method, suitable for large data sets. - The Laplace kernel is defined as k(x) \u221d exp(-abs(x-x0)/w). - Uses Simpson's rule for numerical integration. \"\"\" EXTEND_DATA = 5 * w BINS_PER_W = 20 KERNEL_WIDTH_IN_WS = 15.0 xmin = min ( np . min ( x ), np . min ( y )) - EXTEND_DATA xmax = max ( np . max ( x ), np . max ( y )) + EXTEND_DATA nbins = int ( 0.5 + ( xmax - xmin ) * BINS_PER_W / w ) cx , b = np . histogram ( x , nbins , ( xmin , xmax )) cy , b = np . histogram ( y , nbins , ( xmin , xmax )) db = b [ 1 ] - b [ 0 ] nx = int ( 0.5 + KERNEL_WIDTH_IN_WS * w / db ) kernel = np . zeros ( 2 * nx + 1 ) for i in range ( 2 * nx + 1 ): kx = ( i - nx ) * db kernel [ i ] = np . exp ( - abs ( kx / w )) # kde = unnormalized kernel-density estimator. kde = sp . signal . fftconvolve ( cx , kernel , mode = \"full\" )[ nx : - nx ] kde [ kde < kernel . min ()] = kernel . min () # p = normalized probability distribution. norm = 1.0 / sp . integrate . simpson ( kde , dx = db ) p = kde * norm kde = sp . signal . fftconvolve ( cy , kernel , mode = \"full\" )[ nx : - nx ] kde [ kde < kernel . min ()] = kernel . min () norm = 1.0 / sp . integrate . simpson ( kde , dx = db ) q = kde * norm return - sp . integrate . simpson ( p * np . log ( q ), dx = db )","title":"laplace_cross_entropy_approx"},{"location":"docstrings2/#mass2.mathstat.entropy.laplace_cross_entropy_arrays","text":"Compute the cross-entropy H(P, Q) between two empirical distributions P and Q, where P is estimated from data x using a Laplace kernel, and Q is estimated from data y using a piecewise-constant (top-hat) kernel. This function assumes both x and y are sorted and scaled by the kernel width. The cross-entropy is computed exactly by integrating over all points where the estimated densities change due to the presence of data points in x or y . Parameters: x ( ArrayLike ) \u2013 Sorted array of data points for the P distribution, scaled by kernel width. y ( ArrayLike ) \u2013 Sorted array of data points for the Q distribution, scaled by kernel width. Returns: float \u2013 The exact cross-entropy H(P, Q) between the two distributions. Notes The Laplace kernel is defined as k(x) \u221d exp(-abs(x-x0)/w). The Q distribution uses a top-hat kernel with a nonzero floor to avoid divergences. This function is intended for internal use; see laplace_cross_entropy for the public API. Source code in mass2/mathstat/entropy.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def laplace_cross_entropy_arrays ( x : ArrayLike , y : ArrayLike ) -> float : # noqa: PLR0914 \"\"\"Compute the cross-entropy H(P, Q) between two empirical distributions P and Q, where P is estimated from data `x` using a Laplace kernel, and Q is estimated from data `y` using a piecewise-constant (top-hat) kernel. This function assumes both `x` and `y` are sorted and scaled by the kernel width. The cross-entropy is computed exactly by integrating over all points where the estimated densities change due to the presence of data points in `x` or `y`. Parameters ---------- x : ArrayLike Sorted array of data points for the P distribution, scaled by kernel width. y : ArrayLike Sorted array of data points for the Q distribution, scaled by kernel width. Returns ------- float The exact cross-entropy H(P, Q) between the two distributions. Notes ----- - The Laplace kernel is defined as k(x) \u221d exp(-abs(x-x0)/w). - The Q distribution uses a top-hat kernel with a nonzero floor to avoid divergences. - This function is intended for internal use; see `laplace_cross_entropy` for the public API. \"\"\" # List of all places where q(u) increases or decreases because of a y-point. Qstepwidth = 2 * np . sqrt ( 6 ) ynodes , qstep_is_up = _merge_orderedlists ( y - 0.5 * Qstepwidth , y + 0.5 * Qstepwidth ) # List of all places where p(u) or q(u) changes because of an x- a y-point. nodes , isx = _merge_orderedlists ( x , ynodes ) x = np . asarray ( x ) y = np . asarray ( y ) Nx = len ( x ) Ny = len ( y ) N = Nx + Ny * 2 # Pretend q(u) is never lower than this value, and spread this probability across # the range 10 less than the lowest to 10 more than the highest node. Qmin_sum = 1.0 / np . sqrt ( Ny + 3 ) Qmin = Qmin_sum / ( nodes [ - 1 ] + 10 - ( nodes [ 0 ] - 10 )) Qstep = ( 1.0 - Qmin_sum ) / ( Ny * Qstepwidth ) # Initialize the vectors decayfactor, c, and d. decayfactor = np . zeros ( N , dtype = DTYPE ) for i in range ( 1 , N ): decayfactor [ i ] = np . exp ( nodes [ i - 1 ] - nodes [ i ]) # c requires a left-right pass over all nodes. c = np . zeros ( N , dtype = DTYPE ) stepX = 1.0 / ( 2 * Nx ) j = 0 if isx [ 0 ]: c [ 0 ] = stepX else : j = 1 for i in range ( 1 , N ): factor = decayfactor [ i ] c [ i ] = factor * c [ i - 1 ] if isx [ i ]: c [ i ] += stepX # d requires a right-left pass over all nodes. d = np . zeros ( N , dtype = DTYPE ) if isx [ N - 1 ]: d [ N - 1 ] = stepX for i in range ( N - 2 , - 1 , - 1 ): factor = decayfactor [ i + 1 ] d [ i ] = factor * d [ i + 1 ] if isx [ i ]: d [ i ] += stepX # Now a left-right pass over all nodes to compute the H integral. net_up_qsteps = 0 if not isx [ 0 ]: net_up_qsteps = 1 H = - d [ 0 ] * np . log ( Qmin ) # H due to the open first interval [-inf, nodes[0]] for i in range ( 1 , N ): factor = decayfactor [ i ] q = Qmin + Qstep * net_up_qsteps H -= ( c [ i - 1 ] + d [ i ]) * ( 1 - factor ) * np . log ( q ) if not isx [ i ]: if qstep_is_up [ j ]: net_up_qsteps += 1 else : net_up_qsteps -= 1 j += 1 H -= c [ - 1 ] * np . log ( Qmin ) # H due to the open last interval [nodes[-1], +inf] return H","title":"laplace_cross_entropy_arrays"},{"location":"docstrings2/#mass2.mathstat.entropy.laplace_entropy","text":"Compute the entropy of data set x where the kernel is the Laplace kernel, $k(x) \\propto$ exp(-abs(x-x0)/w). Parameters: x_in ( ArrayLike ) \u2013 The vector of data of which we want the entropy. w ( float , default: 1.0 ) \u2013 The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 approx_mode ( str , default: 'size' ) \u2013 How to balance execution speed and accuracy, by default \"size\" The approx_mode can be one of: exact The exact integral is computed (can take ~0.25 sec per 10^6 values). approx The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. size Uses \"approx\" if len(x)>200000, or \"exact\" otherwise. Returns: float \u2013 The Laplace-kernel entropy. Raises: ValueError \u2013 If the input array x has no values, or w is not positive. Source code in mass2/mathstat/entropy.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def laplace_entropy ( x_in : ArrayLike , w : float = 1.0 , approx_mode : str = \"size\" ) -> float : \"\"\"Compute the entropy of data set `x` where the kernel is the Laplace kernel, $k(x) \\\\propto$ exp(-abs(x-x0)/w). Parameters ---------- x_in : ArrayLike The vector of data of which we want the entropy. w : float, optional The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 approx_mode : str, optional How to balance execution speed and accuracy, by default \"size\" The `approx_mode` can be one of: ``exact`` The exact integral is computed (can take ~0.25 sec per 10^6 values). ``approx`` The integral is approximated by histogramming the data, smoothing that, and using Simpson's rule on the PDF samples that result. ``size`` Uses \"approx\" if len(x)>200000, or \"exact\" otherwise. Returns ------- float The Laplace-kernel entropy. Raises ------ ValueError If the input array `x` has no values, or `w` is not positive. \"\"\" x_in = np . asarray ( x_in ) N = len ( x_in ) if N == 0 : raise ValueError ( \"laplace_entropy(x) needs at least 1 element in `x`.\" ) if w <= 0.0 : raise ValueError ( \"laplace_entropy(x, w) needs `w>0`.\" ) x = np . asarray ( x_in , dtype = DTYPE ) if approx_mode == \"size\" : if N <= 200000 : approx_mode = \"exact\" else : approx_mode = \"approx\" if approx_mode . startswith ( \"exact\" ): return laplace_entropy_array ( x , w ) else : return laplace_entropy_approx ( x , w )","title":"laplace_entropy"},{"location":"docstrings2/#mass2.mathstat.entropy.laplace_entropy_approx","text":"Approximage the entropy of data set x with a binned histogram and the Laplace-distribution kernel-density estimator of the probability distribtion. Parameters: x_in ( ArrayLike ) \u2013 The vector of data of which we want the entropy. w ( float , default: 1.0 ) \u2013 The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 Returns: float \u2013 The approximate Laplace-kernel entropy. Source code in mass2/mathstat/entropy.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def laplace_entropy_approx ( x : ArrayLike , w : float = 1.0 ) -> float : \"\"\"Approximage the entropy of data set `x` with a binned histogram and the Laplace-distribution kernel-density estimator of the probability distribtion. Parameters ---------- x_in : ArrayLike The vector of data of which we want the entropy. w : float, optional The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 Returns ------- float The approximate Laplace-kernel entropy. \"\"\" EXTEND_DATA = 5 * w BINS_PER_W = 20 KERNEL_WIDTH_IN_WS = 15.0 xmin = np . min ( x ) - EXTEND_DATA xmax = np . max ( x ) + EXTEND_DATA nbins = int ( 0.5 + ( xmax - xmin ) * BINS_PER_W / w ) c , b = np . histogram ( x , nbins , ( xmin , xmax )) db = b [ 1 ] - b [ 0 ] nx = int ( 0.5 + KERNEL_WIDTH_IN_WS * w / db ) kernel = np . zeros ( 2 * nx + 1 ) for i in range ( 2 * nx + 1 ): kx = ( i - nx ) * db kernel [ i ] = np . exp ( - np . abs ( kx / w )) # kde = unnormalized kernel-density estimator. kde = sp . signal . fftconvolve ( c , kernel , mode = \"full\" )[ nx : - nx ] minkern = kernel . min () kde [ kde < minkern ] = minkern # p = normalized probability distribution. norm = 1.0 / sp . integrate . simpson ( kde , dx = db ) p = kde * norm return - sp . integrate . simpson ( p * np . log ( p ), dx = db )","title":"laplace_entropy_approx"},{"location":"docstrings2/#mass2.mathstat.entropy.laplace_entropy_array","text":"Compute the entropy of data set x where the kernel is the Laplace kernel, $k(x) \\propto$ exp(-abs(x-x0)/w). Parameters: x_in ( ArrayLike ) \u2013 The vector of data of which we want the entropy. w ( float , default: 1.0 ) \u2013 The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 Returns: float \u2013 The exact Laplace-kernel entropy, regardless of the input array size. Source code in mass2/mathstat/entropy.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 @njit def laplace_entropy_array ( x : ArrayLike , w : float = 1.0 ) -> float : \"\"\"Compute the entropy of data set `x` where the kernel is the Laplace kernel, $k(x) \\\\propto$ exp(-abs(x-x0)/w). Parameters ---------- x_in : ArrayLike The vector of data of which we want the entropy. w : float, optional The width (exponential scale length) of the Laplace distribution to be used in kernel-density estimation, by default 1.0 Returns ------- float The exact Laplace-kernel entropy, regardless of the input array size. \"\"\" x = np . asarray ( x ) N = len ( x ) c = np . zeros ( N , dtype = DTYPE ) d = np . zeros ( N , dtype = DTYPE ) y = np . sort ( x ) / w e = np . exp ( - np . diff ( y )) stepsize = 1.0 / ( 2 * w * N ) c [ 0 ] = stepsize for i in range ( 1 , N ): c [ i ] = e [ i - 1 ] * c [ i - 1 ] + stepsize d [ N - 1 ] = stepsize for i in range ( N - 2 , - 1 , - 1 ): d [ i ] = e [ i ] * d [ i + 1 ] + stepsize H = w * d [ 0 ] * ( 1 - np . log ( d [ 0 ])) + w * c [ N - 1 ] * ( 1 - np . log ( c [ N - 1 ])) for i in range ( N - 1 ): dp = d [ i + 1 ] * e [ i ] r1 = c [ i ] / d [ i + 1 ] e2 = np . sqrt ( e [ i ]) H += 4 * w * np . sqrt ( c [ i ] * dp ) * np . atan (( e2 - 1.0 / e2 ) * r1 ** 0.5 / ( r1 + 1.0 )) H += w * ( dp - c [ i ]) * ( np . log ( c [ i ] + dp ) - 1 ) A , B = d [ i + 1 ], c [ i ] * e [ i ] H -= w * ( A - B ) * ( np . log ( A + B ) - 1 ) return H","title":"laplace_entropy_array"},{"location":"docstrings2/#fitting","text":"mass2.mathstat.fitting Model-fitting utilities. Joe Fowler, NIST","title":"Fitting"},{"location":"docstrings2/#mass2.mathstat.fitting.fit_kink_model","text":"Find the linear least-squares solution for a kinked-linear model. The model is f(x) = a+b(x-k) for x =k, where the 4 parameters are {k,a,b,c}, representing the kink at (x,y)=(k,a) and slopes of b and c for x = k. Given k, the model is linear in the other parameters, which can thus be found exactly by linear algebra. The best value of k is found by use of the Bounded method of the sp.optimize.minimize_scalar() routine. Parameters: x ( ArrayLike ) \u2013 The input data x-values y ( ArrayLike ) \u2013 The input data y-values kbounds ( Optional [ tuple [ float , float ]] , default: None ) \u2013 Bounds on k, by default None. If (u,v), then the minimize_scalar is used to find the best k strictly in u<=k<=v. If None, then use the Brent method, which will start with (b1,b2) as a search bracket where b1 and b2 are the 2nd lowest and 2nd highest values of x. Returns: model_y, abc, X2) where: \u2013 model_y : NDArray[float] an array of the model y-values; kabc : NDArray[float] the best-fit values of the kink location and the 3 linear parameters; X2 : float is the sum of square differences between y and model_y. Raises: ValueError \u2013 if k doesn't satisfy x.min() < k < x.max() Examples: x = np.arange(10, dtype=float) y = np.array(x) truek = 4.6 y[x>truek] = truek y += np.random.default_rng().standard_normal(len(x)) .15 model, (kbest,a,b,c), X2 = fit_kink_model(x, y, kbounds=(3,6)) plt.clf() plt.plot(x, y, \"or\", label=\"Noisy data to be fit\") xi = np.linspace(x[0], kbest, 200) xj = np.linspace(kbest, x[-1], 200) plt.plot(xi, a+b (xi-kbest), \"--k\", label=\"Best-fit kinked model\") plt.plot(xj, a+c*(xj-kbest), \"--k\") plt.legend() Source code in mass2/mathstat/fitting.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def fit_kink_model ( x : ArrayLike , y : ArrayLike , kbounds : tuple [ float , float ] | None = None ) -> tuple [ NDArray , NDArray , float ]: \"\"\"Find the linear least-squares solution for a kinked-linear model. The model is f(x) = a+b(x-k) for x<k and f(x)=a+c(x-k) for x>=k, where the 4 parameters are {k,a,b,c}, representing the kink at (x,y)=(k,a) and slopes of b and c for x<k and x>= k. Given k, the model is linear in the other parameters, which can thus be found exactly by linear algebra. The best value of k is found by use of the Bounded method of the sp.optimize.minimize_scalar() routine. Parameters ---------- x : ArrayLike The input data x-values y : ArrayLike The input data y-values kbounds : Optional[tuple[float, float]], optional Bounds on k, by default None. If (u,v), then the minimize_scalar is used to find the best k strictly in u<=k<=v. If None, then use the Brent method, which will start with (b1,b2) as a search bracket where b1 and b2 are the 2nd lowest and 2nd highest values of x. Returns ------- model_y, abc, X2) where: model_y : NDArray[float] an array of the model y-values; kabc : NDArray[float] the best-fit values of the kink location and the 3 linear parameters; X2 : float is the sum of square differences between y and model_y. Raises ------ ValueError if k doesn't satisfy x.min() < k < x.max() Examples -------- x = np.arange(10, dtype=float) y = np.array(x) truek = 4.6 y[x>truek] = truek y += np.random.default_rng().standard_normal(len(x))*.15 model, (kbest,a,b,c), X2 = fit_kink_model(x, y, kbounds=(3,6)) plt.clf() plt.plot(x, y, \"or\", label=\"Noisy data to be fit\") xi = np.linspace(x[0], kbest, 200) xj = np.linspace(kbest, x[-1], 200) plt.plot(xi, a+b*(xi-kbest), \"--k\", label=\"Best-fit kinked model\") plt.plot(xj, a+c*(xj-kbest), \"--k\") plt.legend() \"\"\" x = np . asarray ( x ) y = np . asarray ( y ) def penalty ( k : float , x : NDArray , y : NDArray ) -> float : \"Extract only the cost function from kink_model()\" _ , _ , X2 = kink_model ( k , x , y ) return X2 if kbounds is None : kbounds = ( x . min (), x . max ()) elif kbounds [ 0 ] < x . min () or kbounds [ 1 ] > x . max (): raise ValueError ( f \"kbounds ( { kbounds } ) must be within the range of x data\" ) optimum = sp . optimize . minimize_scalar ( penalty , args = ( x , y ), method = \"Bounded\" , bounds = kbounds ) kbest = optimum . x model , abc , X2 = kink_model ( kbest , x , y ) return model , np . hstack ([ kbest , abc ]), X2","title":"fit_kink_model"},{"location":"docstrings2/#mass2.mathstat.fitting.kink_model","text":"Compute a kinked-linear model on data {x,y} with kink at x=k. The model is f(x) = a+b(x-k) for x =k, where the 4 parameters are {k,a,b,c}, representing the kink at (x,y)=(k,a) and slopes of b and c for x = k. For a fixed k, the model is linear in the other parameters, whose linear least-squares values can thus be found exactly by linear algebra. This function computes them. Returns (model_y, (a,b,c), X2) where: model_y is an array of the model y-values; (a,b,c) are the best-fit values of the linear parameters; X2 is the sum of square differences between y and model_y. Parameters: k ( float ) \u2013 Location of the kink, in x coordinates x ( ArrayLike ) \u2013 The input data x-values y ( ArrayLike ) \u2013 The input data y-values Returns: model_y, abc, X2) where: \u2013 model_y : NDArray[float] an array of the model y-values; abc : NDArray[float] the best-fit values of the linear parameters; X2 : float is the sum of square differences between y and model_y. Raises: ValueError \u2013 if k doesn't satisfy x.min() < k < x.max() Source code in mass2/mathstat/fitting.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def kink_model ( k : float , x : ArrayLike , y : ArrayLike ) -> tuple [ NDArray , NDArray , float ]: \"\"\"Compute a kinked-linear model on data {x,y} with kink at x=k. The model is f(x) = a+b(x-k) for x<k and f(x)=a+c(x-k) for x>=k, where the 4 parameters are {k,a,b,c}, representing the kink at (x,y)=(k,a) and slopes of b and c for x<k and x>= k. For a fixed k, the model is linear in the other parameters, whose linear least-squares values can thus be found exactly by linear algebra. This function computes them. Returns (model_y, (a,b,c), X2) where: model_y is an array of the model y-values; (a,b,c) are the best-fit values of the linear parameters; X2 is the sum of square differences between y and model_y. Parameters ---------- k : float Location of the kink, in x coordinates x : ArrayLike The input data x-values y : ArrayLike The input data y-values Returns ------- model_y, abc, X2) where: model_y : NDArray[float] an array of the model y-values; abc : NDArray[float] the best-fit values of the linear parameters; X2 : float is the sum of square differences between y and model_y. Raises ------ ValueError if k doesn't satisfy x.min() < k < x.max() \"\"\" x = np . asarray ( x ) y = np . asarray ( y ) xi = x [ x < k ] yi = y [ x < k ] xj = x [ x >= k ] yj = y [ x >= k ] N = len ( x ) if len ( xi ) == 0 or len ( xj ) == 0 : xmin = x . min () xmax = x . max () raise ValueError ( f \"k= { k : g } should be in range [xmin,xmax], or [ { xmin : g } , { xmax : g } ].\" ) dxi = xi - k dxj = xj - k si = dxi . sum () sj = dxj . sum () si2 = ( dxi ** 2 ) . sum () sj2 = ( dxj ** 2 ) . sum () A = np . array ([[ N , si , sj ], [ si , si2 , 0 ], [ sj , 0 , sj2 ]]) v = np . array ([ y . sum (), ( yi * dxi ) . sum (), ( yj * dxj ) . sum ()]) abc = np . linalg . solve ( A , v ) model = np . hstack ([ abc [ 0 ] + abc [ 1 ] * dxi , abc [ 0 ] + abc [ 2 ] * dxj ]) X2 = (( model - y ) ** 2 ) . sum () return model , abc , X2","title":"kink_model"},{"location":"docstrings2/#interpolation","text":"interpolate.py Module mass2.mathstat.interpolate Contains interpolations functions not readily available elsewhere. CubicSpline - Perform an exact cubic spline through the data, with either specified slope at the end of the interval or 'natural boundary conditions' (y''=0 at ends). GPRSpline - Create a smoothing spline based on the theory of Gaussian process regression. Finds the curvature penalty by maximizing the Bayesian marginal likelihood. Intended to supercede SmoothingSpline , but very similar. Differs in how the curvature and data fidelity are balanced. SmoothingSpline - Create a smoothing spline that does not exactly interpolate the data, but finds the cubic spline with lowest \"curvature energy\" among all splines that meet the maximum allowed value of chi-squared. SmoothingSplineLog - Create a SmoothingSpline using the log of the x,y points. NaturalBsplineBasis - A tool for expressing a spline basis using B-splines but also enforcing 'natural boundary conditions'. Joe Fowler, NIST Created Feb 2014","title":"Interpolation"},{"location":"docstrings2/#mass2.mathstat.interpolate.CubicSpline","text":"An exact cubic spline, with either a specified slope or 'natural boundary conditions' (y''=0) at ends of interval. Note that the interface is similar to scipy.interpolate.InterpolatedUnivariateSpline, but the behavior is different. The scipy version will remove the 2nd and 2nd-to-last data points from the set of knots as a way of using the 2 extra degrees of freedom. This class instead sets the 1st or 2nd derivatives at the end of the interval to use the extra degrees of freedom. This code is inspired by section 3.3. of Numerical Recipes, 3rd Edition. Usage: x=np.linspace(4,12,20) y=(x-6)**2+np.random.standard_normal(20) cs = mass2.CubicSpline(x, y) plt.clf() plt.plot(x,y,'ok') xa = np.linspace(0,16,200) plt.plot(xa, cs(xa), 'b-') Source code in mass2/mathstat/interpolate.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 class CubicSpline : \"\"\"An exact cubic spline, with either a specified slope or 'natural boundary conditions' (y''=0) at ends of interval. Note that the interface is similar to scipy.interpolate.InterpolatedUnivariateSpline, but the behavior is different. The scipy version will remove the 2nd and 2nd-to-last data points from the set of knots as a way of using the 2 extra degrees of freedom. This class instead sets the 1st or 2nd derivatives at the end of the interval to use the extra degrees of freedom. This code is inspired by section 3.3. of Numerical Recipes, 3rd Edition. Usage: x=np.linspace(4,12,20) y=(x-6)**2+np.random.standard_normal(20) cs = mass2.CubicSpline(x, y) plt.clf() plt.plot(x,y,'ok') xa = np.linspace(0,16,200) plt.plot(xa, cs(xa), 'b-') \"\"\" def __init__ ( self , x : ArrayLike , y : ArrayLike , yprime1 : float | None = None , yprimeN : float | None = None ): \"\"\"Create an exact cubic spline representation for the function y(x). Parameters ---------- x : ArrayLike Indepdendent variable values. Will be sorted if not increasing. y : ArrayLike Dependent variable values. yprime1 : float | None, optional First derivative at the lower-x boundary, by default None yprimeN : float | None, optional First derivative at the upper-x boundary, by default None. slope of None means to use 'natural boundary conditions' by fixing the second derivative to zero at that boundary. \"\"\" argsort = np . argsort ( x ) self . _x = np . array ( x , dtype = float )[ argsort ] self . _y = np . array ( y , dtype = float )[ argsort ] self . _n = len ( argsort ) self . _y2 = np . zeros ( self . _n , dtype = float ) self . yprime1 = yprime1 self . yprimeN = yprimeN self . _compute_y2 () def _compute_y2 ( self ) -> None : \"\"\"Compute the second derivatives at the knots.\"\"\" self . ystep = self . _y [ 1 :] - self . _y [: - 1 ] self . xstep = self . _x [ 1 :] - self . _x [: - 1 ] u = self . ystep / self . xstep u [ 1 :] -= u [: - 1 ] # For natural boundary conditions, u[0]=y2[0]=0. if self . yprime1 is None : u [ 0 ] = 0 self . _y2 [ 0 ] = 0 else : u [ 0 ] = ( 3.0 / self . xstep [ 0 ]) * ( self . ystep [ 0 ] / self . xstep [ 0 ] - self . yprime1 ) self . _y2 [ 0 ] = - 0.5 for i in range ( 1 , self . _n - 1 ): sig = self . xstep [ i - 1 ] / ( self . _x [ i + 1 ] - self . _x [ i - 1 ]) p = sig * self . _y2 [ i - 1 ] + 2.0 self . _y2 [ i ] = ( sig - 1.0 ) / p u [ i ] = ( 6 * u [ i ] / ( self . _x [ i + 1 ] - self . _x [ i - 1 ]) - sig * u [ i - 1 ]) / p # Again, the following is only for natural boundary conditions if self . yprimeN is None : qn = un = 0.0 else : qn = 0.5 un = ( 3.0 / self . xstep [ - 1 ]) * ( self . yprimeN - self . ystep [ - 1 ] / self . xstep [ - 1 ]) self . _y2 [ self . _n - 1 ] = ( un - qn * u [ self . _n - 2 ]) / ( qn * self . _y2 [ self . _n - 2 ] + 1.0 ) # Backsubstitution: for k in range ( self . _n - 2 , - 1 , - 1 ): self . _y2 [ k ] = self . _y2 [ k ] * self . _y2 [ k + 1 ] + u [ k ] if self . yprime1 is None : self . yprime1 = self . ystep [ 0 ] / self . xstep [ 0 ] - self . xstep [ 0 ] * ( self . _y2 [ 0 ] / 3.0 + self . _y2 [ 1 ] / 6.0 ) if self . yprimeN is None : self . yprimeN = self . ystep [ - 1 ] / self . xstep [ - 1 ] + self . xstep [ - 1 ] * ( self . _y2 [ - 2 ] / 6.0 + self . _y2 [ - 1 ] / 3.0 ) def __call__ ( self , x : ArrayLike | float , der : int = 0 ) -> NDArray : \"\"\"Return the value of the cubic spline (or its derivative) at x. Parameters ---------- x : ArrayLike | float Independent variable value(s) at which to evaluate the spline. der : int, optional Derivative order, by default 0 Returns ------- NDArray Spline result \"\"\" scalar = np . isscalar ( x ) x = np . asarray ( x ) if x . size == 0 : return np . array ([]) elif x . size == 1 : x . shape = ( 1 ,) result = np . zeros_like ( x , dtype = float ) # Find which interval 0,...self._n-2 contains the points (or extrapolates to the points) position = np . searchsorted ( self . _x , x ) - 1 # Here, position == -1 means extrapolate below the first interval. extrap_low = position < 0 if extrap_low . any (): if der == 0 : h = x [ extrap_low ] - self . _x [ 0 ] # will be negative result [ extrap_low ] = self . _y [ 0 ] + h * self . yprime1 elif der == 1 : result [ extrap_low ] = self . yprime1 elif der > 1 : result [ extrap_low ] = 0.0 # position = self._n-1 means extrapolate above the last interval. extrap_hi = position >= self . _n - 1 if extrap_hi . any (): if der == 0 : h = x [ extrap_hi ] - self . _x [ - 1 ] # will be positive result [ extrap_hi ] = self . _y [ - 1 ] + h * self . yprimeN elif der == 1 : result [ extrap_hi ] = self . yprimeN elif der > 1 : result [ extrap_hi ] = 0.0 interp = np . logical_and ( position >= 0 , position < self . _n - 1 ) if interp . any (): klo = position [ interp ] khi = klo + 1 dx = self . xstep [ klo ] a = ( self . _x [ khi ] - x [ interp ]) / dx b = ( x [ interp ] - self . _x [ klo ]) / dx if der == 0 : result [ interp ] = ( a * self . _y [ klo ] + b * self . _y [ khi ] + (( a ** 3 - a ) * self . _y2 [ klo ] + ( b ** 3 - b ) * self . _y2 [ khi ]) * dx * dx / 6.0 ) elif der == 1 : result [ interp ] = ( - self . _y [ klo ] / dx + self . _y [ khi ] / dx + (( - ( a ** 2 ) + 1.0 / 3 ) * self . _y2 [ klo ] + ( b ** 2 - 1.0 / 3 ) * self . _y2 [ khi ]) * dx / 2.0 ) elif der == 2 : result [ interp ] = a * self . _y2 [ klo ] + b * self . _y2 [ khi ] elif der == 3 : result [ interp ] = ( - self . _y2 [ klo ] + self . _y2 [ khi ]) * dx elif der > 3 : result [ interp ] = 0.0 if scalar : result = result [ 0 ] return result def variance ( self , xtest : ArrayLike ) -> NDArray : # noqa: PLR6301 \"\"\"Return a dummy estimate of the variance at points `xtest`.\"\"\" return np . zeros_like ( xtest )","title":"CubicSpline"},{"location":"docstrings2/#mass2.mathstat.interpolate.CubicSpline.__call__","text":"Return the value of the cubic spline (or its derivative) at x. Parameters: x ( ArrayLike | float ) \u2013 Independent variable value(s) at which to evaluate the spline. der ( int , default: 0 ) \u2013 Derivative order, by default 0 Returns: NDArray \u2013 Spline result Source code in mass2/mathstat/interpolate.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def __call__ ( self , x : ArrayLike | float , der : int = 0 ) -> NDArray : \"\"\"Return the value of the cubic spline (or its derivative) at x. Parameters ---------- x : ArrayLike | float Independent variable value(s) at which to evaluate the spline. der : int, optional Derivative order, by default 0 Returns ------- NDArray Spline result \"\"\" scalar = np . isscalar ( x ) x = np . asarray ( x ) if x . size == 0 : return np . array ([]) elif x . size == 1 : x . shape = ( 1 ,) result = np . zeros_like ( x , dtype = float ) # Find which interval 0,...self._n-2 contains the points (or extrapolates to the points) position = np . searchsorted ( self . _x , x ) - 1 # Here, position == -1 means extrapolate below the first interval. extrap_low = position < 0 if extrap_low . any (): if der == 0 : h = x [ extrap_low ] - self . _x [ 0 ] # will be negative result [ extrap_low ] = self . _y [ 0 ] + h * self . yprime1 elif der == 1 : result [ extrap_low ] = self . yprime1 elif der > 1 : result [ extrap_low ] = 0.0 # position = self._n-1 means extrapolate above the last interval. extrap_hi = position >= self . _n - 1 if extrap_hi . any (): if der == 0 : h = x [ extrap_hi ] - self . _x [ - 1 ] # will be positive result [ extrap_hi ] = self . _y [ - 1 ] + h * self . yprimeN elif der == 1 : result [ extrap_hi ] = self . yprimeN elif der > 1 : result [ extrap_hi ] = 0.0 interp = np . logical_and ( position >= 0 , position < self . _n - 1 ) if interp . any (): klo = position [ interp ] khi = klo + 1 dx = self . xstep [ klo ] a = ( self . _x [ khi ] - x [ interp ]) / dx b = ( x [ interp ] - self . _x [ klo ]) / dx if der == 0 : result [ interp ] = ( a * self . _y [ klo ] + b * self . _y [ khi ] + (( a ** 3 - a ) * self . _y2 [ klo ] + ( b ** 3 - b ) * self . _y2 [ khi ]) * dx * dx / 6.0 ) elif der == 1 : result [ interp ] = ( - self . _y [ klo ] / dx + self . _y [ khi ] / dx + (( - ( a ** 2 ) + 1.0 / 3 ) * self . _y2 [ klo ] + ( b ** 2 - 1.0 / 3 ) * self . _y2 [ khi ]) * dx / 2.0 ) elif der == 2 : result [ interp ] = a * self . _y2 [ klo ] + b * self . _y2 [ khi ] elif der == 3 : result [ interp ] = ( - self . _y2 [ klo ] + self . _y2 [ khi ]) * dx elif der > 3 : result [ interp ] = 0.0 if scalar : result = result [ 0 ] return result","title":"__call__"},{"location":"docstrings2/#mass2.mathstat.interpolate.CubicSpline.__init__","text":"Create an exact cubic spline representation for the function y(x). Parameters: x ( ArrayLike ) \u2013 Indepdendent variable values. Will be sorted if not increasing. y ( ArrayLike ) \u2013 Dependent variable values. yprime1 ( float | None , default: None ) \u2013 First derivative at the lower-x boundary, by default None yprimeN ( float | None , default: None ) \u2013 First derivative at the upper-x boundary, by default None. slope of None means to use 'natural boundary conditions' by fixing the second derivative to zero at that boundary. Source code in mass2/mathstat/interpolate.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , x : ArrayLike , y : ArrayLike , yprime1 : float | None = None , yprimeN : float | None = None ): \"\"\"Create an exact cubic spline representation for the function y(x). Parameters ---------- x : ArrayLike Indepdendent variable values. Will be sorted if not increasing. y : ArrayLike Dependent variable values. yprime1 : float | None, optional First derivative at the lower-x boundary, by default None yprimeN : float | None, optional First derivative at the upper-x boundary, by default None. slope of None means to use 'natural boundary conditions' by fixing the second derivative to zero at that boundary. \"\"\" argsort = np . argsort ( x ) self . _x = np . array ( x , dtype = float )[ argsort ] self . _y = np . array ( y , dtype = float )[ argsort ] self . _n = len ( argsort ) self . _y2 = np . zeros ( self . _n , dtype = float ) self . yprime1 = yprime1 self . yprimeN = yprimeN self . _compute_y2 ()","title":"__init__"},{"location":"docstrings2/#mass2.mathstat.interpolate.CubicSpline.variance","text":"Return a dummy estimate of the variance at points xtest . Source code in mass2/mathstat/interpolate.py 201 202 203 def variance ( self , xtest : ArrayLike ) -> NDArray : # noqa: PLR6301 \"\"\"Return a dummy estimate of the variance at points `xtest`.\"\"\" return np . zeros_like ( xtest )","title":"variance"},{"location":"docstrings2/#mass2.mathstat.interpolate.GPRSpline","text":"Bases: CubicSpline A callable object that performs a smoothing cubic spline operation The smoothing spline is the cubic spline minimizing the \"curvature energy\" subject to a constraint that the maximum allowed chi-squared is equal to the number of data points. Here curvature energy is defined as the integral of the square of the second derivative from the lowest to the highest knots. The value of sigmaf fixes the square root of the \"function variance\". Small values of sigmaf correspond to large penalties on the curvature, so they emphasize low curvature. Large sigmaf places emphasis on fidelity to the data and will have relatively higher curvature (and a higher uncertainty on the derived curve). Setting sigmaf=None (the default) will choose the value that maximizes the Bayesian marginal likelihood of the data and is probably smart. For further discussion, see Sections 2.2, 2.7, and 6.3 of Rasmussen, C. E., & Williams, K. I. (2006). Gaussian Processes for Machine Learning. Retrieved from http://www.gaussianprocess.org/gpml/chapters/ This object is very similar to SmoothingSpline in this module but is based on Gaussian Process Regression theory. It improves on SmoothingSpline in that: 1. The curvature/data fidelity trade-off is chosen by more principaled, Bayesian means. 2. The uncertainty in the spline curve is estimated by GPR theory. Source code in mass2/mathstat/interpolate.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 class GPRSpline ( CubicSpline ): \"\"\"A callable object that performs a smoothing cubic spline operation The smoothing spline is the cubic spline minimizing the \"curvature energy\" subject to a constraint that the maximum allowed chi-squared is equal to the number of data points. Here curvature energy is defined as the integral of the square of the second derivative from the lowest to the highest knots. The value of `sigmaf` fixes the square root of the \"function variance\". Small values of `sigmaf` correspond to large penalties on the curvature, so they emphasize low curvature. Large `sigmaf` places emphasis on fidelity to the data and will have relatively higher curvature (and a higher uncertainty on the derived curve). Setting `sigmaf=None` (the default) will choose the value that maximizes the Bayesian marginal likelihood of the data and is probably smart. For further discussion, see Sections 2.2, 2.7, and 6.3 of Rasmussen, C. E., & Williams, K. I. (2006). Gaussian Processes for Machine Learning. Retrieved from http://www.gaussianprocess.org/gpml/chapters/ This object is very similar to `SmoothingSpline` in this module but is based on Gaussian Process Regression theory. It improves on `SmoothingSpline` in that: 1. The curvature/data fidelity trade-off is chosen by more principaled, Bayesian means. 2. The uncertainty in the spline curve is estimated by GPR theory. \"\"\" def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , sigmaf : float | None = None ): \"\"\"Set up the Gaussian Process Regression spline. Parameters ---------- x : ArrayLike Indepdendent variable values. Will be sorted if not increasing. y : ArrayLike Dependent variable values. dy : ArrayLike Uncertainties in y values. dx : ArrayLike | None, optional Uncertainties in x values, by default None sigmaf : float | None, optional Allowed function variance, or None to maximize the data likelihood, by default None \"\"\" self . x = np . array ( x ) self . y = np . array ( y ) self . dy = np . array ( dy ) self . Nk = len ( self . x ) assert self . Nk == len ( self . y ) assert self . Nk == len ( self . dy ) if dx is None : self . dx = np . zeros_like ( dy ) self . err = np . array ( np . abs ( dy )) else : self . dx = np . array ( dx ) roughfit = np . polyfit ( self . x , self . y , 2 ) slope = np . poly1d ( np . polyder ( roughfit , 1 ))( self . x ) self . err = np . sqrt (( self . dx * slope ) ** 2 + self . dy ** 2 ) assert self . Nk == len ( self . dx ) assert self . Nk == len ( self . err ) if sigmaf is None : sigmaf = self . best_sigmaf () self . sigmaf = sigmaf H = np . vstack (( np . ones_like ( self . x ), self . x )) K = np . zeros (( self . Nk , self . Nk ), dtype = float ) sf2 = sigmaf ** 2 for i in range ( self . Nk ): K [ i , i ] = sf2 * k_spline ( self . x [ i ], self . x [ i ]) for j in range ( i + 1 , self . Nk ): K [ i , j ] = K [ j , i ] = sf2 * k_spline ( self . x [ i ], self . x [ j ]) Ky = K + np . diag ( self . err ** 2 ) L = np . linalg . cholesky ( Ky ) LH = np . linalg . solve ( L , H . T ) A = LH . T . dot ( LH ) KinvHT = np . linalg . solve ( L . T , LH ) self . L = L self . A = A self . KinvHT = KinvHT beta = np . linalg . solve ( A , KinvHT . T ) . dot ( self . y ) # Compute at test points = self.x # We know that these are the knots of a natural cubic spline R = H - KinvHT . T . dot ( K ) fbar = np . linalg . solve ( L . T , np . linalg . solve ( L , K )) . T . dot ( y ) gbar = fbar + R . T . dot ( beta ) CubicSpline . __init__ ( self , self . x , gbar ) def best_sigmaf ( self ) -> float : \"\"\"Return the sigmaf value that maximizes the marginal Bayesian likelihood.\"\"\" guess = np . median ( self . err / self . y ) result = sp . optimize . minimize_scalar ( lambda x : - self . _marginal_like ( x ), [ guess / 1e4 , guess * 1e4 ]) if result . success : # _marginal_like depends only on the abs(argument), so take minimizer as positive. return np . abs ( result . x ) raise ( ValueError ( \"Could not maximimze the marginal likelihood\" )) def _marginal_like ( self , sigmaf : float ) -> float : \"\"\"Compute the marginal likelihood of the data given sigmaf. Parameters ---------- sigmaf : float The square root of the function variance Returns ------- float The marginal likelihood (up to an additive constant) \"\"\" H = np . vstack (( np . ones_like ( self . x ), self . x )) K = np . zeros (( self . Nk , self . Nk ), dtype = float ) sf2 = sigmaf ** 2 for i in range ( self . Nk ): K [ i , i ] = sf2 * k_spline ( self . x [ i ], self . x [ i ]) for j in range ( i + 1 , self . Nk ): K [ i , j ] = K [ j , i ] = sf2 * k_spline ( self . x [ i ], self . x [ j ]) Ky = K + np . diag ( self . err ** 2 ) L = np . linalg . cholesky ( Ky ) LH = np . linalg . solve ( L , H . T ) A = LH . T . dot ( LH ) KinvHT = np . linalg . solve ( L . T , LH ) C = KinvHT . dot ( np . linalg . solve ( A , KinvHT . T )) yCy = self . y . dot ( C . dot ( self . y )) Linvy = np . linalg . solve ( L , self . y ) yKinvy = Linvy . dot ( Linvy ) return - 0.5 * (( self . Nk - 2 ) * np . log ( 2 * np . pi ) + np . linalg . slogdet ( A )[ 1 ] + np . linalg . slogdet ( Ky )[ 1 ] - yCy + yKinvy ) def variance ( self , xtest : ArrayLike ) -> NDArray : \"\"\"Returns the variance for function evaluations at the test points `xtest`. This equals the diagonal of `self.covariance(xtest)`, but for large test sets, this method computes only the diagonal and should therefore be faster.\"\"\" v = [] xtest = np . asarray ( xtest ) for x in np . asarray ( xtest ): Ktest = self . sigmaf ** 2 * k_spline ( x , self . x ) LinvKtest = np . linalg . solve ( self . L , Ktest ) cov_ftest = self . sigmaf ** 2 * k_spline ( x , x ) - ( LinvKtest ** 2 ) . sum () R = np . array (( 1 , x )) - self . KinvHT . T . dot ( Ktest ) v . append ( cov_ftest + R . dot ( np . linalg . solve ( self . A , R ))) if np . isscalar ( xtest ): return v [ 0 ] return np . array ( v ) def covariance ( self , xtest : ArrayLike ) -> NDArray : \"\"\"Returns the covariance between function evaluations at the test points `xtest`.\"\"\" if np . isscalar ( xtest ): return self . variance ( xtest ) xtest = np . asarray ( xtest ) Ktest = self . sigmaf ** 2 * np . vstack ([ k_spline ( x , self . x ) for x in xtest ]) . T LinvKtest = np . linalg . solve ( self . L , Ktest ) cov_ftest = self . sigmaf ** 2 * np . vstack ([ k_spline ( x , xtest ) for x in xtest ]) cov_ftest -= LinvKtest . T . dot ( LinvKtest ) R = np . vstack (( np . ones ( len ( xtest )), xtest )) R -= self . KinvHT . T . dot ( Ktest ) return cov_ftest + R . T . dot ( np . linalg . solve ( self . A , R ))","title":"GPRSpline"},{"location":"docstrings2/#mass2.mathstat.interpolate.GPRSpline.__init__","text":"Set up the Gaussian Process Regression spline. Parameters: x ( ArrayLike ) \u2013 Indepdendent variable values. Will be sorted if not increasing. y ( ArrayLike ) \u2013 Dependent variable values. dy ( ArrayLike ) \u2013 Uncertainties in y values. dx ( ArrayLike | None , default: None ) \u2013 Uncertainties in x values, by default None sigmaf ( float | None , default: None ) \u2013 Allowed function variance, or None to maximize the data likelihood, by default None Source code in mass2/mathstat/interpolate.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , sigmaf : float | None = None ): \"\"\"Set up the Gaussian Process Regression spline. Parameters ---------- x : ArrayLike Indepdendent variable values. Will be sorted if not increasing. y : ArrayLike Dependent variable values. dy : ArrayLike Uncertainties in y values. dx : ArrayLike | None, optional Uncertainties in x values, by default None sigmaf : float | None, optional Allowed function variance, or None to maximize the data likelihood, by default None \"\"\" self . x = np . array ( x ) self . y = np . array ( y ) self . dy = np . array ( dy ) self . Nk = len ( self . x ) assert self . Nk == len ( self . y ) assert self . Nk == len ( self . dy ) if dx is None : self . dx = np . zeros_like ( dy ) self . err = np . array ( np . abs ( dy )) else : self . dx = np . array ( dx ) roughfit = np . polyfit ( self . x , self . y , 2 ) slope = np . poly1d ( np . polyder ( roughfit , 1 ))( self . x ) self . err = np . sqrt (( self . dx * slope ) ** 2 + self . dy ** 2 ) assert self . Nk == len ( self . dx ) assert self . Nk == len ( self . err ) if sigmaf is None : sigmaf = self . best_sigmaf () self . sigmaf = sigmaf H = np . vstack (( np . ones_like ( self . x ), self . x )) K = np . zeros (( self . Nk , self . Nk ), dtype = float ) sf2 = sigmaf ** 2 for i in range ( self . Nk ): K [ i , i ] = sf2 * k_spline ( self . x [ i ], self . x [ i ]) for j in range ( i + 1 , self . Nk ): K [ i , j ] = K [ j , i ] = sf2 * k_spline ( self . x [ i ], self . x [ j ]) Ky = K + np . diag ( self . err ** 2 ) L = np . linalg . cholesky ( Ky ) LH = np . linalg . solve ( L , H . T ) A = LH . T . dot ( LH ) KinvHT = np . linalg . solve ( L . T , LH ) self . L = L self . A = A self . KinvHT = KinvHT beta = np . linalg . solve ( A , KinvHT . T ) . dot ( self . y ) # Compute at test points = self.x # We know that these are the knots of a natural cubic spline R = H - KinvHT . T . dot ( K ) fbar = np . linalg . solve ( L . T , np . linalg . solve ( L , K )) . T . dot ( y ) gbar = fbar + R . T . dot ( beta ) CubicSpline . __init__ ( self , self . x , gbar )","title":"__init__"},{"location":"docstrings2/#mass2.mathstat.interpolate.GPRSpline.best_sigmaf","text":"Return the sigmaf value that maximizes the marginal Bayesian likelihood. Source code in mass2/mathstat/interpolate.py 300 301 302 303 304 305 306 307 def best_sigmaf ( self ) -> float : \"\"\"Return the sigmaf value that maximizes the marginal Bayesian likelihood.\"\"\" guess = np . median ( self . err / self . y ) result = sp . optimize . minimize_scalar ( lambda x : - self . _marginal_like ( x ), [ guess / 1e4 , guess * 1e4 ]) if result . success : # _marginal_like depends only on the abs(argument), so take minimizer as positive. return np . abs ( result . x ) raise ( ValueError ( \"Could not maximimze the marginal likelihood\" ))","title":"best_sigmaf"},{"location":"docstrings2/#mass2.mathstat.interpolate.GPRSpline.covariance","text":"Returns the covariance between function evaluations at the test points xtest . Source code in mass2/mathstat/interpolate.py 357 358 359 360 361 362 363 364 365 366 367 368 369 def covariance ( self , xtest : ArrayLike ) -> NDArray : \"\"\"Returns the covariance between function evaluations at the test points `xtest`.\"\"\" if np . isscalar ( xtest ): return self . variance ( xtest ) xtest = np . asarray ( xtest ) Ktest = self . sigmaf ** 2 * np . vstack ([ k_spline ( x , self . x ) for x in xtest ]) . T LinvKtest = np . linalg . solve ( self . L , Ktest ) cov_ftest = self . sigmaf ** 2 * np . vstack ([ k_spline ( x , xtest ) for x in xtest ]) cov_ftest -= LinvKtest . T . dot ( LinvKtest ) R = np . vstack (( np . ones ( len ( xtest )), xtest )) R -= self . KinvHT . T . dot ( Ktest ) return cov_ftest + R . T . dot ( np . linalg . solve ( self . A , R ))","title":"covariance"},{"location":"docstrings2/#mass2.mathstat.interpolate.GPRSpline.variance","text":"Returns the variance for function evaluations at the test points xtest . This equals the diagonal of self.covariance(xtest) , but for large test sets, this method computes only the diagonal and should therefore be faster. Source code in mass2/mathstat/interpolate.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 def variance ( self , xtest : ArrayLike ) -> NDArray : \"\"\"Returns the variance for function evaluations at the test points `xtest`. This equals the diagonal of `self.covariance(xtest)`, but for large test sets, this method computes only the diagonal and should therefore be faster.\"\"\" v = [] xtest = np . asarray ( xtest ) for x in np . asarray ( xtest ): Ktest = self . sigmaf ** 2 * k_spline ( x , self . x ) LinvKtest = np . linalg . solve ( self . L , Ktest ) cov_ftest = self . sigmaf ** 2 * k_spline ( x , x ) - ( LinvKtest ** 2 ) . sum () R = np . array (( 1 , x )) - self . KinvHT . T . dot ( Ktest ) v . append ( cov_ftest + R . dot ( np . linalg . solve ( self . A , R ))) if np . isscalar ( xtest ): return v [ 0 ] return np . array ( v )","title":"variance"},{"location":"docstrings2/#mass2.mathstat.interpolate.NaturalBsplineBasis","text":"Represent a cubic B-spline basis in 1D with natural boundary conditions. That is, f''(x)=0 at the first and last knots. This constraint reduces the effective number of basis functions from (2+Nknots) to Nknots. Usage: knots = [0,5,8,9,10,12] basis = NaturalBsplineBasis(knots) x = np.linspace(0, 12, 200) plt.clf() for id in range(len(knots)): plt.plot(x, basis(x, id)) Source code in mass2/mathstat/interpolate.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 class NaturalBsplineBasis : \"\"\"Represent a cubic B-spline basis in 1D with natural boundary conditions. That is, f''(x)=0 at the first and last knots. This constraint reduces the effective number of basis functions from (2+Nknots) to Nknots. Usage: knots = [0,5,8,9,10,12] basis = NaturalBsplineBasis(knots) x = np.linspace(0, 12, 200) plt.clf() for id in range(len(knots)): plt.plot(x, basis(x, id)) \"\"\" def __init__ ( self , knots : ArrayLike ): \"\"\"Initialization requires only the list of knots.\"\"\" knots = np . asarray ( knots ) Nk = len ( knots ) b , e = knots [ 0 ], knots [ - 1 ] padknots = np . hstack ([[ b , b , b ], knots , [ e , e , e ]]) # Combinations of basis function #1 into 2 and 3 (and #N+2 into N+1 # and N) are used to enforce the natural B.C. of f''(x)=0 at the ends. lowfpp = np . zeros ( 3 , dtype = float ) hifpp = np . zeros ( 3 , dtype = float ) for i in ( 0 , 1 , 2 ): scoef = np . zeros ( Nk + 2 , dtype = float ) scoef [ i ] = 1.0 lowfpp [ i ] = splev ( b , sp . interpolate . BSpline ( padknots , scoef , 3 ), der = 2 ) for i in ( 0 , 1 , 2 ): scoef = np . zeros ( Nk + 2 , dtype = float ) scoef [ Nk + 1 - i ] = 1.0 # go from last to 3rd-to-last hifpp [ i ] = splev ( e , sp . interpolate . BSpline ( padknots , scoef , 3 ), der = 2 ) self . coef_b = - lowfpp [ 1 : 3 ] / lowfpp [ 0 ] self . coef_e = - hifpp [ 1 : 3 ] / hifpp [ 0 ] self . Nk = Nk self . knots = np . array ( knots ) self . padknots = padknots def __call__ ( self , x : ArrayLike , id : int , der : int = 0 ) -> NDArray : \"\"\"Compute the Basis-spline at the points `x` for basis function `id`, or its derivative of degree `der`. Parameters ---------- x : ArrayLike Independent variable values at which to evaluate the basis function. id : int Which basis function to evaluate, 0 <= id < Nk der : int, optional Derivative degree, by default 0 Returns ------- NDArray Basis function values (or derivative values) at `x`. Raises ------ ValueError If `id` is not in the range 0 <= id < Nk \"\"\" if id < 0 or id >= self . Nk : raise ValueError ( f \"Require 0 <= id < Nk= { self . Nk } \" ) coef = np . zeros ( self . Nk + 2 , dtype = float ) coef [ id + 1 ] = 1.0 if id < 2 : coef [ 0 ] = self . coef_b [ id ] elif id >= self . Nk - 2 : coef [ - 1 ] = self . coef_e [ self . Nk - id - 1 ] return splev ( x , ( self . padknots , coef , 3 ), der = der ) def values_matrix ( self , der : int = 0 ) -> NDArray : \"\"\"Return matrix M where M_ij = value at knot i for basis function j. If der>0, then return the derivative of that order instead of the value.\"\"\" # Note the array is naturally built by vstack as the Transpose of what we want. return np . vstack ([ self ( self . knots , id , der = der ) for id in range ( self . Nk )]) . T def expand_coeff ( self , beta : NDArray ) -> NDArray : \"\"\"Given coefficients of this length-Nk basis, return the coefficients needed by FITPACK, which are of length Nk+2.\"\"\" first = beta [ 0 ] * self . coef_b [ 0 ] + beta [ 1 ] * self . coef_b [ 1 ] last = beta [ - 1 ] * self . coef_e [ 0 ] + beta [ - 2 ] * self . coef_e [ 1 ] return np . hstack ([ first , beta , last ])","title":"NaturalBsplineBasis"},{"location":"docstrings2/#mass2.mathstat.interpolate.NaturalBsplineBasis.__call__","text":"Compute the Basis-spline at the points x for basis function id , or its derivative of degree der . Parameters: x ( ArrayLike ) \u2013 Independent variable values at which to evaluate the basis function. id ( int ) \u2013 Which basis function to evaluate, 0 <= id < Nk der ( int , default: 0 ) \u2013 Derivative degree, by default 0 Returns: NDArray \u2013 Basis function values (or derivative values) at x . Raises: ValueError \u2013 If id is not in the range 0 <= id < Nk Source code in mass2/mathstat/interpolate.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 def __call__ ( self , x : ArrayLike , id : int , der : int = 0 ) -> NDArray : \"\"\"Compute the Basis-spline at the points `x` for basis function `id`, or its derivative of degree `der`. Parameters ---------- x : ArrayLike Independent variable values at which to evaluate the basis function. id : int Which basis function to evaluate, 0 <= id < Nk der : int, optional Derivative degree, by default 0 Returns ------- NDArray Basis function values (or derivative values) at `x`. Raises ------ ValueError If `id` is not in the range 0 <= id < Nk \"\"\" if id < 0 or id >= self . Nk : raise ValueError ( f \"Require 0 <= id < Nk= { self . Nk } \" ) coef = np . zeros ( self . Nk + 2 , dtype = float ) coef [ id + 1 ] = 1.0 if id < 2 : coef [ 0 ] = self . coef_b [ id ] elif id >= self . Nk - 2 : coef [ - 1 ] = self . coef_e [ self . Nk - id - 1 ] return splev ( x , ( self . padknots , coef , 3 ), der = der )","title":"__call__"},{"location":"docstrings2/#mass2.mathstat.interpolate.NaturalBsplineBasis.__init__","text":"Initialization requires only the list of knots. Source code in mass2/mathstat/interpolate.py 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def __init__ ( self , knots : ArrayLike ): \"\"\"Initialization requires only the list of knots.\"\"\" knots = np . asarray ( knots ) Nk = len ( knots ) b , e = knots [ 0 ], knots [ - 1 ] padknots = np . hstack ([[ b , b , b ], knots , [ e , e , e ]]) # Combinations of basis function #1 into 2 and 3 (and #N+2 into N+1 # and N) are used to enforce the natural B.C. of f''(x)=0 at the ends. lowfpp = np . zeros ( 3 , dtype = float ) hifpp = np . zeros ( 3 , dtype = float ) for i in ( 0 , 1 , 2 ): scoef = np . zeros ( Nk + 2 , dtype = float ) scoef [ i ] = 1.0 lowfpp [ i ] = splev ( b , sp . interpolate . BSpline ( padknots , scoef , 3 ), der = 2 ) for i in ( 0 , 1 , 2 ): scoef = np . zeros ( Nk + 2 , dtype = float ) scoef [ Nk + 1 - i ] = 1.0 # go from last to 3rd-to-last hifpp [ i ] = splev ( e , sp . interpolate . BSpline ( padknots , scoef , 3 ), der = 2 ) self . coef_b = - lowfpp [ 1 : 3 ] / lowfpp [ 0 ] self . coef_e = - hifpp [ 1 : 3 ] / hifpp [ 0 ] self . Nk = Nk self . knots = np . array ( knots ) self . padknots = padknots","title":"__init__"},{"location":"docstrings2/#mass2.mathstat.interpolate.NaturalBsplineBasis.expand_coeff","text":"Given coefficients of this length-Nk basis, return the coefficients needed by FITPACK, which are of length Nk+2. Source code in mass2/mathstat/interpolate.py 451 452 453 454 455 456 def expand_coeff ( self , beta : NDArray ) -> NDArray : \"\"\"Given coefficients of this length-Nk basis, return the coefficients needed by FITPACK, which are of length Nk+2.\"\"\" first = beta [ 0 ] * self . coef_b [ 0 ] + beta [ 1 ] * self . coef_b [ 1 ] last = beta [ - 1 ] * self . coef_e [ 0 ] + beta [ - 2 ] * self . coef_e [ 1 ] return np . hstack ([ first , beta , last ])","title":"expand_coeff"},{"location":"docstrings2/#mass2.mathstat.interpolate.NaturalBsplineBasis.values_matrix","text":"Return matrix M where M_ij = value at knot i for basis function j. If der>0, then return the derivative of that order instead of the value. Source code in mass2/mathstat/interpolate.py 445 446 447 448 449 def values_matrix ( self , der : int = 0 ) -> NDArray : \"\"\"Return matrix M where M_ij = value at knot i for basis function j. If der>0, then return the derivative of that order instead of the value.\"\"\" # Note the array is naturally built by vstack as the Transpose of what we want. return np . vstack ([ self ( self . knots , id , der = der ) for id in range ( self . Nk )]) . T","title":"values_matrix"},{"location":"docstrings2/#mass2.mathstat.interpolate.SmoothingSpline","text":"A callable object that performs a smoothing cubic spline operation, using the NaturalBsplineBasis object for the basis representation of splines. The smoothing spline is the cubic spline minimizing the \"curvature energy\" subject to a constraint that the maximum allowed chi-squared is equal to the number of data points. Here curvature energy is defined as the integral of the square of the second derivative from the lowest to the highest knots. For a proof see Reinsch, C. H. (1967). \"Smoothing by spline functions.\" Numerische Mathematik, 10(3), 177-183. http://doi.org/10.1007/BF02162161 Source code in mass2/mathstat/interpolate.py 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 class SmoothingSpline : \"\"\"A callable object that performs a smoothing cubic spline operation, using the NaturalBsplineBasis object for the basis representation of splines. The smoothing spline is the cubic spline minimizing the \"curvature energy\" subject to a constraint that the maximum allowed chi-squared is equal to the number of data points. Here curvature energy is defined as the integral of the square of the second derivative from the lowest to the highest knots. For a proof see Reinsch, C. H. (1967). \"Smoothing by spline functions.\" Numerische Mathematik, 10(3), 177-183. http://doi.org/10.1007/BF02162161 \"\"\" def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , maxchisq : float | None = None ): \"\"\"Smoothing spline for data {x,y} with errors {dy} on the y values and {dx} on the x values (or zero if not given). If dx errors are given, a global quadratic fit is done to the data to estimate the local slope. If that's a poor choice, then you should combine your dx and dy errors to create a sensible single error list, and you should pass that in as dy. maxchisq specifies the largest allowed value of chi-squared (the sum of the squares of the differences y_i-f(x_i), divided by the variance v_i). If not given, this defaults to the number of data values. When a (weighted) least squares fit of a line to the data meets the maxchisq constraint, then the actual chi-squared will be less than maxchisq. \"\"\" self . x = np . array ( x ) # copy self . y = np . array ( y ) self . dy = np . array ( dy ) if dx is None : err = np . array ( np . abs ( dy )) dx = np . zeros_like ( err ) else : roughfit = np . polyfit ( self . x , self . y , 2 ) slope = np . poly1d ( np . polyder ( roughfit , 1 ))( x ) err = np . sqrt (( np . asarray ( dx ) * slope ) ** 2 + self . dy ** 2 ) self . xscale = ( self . x ** 2 ) . mean () ** 0.5 self . x /= self . xscale self . dx = np . array ( dx ) / self . xscale self . err = err self . Nk = len ( self . x ) if maxchisq is None : self . maxchisq = float ( self . Nk ) else : self . maxchisq = maxchisq self . basis = NaturalBsplineBasis ( self . x ) self . N0 = self . basis . values_matrix ( 0 ) self . N2 = self . basis . values_matrix ( 2 ) self . Omega = self . _compute_Omega ( self . x , self . N2 ) self . smooth ( chisq = self . maxchisq ) @staticmethod def _compute_Omega ( knots : NDArray , N2 : NDArray ) -> NDArray : \"\"\"Given the matrix M2 of second derivates at the knots (that is, M2_ij is the value of B_j''(x_i), second derivative of basis function #j at knot i), compute the matrix Omega, where Omega_ij is the integral over the entire domain of the product B_i''(x) B_j''(x). This can be done because each B_i(x) is piecewise linear, with the slope changes at each knot location.\"\"\" Nk = len ( knots ) assert N2 . shape [ 0 ] == Nk assert N2 . shape [ 1 ] == Nk Omega = np . zeros_like ( N2 ) for i in range ( Nk ): for j in range ( i + 1 ): for k in range ( Nk - 1 ): Omega [ i , j ] += ( N2 [ k + 1 , i ] * N2 [ k , j ] + N2 [ k + 1 , j ] * N2 [ k , i ]) * ( knots [ k + 1 ] - knots [ k ]) / 6.0 for k in range ( Nk ): Omega [ i , j ] += N2 [ k , i ] * N2 [ k , j ] * ( knots [ min ( k + 1 , Nk - 1 )] - knots [ max ( 0 , k - 1 )]) / 3.0 Omega [ j , i ] = Omega [ i , j ] return Omega def smooth ( self , chisq : float | None = None ) -> None : \"\"\"Choose the value of the curve at the knots so as to achieve the smallest possible curvature subject to the constraint that the sum over all {x,y} pairs S = [(y-f(x))/dy]^2 <= chisq\"\"\" if chisq is None : chisq = self . Nk Dinv = self . err ** ( - 2 ) # Vector but stands for diagonals of a diagonal matrix. NTDinv = self . N0 . T * Dinv lhs = np . dot ( NTDinv , self . N0 ) rhs = np . dot ( self . N0 . T , Dinv * self . y ) def best_params ( p : NDArray ) -> NDArray : \"\"\"Return the best-fit parameters for a given curvature penalty p.\"\"\" return np . linalg . solve ( p * ( lhs - self . Omega ) + self . Omega , p * rhs ) def chisq_difference ( p : NDArray , target_chisq : float ) -> float : \"\"\"Return the difference between the chi-squared for curvature penalty p and the target chi-squared.\"\"\" # If curvature is too small, the computation can become singular. # Avoid this by returning a crazy-high chisquared, as needed. try : beta = best_params ( p ) except np . linalg . LinAlgError : return 1e99 ys = np . dot ( self . N0 , beta ) chisq = np . sum ((( self . y - ys ) / self . err ) ** 2 ) return chisq - target_chisq mincurvature = 1e-20 pbest = sp . optimize . brentq ( chisq_difference , mincurvature , 1 , args = ( chisq ,)) beta = best_params ( pbest ) self . coeff = self . basis . expand_coeff ( beta ) ys = np . dot ( self . N0 , beta ) self . actualchisq = np . sum ((( self . y - ys ) / self . err ) ** 2 ) # Store the linear extrapolation outside the knotted region. endpoints = np . array ([ self . x [ 0 ], self . x [ - 1 ]]) * self . xscale val = self . __eval ( endpoints , 0 , allow_extrapolate = False ) slope = self . __eval ( endpoints , 1 , allow_extrapolate = False ) * self . xscale self . lowline = np . poly1d ([ slope [ 0 ], val [ 0 ]]) self . highline = np . poly1d ([ slope [ 1 ], val [ 1 ]]) def __eval ( self , x : ArrayLike , der : int = 0 , allow_extrapolate : bool = True ) -> NDArray : \"\"\"Return the value of (the `der`th derivative of) the smoothing spline at data points `x`.\"\"\" scalar = np . isscalar ( x ) x = np . asarray ( x ) x /= self . xscale splresult = splev ( x , ( self . basis . padknots , self . coeff , 3 ), der = der ) low = x < self . x [ 0 ] high = x > self . x [ - 1 ] if np . any ( low ) and allow_extrapolate : if der == 0 : splresult [ low ] = self . lowline ( x [ low ] - self . x [ 0 ]) elif der == 1 : splresult [ low ] = self . lowline . coeffs [ 0 ] elif der >= 2 : splresult [ low ] = 0.0 if np . any ( high ) and allow_extrapolate : if der == 0 : splresult [ high ] = self . highline ( x [ high ] - self . x [ - 1 ]) elif der == 1 : splresult [ high ] = self . highline . coeffs [ 0 ] elif der >= 2 : splresult [ high ] = 0.0 if der > 0 : splresult /= self . xscale ** der if scalar : splresult = splresult [()] return splresult def __call__ ( self , x : ArrayLike , der : int = 0 ) -> NDArray : \"\"\"Return the value of (the `der`th derivative of) the smoothing spline at data points `x`.\"\"\" return self . __eval ( x , der = der )","title":"SmoothingSpline"},{"location":"docstrings2/#mass2.mathstat.interpolate.SmoothingSpline.__call__","text":"Return the value of (the der th derivative of) the smoothing spline at data points x . Source code in mass2/mathstat/interpolate.py 608 609 610 611 def __call__ ( self , x : ArrayLike , der : int = 0 ) -> NDArray : \"\"\"Return the value of (the `der`th derivative of) the smoothing spline at data points `x`.\"\"\" return self . __eval ( x , der = der )","title":"__call__"},{"location":"docstrings2/#mass2.mathstat.interpolate.SmoothingSpline.__eval","text":"Return the value of (the der th derivative of) the smoothing spline at data points x . Source code in mass2/mathstat/interpolate.py 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 def __eval ( self , x : ArrayLike , der : int = 0 , allow_extrapolate : bool = True ) -> NDArray : \"\"\"Return the value of (the `der`th derivative of) the smoothing spline at data points `x`.\"\"\" scalar = np . isscalar ( x ) x = np . asarray ( x ) x /= self . xscale splresult = splev ( x , ( self . basis . padknots , self . coeff , 3 ), der = der ) low = x < self . x [ 0 ] high = x > self . x [ - 1 ] if np . any ( low ) and allow_extrapolate : if der == 0 : splresult [ low ] = self . lowline ( x [ low ] - self . x [ 0 ]) elif der == 1 : splresult [ low ] = self . lowline . coeffs [ 0 ] elif der >= 2 : splresult [ low ] = 0.0 if np . any ( high ) and allow_extrapolate : if der == 0 : splresult [ high ] = self . highline ( x [ high ] - self . x [ - 1 ]) elif der == 1 : splresult [ high ] = self . highline . coeffs [ 0 ] elif der >= 2 : splresult [ high ] = 0.0 if der > 0 : splresult /= self . xscale ** der if scalar : splresult = splresult [()] return splresult","title":"__eval"},{"location":"docstrings2/#mass2.mathstat.interpolate.SmoothingSpline.__init__","text":"Smoothing spline for data {x,y} with errors {dy} on the y values and {dx} on the x values (or zero if not given). If dx errors are given, a global quadratic fit is done to the data to estimate the local slope. If that's a poor choice, then you should combine your dx and dy errors to create a sensible single error list, and you should pass that in as dy. maxchisq specifies the largest allowed value of chi-squared (the sum of the squares of the differences y_i-f(x_i), divided by the variance v_i). If not given, this defaults to the number of data values. When a (weighted) least squares fit of a line to the data meets the maxchisq constraint, then the actual chi-squared will be less than maxchisq. Source code in mass2/mathstat/interpolate.py 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , maxchisq : float | None = None ): \"\"\"Smoothing spline for data {x,y} with errors {dy} on the y values and {dx} on the x values (or zero if not given). If dx errors are given, a global quadratic fit is done to the data to estimate the local slope. If that's a poor choice, then you should combine your dx and dy errors to create a sensible single error list, and you should pass that in as dy. maxchisq specifies the largest allowed value of chi-squared (the sum of the squares of the differences y_i-f(x_i), divided by the variance v_i). If not given, this defaults to the number of data values. When a (weighted) least squares fit of a line to the data meets the maxchisq constraint, then the actual chi-squared will be less than maxchisq. \"\"\" self . x = np . array ( x ) # copy self . y = np . array ( y ) self . dy = np . array ( dy ) if dx is None : err = np . array ( np . abs ( dy )) dx = np . zeros_like ( err ) else : roughfit = np . polyfit ( self . x , self . y , 2 ) slope = np . poly1d ( np . polyder ( roughfit , 1 ))( x ) err = np . sqrt (( np . asarray ( dx ) * slope ) ** 2 + self . dy ** 2 ) self . xscale = ( self . x ** 2 ) . mean () ** 0.5 self . x /= self . xscale self . dx = np . array ( dx ) / self . xscale self . err = err self . Nk = len ( self . x ) if maxchisq is None : self . maxchisq = float ( self . Nk ) else : self . maxchisq = maxchisq self . basis = NaturalBsplineBasis ( self . x ) self . N0 = self . basis . values_matrix ( 0 ) self . N2 = self . basis . values_matrix ( 2 ) self . Omega = self . _compute_Omega ( self . x , self . N2 ) self . smooth ( chisq = self . maxchisq )","title":"__init__"},{"location":"docstrings2/#mass2.mathstat.interpolate.SmoothingSpline.smooth","text":"Choose the value of the curve at the knots so as to achieve the smallest possible curvature subject to the constraint that the sum over all {x,y} pairs S = [(y-f(x))/dy]^2 <= chisq Source code in mass2/mathstat/interpolate.py 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 def smooth ( self , chisq : float | None = None ) -> None : \"\"\"Choose the value of the curve at the knots so as to achieve the smallest possible curvature subject to the constraint that the sum over all {x,y} pairs S = [(y-f(x))/dy]^2 <= chisq\"\"\" if chisq is None : chisq = self . Nk Dinv = self . err ** ( - 2 ) # Vector but stands for diagonals of a diagonal matrix. NTDinv = self . N0 . T * Dinv lhs = np . dot ( NTDinv , self . N0 ) rhs = np . dot ( self . N0 . T , Dinv * self . y ) def best_params ( p : NDArray ) -> NDArray : \"\"\"Return the best-fit parameters for a given curvature penalty p.\"\"\" return np . linalg . solve ( p * ( lhs - self . Omega ) + self . Omega , p * rhs ) def chisq_difference ( p : NDArray , target_chisq : float ) -> float : \"\"\"Return the difference between the chi-squared for curvature penalty p and the target chi-squared.\"\"\" # If curvature is too small, the computation can become singular. # Avoid this by returning a crazy-high chisquared, as needed. try : beta = best_params ( p ) except np . linalg . LinAlgError : return 1e99 ys = np . dot ( self . N0 , beta ) chisq = np . sum ((( self . y - ys ) / self . err ) ** 2 ) return chisq - target_chisq mincurvature = 1e-20 pbest = sp . optimize . brentq ( chisq_difference , mincurvature , 1 , args = ( chisq ,)) beta = best_params ( pbest ) self . coeff = self . basis . expand_coeff ( beta ) ys = np . dot ( self . N0 , beta ) self . actualchisq = np . sum ((( self . y - ys ) / self . err ) ** 2 ) # Store the linear extrapolation outside the knotted region. endpoints = np . array ([ self . x [ 0 ], self . x [ - 1 ]]) * self . xscale val = self . __eval ( endpoints , 0 , allow_extrapolate = False ) slope = self . __eval ( endpoints , 1 , allow_extrapolate = False ) * self . xscale self . lowline = np . poly1d ([ slope [ 0 ], val [ 0 ]]) self . highline = np . poly1d ([ slope [ 1 ], val [ 1 ]])","title":"smooth"},{"location":"docstrings2/#mass2.mathstat.interpolate.SmoothingSplineLog","text":"A smoothing spline in log-log space. Source code in mass2/mathstat/interpolate.py 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 class SmoothingSplineLog : \"\"\"A smoothing spline in log-log space.\"\"\" def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , maxchisq : float | None = None ): \"\"\"Set up a smoothing spline in log-log space. Parameters ---------- x : ArrayLike Independent variable values. Must be positive and will be sorted if not increasing. y : ArrayLike Dependent variable values. Must be positive. dy : ArrayLike Uncertainties in y values. dx : ArrayLike | None, optional Uncertainties in x values, by default None maxchisq : float | None, optional Maximum allowed chi^2 value, by default None Raises ------ ValueError If any x or y values are not positive. \"\"\" x = np . asarray ( x ) y = np . asarray ( y ) dy = np . asarray ( dy ) if np . any ( x <= 0 ) or np . any ( y <= 0 ): raise ValueError ( \"The x and y data must all be positive to use a SmoothingSplineLog\" ) if dx is not None : dx = np . asarray ( dx ) / x self . linear_model = SmoothingSpline ( np . log ( x ), np . log ( y ), dy / y , dx , maxchisq = maxchisq ) def __call__ ( self , x : ArrayLike , der : int = 0 ) -> NDArray : \"\"\"Compute the log-log smoothing spline or its derivative at the points `x`. Parameters ---------- x : ArrayLike Independent variable values at which to evaluate the spline. der : int, optional Derivative degree, by default 0 Returns ------- NDArray Smoothing spline values (or derivative values) at `x`. \"\"\" return np . exp ( self . linear_model ( np . log ( x ), der = der ))","title":"SmoothingSplineLog"},{"location":"docstrings2/#mass2.mathstat.interpolate.SmoothingSplineLog.__call__","text":"Compute the log-log smoothing spline or its derivative at the points x . Parameters: x ( ArrayLike ) \u2013 Independent variable values at which to evaluate the spline. der ( int , default: 0 ) \u2013 Derivative degree, by default 0 Returns: NDArray \u2013 Smoothing spline values (or derivative values) at x . Source code in mass2/mathstat/interpolate.py 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 def __call__ ( self , x : ArrayLike , der : int = 0 ) -> NDArray : \"\"\"Compute the log-log smoothing spline or its derivative at the points `x`. Parameters ---------- x : ArrayLike Independent variable values at which to evaluate the spline. der : int, optional Derivative degree, by default 0 Returns ------- NDArray Smoothing spline values (or derivative values) at `x`. \"\"\" return np . exp ( self . linear_model ( np . log ( x ), der = der ))","title":"__call__"},{"location":"docstrings2/#mass2.mathstat.interpolate.SmoothingSplineLog.__init__","text":"Set up a smoothing spline in log-log space. Parameters: x ( ArrayLike ) \u2013 Independent variable values. Must be positive and will be sorted if not increasing. y ( ArrayLike ) \u2013 Dependent variable values. Must be positive. dy ( ArrayLike ) \u2013 Uncertainties in y values. dx ( ArrayLike | None , default: None ) \u2013 Uncertainties in x values, by default None maxchisq ( float | None , default: None ) \u2013 Maximum allowed chi^2 value, by default None Raises: ValueError \u2013 If any x or y values are not positive. Source code in mass2/mathstat/interpolate.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def __init__ ( self , x : ArrayLike , y : ArrayLike , dy : ArrayLike , dx : ArrayLike | None = None , maxchisq : float | None = None ): \"\"\"Set up a smoothing spline in log-log space. Parameters ---------- x : ArrayLike Independent variable values. Must be positive and will be sorted if not increasing. y : ArrayLike Dependent variable values. Must be positive. dy : ArrayLike Uncertainties in y values. dx : ArrayLike | None, optional Uncertainties in x values, by default None maxchisq : float | None, optional Maximum allowed chi^2 value, by default None Raises ------ ValueError If any x or y values are not positive. \"\"\" x = np . asarray ( x ) y = np . asarray ( y ) dy = np . asarray ( dy ) if np . any ( x <= 0 ) or np . any ( y <= 0 ): raise ValueError ( \"The x and y data must all be positive to use a SmoothingSplineLog\" ) if dx is not None : dx = np . asarray ( dx ) / x self . linear_model = SmoothingSpline ( np . log ( x ), np . log ( y ), dy / y , dx , maxchisq = maxchisq )","title":"__init__"},{"location":"docstrings2/#mass2.mathstat.interpolate.k_spline","text":"Compute the spline covariance kernel, R&W eq 6.28. Source code in mass2/mathstat/interpolate.py 206 207 208 209 def k_spline ( x : NDArray , y : NDArray ) -> NDArray : \"\"\"Compute the spline covariance kernel, R&W eq 6.28.\"\"\" v = np . minimum ( x , y ) return v ** 3 / 3 + v ** 2 / 2 * np . abs ( x - y )","title":"k_spline"},{"location":"docstrings2/#power-spectra","text":"A class and functions to compute a power spectrum using some of the sophistications given in Numerical Recipes, including windowing and overlapping data segments. Use the class PowerSpectrum in the case that you are compute-limited and PowerSpectrumOverlap in the case that you are data-limited. The latter uses k segments of data two segments at a time to make (k-1) estimates and makes fuller use of all data (except in the first and last segment). Joe Fowler, NIST October 13, 2010 Usage: import power_spectrum as ps import pylab as plt N=1024 M=N/4 data=np.random.default_rng().standard_normal(N) spec = ps.PowerSpectrum(M, dt=1e-6) window = ps.hann(2 M) for i in range(3): spec.addDataSegment(data[i M : (i+2)*M], window=window) plt.plot(spec.frequencies(), spec.spectrum()) Or you can use the convenience function that hides the class objects from you and simply returns a (frequency,spectrum) pair of arrays: N=1024 data=np.random.default_rng().standard_normal(N) plt.clf() for i in (2,4,8,1): f,s = ps.computeSpectrum(data, segfactor=i, dt=1e-6, window=np.hanning) plt.plot(f, s) Window choices are: bartlett - Triangle shape hann - Sine-squared hamming - 0.08 + 0.92*(sine-squared) welch - Parabolic None - Square (no windowing) *** - Any other vector of length 2m OR any callable accepting 2m as an argument and returning a sequence of that length. Each window take an argument (n), the number of data points per segment. When using the PowerSpectrum or PowerSpectrumOverlap classes or the convenience function computeSpectrum, you have a choice. You can call the window and pass in the resulting vector, or you can pass in the callable function itself. It is allowed to use different windows on different data segments, though honestly that would be really weird.","title":"Power spectra"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrum","text":"Object for accumulating power spectrum estimates from one or more data segments. If you want to use multiple overlapping segments, use class PowerSpectrumOvelap. Based on Num Rec 3rd Ed section 13.4 Source code in mass2/mathstat/power_spectrum.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class PowerSpectrum : \"\"\"Object for accumulating power spectrum estimates from one or more data segments. If you want to use multiple overlapping segments, use class PowerSpectrumOvelap. Based on Num Rec 3rd Ed section 13.4\"\"\" def __init__ ( self , m : int , dt : float | None = 1.0 ): \"\"\"Sets up to estimate PSD at m+1 frequencies (counting DC) given data segments of length 2m. Optional dt is the time step Delta\"\"\" self . m = m self . m2 = 2 * m self . nsegments = 0 self . specsum = np . zeros ( m + 1 , dtype = float ) if dt is None : self . dt = 1.0 else : self . dt = dt def copy ( self ) -> \"PowerSpectrum\" : \"\"\"Return a copy of the object. Handy when coding and you don't want to recompute everything, but you do want to update the method definitions.\"\"\" c = PowerSpectrum ( self . m , dt = self . dt ) c . __dict__ . update ( self . __dict__ ) return c def addDataSegment ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a data segment of length 2m using the window function given. window can be None (square window), a callable taking the length and returning a sequence, or a sequence.\"\"\" if len ( data ) != self . m2 : raise ValueError ( f \"wrong size data segment. len(data)= { len ( data ) } but require { self . m2 } \" ) if np . isnan ( data ) . any (): raise ValueError ( \"data contains NaN\" ) if window is None : w = np . ones ( self . m2 ) elif callable ( window ): w = window ( self . m2 ) elif isinstance ( window , np . ndarray ): assert len ( window ) == self . m2 w = window else : raise TypeError ( \"Window not understood\" ) wksp = w * data sum_window = ( w ** 2 ) . sum () scale_factor = 2.0 / ( sum_window * self . m2 ) if True : # we want real units scale_factor *= self . dt * self . m2 wksp = np . fft . rfft ( wksp ) # The first line adds 2x too much to the first/last bins. ps = np . abs ( wksp ) ** 2 self . specsum += scale_factor * ps self . nsegments += 1 def addLongData ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a long vector of data as non-overlapping segments of length 2m.\"\"\" data = np . asarray ( data ) nt = len ( data ) nk = nt // self . m2 for k in range ( nk ): noff = k * self . m2 self . addDataSegment ( data [ noff : noff + self . m2 ], window = window ) def spectrum ( self , nbins : int | None = None ) -> NDArray : \"\"\"If <nbins> is given, the data are averaged into <nbins> bins.\"\"\" if nbins is None : return self . specsum / self . nsegments if nbins > self . m : raise ValueError ( f \"Cannot rebin into more than m= { self . m } bins\" ) newbin = np . asarray ( 0.5 + np . arange ( self . m + 1 , dtype = float ) / ( self . m + 1 ) * nbins , dtype = int ) result = np . zeros ( nbins + 1 , dtype = float ) for i in range ( nbins + 1 ): result [ i ] = self . specsum [ newbin == i ] . mean () return result / self . nsegments def autocorrelation ( self ) -> None : \"\"\"Return the autocorrelation (the DFT of this power spectrum)\"\"\" raise NotImplementedError ( \"The autocorrelation method is not yet implemented.\" ) def frequencies ( self , nbins : int | None = None ) -> NDArray : \"\"\"If <nbins> is given, the data are averaged into <nbins> bins.\"\"\" if nbins is None : nbins = self . m if nbins > self . m : raise ValueError ( f \"Cannot rebin into more than m= { self . m } bins\" ) return np . arange ( nbins + 1 , dtype = float ) / ( 2 * self . dt * nbins ) def plot ( self , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , ** plotkwarg : Any , ) -> None : \"\"\"Plot the power spectrum (or its square root) on a log-log plot. Parameters ---------- axis : plt.Axes | None, optional Axes to plot on, or if None create a new figure, by default None arb_to_unit_scale_and_label : tuple[int, str], optional rescale the sqrt(PSD) by this amoutn and label it such, by default (1, \"arb\") sqrt_psd : bool, optional Whether to take the square root of the PSD, by default True \"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . spectrum ()[ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies ()[ 1 :] if sqrt_psd : axis . plot ( freq , np . sqrt ( psd ), ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( freq , psd , ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) plt . loglog () axis . grid () axis . set_xlabel ( \"Frequency (Hz)\" )","title":"PowerSpectrum"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrum.__init__","text":"Sets up to estimate PSD at m+1 frequencies (counting DC) given data segments of length 2m. Optional dt is the time step Delta Source code in mass2/mathstat/power_spectrum.py 74 75 76 77 78 79 80 81 82 83 84 def __init__ ( self , m : int , dt : float | None = 1.0 ): \"\"\"Sets up to estimate PSD at m+1 frequencies (counting DC) given data segments of length 2m. Optional dt is the time step Delta\"\"\" self . m = m self . m2 = 2 * m self . nsegments = 0 self . specsum = np . zeros ( m + 1 , dtype = float ) if dt is None : self . dt = 1.0 else : self . dt = dt","title":"__init__"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrum.addDataSegment","text":"Process a data segment of length 2m using the window function given. window can be None (square window), a callable taking the length and returning a sequence, or a sequence. Source code in mass2/mathstat/power_spectrum.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def addDataSegment ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a data segment of length 2m using the window function given. window can be None (square window), a callable taking the length and returning a sequence, or a sequence.\"\"\" if len ( data ) != self . m2 : raise ValueError ( f \"wrong size data segment. len(data)= { len ( data ) } but require { self . m2 } \" ) if np . isnan ( data ) . any (): raise ValueError ( \"data contains NaN\" ) if window is None : w = np . ones ( self . m2 ) elif callable ( window ): w = window ( self . m2 ) elif isinstance ( window , np . ndarray ): assert len ( window ) == self . m2 w = window else : raise TypeError ( \"Window not understood\" ) wksp = w * data sum_window = ( w ** 2 ) . sum () scale_factor = 2.0 / ( sum_window * self . m2 ) if True : # we want real units scale_factor *= self . dt * self . m2 wksp = np . fft . rfft ( wksp ) # The first line adds 2x too much to the first/last bins. ps = np . abs ( wksp ) ** 2 self . specsum += scale_factor * ps self . nsegments += 1","title":"addDataSegment"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrum.addLongData","text":"Process a long vector of data as non-overlapping segments of length 2m. Source code in mass2/mathstat/power_spectrum.py 125 126 127 128 129 130 131 132 def addLongData ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a long vector of data as non-overlapping segments of length 2m.\"\"\" data = np . asarray ( data ) nt = len ( data ) nk = nt // self . m2 for k in range ( nk ): noff = k * self . m2 self . addDataSegment ( data [ noff : noff + self . m2 ], window = window )","title":"addLongData"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrum.autocorrelation","text":"Return the autocorrelation (the DFT of this power spectrum) Source code in mass2/mathstat/power_spectrum.py 147 148 149 def autocorrelation ( self ) -> None : \"\"\"Return the autocorrelation (the DFT of this power spectrum)\"\"\" raise NotImplementedError ( \"The autocorrelation method is not yet implemented.\" )","title":"autocorrelation"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrum.copy","text":"Return a copy of the object. Handy when coding and you don't want to recompute everything, but you do want to update the method definitions. Source code in mass2/mathstat/power_spectrum.py 86 87 88 89 90 91 92 93 def copy ( self ) -> \"PowerSpectrum\" : \"\"\"Return a copy of the object. Handy when coding and you don't want to recompute everything, but you do want to update the method definitions.\"\"\" c = PowerSpectrum ( self . m , dt = self . dt ) c . __dict__ . update ( self . __dict__ ) return c","title":"copy"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrum.frequencies","text":"If is given, the data are averaged into bins. Source code in mass2/mathstat/power_spectrum.py 151 152 153 154 155 156 157 def frequencies ( self , nbins : int | None = None ) -> NDArray : \"\"\"If <nbins> is given, the data are averaged into <nbins> bins.\"\"\" if nbins is None : nbins = self . m if nbins > self . m : raise ValueError ( f \"Cannot rebin into more than m= { self . m } bins\" ) return np . arange ( nbins + 1 , dtype = float ) / ( 2 * self . dt * nbins )","title":"frequencies"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrum.plot","text":"Plot the power spectrum (or its square root) on a log-log plot. Parameters: axis ( Axes | None , default: None ) \u2013 Axes to plot on, or if None create a new figure, by default None arb_to_unit_scale_and_label ( tuple [ int , str ] , default: (1, 'arb') ) \u2013 rescale the sqrt(PSD) by this amoutn and label it such, by default (1, \"arb\") sqrt_psd ( bool , default: True ) \u2013 Whether to take the square root of the PSD, by default True Source code in mass2/mathstat/power_spectrum.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def plot ( self , axis : plt . Axes | None = None , arb_to_unit_scale_and_label : tuple [ int , str ] = ( 1 , \"arb\" ), sqrt_psd : bool = True , ** plotkwarg : Any , ) -> None : \"\"\"Plot the power spectrum (or its square root) on a log-log plot. Parameters ---------- axis : plt.Axes | None, optional Axes to plot on, or if None create a new figure, by default None arb_to_unit_scale_and_label : tuple[int, str], optional rescale the sqrt(PSD) by this amoutn and label it such, by default (1, \"arb\") sqrt_psd : bool, optional Whether to take the square root of the PSD, by default True \"\"\" if axis is None : plt . figure () axis = plt . gca () arb_to_unit_scale , unit_label = arb_to_unit_scale_and_label psd = self . spectrum ()[ 1 :] * ( arb_to_unit_scale ** 2 ) freq = self . frequencies ()[ 1 :] if sqrt_psd : axis . plot ( freq , np . sqrt ( psd ), ** plotkwarg ) axis . set_ylabel ( f \"Power Spectral Density ( { unit_label } $/ \\\\ sqrt {{ Hz }} $)\" ) else : axis . plot ( freq , psd , ** plotkwarg ) axis . set_ylabel ( f \"Amplitude Spectral Density ( { unit_label } $^2$ Hz$^ {{ -1 }} $)\" ) plt . loglog () axis . grid () axis . set_xlabel ( \"Frequency (Hz)\" )","title":"plot"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrum.spectrum","text":"If is given, the data are averaged into bins. Source code in mass2/mathstat/power_spectrum.py 134 135 136 137 138 139 140 141 142 143 144 145 def spectrum ( self , nbins : int | None = None ) -> NDArray : \"\"\"If <nbins> is given, the data are averaged into <nbins> bins.\"\"\" if nbins is None : return self . specsum / self . nsegments if nbins > self . m : raise ValueError ( f \"Cannot rebin into more than m= { self . m } bins\" ) newbin = np . asarray ( 0.5 + np . arange ( self . m + 1 , dtype = float ) / ( self . m + 1 ) * nbins , dtype = int ) result = np . zeros ( nbins + 1 , dtype = float ) for i in range ( nbins + 1 ): result [ i ] = self . specsum [ newbin == i ] . mean () return result / self . nsegments","title":"spectrum"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrumOverlap","text":"Bases: PowerSpectrum Object for power spectral estimation using overlapping data segments. User sends non-overlapping segments of length m, and they are processed in pairs of length 2m with overlap (except on the first and last segment). Source code in mass2/mathstat/power_spectrum.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class PowerSpectrumOverlap ( PowerSpectrum ): \"\"\"Object for power spectral estimation using overlapping data segments. User sends non-overlapping segments of length m, and they are processed in pairs of length 2m with overlap (except on the first and last segment). \"\"\" def __init__ ( self , m : int , dt : float | None = 1.0 ): \"\"\"Sets up an object to accumulate a power spectrum estimate. Parameters ---------- m : int Create a PSD estimate with m+1 frequency bins (counting DC) dt : float | None, optional Time sample period in seconds, by default 1.0 \"\"\" PowerSpectrum . __init__ ( self , m , dt = dt ) self . first = True def addDataSegment ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"Process a data segment of length m using window.\" if self . first : self . first = False self . fullseg = np . concatenate (( np . zeros_like ( data ), np . array ( data ))) else : self . fullseg [ 0 : self . m ] = self . fullseg [ self . m :] self . fullseg [ self . m :] = data PowerSpectrum . addDataSegment ( self , self . fullseg , window = window ) def addLongData ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a long vector of data as overlapping segments of length 2m.\"\"\" nt = len ( data ) nk = ( nt - 1 ) // self . m if nk > 1 : delta_el = ( nt - self . m2 ) / ( nk - 1.0 ) else : delta_el = 0.0 for k in range ( nk ): noff = int ( k * delta_el + 0.5 ) PowerSpectrum . addDataSegment ( self , data [ noff : noff + self . m2 ], window = window )","title":"PowerSpectrumOverlap"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrumOverlap.__init__","text":"Sets up an object to accumulate a power spectrum estimate. Parameters: m ( int ) \u2013 Create a PSD estimate with m+1 frequency bins (counting DC) dt ( float | None , default: 1.0 ) \u2013 Time sample period in seconds, by default 1.0 Source code in mass2/mathstat/power_spectrum.py 202 203 204 205 206 207 208 209 210 211 212 213 def __init__ ( self , m : int , dt : float | None = 1.0 ): \"\"\"Sets up an object to accumulate a power spectrum estimate. Parameters ---------- m : int Create a PSD estimate with m+1 frequency bins (counting DC) dt : float | None, optional Time sample period in seconds, by default 1.0 \"\"\" PowerSpectrum . __init__ ( self , m , dt = dt ) self . first = True","title":"__init__"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrumOverlap.addDataSegment","text":"Process a data segment of length m using window. Source code in mass2/mathstat/power_spectrum.py 215 216 217 218 219 220 221 222 223 def addDataSegment ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"Process a data segment of length m using window.\" if self . first : self . first = False self . fullseg = np . concatenate (( np . zeros_like ( data ), np . array ( data ))) else : self . fullseg [ 0 : self . m ] = self . fullseg [ self . m :] self . fullseg [ self . m :] = data PowerSpectrum . addDataSegment ( self , self . fullseg , window = window )","title":"addDataSegment"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.PowerSpectrumOverlap.addLongData","text":"Process a long vector of data as overlapping segments of length 2m. Source code in mass2/mathstat/power_spectrum.py 225 226 227 228 229 230 231 232 233 234 235 236 def addLongData ( self , data : NDArray , window : Callable | NDArray | None = None ) -> None : \"\"\"Process a long vector of data as overlapping segments of length 2m.\"\"\" nt = len ( data ) nk = ( nt - 1 ) // self . m if nk > 1 : delta_el = ( nt - self . m2 ) / ( nk - 1.0 ) else : delta_el = 0.0 for k in range ( nk ): noff = int ( k * delta_el + 0.5 ) PowerSpectrum . addDataSegment ( self , data [ noff : noff + self . m2 ], window = window )","title":"addLongData"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.bartlett","text":"A Bartlett window (triangle shape) of length n Source code in mass2/mathstat/power_spectrum.py 242 243 244 def bartlett ( n : int ) -> NDArray : \"\"\"A Bartlett window (triangle shape) of length n\"\"\" return np . bartlett ( n )","title":"bartlett"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.computeSpectrum","text":"Convenience function to compute the power spectrum of a single data array. Args: Data for finding the spectrum How many segments to break up the data into. The spectrum will be found on each consecutive pair of segments and will be averaged over all pairs. The sample spacing, in time. The window function to apply. Should be a function that accepts a number of samples and returns an array of that length. Possible values are bartlett, welch, hann, and hamming in this module, or use a function of your choosing. Returns: Either the PSD estimate as an array (non-negative frequencies only), OR the tuple (frequencies, PSD). The latter returns when is not None. Source code in mass2/mathstat/power_spectrum.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def computeSpectrum ( data : ArrayLike , segfactor : int = 1 , dt : float | None = None , window : Callable | ArrayLike | None = None ) -> tuple [ NDArray , NDArray ]: \"\"\"Convenience function to compute the power spectrum of a single data array. Args: <data> Data for finding the spectrum <segfactor> How many segments to break up the data into. The spectrum will be found on each consecutive pair of segments and will be averaged over all pairs. <dt> The sample spacing, in time. <window> The window function to apply. Should be a function that accepts a number of samples and returns an array of that length. Possible values are bartlett, welch, hann, and hamming in this module, or use a function of your choosing. Returns: Either the PSD estimate as an array (non-negative frequencies only), *OR* the tuple (frequencies, PSD). The latter returns when <dt> is not None. \"\"\" data = np . asarray ( data ) N = len ( data ) M = N // ( 2 * segfactor ) window_length = 2 * M if window is None : w = np . ones ( window_length , dtype = float ) elif callable ( window ): w = window ( window_length ) elif isinstance ( window , np . ndarray ): assert len ( window ) == window_length w = np . array ( window ) else : raise TypeError ( \"Window not understood\" ) if segfactor == 1 : spec = PowerSpectrum ( M , dt = dt ) # Ensure that the datasegment has even length spec . addDataSegment ( data [: 2 * M ], window = w ) else : spec = PowerSpectrumOverlap ( M , dt = dt ) for i in range ( 2 * segfactor - 1 ): spec . addDataSegment ( data [ i * M : ( i + 1 ) * M ], window = w ) if dt is None : return np . zeros ( M ), spec . spectrum () else : return spec . frequencies (), spec . spectrum ()","title":"computeSpectrum"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.demo","text":"Plot a demonstration power spectrum with different segmentations. Parameters: N ( int , default: 1024 ) \u2013 Length of the white-noise random data vector, by default 1024 window ( Callable | ArrayLike | None , default: hanning ) \u2013 Window function to apply, by default np.hanning Source code in mass2/mathstat/power_spectrum.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def demo ( N : int = 1024 , window : Callable | ArrayLike | None = np . hanning ) -> None : \"\"\"Plot a demonstration power spectrum with different segmentations. Parameters ---------- N : int, optional Length of the white-noise random data vector, by default 1024 window : Callable | ArrayLike | None, optional Window function to apply, by default np.hanning \"\"\" data = np . random . default_rng () . standard_normal ( N ) plt . clf () for i in ( 2 , 4 , 8 , 1 ): f , s = computeSpectrum ( data , segfactor = i , dt = 1e0 , window = window ) plt . plot ( f , s )","title":"demo"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.hamming","text":"A Hamming window (0.08 + 0.92*sine-squared) of length n Source code in mass2/mathstat/power_spectrum.py 260 261 262 def hamming ( n : int ) -> NDArray : \"\"\"A Hamming window (0.08 + 0.92*sine-squared) of length n\"\"\" return np . hamming ( n )","title":"hamming"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.hann","text":"A Hann window (sine-squared) of length n Source code in mass2/mathstat/power_spectrum.py 252 253 254 255 256 257 def hann ( n : int ) -> NDArray : \"\"\"A Hann window (sine-squared) of length n\"\"\" # twopi = np.pi*2 # i = np.arange(n, dtype=float) # return 0.5*(1.0-np.cos(i*twopi/(n-1))) return np . hanning ( n )","title":"hann"},{"location":"docstrings2/#mass2.mathstat.power_spectrum.welch","text":"A Welch window (parabolic) of length n Source code in mass2/mathstat/power_spectrum.py 247 248 249 def welch ( n : int ) -> NDArray : \"\"\"A Welch window (parabolic) of length n\"\"\" return 1 - ( 2 * np . arange ( n , dtype = float ) / ( n - 1.0 ) - 1 ) ** 2","title":"welch"},{"location":"docstrings2/#robust-statistics","text":"mass2.mathstat.robust Functions from the field of robust statistics. Location estimators: bisquare_weighted_mean - Mean with weights given by the bisquare rho function. huber_weighted_mean - Mean with weights given by Huber's rho function. trimean - Tukey's trimean, the average of the median and the midhinge. shorth_range - Primarily a dispersion estimator, but location=True gives a (poor) location. Dispersion estimators: median_abs_dev - Median absolute deviation from the median. shorth_range - Length of the shortest closed interval containing at least half the data. Qscale - Normalized Rousseeuw & Croux Q statistic, from the 25%ile of all 2-point distances. Utility functions: high_median - Weighted median Recommendations: For location, suggest the bisquare_weighted_mean with k=3.9*sigma, if you can make any reasonable guess as to the Gaussian-like width sigma. If not, trimean is a good second choice, though less efficient. For dispersion, the Qscale is very efficient for nearly Gaussian data. The median_abs_dev is the most robust though less efficient. If Qscale doesn't work, then short_range is a good second choice. Created on Feb 9, 2012 Rewritten with Numba Jan 23, 2025 @author: fowlerj","title":"Robust statistics"},{"location":"docstrings2/#mass2.mathstat.robust.Qscale","text":"Compute the robust estimator of scale Q of Rousseeuw and Croux using only O(n log n) memory and computations. A naive implementation is O(n^2) in both memory and computations. Args: x: The data set, an unsorted sequence of values. sort_inplace: Whether it is okay for the function to reorder the set . If True, must be a np.ndarray (or ValueError is raised). Q is defined as d_n * 2.2219 * {|xi-xj|; i<j}_k, where {a}_k means the kth order-statistic of the set {a}, this set is that of the distances between all (n 2) possible pairs of data in {x} n=# of observations in set {x}, k = (n choose 2)/4, 2.2219 makes Q consistent for sigma in normal distributions as n-->infinity, and d_n is a correction factor to the 2.2219 when n is not large. This function does apply the correction factors to make Q consistent with sigma for a Gaussian distribution. Technique from C. Croux & P. Rousseeuw in Comp. Stat Vol 1 (1992) ed. Dodge & Whittaker, Heidelberg: Physica-Verlag pages 411-428. Available at ftp://ftp.win.ua.ac.be/pub/preprints/92/Timeff92.pdf The estimator is further studied in Rousseeuw & Croux, J Am. Stat. Assoc 88 (1993), pp 1273-1283. Source code in mass2/mathstat/robust.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def Qscale ( x : ArrayLike , sort_inplace : bool = False ) -> float : \"\"\"Compute the robust estimator of scale Q of Rousseeuw and Croux using only O(n log n) memory and computations. A naive implementation is O(n^2) in both memory and computations. Args: x: The data set, an unsorted sequence of values. sort_inplace: Whether it is okay for the function to reorder the set <x>. If True, <x> must be a np.ndarray (or ValueError is raised). Q is defined as d_n * 2.2219 * {|xi-xj|; i<j}_k, where {a}_k means the kth order-statistic of the set {a}, this set is that of the distances between all (n 2) possible pairs of data in {x} n=# of observations in set {x}, k = (n choose 2)/4, 2.2219 makes Q consistent for sigma in normal distributions as n-->infinity, and d_n is a correction factor to the 2.2219 when n is not large. This function does apply the correction factors to make Q consistent with sigma for a Gaussian distribution. Technique from C. Croux & P. Rousseeuw in Comp. Stat Vol 1 (1992) ed. Dodge & Whittaker, Heidelberg: Physica-Verlag pages 411-428. Available at ftp://ftp.win.ua.ac.be/pub/preprints/92/Timeff92.pdf The estimator is further studied in Rousseeuw & Croux, J Am. Stat. Assoc 88 (1993), pp 1273-1283. \"\"\" if not sort_inplace : x = np . array ( x ) elif not isinstance ( x , np . ndarray ): raise ValueError ( \"sort_inplace cannot be True unless the data set x is a np.ndarray.\" ) x . sort () n = len ( x ) if n < 2 : raise ValueError ( \"Data set <x> must contain at least 2 values!\" ) h = n // 2 + 1 target_k = h * ( h - 1 ) // 2 - 1 # -1 so that order count can start at 0 instead of conventional 1,2,3... # Compute the n-dependent prefactor to make Q consistent with sigma of a Gaussian. prefactor = 2.2219 if n <= 9 : prefactor *= [ 0 , 0 , 0.399 , 0.994 , 0.512 , 0.844 , 0.611 , 0.857 , 0.669 , 0.872 ][ n ] elif n % 2 == 1 : prefactor *= n / ( n + 1.4 ) else : prefactor *= n / ( n + 3.8 ) # Now down to business finding the 25%ile of |xi - xj| for i<j (or equivalently, for i != j) # Imagine the upper triangle of the matrix Aij = xj - xi (upper means j>i). # If the set is sorted such that xi <= x(i+1) for any i, then the upper triangle of Aij contains # exactly those distances from which we need the k=n/4 order statistic. # For small lists, too many boundary problems arise. Just do it the slow way: if n <= 5 : dist = np . hstack ([ x [ j ] - x [: j ] for j in range ( 1 , n )]) assert len ( dist ) == ( n * ( n - 1 )) // 2 dist . sort () return dist [ target_k ] * prefactor q , npasses = _Qscale_subroutine ( x , n , target_k ) if npasses > n : raise RuntimeError ( f \"Qscale tried { npasses } distances, which is too many\" ) return q * prefactor","title":"Qscale"},{"location":"docstrings2/#mass2.mathstat.robust.bisquare_weighted_mean","text":"The bisquare weighted mean of the data with a k-value of . Args: x (array): data values to be summarized k (number): give zero weight to values at least distance k from the weighted mean. center (number): an initial guess at the weighted mean. If None, then the data median will be used (default None). tol (number): tolerance on the estimator (see below; default None) A sensible choice of is 3 to 5 times the rms width or 1.3 to 2 times the full width at half max of a peak. For strictly Gaussian data, the choices of k= 3.14, 3.88, and 4.68 times sigma will be 80%, 90%, and 95% efficient. The answer is found iteratively, revised until it changes by less than . If is None (the default), then will use 1e-5 times the median absolute deviation of about its median. Data values a distance of more than from the weighted mean are given no weight. Source code in mass2/mathstat/robust.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def bisquare_weighted_mean ( x : ArrayLike , k : float , center : float | None = None , tol : float | None = None ) -> float : \"\"\"The bisquare weighted mean of the data <x> with a k-value of <k>. Args: x (array): data values to be summarized k (number): give zero weight to values at least distance k from the weighted mean. center (number): an initial guess at the weighted mean. If None, then the data median will be used (default None). tol (number): tolerance on the estimator (see below; default None) A sensible choice of <k> is 3 to 5 times the rms width or 1.3 to 2 times the full width at half max of a peak. For strictly Gaussian data, the choices of k= 3.14, 3.88, and 4.68 times sigma will be 80%, 90%, and 95% efficient. The answer is found iteratively, revised until it changes by less than <tol>. If <tol> is None (the default), then <tol> will use 1e-5 times the median absolute deviation of <x> about its median. Data values a distance of more than <k> from the weighted mean are given no weight. \"\"\" x = np . asarray ( x ) if center is None : center = np . median ( x ) if tol is None : tol = 1e-5 * median_abs_dev ( x , normalize = True ) for _iteration in range ( 100 ): weights = ( 1 - (( x - center ) / k ) ** 2.0 ) ** 2.0 weights [ np . abs ( x - center ) > k ] = 0.0 newcenter = ( weights * x ) . sum () / weights . sum () if abs ( newcenter - center ) < tol : return newcenter center = newcenter raise RuntimeError ( \"bisquare_weighted_mean used too many iterations. \\n \" + \"Consider using higher <tol> or better <center>, or change to trimean(x).\" )","title":"bisquare_weighted_mean"},{"location":"docstrings2/#mass2.mathstat.robust.high_median","text":"Compute the weighted high median of data set x with weights . Returns: The smallest x[j] such that the sum of all weights for data with x[i] <= x[j] is strictly greater than half the total weight. If return_index is True, then the chosen index is returned also as (highmed, index). Source code in mass2/mathstat/robust.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def high_median ( x : ArrayLike , weights : ArrayLike | None = None , return_index : bool = False ) -> float | tuple [ float , int ]: \"\"\"Compute the weighted high median of data set x with weights <weights>. Returns: The smallest x[j] such that the sum of all weights for data with x[i] <= x[j] is strictly greater than half the total weight. If return_index is True, then the chosen index is returned also as (highmed, index). \"\"\" x = np . asarray ( x ) sort_idx = x . argsort () # now x[sort_idx] is sorted n = len ( x ) if weights is None : weights = np . ones ( n , dtype = float ) else : weights = np . asarray ( weights , dtype = float ) ri = _high_median ( sort_idx , weights , n ) if return_index : return x [ ri ], ri return x [ ri ]","title":"high_median"},{"location":"docstrings2/#mass2.mathstat.robust.huber_weighted_mean","text":"Huber's weighted mean of the data with a k-value of . Args: x (array): data values to be summarized k (number): give zero weight to values at least distance k from the weighted mean. center (number): an initial guess at the weighted mean. If None, then the data median will be used (default None). tol (number): tolerance on the estimator (see below; default None) A sensible choice of is 1 to 1.5 times the rms width or 0.4 to 0.6 times the full width at half max of a peak. For strictly Gaussian data, the choices of k=1.0 and 1.4 sigma give ... The answer is found iteratively, revised until it changes by less than . If is None (the default), then will use 1e-5 times the median absolute deviation of about its median. Data values a distance of more than from the weighted mean are given no weight. Source code in mass2/mathstat/robust.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def huber_weighted_mean ( x : ArrayLike , k : float , center : float | None = None , tol : float | None = None ) -> float : \"\"\"Huber's weighted mean of the data <x> with a k-value of <k>. Args: x (array): data values to be summarized k (number): give zero weight to values at least distance k from the weighted mean. center (number): an initial guess at the weighted mean. If None, then the data median will be used (default None). tol (number): tolerance on the estimator (see below; default None) A sensible choice of <k> is 1 to 1.5 times the rms width or 0.4 to 0.6 times the full width at half max of a peak. For strictly Gaussian data, the choices of k=1.0 and 1.4 sigma give ... The answer is found iteratively, revised until it changes by less than <tol>. If <tol> is None (the default), then <tol> will use 1e-5 times the median absolute deviation of <x> about its median. Data values a distance of more than <k> from the weighted mean are given no weight. \"\"\" x = np . asarray ( x ) if center is None : center = np . median ( x ) if tol is None : tol = 1e-5 * median_abs_dev ( x , normalize = True ) for _iteration in range ( 100 ): weights = np . asarray (( 1.0 * k ) / np . abs ( x - center )) weights [ weights > 1.0 ] = 1.0 newcenter = ( weights * x ) . sum () / weights . sum () if abs ( newcenter - center ) < tol : return newcenter center = newcenter raise RuntimeError ( \"huber_weighted_mean used too many iterations. \\n \" + \"Consider using higher <tol> or better <center>, or change to trimean(x).\" )","title":"huber_weighted_mean"},{"location":"docstrings2/#mass2.mathstat.robust.median_abs_dev","text":"Median absolute deviation (from the median) of data vector. Args: x (array): data to be summarized. normalize (bool): if True, then return MAD/0.675, which scaling makes the statistic consistent with the standard deviation for an asymptotically large sample of Gaussian deviates (default False). Source code in mass2/mathstat/robust.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def median_abs_dev ( x : ArrayLike , normalize : bool = False ) -> float : \"\"\"Median absolute deviation (from the median) of data vector. Args: x (array): data to be summarized. normalize (bool): if True, then return MAD/0.675, which scaling makes the statistic consistent with the standard deviation for an asymptotically large sample of Gaussian deviates (default False). \"\"\" x = np . asarray ( x ) mad = np . median ( np . abs ( x - np . median ( x ))) if normalize : return mad / 0.674480 # Half of the normal distribution has abs(x-mu) < 0.674480*sigma return mad","title":"median_abs_dev"},{"location":"docstrings2/#mass2.mathstat.robust.shorth_range","text":"Returns the Shortest Half (shorth) Range, a robust estimator of dispersion. The Shortest Half of a data set {x} means that closed interval [a,b] where (1) a and b are both elements of the data set, (2) at least half of the elements are in the closed interval, and (3) which minimizes the length of the closed interval (b-a). The shorth range is (b-a). See mass2.mathstat.robust.shorth_information for further explanation and references in the literature. Args: x (array): The data set under study. Must be a sequence of values. normalize (bool): If False (default), then return the actual range b-a. If True, then the range will be divided by 1.348960, which normalizes the range to be a consistent estimator of the parameter sigma in the case of an exact Gaussian distribution. (A small correction of order 1/N is applied, too, which mostly corrects for bias at modest values of the sample size N.) sort_inplace - Permit this function to reorder the data set . If False (default), then x will be copied and the copy will be sorted. (Note that if is not a np.ndarray, an error will be raised if is True.) location - Whether to return two location estimators in addition to the dispersion estimator. (default False). Returns: shorth range if evaluates to False; otherwise returns: (shorth range, shorth mean, shorth center) In this, shorth mean is the mean of all samples in the closed range [a,b], and shorth center = (a+b)/2. Beware that both of these location estimators have the undesirable property that their asymptotic standard deviation improves only as N^(-1/3) rather than the more usual N^(-1/2). So it is not a very good idea to use them as location estimators. They are really only included here for testing just how useless they are. Source code in mass2/mathstat/robust.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def shorth_range ( x : ArrayLike , normalize : bool = False , sort_inplace : bool = False , location : bool = False ) -> float | tuple [ float , float , float ]: \"\"\"Returns the Shortest Half (shorth) Range, a robust estimator of dispersion. The Shortest Half of a data set {x} means that closed interval [a,b] where (1) a and b are both elements of the data set, (2) at least half of the elements are in the closed interval, and (3) which minimizes the length of the closed interval (b-a). The shorth range is (b-a). See mass2.mathstat.robust.shorth_information for further explanation and references in the literature. Args: x (array): The data set under study. Must be a sequence of values. normalize (bool): If False (default), then return the actual range b-a. If True, then the range will be divided by 1.348960, which normalizes the range to be a consistent estimator of the parameter sigma in the case of an exact Gaussian distribution. (A small correction of order 1/N is applied, too, which mostly corrects for bias at modest values of the sample size N.) sort_inplace - Permit this function to reorder the data set <x>. If False (default), then x will be copied and the copy will be sorted. (Note that if <x> is not a np.ndarray, an error will be raised if <sort_inplace> is True.) location - Whether to return two location estimators in addition to the dispersion estimator. (default False). Returns: shorth range if <location> evaluates to False; otherwise returns: (shorth range, shorth mean, shorth center) In this, shorth mean is the mean of all samples in the closed range [a,b], and shorth center = (a+b)/2. Beware that both of these location estimators have the undesirable property that their asymptotic standard deviation improves only as N^(-1/3) rather than the more usual N^(-1/2). So it is not a very good idea to use them as location estimators. They are really only included here for testing just how useless they are. \"\"\" if not sort_inplace : x = np . array ( x ) elif not isinstance ( x , np . ndarray ): raise ValueError ( \"sort_inplace cannot be True unless the data set x is a np.ndarray.\" ) x . sort () n = len ( x ) # Number of data values nhalves = int (( n + 1 ) / 2 ) # Number of minimal intervals containing at least half the data nobs = 1 + int ( n / 2 ) # Number of data values in each minimal interval range_each_half = x [ n - nhalves : n ] - x [ 0 : nhalves ] idxa = range_each_half . argmin () a , b = x [ idxa ], x [ idxa + nobs - 1 ] shorth_range = b - a if normalize : shorth_range /= 2 * 0.674480 # Asymptotic expectation for normal data: sigma*2*0.674480 # The value 2*0.674480 is twice the inverse cumulative normal distribution at 0.75. That is, # the middle 50% of a normal distribution are within \u00b10.674480*sigma of the mean. # The small-n corrections depend on n mod 4. See Rousseeuw & Lerow 1988. # These are not at all clear from the text of the paper (see table on p. 115 # if you want to try to decode them). if n % 4 == 0 : shorth_range *= ( n + 1.0 ) / n elif n % 4 == 1 : shorth_range *= ( n + 1.0 ) / ( n - 1.0 ) elif n % 4 == 2 : shorth_range *= ( n + 1.0 ) / n else : shorth_range *= ( n + 1.0 ) / ( n - 1.0 ) if location : return shorth_range , x [ idxa : idxa + nobs ] . mean (), 0.5 * ( a + b ) return shorth_range","title":"shorth_range"},{"location":"docstrings2/#mass2.mathstat.robust.trimean","text":"Return Tukey's trimean for a data set , a measure of its central tendency (\"location\" or \"center\"). If (q1,q2,q3) are the quartiles (i.e., the 25%ile, median, and 75 %ile), the trimean is (q1+q3)/4 + q2/2. Source code in mass2/mathstat/robust.py 121 122 123 124 125 126 127 128 129 130 131 def trimean ( x : ArrayLike ) -> float : \"\"\"Return Tukey's trimean for a data set <x>, a measure of its central tendency (\"location\" or \"center\"). If (q1,q2,q3) are the quartiles (i.e., the 25%ile, median, and 75 %ile), the trimean is (q1+q3)/4 + q2/2. \"\"\" x = np . asarray ( x ) q1 , q2 , q3 = [ np . percentile ( x , per ) for per in ( 25 , 50 , 75 )] trimean = 0.25 * ( q1 + q3 ) + 0.5 * q2 return trimean","title":"trimean"},{"location":"filtering/","text":"Optimal filtering The FilterMaker interface The optimal filtering interface FilterMaker is based on Python dataclasses , a modern approach to Python objects based on having set of attributes fixed at creation time. When the dataclass is \"frozen\" (as in this case), it also does not allow changing the values of these attributes. Our intention with the new API is offer a range of objects that can perform optimal filtering, or create objects that do, but rejecting the proliferation of incompatible features that used to appear in slightly different flavors of filters. This API consists of two key objects: The Filter is a specific implementation of an optimal filter, designed to be used in one-lag or five-lag mode, and with fixed choices about low-pass filtering of the filter's values, or about giving zero weight to a number of initial or final samples in a record. Offers a filter_records(r) method to apply its optimal filter to one or more pulse records r . When r is a 2d array, each row corresponds to a pulse record. The FilterMaker contains a model of one channel's signal and noise. It is able to create various objects of the type Filter (or subtypes thereof). The user first creates a FilterMaker from the analyzed noise and signal, then uses it to generate an optimal filter with the desired properties. The filter will be an object whose type is a subclass of Filter , either Filter5Lag or FilterATS . That object has a method filter_records(...) . Usage looks like the following: import numpy as np import mass2 n = 500 Maxsignal = 1000.0 sigma_noise = 1.0 tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.core.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) filt_5lag = maker.compute_5lag() print(f\"Filter peak value: {filt_5lag.nominal_peak:.1f}\") print(f\"Filter rms value: {filt_5lag.variance**0.5:.4f}\") print(f\"Filter predicted V/dV (FWHM): {filt_5lag.predicted_v_over_dv:.4f}\") from numpy.testing import assert_allclose assert_allclose(filt_5lag.nominal_peak, 1000) assert_allclose(filt_5lag.variance**0.5, 0.1549, rtol=1e-3) assert_allclose(filt_5lag.predicted_v_over_dv, 2741.6517) # mkdocs: render # Here's what the filter looks like: a pulse minus a constant. filt_5lag.plot() This code produces a filter maker maker and an optimal filter filt_5lag and generates the following output: Filter peak value: 1000.0 Filter rms value: 0.1549 Filter predicted V/dV (FWHM): 2741.6517 A test of normalization and filter variance import numpy as np import mass2 def verify_close(x, y, rtol=1e-5, topic=None): if topic is not None: print(f\"Checking {topic:20s}: \", end=\"\") isclose = np.isclose(x, y, rtol=rtol) print(f\"x={x:.4e}, y={y:.4e} are close to each other? {isclose}\") assert isclose def test_mass_5lag_filters(Maxsignal=100.0, sigma_noise=1.0, n=500): tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.core.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) filt_5lag = maker.compute_5lag() # Check filter's normalization f = filt_5lag.values verify_close(Maxsignal, f.dot(truncated_signal), rtol=1e-5, topic = \"Filter normalization\") # Check filter's variance expected_dV = sigma_noise / n**0.5 * signal.max()/truncated_signal.std() verify_close(expected_dV, filt_5lag.variance**0.5, rtol=1e-5, topic=\"Expected variance\") # Check filter's V/dV calculation fwhm_sigma_ratio = np.sqrt(8*np.log(2)) expected_V_dV = Maxsignal / (expected_dV * fwhm_sigma_ratio) verify_close(expected_V_dV, filt_5lag.predicted_v_over_dv, rtol=1e-5, topic=\"Expected V/\\u03b4v\") print() test_mass_5lag_filters(100, 1.0, 500) test_mass_5lag_filters(400, 1.0, 500) test_mass_5lag_filters(100, 1.0, 1000) test_mass_5lag_filters(100, 2.0, 500) These four tests should yield the following output: Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=2.7417e+02, y=2.7417e+02 are close to each other? True Checking Filter normalization: x=4.0000e+02, y=4.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.0967e+03, y=1.0967e+03 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.0963e-01, y=1.0963e-01 are close to each other? True Checking Expected V/\u03b4v : x=3.8734e+02, y=3.8734e+02 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=3.0978e-01, y=3.0978e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.3708e+02, y=1.3708e+02 are close to each other? True","title":"Optimal filtering"},{"location":"filtering/#optimal-filtering","text":"","title":"Optimal filtering"},{"location":"filtering/#the-filtermaker-interface","text":"The optimal filtering interface FilterMaker is based on Python dataclasses , a modern approach to Python objects based on having set of attributes fixed at creation time. When the dataclass is \"frozen\" (as in this case), it also does not allow changing the values of these attributes. Our intention with the new API is offer a range of objects that can perform optimal filtering, or create objects that do, but rejecting the proliferation of incompatible features that used to appear in slightly different flavors of filters. This API consists of two key objects: The Filter is a specific implementation of an optimal filter, designed to be used in one-lag or five-lag mode, and with fixed choices about low-pass filtering of the filter's values, or about giving zero weight to a number of initial or final samples in a record. Offers a filter_records(r) method to apply its optimal filter to one or more pulse records r . When r is a 2d array, each row corresponds to a pulse record. The FilterMaker contains a model of one channel's signal and noise. It is able to create various objects of the type Filter (or subtypes thereof). The user first creates a FilterMaker from the analyzed noise and signal, then uses it to generate an optimal filter with the desired properties. The filter will be an object whose type is a subclass of Filter , either Filter5Lag or FilterATS . That object has a method filter_records(...) . Usage looks like the following: import numpy as np import mass2 n = 500 Maxsignal = 1000.0 sigma_noise = 1.0 tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.core.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) filt_5lag = maker.compute_5lag() print(f\"Filter peak value: {filt_5lag.nominal_peak:.1f}\") print(f\"Filter rms value: {filt_5lag.variance**0.5:.4f}\") print(f\"Filter predicted V/dV (FWHM): {filt_5lag.predicted_v_over_dv:.4f}\") from numpy.testing import assert_allclose assert_allclose(filt_5lag.nominal_peak, 1000) assert_allclose(filt_5lag.variance**0.5, 0.1549, rtol=1e-3) assert_allclose(filt_5lag.predicted_v_over_dv, 2741.6517) # mkdocs: render # Here's what the filter looks like: a pulse minus a constant. filt_5lag.plot() This code produces a filter maker maker and an optimal filter filt_5lag and generates the following output: Filter peak value: 1000.0 Filter rms value: 0.1549 Filter predicted V/dV (FWHM): 2741.6517","title":"The FilterMaker interface"},{"location":"filtering/#a-test-of-normalization-and-filter-variance","text":"import numpy as np import mass2 def verify_close(x, y, rtol=1e-5, topic=None): if topic is not None: print(f\"Checking {topic:20s}: \", end=\"\") isclose = np.isclose(x, y, rtol=rtol) print(f\"x={x:.4e}, y={y:.4e} are close to each other? {isclose}\") assert isclose def test_mass_5lag_filters(Maxsignal=100.0, sigma_noise=1.0, n=500): tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.core.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) filt_5lag = maker.compute_5lag() # Check filter's normalization f = filt_5lag.values verify_close(Maxsignal, f.dot(truncated_signal), rtol=1e-5, topic = \"Filter normalization\") # Check filter's variance expected_dV = sigma_noise / n**0.5 * signal.max()/truncated_signal.std() verify_close(expected_dV, filt_5lag.variance**0.5, rtol=1e-5, topic=\"Expected variance\") # Check filter's V/dV calculation fwhm_sigma_ratio = np.sqrt(8*np.log(2)) expected_V_dV = Maxsignal / (expected_dV * fwhm_sigma_ratio) verify_close(expected_V_dV, filt_5lag.predicted_v_over_dv, rtol=1e-5, topic=\"Expected V/\\u03b4v\") print() test_mass_5lag_filters(100, 1.0, 500) test_mass_5lag_filters(400, 1.0, 500) test_mass_5lag_filters(100, 1.0, 1000) test_mass_5lag_filters(100, 2.0, 500) These four tests should yield the following output: Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=2.7417e+02, y=2.7417e+02 are close to each other? True Checking Filter normalization: x=4.0000e+02, y=4.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.0967e+03, y=1.0967e+03 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.0963e-01, y=1.0963e-01 are close to each other? True Checking Expected V/\u03b4v : x=3.8734e+02, y=3.8734e+02 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=3.0978e-01, y=3.0978e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.3708e+02, y=1.3708e+02 are close to each other? True","title":"A test of normalization and filter variance"},{"location":"fluorescence/","text":"Fluorescence Lines Mass includes numerous features to help you analyze and model the fluorescence emission of various elements. Mass can Approximate the shape of the fluorescence line emission for certain lines (particularly the K-alpha and K-beta lines of elements from Mg to Zn, or Z=12 to 30). Generate random deviates, drawn from these same energy distributions. Fit a measured spectrum to on of these energy distributions. Examples 1. Plot the distribution Objects of the SpectralLine class are callable, and return their PDF given the energy as an array or scalar argument. # mkdocs: render import mass2 import numpy as np import pylab as plt spectrum = mass2.spectra[\"MnKAlpha\"] plt.clf() axis=plt.gca() cm = plt.cm.magma for fwhm in (3,4,5,6,8,10): spectrum.plot(axis=axis,components=False,label=f\"FWHM: {fwhm} eV\", setylim=False, instrument_gaussian_fwhm=fwhm, color=cm(fwhm/10-0.2)); plt.legend(loc=\"upper left\") plt.title(\"Mn K$\\\\alpha$ distribution at various resolutions\") plt.xlabel(\"Energy (eV)\") 2. Generate random deviates from a fluorescence line shape Objects of the SpectralLine class roughly copy the API of the scipy type scipy.stats.rv_continuous and offer some of the methods, such as pdf , rvs .: # mkdocs: render energies0 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=0) energies3 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=3) energies6 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=6) plt.clf() erange=(5840, 5940) for E, color in zip((energies0, energies3, energies6), (\"r\", \"b\", \"purple\")): contents, bin_edges, _ = plt.hist(E, 200, range=erange, histtype=\"step\", color=color) plt.xlabel(\"Energy (eV)\") plt.ylabel(\"Counts per bin\") plt.xlim((erange[0], erange[1])) 3. Fit data to a fluorescence line model # mkdocs: render model = mass2.spectra[\"MnKAlpha\"].model() contents3, bins = np.histogram(energies3, 200, range=erange) bin_ctr = bins[:-1] + 0.5 * (bins[1] - bins[0]) guess_params = model.guess(contents3, bin_ctr, dph_de=1.0) result = model.fit(contents3, guess_params, bin_centers=bin_ctr) result.plotm() print(result.best_values) fwhm = result.params[\"fwhm\"] print(f\"Estimated resolution (FWHM) = {fwhm.value}\u00b1{fwhm.stderr}\")","title":"Atomic fluorescence"},{"location":"fluorescence/#fluorescence-lines","text":"Mass includes numerous features to help you analyze and model the fluorescence emission of various elements. Mass can Approximate the shape of the fluorescence line emission for certain lines (particularly the K-alpha and K-beta lines of elements from Mg to Zn, or Z=12 to 30). Generate random deviates, drawn from these same energy distributions. Fit a measured spectrum to on of these energy distributions.","title":"Fluorescence Lines"},{"location":"fluorescence/#examples","text":"","title":"Examples"},{"location":"fluorescence/#1-plot-the-distribution","text":"Objects of the SpectralLine class are callable, and return their PDF given the energy as an array or scalar argument. # mkdocs: render import mass2 import numpy as np import pylab as plt spectrum = mass2.spectra[\"MnKAlpha\"] plt.clf() axis=plt.gca() cm = plt.cm.magma for fwhm in (3,4,5,6,8,10): spectrum.plot(axis=axis,components=False,label=f\"FWHM: {fwhm} eV\", setylim=False, instrument_gaussian_fwhm=fwhm, color=cm(fwhm/10-0.2)); plt.legend(loc=\"upper left\") plt.title(\"Mn K$\\\\alpha$ distribution at various resolutions\") plt.xlabel(\"Energy (eV)\")","title":"1. Plot the distribution"},{"location":"fluorescence/#2-generate-random-deviates-from-a-fluorescence-line-shape","text":"Objects of the SpectralLine class roughly copy the API of the scipy type scipy.stats.rv_continuous and offer some of the methods, such as pdf , rvs .: # mkdocs: render energies0 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=0) energies3 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=3) energies6 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=6) plt.clf() erange=(5840, 5940) for E, color in zip((energies0, energies3, energies6), (\"r\", \"b\", \"purple\")): contents, bin_edges, _ = plt.hist(E, 200, range=erange, histtype=\"step\", color=color) plt.xlabel(\"Energy (eV)\") plt.ylabel(\"Counts per bin\") plt.xlim((erange[0], erange[1]))","title":"2. Generate random deviates from a fluorescence line shape"},{"location":"fluorescence/#3-fit-data-to-a-fluorescence-line-model","text":"# mkdocs: render model = mass2.spectra[\"MnKAlpha\"].model() contents3, bins = np.histogram(energies3, 200, range=erange) bin_ctr = bins[:-1] + 0.5 * (bins[1] - bins[0]) guess_params = model.guess(contents3, bin_ctr, dph_de=1.0) result = model.fit(contents3, guess_params, bin_centers=bin_ctr) result.plotm() print(result.best_values) fwhm = result.params[\"fwhm\"] print(f\"Estimated resolution (FWHM) = {fwhm.value}\u00b1{fwhm.stderr}\")","title":"3. Fit data to a fluorescence line model"},{"location":"getting_started/","text":"Getting started with Mass2 The Microcalorimeter Analysis Software Suite, or Mass, is a library designed for the analysis of data generated by superconducting microcalorimeters, such as Transition-Edge Sensors (TESs) and Thermal Kinetic-Inductance Detectors (TKIDs). Key features and methods Mass2 represents microcalorimeter pulse data using a Pola.rs DataFrame. Each row represents a single pulse, and there may be an arbitrary number of columns representing \"per pulse quantities\". The raw pulse record may be stored in the DataFrame as well; Mass2 uses \"memmaps\" to limit memory usage in this case. The use of the Polars DataFrame enables many best-in-class methods for slicing and dicing your data. It abstracts away the input and output file formats, allowing Mass2 to easily handle data from many different file formats (LJH, OFF, .bin, and anything you can make a numpy array from). It enables export/reload from many formats as well. When in doubt, we suggest using the Apache Parquet file format to save data in a native data frame structure. Mass2 is designed to do analysis on arrays of microcalorimeters, so Mass2 provides APIs and algorithms that can perform equivalent operations on hundreds of distinct channels all at once. Mass2 creates and applies optimal filters. Mass2 creates and applies energy-calibration functions. Mass2 Fits known fluoresence-line shapes to measured spectra, and it can generate calibrations from the fits. Mass2 supports two modes of operation: From scratch, where you analyze detector data for the first time assuming no prior knowledge and From Recipes, where you \"replay\" an analysis Recipe created in the other mode. In Recipe mode, there is no learning from data, it simply does the same transformations to your DataFrame to produce a new DataFrame. This is intended for use in instruments that operate the same way day in and day out. Mass2 uses many programming best practices, including automated testing and doc tests. We also have adopted an immutable-first approach, where the majority of Python objects are immutable, like the Polars DataFrame. Data transformations are reprented by binding a new variable for example ch2 = ch.do_something() . This makes the code easier to read and reason about. Comparison of Mass versions 2 vs 1 Mass2 builds on many lessons learned developing Mass and adopts many newer practices. Mass was created starting in February 2011 with the goal of analyzing data from a 16-pixel array. It was, in many ways, an inflexible, custom-built Data Science library before Data Science was a common term. Pandas, a popular DataFrame library, existed in 2011 but was not widely known and had only one major contributor. Changes from Mass version 1 include: Mass2 takes a different view of data wrangling and bookkeeping than version 1, leveraging the power of the Pola.rs Dataframe for this purpose. Polars is a high-performance modern dataframe library. It organizes the data structures and provides data I/O. This change increases efficiency and performance for some activities. More importantly, using a Dataframe makes the arrays of per-pulse quantities both flexible (you can add fields of your own design on equal footing with standard quantities) and clearer (you can easily find a complete list of all such quantities, plus data types and statistics). Mass2 no longer uses HDF5 datasets to automatically cache analysis results alongside the raw data. This change removes a valuable, but often maddeningly imperfect feature. We think the cost worth paying. There are big benefits in clarity and avoiding both errors and hidden side effects. Mass2 give access to the native save/load features of Polars, making it easy to preserve and recover subsets or complete sets of analyzed data. The preferred file format is Apache Parquet , but several other choices like CSV are supported and can be used for interchange with frameworks other than Mass. Many data structures in Mass2 use Python's \"frozen dataclasses\" features, making them immutable. With immutable objects, it is much easier to avoid inconsistent state and sneaky bugs. They will take some getting used to at first, but (just as with the lack of HDF5 backing) we believe the benefits are worth the costs. Mass2 tries to expose all functionality as a \"bare function\" that takes arrays as argument, then also exposes that with a convenient ch.do_something() API; in Mass version 1, many algorithms were implemented as methods of a data class and thus impossible to use on their own. Some things have not changed. Mass still makes heavy use of the libraries: Numpy . For numerical arrays and scientific computation. Scipy . Additional algorithms for scientific computation. Matplotlib . For high-quality plotting. LMFIT . Convenient non-linear curve fitting. Mass also still: Lets you operate on many microcalorimeter data channels in parallel with a single function call. Works from full pulse records in LJH files, or pre-filtered pulses in OFF files. Generates standard pulse-summary quantities. Creates and applies optimal filters. Creates and applies energy-calibration functions. Fits known fluorescence-line shapes to measured spectra. This guide is based on the evolving API of Mass2, still under development. How to load pulse data Raw pulse data may be stored in the custom LJH format . An Optimally Filtered Format (\"OFF\") has also been used, in which the Dastard data acquisition program performs optimal filtering in real time. Mass2 can work with data of both types, though of course any process that requires access to the full raw pulse record will fail on an OFF file. Mass2 also has some capability to work with untriggered data stored in .bin or similar files. (Other formats could be added to Mass2 in the future, if necessary.) How to load full pulses from LJH files Mass2 can read LJH files, whether the modern version 2.2, or the earlier 2.1 or 2.0 versions. The older versions have much reduced time resolution and no ability to connect with external triggers; for the most part, they were superceded in late 2015 by the latest verison. The file version is automatically detected by Mass, and you should not have to think about it. For this getting started guide, we'll assume you have the pulsedata package installed. (This should happen automatically when you install Mass2.) It's a standard source of a small number of LJH and OFF example files. Often, you want to load all the data files in a directory, possibly with companion noise files from another. That looks like this: # mkdocs: render import numpy as np import pylab as plt import polars as pl import pulsedata import mass2 pn_pair = pulsedata.pulse_noise_ljh_pairs[\"bessy_20240727\"] data = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder ) The value returned, data , is a mass2.Channels object. At its heart is data.channels , a dictionary mapping from channel numbers to single-sensor objects of the type mass2.Channel . When you create the Channels , there are options to exclude certain channels with problems, or limit the result to some maximum number of channels. For these needs, you can use the arguments limit: int and/or exclude_ch_nums: Iterable[int] . For instance, the following will return a Channels that omits channel 4220 and limits the output to no more than 5 channels (in fact, there will be only 1, given the data source has only one channel other than number 4220): less_data = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder, limit=5, exclude_ch_nums=[4220] ) Advanced cases, for deep exploration If you want to create a single mass2.Channel object (not created as part of a mass2.Channels object): pfile = pn_pair.pulse_folder / \"20240727_run0002_chan4220.ljh\" nfile = pn_pair.noise_folder / \"20240727_run0000_chan4220.ljh\" # the noise file is optional ch = mass2.Channel.from_ljh(pfile, nfile) To open a single LJH file and study it as a pure file, you can use the internal class LJHFile , like this: # mkdocs: render ljh = mass2.LJHFile.open(pn_pair.pulse_folder/\"20240727_run0002_chan4220.ljh\") print(ljh.npulses, ljh.npresamples, ljh.nsamples) print(ljh.is_continuous) print(ljh.dtype) plt.clf() y = ljh.read_trace(30) plt.plot(y, \".r\") How to load pre-filtered pulses from OFF files OFF files do not record the entire microcalorimeter pulse, but only a summary of it. This means that no analysis steps that require the raw pulse can be performed. Also, many of the per-pulse summary quantities that exist in an OFF file have different names (and perhaps slightly different definitions) than the corresponding fields computed in a standard LJH-based analysis. Use of OFF files is not a primary goal of Mass2 in the initial release, but they are supported for legacy analysis. # Load one OFF file into an `OffFile` object' p = pn_pair.pulse_folder off = mass2.core.OffFile(p / \"20240727_run0002_chan4220.off\") # Load multiple OFF files info a `Channels` object import glob files = glob.glob(str(p / \"*_chan*.off\")) offdata = mass2.Channels.from_off_paths(files, description=\"OFF file demo\") # Here's the dataframe for a single channel's per-pulse data given an OFF input ch = offdata.ch0 print(ch.df) How to load additional information Most data sets come with experiment-state labels, and some also have external trigger timing. Both sets of information are universal across channels, in the sense that a single table of state labels or external trigger times is relevant to all channels in that data acquisition. Therefore, they are best loaded for all channels at once. Here is the first time we see a consequence of the immutability of the Channel and Channels classes: instead of adding information to an existing object, we have to create a new object, a copy of what came before but with added information. In this case, the added information will be new data columns in the dataframe of each channel. But to use the new, enhanced object, we have to re-assign the name data to the new object. Thus: # mkdocs: render # Load the *_experiment_state.txt file, assuming it has the usual name and path data = data.with_experiment_state_by_path() ch = data.ch0 print(ch.df.columns) states = ch.df[\"state_label\"] print(states.dtype) print(states.unique(), states.unique_counts()) assert states.unique()[-2] == \"SCAN3\" assert states.unique_counts()[-2] == 42433 The output reads: ['timestamp', 'pulse', 'subframecount', 'state_label'] Categorical shape: (6,) Series: 'state_label' [cat] [ null \"START\" \"CAL2\" \"PAUSE\" \"SCAN3\" \"SCAN4\" ] shape: (6,) Series: 'state_label' [u32] [ 4 4 11468 40854 42433 5237 ] Notice that Polars saves the state labels as a \"Categorical\" type . This type improves performance when the values are strings known to take on one of a limited number of possibilities (far fewer than the number of rows in a table). Here's how you would load external trigger, if this example data set had external triggers: # exttrig_path = p / \"20240727_run0002_external_trigger.bin\" # data = data.with_external_trigger_by_path(exttrig_path) What is in the Channel and Channels objects The most important part of Channels is a dictionary mapping channel numbers each to a Channel object. You can access a specific channel, or the \"first\", like this: ch = data.channels[4220] # or if you just want any rando channel, this returns the first channel in order of creation (normally, the one with the lowest channel #) arb_channel = data.ch0 Other features include a dictionary of channels declared \"bad\" print(f\"There are {len(data.bad_channels)} bad channels in the data at first.\") data_withbad = data.set_bad(4219, \"We think chan 4219 is no good at all.\") print(f\"There are {len(data_withbad.bad_channels)} bad channels if we capriciously declare one bad.\") Bad channels are used to prevent a few problematic channels from derailing a whole analysis, so when one channel has an error with an analysis step, it can be marked bad and the rest of the channels can proceed to the next analysis step. It is easy to join the data from successive experiments into a single concatenated object. This example just joins a second copy of the same data to itself, because the pulsedata repository doesn't happen to have more data from the same experiment. later_data = data data_merged = data.concat_data(later_data) The most important part of each Channel object is the underlying per-pulse dataframe. This code prints that DataFrame , then the facts specific to the sensor channel as a whole, first as an object, then a dataframe: print(ch.df) print(ch.header) print(ch.header.df) # Here are the pulse dataframe's column names, the contents of the subframecount column, and the min/mean/max of the timestamp columns print(ch.df.columns) print(ch.df[\"subframecount\"]) ts = ch.df[\"timestamp\"] print(ts.min(), ts.mean(), ts.max()) assert len(ts) == 100_000 The above commands produce the output: shape: (100_000, 4) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 timestamp \u2506 pulse \u2506 subframecount \u2506 state_label \u2502 \u2502 --- \u2506 --- \u2506 --- \u2506 --- \u2502 \u2502 datetime[\u03bcs] \u2506 array[u16, 500] \u2506 u64 \u2506 cat \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 2024-07-27 13:30:44.451920 \u2506 [6063, 6076, \u2026 6655] \u2506 1519640266112 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.472140 \u2506 [6071, 6065, \u2026 6670] \u2506 1519640589632 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.475788 \u2506 [6120, 6123, \u2026 6215] \u2506 1519640648000 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.497137 \u2506 [6074, 6065, \u2026 6660] \u2506 1519640944448 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.573915 \u2506 [6074, 6072, \u2026 6672] \u2506 1519642206080 \u2506 START \u2502 \u2502 \u2026 \u2506 \u2026 \u2506 \u2026 \u2506 \u2026 \u2502 \u2502 2024-07-27 16:11:55.982246 \u2506 [6065, 6063, \u2026 6278] \u2506 1674384771712 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:55.986849 \u2506 [6077, 6089, \u2026 6662] \u2506 1674384833216 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.054243 \u2506 [6058, 6048, \u2026 6637] \u2506 1674385923008 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.080575 \u2506 [6067, 6064, \u2026 6639] \u2506 1674386344320 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.220080 \u2506 [6062, 6055, \u2026 6483] \u2506 1674388576448 \u2506 PAUSE \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ChannelHeader(description='20240727_run0002_chan4219.ljh', ch_num=4219, frametime_s=4e-06, n_presamples=250, n_samples=500) shape: (1, 27) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Save File Format \u2506 Software Version \u2506 Software Git Hash \u2506 Data source \u2506 \u2026 \u2506 Pixel Name \u2506 Timebase \u2506 Filename \u2506 continuous \u2502 \u2502 Version \u2506 --- \u2506 --- \u2506 --- \u2506 \u2506 --- \u2506 --- \u2506 --- \u2506 --- \u2502 \u2502 --- \u2506 str \u2506 str \u2506 str \u2506 \u2506 str \u2506 f64 \u2506 str \u2506 bool \u2502 \u2502 str \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 2.2.1 \u2506 DASTARD version 0.3.2 \u2506 508fd92 \u2506 Abaco \u2506 \u2026 \u2506 \u2506 0.000004 \u2506 /Users/fowlerj/qsp/lib/ \u2506 false \u2502 \u2502 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 python3\u2026 \u2506 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ['timestamp', 'pulse', 'subframecount', 'state_label'] shape: (100_000,) Series: 'subframecount' [u64] [ 1519640266112 1519640589632 1519640648000 1519640944448 1519642206080 \u2026 1674384771712 1674384833216 1674385923008 1674386344320 1674388576448 ] 2024-07-27 13:30:44.451920 2024-07-27 14:43:53.618149 2024-07-27 16:11:56.220080 Each column (e.g., ch.df[\"timestamp\"] ) is a Polars \"series\" (type: pl.Series ). See the Polars documentation to learn about the enormous range of operations you can perform on dataframes and data series. The library is optimized for parallel operation and offers database-like queries to select and join frames. It offers the option of lazy queries, where a query is built up step-by-step then optimized before being executed. We cannot stress enough how thoroughly worth it it is for a Mass2 user to learn and take advantage of the Polars library to make any query, no matter how simple or complex. Polars is excellent. Analyzing pulse data One of the design goals for Mass2 was to enable pulse analysis in each of two distinct situations: Analysis of a data set from a new instrument, or a new configuration (e.g., a changed bath temperature or bias voltage), or an unfamiliar sample (especially a new calibration target). In this case, we don't yet know the appropriate optimal filter to use for each sensor, or how to convert measured pulse sizes into photon energies. We might also not even know the optimal set of analysis choices to make, such as whether arrival-time (\"phase\") correction is required. Analysis of new data from an established instrument in a known configuration with a known energy-calibration curve. In this case, we can apply an analysis \"recipe\" learned from the other case. It is assumed that this earlier recipe was learned from similar data, and that it will therefore produce good, if imperfect, results. The results should be good enough to use during real-time, or \"online\" data analysis to assess data quality, even if they have to be improved offline for a final result. This section concentrate on the first case, analysis of new data for which no established recipe exists. For the second case, see Re-using a recipe and Storing a recipe . Summarizing pulses The Channel.summarize_pulses() method returns a new Channel with a much enhanced dataframe that has nearly a dozen new columns, such as [\"pretrig_mean\", \"pulse_rms\", ...] . To use this method on all the Channel s in a Channels object, we make use of the handy Channels.map() method. It takes a single, callable argument f which must accept a Channel object and returns a modified version of it. We also often follow this up by a function that attaches a \"good expression\" to each channel. Here we'll consider pulses \"good\" when their pretrigger rms and their postpeak derivatives are no more than 8-sigma off the median. These two steps can be performed with map() like this: # mkdocs: render def summarize_and_cut(ch: mass2.Channel) -> mass2.Channel: return ( ch.summarize_pulses() .with_good_expr_pretrig_rms_and_postpeak_deriv(8, 8) ) data = data.map(summarize_and_cut) # Plot a distribution ch = data.ch0 prms = ch.df[\"pulse_rms\"] hist_range = range=np.percentile(prms, [0.5, 99.5]) bin_edges = np.linspace(hist_range[0], hist_range[1], 1000) ch.plot_hist(\"pulse_rms\", bin_edges) plt.xlabel(\"Pulse rms (arbs)\") plt.title(f\"The middle 99% of pulse rms values for channel {ch.header.ch_num}\") You may either re-use the same variable name, e.g., data=data.do_something() here, or use a new variable name such as data2 . Most operations only add to the fields in a data frame, without modifying or removing any existing fields, so preserving the old object is not usually useful. It can be useful to build up an analysis in stages, however, checkpointing with new variables names as you go. Many of the example notebooks use data , data2 , data3 as checkpoints. Computing and applying optimal filters To compute an optimal filter for each channel, one must analyze the noise (to learn its autocorrelation function) and create a model of pulses (typically, the model is an average pulse, possibly computed over a selected subset of the records). The simplest approach is to use the Channel.filter5lag method. It analyzes noise a standard way, and it computes an average of all pulses designated \"good\" in the previous step. If you wanted to restrict those pulses further, an optional use_expr argument lets you pass in a Polars expression to reject pulses outside a certain range of times, or experiment states, or to select based on a range of pulse_rms values. The good_expr is remembered through each step of the analsys, while the use_expr is used only for a single step of the analysis. We'll skip the \"use\" here and instead take the default choice of using all good pulses: # mkdocs: render def do_filter(ch: mass2.Channel) -> mass2.Channel: return ch.filter5lag(f_3db=10000) data = data.map(do_filter) The operation above (or the application of any 5-lag filter) will add new fields (\"5lagx\", \"5lagy\") to the channel data frame. It will also add a new, last \"step\" in each channel's analysis \"recipe\". See more in Saving your results below. Here's how you can look at the last step and make its built-in debug plot. For optimal filter construction, the debug plot is simply a plot of the filter created in the step. # mkdocs: render ch = data.ch0 step = ch.steps[-1] step.dbg_plot(ch.df) The step also contains both the new optimal filter and the FilterMaker used to create it. You can look at the latter to find the noise correlation and spectrum, and the signal model--all the ingredients needed to make optimal filters: # mkdocs: render maker = step.filter_maker plt.clf() plt.subplot(221) plt.plot(maker.noise_autocorr[:100], \".-b\") plt.plot(0, maker.noise_autocorr[0], \"ok\") plt.title(\"Noise autocorrelation\") plt.xlabel(f\"Lags (each lag = {maker.sample_time_sec*1e6:.2f} \u00b5s)\") plt.subplot(222) freq_khz = np.linspace(0, 0.5*1e-3/maker.sample_time_sec, len(maker.noise_psd)) plt.loglog(freq_khz[1:], maker.noise_psd[1:], \"-g\") plt.xlabel(\"Frequency (kHz)\") plt.title(\"Noise power spectral density\") plt.ylabel(\"Noise PSD (arbs$^2$ / Hz)\") plt.subplot(212) t_ms = (np.arange(ch.header.n_samples)-ch.header.n_presamples)*maker.sample_time_sec*1000 plt.plot(t_ms, maker.signal_model, \"r\") plt.title(\"Model pulse\") plt.xlabel(\"Time after trigger (ms)\") plt.ylabel(\"Signal (arbs)\") plt.tight_layout() Corrections and energy calibration We can apply the drift correction based on pretrigger mean to the filtered values, and also perform a rough calibration. That might look like: # mkdocs: render def dc_and_rough_cal2(ch: mass2.Channel) -> mass2.Channel: import polars as pl use_cal = (pl.col(\"state_label\") == \"CAL2\") use_dc = pl.lit(True) line_names = [\"CKAlpha\", \"NKAlpha\", \"OKAlpha\", \"FeLl\", \"FeLAlpha\", \"FeLBeta\", \"NiLAlpha\", \"NiLBeta\", \"CuLAlpha\", \"CuLBeta\", 980] return ( ch.rough_cal_combinatoric( line_names=line_names, uncalibrated_col=\"pulse_rms\", calibrated_col=\"energy_pulse_rms\", ph_smoothing_fwhm=6, use_expr=use_cal, ) .driftcorrect( indicator_col=\"pretrig_mean\", uncorrected_col=\"5lagy\", use_expr=use_dc) .rough_cal_combinatoric( line_names, uncalibrated_col=\"5lagy_dc\", calibrated_col=\"energy_5lagy_dc\", ph_smoothing_fwhm=6, use_expr=use_cal, ) ) data = data.map(dc_and_rough_cal2) ch = data.ch0 plt.clf() use = (pl.col(\"state_label\") == \"CAL2\") ax = plt.subplot(211) ch.plot_hist(\"energy_pulse_rms\", np.linspace(0, 1000, 1001), axis=ax, use_expr=use) plt.title(\"Rough-calibrated energy (not optimally filtered)\") plt.xlabel(\"Rough energy (eV)\") ax = plt.subplot(212) ch.plot_hist(\"5lagy_dc\", np.linspace(0, 2600, 1001), axis=ax, use_expr=use) plt.title(\"Uncalibrated, optimally filtered pulse heights\") plt.xlabel(\"Pulse heights (arbs)\") plt.tight_layout() Now, an improved calibration can be achieved with actual fits to the known calibration lines. (One subtle aspect of this improved calibration is that there must be an existing calibration from the same field into energy that you want to use for the final. In this case, we must have that second rough_cal_combinatoric above, because only that second one uses the field 5lagy_dc as its input. This allows the multifit calibration routine to know about the energy scale well enough to find the lines to fit. Furthermore, we have to track which step it was whose calibration we are improving upon. Here, that is the previous step, called -1 in python. The first rough calibration wouldn't meet our needs, because it is a calibration from pulse_rms into energy, a totally different mapping.) # mkdocs: render def do_multifit(ch: mass2.Channel) -> mass2.Channel: import mass2 import polars as pl STEP_WITH_ROUGH_CAL = -1 multifit = mass2.MultiFit( default_fit_width=80, default_use_expr=(pl.col(\"state_label\") == \"CAL2\"), default_bin_size=0.3, ) line_names = [\"CKAlpha\", \"NKAlpha\", \"OKAlpha\", \"FeLl\", \"FeLAlpha\", \"NiLAlpha\", \"CuLAlpha\", 980.0] dhigh = {\"CuLAlpha\": 12, \"NiLAlpha\": 12} dlow = {980.0: 20} for line in line_names: multifit = multifit.with_line(line, dlo=dlow.get(line, None), dhi=dhigh.get(line, None)) return ch.multifit_mass_cal(multifit, STEP_WITH_ROUGH_CAL, \"energy_5lagy_best\") data = data.map(do_multifit) ch = data.ch0 plt.clf() ax1 = plt.subplot(211) use_cal = (pl.col(\"state_label\") == \"CAL2\") edges = np.linspace(0, 1000, 1001) ch.plot_hist(\"energy_5lagy_best\", edges, axis=ax1, use_expr=use_cal) plt.title(\"Calibrated energy, state='CAL2'\") plt.xlabel(\"Pulse energy (eV)\") ax2 = plt.subplot(212, sharex=ax1) use_noncal = (pl.col(\"state_label\") == \"SCAN3\") ch.plot_hist(\"energy_5lagy_best\", edges, axis=ax2, use_expr=use_noncal) plt.title(\"Calibrated energy, state='SCAN3'\") plt.xlabel(\"Pulse energy (eV)\") plt.xlim([0, 1000]) plt.tight_layout() Saving results and the Recipe system Mass2 is designed to make storing results easy, both the the quantities produced in analysis and the steps taken to compute them. This section explains how and explores the saving of steps and of numerical results. Analysis \"steps\" have been mentioned before. The big idea is that each channel is associated with a Recipe containing all the operations and parameters used to create it, starting from the raw pulse records in an Raw Data file. (Raw Data files are never modified by Mass2.) This recipe consists of one or more steps to be applied in a fixed order. Each step takes for input fields that existed in the channel's dataframe before the step, and it creates one or more new fields that exist in the dataframe afterwards. Importantly, a recipe can be stored to disk, and applied in the future, either to the same data set, or to another. Warning: One catch is that each recipe is specific to one sensor, and they are tracked only by channel number. If the set of active sensors changes after the learning phase, any new sensors will lack a recipe. Worse, if a probe of the \u00b5MUX resonator frequencies leads to renumbering of the sensors, then we won't be able to find the right recipe for most (or all) sensors. Caching computed data (work in progress) Sometimes you run an analysis that you consider \"final\"; you want to keep the results only, and you expect (hope?) never to look at the raw LJH files again. We are still working out a standard system for caching computed data: how to name the files, how to combine them, etc. Here are two approaches that employ Parquet files. One stores a file per channel, and the other stores all data in a single file. Approach A: one file per channel Here we store each channel's dataframe in a separate file. Notice that we want to drop the column representing the raw pulse data, because the output would otherwise be far too large (and redundant). Probably it's fine to drop the subframe count, too, so we do that here. # mkdocs: render # For automated tests, we want the output in a temporary directory import tempfile, os output_dir = tempfile.TemporaryDirectory(prefix=\"mass2_getting_started\") print(f\"Test output lives in '{output_dir.name}'\") columns_to_drop = (\"pulse\", \"subframecount\") for ch_num, ch in data.channels.items(): filename = os.path.join(output_dir.name, f\"output_test_chan{ch_num}.parquet\") df = ch.df.drop(columns_to_drop) df.write_parquet(filename) You can pass options to write_parquet to control compression and other aspects of the file. If you prefer to write out only a specified subset of the fields in the dataframes, replace the .drop(columns_to_drop) in the code above with a .select(columns_to_keep) operation. All the methods that act on dataframes are possible: drop to remove specific columns select to keep specific columns limit to write only a maximum number of rows filter to select rows according to a Polars expression alias to rename columns and many others. TODO: we should work on a way to make it easier to load up these files and start analyzing where you left off. One might match up these files with an LJH file, to permit creation of a new Channel object. For now, this sort of data caching is limited to when you just need the data, not the full framework of Mass2. (But keep in mind, the Polars framework is really powerful, and there's a lot you could imagine doing with just the data.) Approach B: one file for all channels This is similar, except that we use some new Polars tricks: polars.DataFrame.with_columns() to append a new column containing this channel's number (we will need it later, because we're about to mix all channels into a new dataframe) polars.concat() to join rows from multiple dataframes columns_to_drop = (\"pulse\", \"subframecount\") all_df = [] for ch_num, ch in data.channels.items(): df = ch.df.drop(columns_to_drop).with_columns(ch_num=pl.lit(ch_num)) all_df.append(df) filename = os.path.join(output_dir.name, \"output_test_allchan.parquet\") pl.concat(all_df).write_parquet(filename) Storing analysis recipes The system for storing analysis recipes is extremely simple. The following will save the full analysis recipe for each channel in the data object. recipe_filename = os.path.join(output_dir.name, \"full_recipes.pkl\") data.save_recipes(recipe_filename) Beware that this operation will drop all \"debug information\" in certain steps in the recipe (generally, any steps that store a lot of data that's used not for performing the step but only for debugging). The benefit is that the recipes file is smaller, and faster to save and load. The cost is that debugging plots cannot be made after the recipes are reloaded. You probably save a recipe because you've already debugged its steps, so this is usually an acceptable tradeoff. If you don't like it, use the drop_debug=False argument. By default all steps in a recipe are saved, but you might not want or need this. For instance, in the examples given on this page, we computed two \"rough\" calibrations and a final one. The two rough ones are \"dead ends\" if you only care about the final energy. When you are saving a recipe for later use on future data sets in a real-time pipeline, you might want to store a minimal recipe--one that omits any steps not needed to reach a few critical fields. Perhaps you have no need to compute anything that doesn't directly lead to the final energy value. If you omit dead ends from the stored recipes, then not only is the file smaller, but the computation required to run the recipes is reduced. You might prefer instead: required_fields = {\"energy_5lagy_best\", \"pretrig_mean\"} trimmed_recipe_filename = os.path.join(output_dir.name, \"trimmed_recipes.pkl\") data.save_recipes(trimmed_recipe_filename, required_fields=required_fields) This version will save all the steps that produce the two specified required_fields , and all steps that the depend on. The two rough calibration steps, however, will be trimmed away as dead ends. (In this instance, having the pretrigger mean as a required field is superfluous. It's already required in the drift correction step, which is required before the step that takes drift-corrected optimally-filtered pulse height into energy.) Re-using a recipe (on the same or new data) Once a mass2.Channels object is created from raw LJH files, an existing recipe can be loaded from a file and executed on the raw data. The result will be that the dataframe for each Channel will now contain the derived fields, including {\"energy_5lagy_best\", \"pretrig_mean\"} , as well as any fields computed along with them, or on the direct path to them. pn_pair = pulsedata.pulse_noise_ljh_pairs[\"bessy_20240727\"] data_replay = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder ).with_experiment_state_by_path() data_replay = data_replay.load_recipes(trimmed_recipe_filename) # Verify that the expected columns exist in the replay and have identical values df1 = data.ch0.df df2 = data_replay.ch0.df assert df2.equals(df1.select(df2.columns)) Further examples There are more examples of how one can use the Mass2 approach to data analysis in the examples/ directory, in the form of Marimo notebooks . Many examples in the top level of that directory have been designed for use on data found in the https://github.com/ggggggggg/pulsedata repository, so they should work for any user of Mass. Other examples rely on data that isn't public, but they can still be useful training materials for ways to use Mass2. Marimo is similar to Jupyter, but it has some nice advantages. It stores notebooks as pure Python (vs JSON for Jupyter). It uses a reactive execution model with an understanding of the data-flow dependency graph. This fixes a lot of the hidden-state and unexpected behavior that often come with Jupyter's linear-flow (top-to-bottom) execution model. Another really cool Marimo feature (though it's still a bit buggy) is that Matplotlib figure can be rendered and made interactive in the web browser (rather than having to explicitly set x/y limits and re-run a cell)--the result is closer to what you expect from an iPython session. Marimo is easy to start from the command line. The marimo shell command will start a marimo server running, which both serves web pages and runs the underlying Python code. Here are a few examples of how to start marimo: If you add pip scripts to your path (following the confusing documentation from Python or possible the helpful error message pip or uv prints out) you may: mass2-examples OR on windows mass2-examples.exe cd ~/where/I/stored/mass2 # Each of the following commands starts a local Marimo webserver, # then opens a brower tab pointing to it. They also... # ...display a chooser to select whatever example you like: marimo edit examples # ...display one specific example notebook, with (editable) code: marimo edit examples/bessy_20240727.py # ...display a notebook's results, but not the code that created it: marimo run examples/ebit_juy2024_from_off.py We strongly suggest exploring a couple of these notebooks as you launch your adventure with Mass2.","title":"Getting started"},{"location":"getting_started/#getting-started-with-mass2","text":"The Microcalorimeter Analysis Software Suite, or Mass, is a library designed for the analysis of data generated by superconducting microcalorimeters, such as Transition-Edge Sensors (TESs) and Thermal Kinetic-Inductance Detectors (TKIDs).","title":"Getting started with Mass2"},{"location":"getting_started/#key-features-and-methods","text":"Mass2 represents microcalorimeter pulse data using a Pola.rs DataFrame. Each row represents a single pulse, and there may be an arbitrary number of columns representing \"per pulse quantities\". The raw pulse record may be stored in the DataFrame as well; Mass2 uses \"memmaps\" to limit memory usage in this case. The use of the Polars DataFrame enables many best-in-class methods for slicing and dicing your data. It abstracts away the input and output file formats, allowing Mass2 to easily handle data from many different file formats (LJH, OFF, .bin, and anything you can make a numpy array from). It enables export/reload from many formats as well. When in doubt, we suggest using the Apache Parquet file format to save data in a native data frame structure. Mass2 is designed to do analysis on arrays of microcalorimeters, so Mass2 provides APIs and algorithms that can perform equivalent operations on hundreds of distinct channels all at once. Mass2 creates and applies optimal filters. Mass2 creates and applies energy-calibration functions. Mass2 Fits known fluoresence-line shapes to measured spectra, and it can generate calibrations from the fits. Mass2 supports two modes of operation: From scratch, where you analyze detector data for the first time assuming no prior knowledge and From Recipes, where you \"replay\" an analysis Recipe created in the other mode. In Recipe mode, there is no learning from data, it simply does the same transformations to your DataFrame to produce a new DataFrame. This is intended for use in instruments that operate the same way day in and day out. Mass2 uses many programming best practices, including automated testing and doc tests. We also have adopted an immutable-first approach, where the majority of Python objects are immutable, like the Polars DataFrame. Data transformations are reprented by binding a new variable for example ch2 = ch.do_something() . This makes the code easier to read and reason about.","title":"Key features and methods"},{"location":"getting_started/#comparison-of-mass-versions-2-vs-1","text":"Mass2 builds on many lessons learned developing Mass and adopts many newer practices. Mass was created starting in February 2011 with the goal of analyzing data from a 16-pixel array. It was, in many ways, an inflexible, custom-built Data Science library before Data Science was a common term. Pandas, a popular DataFrame library, existed in 2011 but was not widely known and had only one major contributor. Changes from Mass version 1 include: Mass2 takes a different view of data wrangling and bookkeeping than version 1, leveraging the power of the Pola.rs Dataframe for this purpose. Polars is a high-performance modern dataframe library. It organizes the data structures and provides data I/O. This change increases efficiency and performance for some activities. More importantly, using a Dataframe makes the arrays of per-pulse quantities both flexible (you can add fields of your own design on equal footing with standard quantities) and clearer (you can easily find a complete list of all such quantities, plus data types and statistics). Mass2 no longer uses HDF5 datasets to automatically cache analysis results alongside the raw data. This change removes a valuable, but often maddeningly imperfect feature. We think the cost worth paying. There are big benefits in clarity and avoiding both errors and hidden side effects. Mass2 give access to the native save/load features of Polars, making it easy to preserve and recover subsets or complete sets of analyzed data. The preferred file format is Apache Parquet , but several other choices like CSV are supported and can be used for interchange with frameworks other than Mass. Many data structures in Mass2 use Python's \"frozen dataclasses\" features, making them immutable. With immutable objects, it is much easier to avoid inconsistent state and sneaky bugs. They will take some getting used to at first, but (just as with the lack of HDF5 backing) we believe the benefits are worth the costs. Mass2 tries to expose all functionality as a \"bare function\" that takes arrays as argument, then also exposes that with a convenient ch.do_something() API; in Mass version 1, many algorithms were implemented as methods of a data class and thus impossible to use on their own. Some things have not changed. Mass still makes heavy use of the libraries: Numpy . For numerical arrays and scientific computation. Scipy . Additional algorithms for scientific computation. Matplotlib . For high-quality plotting. LMFIT . Convenient non-linear curve fitting. Mass also still: Lets you operate on many microcalorimeter data channels in parallel with a single function call. Works from full pulse records in LJH files, or pre-filtered pulses in OFF files. Generates standard pulse-summary quantities. Creates and applies optimal filters. Creates and applies energy-calibration functions. Fits known fluorescence-line shapes to measured spectra. This guide is based on the evolving API of Mass2, still under development.","title":"Comparison of Mass versions 2 vs 1"},{"location":"getting_started/#how-to-load-pulse-data","text":"Raw pulse data may be stored in the custom LJH format . An Optimally Filtered Format (\"OFF\") has also been used, in which the Dastard data acquisition program performs optimal filtering in real time. Mass2 can work with data of both types, though of course any process that requires access to the full raw pulse record will fail on an OFF file. Mass2 also has some capability to work with untriggered data stored in .bin or similar files. (Other formats could be added to Mass2 in the future, if necessary.)","title":"How to load pulse data"},{"location":"getting_started/#how-to-load-full-pulses-from-ljh-files","text":"Mass2 can read LJH files, whether the modern version 2.2, or the earlier 2.1 or 2.0 versions. The older versions have much reduced time resolution and no ability to connect with external triggers; for the most part, they were superceded in late 2015 by the latest verison. The file version is automatically detected by Mass, and you should not have to think about it. For this getting started guide, we'll assume you have the pulsedata package installed. (This should happen automatically when you install Mass2.) It's a standard source of a small number of LJH and OFF example files. Often, you want to load all the data files in a directory, possibly with companion noise files from another. That looks like this: # mkdocs: render import numpy as np import pylab as plt import polars as pl import pulsedata import mass2 pn_pair = pulsedata.pulse_noise_ljh_pairs[\"bessy_20240727\"] data = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder ) The value returned, data , is a mass2.Channels object. At its heart is data.channels , a dictionary mapping from channel numbers to single-sensor objects of the type mass2.Channel . When you create the Channels , there are options to exclude certain channels with problems, or limit the result to some maximum number of channels. For these needs, you can use the arguments limit: int and/or exclude_ch_nums: Iterable[int] . For instance, the following will return a Channels that omits channel 4220 and limits the output to no more than 5 channels (in fact, there will be only 1, given the data source has only one channel other than number 4220): less_data = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder, limit=5, exclude_ch_nums=[4220] )","title":"How to load full pulses from LJH files"},{"location":"getting_started/#advanced-cases-for-deep-exploration","text":"If you want to create a single mass2.Channel object (not created as part of a mass2.Channels object): pfile = pn_pair.pulse_folder / \"20240727_run0002_chan4220.ljh\" nfile = pn_pair.noise_folder / \"20240727_run0000_chan4220.ljh\" # the noise file is optional ch = mass2.Channel.from_ljh(pfile, nfile) To open a single LJH file and study it as a pure file, you can use the internal class LJHFile , like this: # mkdocs: render ljh = mass2.LJHFile.open(pn_pair.pulse_folder/\"20240727_run0002_chan4220.ljh\") print(ljh.npulses, ljh.npresamples, ljh.nsamples) print(ljh.is_continuous) print(ljh.dtype) plt.clf() y = ljh.read_trace(30) plt.plot(y, \".r\")","title":"Advanced cases, for deep exploration"},{"location":"getting_started/#how-to-load-pre-filtered-pulses-from-off-files","text":"OFF files do not record the entire microcalorimeter pulse, but only a summary of it. This means that no analysis steps that require the raw pulse can be performed. Also, many of the per-pulse summary quantities that exist in an OFF file have different names (and perhaps slightly different definitions) than the corresponding fields computed in a standard LJH-based analysis. Use of OFF files is not a primary goal of Mass2 in the initial release, but they are supported for legacy analysis. # Load one OFF file into an `OffFile` object' p = pn_pair.pulse_folder off = mass2.core.OffFile(p / \"20240727_run0002_chan4220.off\") # Load multiple OFF files info a `Channels` object import glob files = glob.glob(str(p / \"*_chan*.off\")) offdata = mass2.Channels.from_off_paths(files, description=\"OFF file demo\") # Here's the dataframe for a single channel's per-pulse data given an OFF input ch = offdata.ch0 print(ch.df)","title":"How to load pre-filtered pulses from OFF files"},{"location":"getting_started/#how-to-load-additional-information","text":"Most data sets come with experiment-state labels, and some also have external trigger timing. Both sets of information are universal across channels, in the sense that a single table of state labels or external trigger times is relevant to all channels in that data acquisition. Therefore, they are best loaded for all channels at once. Here is the first time we see a consequence of the immutability of the Channel and Channels classes: instead of adding information to an existing object, we have to create a new object, a copy of what came before but with added information. In this case, the added information will be new data columns in the dataframe of each channel. But to use the new, enhanced object, we have to re-assign the name data to the new object. Thus: # mkdocs: render # Load the *_experiment_state.txt file, assuming it has the usual name and path data = data.with_experiment_state_by_path() ch = data.ch0 print(ch.df.columns) states = ch.df[\"state_label\"] print(states.dtype) print(states.unique(), states.unique_counts()) assert states.unique()[-2] == \"SCAN3\" assert states.unique_counts()[-2] == 42433 The output reads: ['timestamp', 'pulse', 'subframecount', 'state_label'] Categorical shape: (6,) Series: 'state_label' [cat] [ null \"START\" \"CAL2\" \"PAUSE\" \"SCAN3\" \"SCAN4\" ] shape: (6,) Series: 'state_label' [u32] [ 4 4 11468 40854 42433 5237 ] Notice that Polars saves the state labels as a \"Categorical\" type . This type improves performance when the values are strings known to take on one of a limited number of possibilities (far fewer than the number of rows in a table). Here's how you would load external trigger, if this example data set had external triggers: # exttrig_path = p / \"20240727_run0002_external_trigger.bin\" # data = data.with_external_trigger_by_path(exttrig_path)","title":"How to load additional information"},{"location":"getting_started/#what-is-in-the-channel-and-channels-objects","text":"The most important part of Channels is a dictionary mapping channel numbers each to a Channel object. You can access a specific channel, or the \"first\", like this: ch = data.channels[4220] # or if you just want any rando channel, this returns the first channel in order of creation (normally, the one with the lowest channel #) arb_channel = data.ch0 Other features include a dictionary of channels declared \"bad\" print(f\"There are {len(data.bad_channels)} bad channels in the data at first.\") data_withbad = data.set_bad(4219, \"We think chan 4219 is no good at all.\") print(f\"There are {len(data_withbad.bad_channels)} bad channels if we capriciously declare one bad.\") Bad channels are used to prevent a few problematic channels from derailing a whole analysis, so when one channel has an error with an analysis step, it can be marked bad and the rest of the channels can proceed to the next analysis step. It is easy to join the data from successive experiments into a single concatenated object. This example just joins a second copy of the same data to itself, because the pulsedata repository doesn't happen to have more data from the same experiment. later_data = data data_merged = data.concat_data(later_data) The most important part of each Channel object is the underlying per-pulse dataframe. This code prints that DataFrame , then the facts specific to the sensor channel as a whole, first as an object, then a dataframe: print(ch.df) print(ch.header) print(ch.header.df) # Here are the pulse dataframe's column names, the contents of the subframecount column, and the min/mean/max of the timestamp columns print(ch.df.columns) print(ch.df[\"subframecount\"]) ts = ch.df[\"timestamp\"] print(ts.min(), ts.mean(), ts.max()) assert len(ts) == 100_000 The above commands produce the output: shape: (100_000, 4) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 timestamp \u2506 pulse \u2506 subframecount \u2506 state_label \u2502 \u2502 --- \u2506 --- \u2506 --- \u2506 --- \u2502 \u2502 datetime[\u03bcs] \u2506 array[u16, 500] \u2506 u64 \u2506 cat \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 2024-07-27 13:30:44.451920 \u2506 [6063, 6076, \u2026 6655] \u2506 1519640266112 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.472140 \u2506 [6071, 6065, \u2026 6670] \u2506 1519640589632 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.475788 \u2506 [6120, 6123, \u2026 6215] \u2506 1519640648000 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.497137 \u2506 [6074, 6065, \u2026 6660] \u2506 1519640944448 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.573915 \u2506 [6074, 6072, \u2026 6672] \u2506 1519642206080 \u2506 START \u2502 \u2502 \u2026 \u2506 \u2026 \u2506 \u2026 \u2506 \u2026 \u2502 \u2502 2024-07-27 16:11:55.982246 \u2506 [6065, 6063, \u2026 6278] \u2506 1674384771712 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:55.986849 \u2506 [6077, 6089, \u2026 6662] \u2506 1674384833216 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.054243 \u2506 [6058, 6048, \u2026 6637] \u2506 1674385923008 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.080575 \u2506 [6067, 6064, \u2026 6639] \u2506 1674386344320 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.220080 \u2506 [6062, 6055, \u2026 6483] \u2506 1674388576448 \u2506 PAUSE \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ChannelHeader(description='20240727_run0002_chan4219.ljh', ch_num=4219, frametime_s=4e-06, n_presamples=250, n_samples=500) shape: (1, 27) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Save File Format \u2506 Software Version \u2506 Software Git Hash \u2506 Data source \u2506 \u2026 \u2506 Pixel Name \u2506 Timebase \u2506 Filename \u2506 continuous \u2502 \u2502 Version \u2506 --- \u2506 --- \u2506 --- \u2506 \u2506 --- \u2506 --- \u2506 --- \u2506 --- \u2502 \u2502 --- \u2506 str \u2506 str \u2506 str \u2506 \u2506 str \u2506 f64 \u2506 str \u2506 bool \u2502 \u2502 str \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 2.2.1 \u2506 DASTARD version 0.3.2 \u2506 508fd92 \u2506 Abaco \u2506 \u2026 \u2506 \u2506 0.000004 \u2506 /Users/fowlerj/qsp/lib/ \u2506 false \u2502 \u2502 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 python3\u2026 \u2506 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ['timestamp', 'pulse', 'subframecount', 'state_label'] shape: (100_000,) Series: 'subframecount' [u64] [ 1519640266112 1519640589632 1519640648000 1519640944448 1519642206080 \u2026 1674384771712 1674384833216 1674385923008 1674386344320 1674388576448 ] 2024-07-27 13:30:44.451920 2024-07-27 14:43:53.618149 2024-07-27 16:11:56.220080 Each column (e.g., ch.df[\"timestamp\"] ) is a Polars \"series\" (type: pl.Series ). See the Polars documentation to learn about the enormous range of operations you can perform on dataframes and data series. The library is optimized for parallel operation and offers database-like queries to select and join frames. It offers the option of lazy queries, where a query is built up step-by-step then optimized before being executed. We cannot stress enough how thoroughly worth it it is for a Mass2 user to learn and take advantage of the Polars library to make any query, no matter how simple or complex. Polars is excellent.","title":"What is in the Channel and Channels objects"},{"location":"getting_started/#analyzing-pulse-data","text":"One of the design goals for Mass2 was to enable pulse analysis in each of two distinct situations: Analysis of a data set from a new instrument, or a new configuration (e.g., a changed bath temperature or bias voltage), or an unfamiliar sample (especially a new calibration target). In this case, we don't yet know the appropriate optimal filter to use for each sensor, or how to convert measured pulse sizes into photon energies. We might also not even know the optimal set of analysis choices to make, such as whether arrival-time (\"phase\") correction is required. Analysis of new data from an established instrument in a known configuration with a known energy-calibration curve. In this case, we can apply an analysis \"recipe\" learned from the other case. It is assumed that this earlier recipe was learned from similar data, and that it will therefore produce good, if imperfect, results. The results should be good enough to use during real-time, or \"online\" data analysis to assess data quality, even if they have to be improved offline for a final result. This section concentrate on the first case, analysis of new data for which no established recipe exists. For the second case, see Re-using a recipe and Storing a recipe .","title":"Analyzing pulse data"},{"location":"getting_started/#summarizing-pulses","text":"The Channel.summarize_pulses() method returns a new Channel with a much enhanced dataframe that has nearly a dozen new columns, such as [\"pretrig_mean\", \"pulse_rms\", ...] . To use this method on all the Channel s in a Channels object, we make use of the handy Channels.map() method. It takes a single, callable argument f which must accept a Channel object and returns a modified version of it. We also often follow this up by a function that attaches a \"good expression\" to each channel. Here we'll consider pulses \"good\" when their pretrigger rms and their postpeak derivatives are no more than 8-sigma off the median. These two steps can be performed with map() like this: # mkdocs: render def summarize_and_cut(ch: mass2.Channel) -> mass2.Channel: return ( ch.summarize_pulses() .with_good_expr_pretrig_rms_and_postpeak_deriv(8, 8) ) data = data.map(summarize_and_cut) # Plot a distribution ch = data.ch0 prms = ch.df[\"pulse_rms\"] hist_range = range=np.percentile(prms, [0.5, 99.5]) bin_edges = np.linspace(hist_range[0], hist_range[1], 1000) ch.plot_hist(\"pulse_rms\", bin_edges) plt.xlabel(\"Pulse rms (arbs)\") plt.title(f\"The middle 99% of pulse rms values for channel {ch.header.ch_num}\") You may either re-use the same variable name, e.g., data=data.do_something() here, or use a new variable name such as data2 . Most operations only add to the fields in a data frame, without modifying or removing any existing fields, so preserving the old object is not usually useful. It can be useful to build up an analysis in stages, however, checkpointing with new variables names as you go. Many of the example notebooks use data , data2 , data3 as checkpoints.","title":"Summarizing pulses"},{"location":"getting_started/#computing-and-applying-optimal-filters","text":"To compute an optimal filter for each channel, one must analyze the noise (to learn its autocorrelation function) and create a model of pulses (typically, the model is an average pulse, possibly computed over a selected subset of the records). The simplest approach is to use the Channel.filter5lag method. It analyzes noise a standard way, and it computes an average of all pulses designated \"good\" in the previous step. If you wanted to restrict those pulses further, an optional use_expr argument lets you pass in a Polars expression to reject pulses outside a certain range of times, or experiment states, or to select based on a range of pulse_rms values. The good_expr is remembered through each step of the analsys, while the use_expr is used only for a single step of the analysis. We'll skip the \"use\" here and instead take the default choice of using all good pulses: # mkdocs: render def do_filter(ch: mass2.Channel) -> mass2.Channel: return ch.filter5lag(f_3db=10000) data = data.map(do_filter) The operation above (or the application of any 5-lag filter) will add new fields (\"5lagx\", \"5lagy\") to the channel data frame. It will also add a new, last \"step\" in each channel's analysis \"recipe\". See more in Saving your results below. Here's how you can look at the last step and make its built-in debug plot. For optimal filter construction, the debug plot is simply a plot of the filter created in the step. # mkdocs: render ch = data.ch0 step = ch.steps[-1] step.dbg_plot(ch.df) The step also contains both the new optimal filter and the FilterMaker used to create it. You can look at the latter to find the noise correlation and spectrum, and the signal model--all the ingredients needed to make optimal filters: # mkdocs: render maker = step.filter_maker plt.clf() plt.subplot(221) plt.plot(maker.noise_autocorr[:100], \".-b\") plt.plot(0, maker.noise_autocorr[0], \"ok\") plt.title(\"Noise autocorrelation\") plt.xlabel(f\"Lags (each lag = {maker.sample_time_sec*1e6:.2f} \u00b5s)\") plt.subplot(222) freq_khz = np.linspace(0, 0.5*1e-3/maker.sample_time_sec, len(maker.noise_psd)) plt.loglog(freq_khz[1:], maker.noise_psd[1:], \"-g\") plt.xlabel(\"Frequency (kHz)\") plt.title(\"Noise power spectral density\") plt.ylabel(\"Noise PSD (arbs$^2$ / Hz)\") plt.subplot(212) t_ms = (np.arange(ch.header.n_samples)-ch.header.n_presamples)*maker.sample_time_sec*1000 plt.plot(t_ms, maker.signal_model, \"r\") plt.title(\"Model pulse\") plt.xlabel(\"Time after trigger (ms)\") plt.ylabel(\"Signal (arbs)\") plt.tight_layout()","title":"Computing and applying optimal filters"},{"location":"getting_started/#corrections-and-energy-calibration","text":"We can apply the drift correction based on pretrigger mean to the filtered values, and also perform a rough calibration. That might look like: # mkdocs: render def dc_and_rough_cal2(ch: mass2.Channel) -> mass2.Channel: import polars as pl use_cal = (pl.col(\"state_label\") == \"CAL2\") use_dc = pl.lit(True) line_names = [\"CKAlpha\", \"NKAlpha\", \"OKAlpha\", \"FeLl\", \"FeLAlpha\", \"FeLBeta\", \"NiLAlpha\", \"NiLBeta\", \"CuLAlpha\", \"CuLBeta\", 980] return ( ch.rough_cal_combinatoric( line_names=line_names, uncalibrated_col=\"pulse_rms\", calibrated_col=\"energy_pulse_rms\", ph_smoothing_fwhm=6, use_expr=use_cal, ) .driftcorrect( indicator_col=\"pretrig_mean\", uncorrected_col=\"5lagy\", use_expr=use_dc) .rough_cal_combinatoric( line_names, uncalibrated_col=\"5lagy_dc\", calibrated_col=\"energy_5lagy_dc\", ph_smoothing_fwhm=6, use_expr=use_cal, ) ) data = data.map(dc_and_rough_cal2) ch = data.ch0 plt.clf() use = (pl.col(\"state_label\") == \"CAL2\") ax = plt.subplot(211) ch.plot_hist(\"energy_pulse_rms\", np.linspace(0, 1000, 1001), axis=ax, use_expr=use) plt.title(\"Rough-calibrated energy (not optimally filtered)\") plt.xlabel(\"Rough energy (eV)\") ax = plt.subplot(212) ch.plot_hist(\"5lagy_dc\", np.linspace(0, 2600, 1001), axis=ax, use_expr=use) plt.title(\"Uncalibrated, optimally filtered pulse heights\") plt.xlabel(\"Pulse heights (arbs)\") plt.tight_layout() Now, an improved calibration can be achieved with actual fits to the known calibration lines. (One subtle aspect of this improved calibration is that there must be an existing calibration from the same field into energy that you want to use for the final. In this case, we must have that second rough_cal_combinatoric above, because only that second one uses the field 5lagy_dc as its input. This allows the multifit calibration routine to know about the energy scale well enough to find the lines to fit. Furthermore, we have to track which step it was whose calibration we are improving upon. Here, that is the previous step, called -1 in python. The first rough calibration wouldn't meet our needs, because it is a calibration from pulse_rms into energy, a totally different mapping.) # mkdocs: render def do_multifit(ch: mass2.Channel) -> mass2.Channel: import mass2 import polars as pl STEP_WITH_ROUGH_CAL = -1 multifit = mass2.MultiFit( default_fit_width=80, default_use_expr=(pl.col(\"state_label\") == \"CAL2\"), default_bin_size=0.3, ) line_names = [\"CKAlpha\", \"NKAlpha\", \"OKAlpha\", \"FeLl\", \"FeLAlpha\", \"NiLAlpha\", \"CuLAlpha\", 980.0] dhigh = {\"CuLAlpha\": 12, \"NiLAlpha\": 12} dlow = {980.0: 20} for line in line_names: multifit = multifit.with_line(line, dlo=dlow.get(line, None), dhi=dhigh.get(line, None)) return ch.multifit_mass_cal(multifit, STEP_WITH_ROUGH_CAL, \"energy_5lagy_best\") data = data.map(do_multifit) ch = data.ch0 plt.clf() ax1 = plt.subplot(211) use_cal = (pl.col(\"state_label\") == \"CAL2\") edges = np.linspace(0, 1000, 1001) ch.plot_hist(\"energy_5lagy_best\", edges, axis=ax1, use_expr=use_cal) plt.title(\"Calibrated energy, state='CAL2'\") plt.xlabel(\"Pulse energy (eV)\") ax2 = plt.subplot(212, sharex=ax1) use_noncal = (pl.col(\"state_label\") == \"SCAN3\") ch.plot_hist(\"energy_5lagy_best\", edges, axis=ax2, use_expr=use_noncal) plt.title(\"Calibrated energy, state='SCAN3'\") plt.xlabel(\"Pulse energy (eV)\") plt.xlim([0, 1000]) plt.tight_layout()","title":"Corrections and energy calibration"},{"location":"getting_started/#saving-results-and-the-recipe-system","text":"Mass2 is designed to make storing results easy, both the the quantities produced in analysis and the steps taken to compute them. This section explains how and explores the saving of steps and of numerical results. Analysis \"steps\" have been mentioned before. The big idea is that each channel is associated with a Recipe containing all the operations and parameters used to create it, starting from the raw pulse records in an Raw Data file. (Raw Data files are never modified by Mass2.) This recipe consists of one or more steps to be applied in a fixed order. Each step takes for input fields that existed in the channel's dataframe before the step, and it creates one or more new fields that exist in the dataframe afterwards. Importantly, a recipe can be stored to disk, and applied in the future, either to the same data set, or to another. Warning: One catch is that each recipe is specific to one sensor, and they are tracked only by channel number. If the set of active sensors changes after the learning phase, any new sensors will lack a recipe. Worse, if a probe of the \u00b5MUX resonator frequencies leads to renumbering of the sensors, then we won't be able to find the right recipe for most (or all) sensors.","title":"Saving results and the Recipe system"},{"location":"getting_started/#caching-computed-data-work-in-progress","text":"Sometimes you run an analysis that you consider \"final\"; you want to keep the results only, and you expect (hope?) never to look at the raw LJH files again. We are still working out a standard system for caching computed data: how to name the files, how to combine them, etc. Here are two approaches that employ Parquet files. One stores a file per channel, and the other stores all data in a single file. Approach A: one file per channel Here we store each channel's dataframe in a separate file. Notice that we want to drop the column representing the raw pulse data, because the output would otherwise be far too large (and redundant). Probably it's fine to drop the subframe count, too, so we do that here. # mkdocs: render # For automated tests, we want the output in a temporary directory import tempfile, os output_dir = tempfile.TemporaryDirectory(prefix=\"mass2_getting_started\") print(f\"Test output lives in '{output_dir.name}'\") columns_to_drop = (\"pulse\", \"subframecount\") for ch_num, ch in data.channels.items(): filename = os.path.join(output_dir.name, f\"output_test_chan{ch_num}.parquet\") df = ch.df.drop(columns_to_drop) df.write_parquet(filename) You can pass options to write_parquet to control compression and other aspects of the file. If you prefer to write out only a specified subset of the fields in the dataframes, replace the .drop(columns_to_drop) in the code above with a .select(columns_to_keep) operation. All the methods that act on dataframes are possible: drop to remove specific columns select to keep specific columns limit to write only a maximum number of rows filter to select rows according to a Polars expression alias to rename columns and many others. TODO: we should work on a way to make it easier to load up these files and start analyzing where you left off. One might match up these files with an LJH file, to permit creation of a new Channel object. For now, this sort of data caching is limited to when you just need the data, not the full framework of Mass2. (But keep in mind, the Polars framework is really powerful, and there's a lot you could imagine doing with just the data.) Approach B: one file for all channels This is similar, except that we use some new Polars tricks: polars.DataFrame.with_columns() to append a new column containing this channel's number (we will need it later, because we're about to mix all channels into a new dataframe) polars.concat() to join rows from multiple dataframes columns_to_drop = (\"pulse\", \"subframecount\") all_df = [] for ch_num, ch in data.channels.items(): df = ch.df.drop(columns_to_drop).with_columns(ch_num=pl.lit(ch_num)) all_df.append(df) filename = os.path.join(output_dir.name, \"output_test_allchan.parquet\") pl.concat(all_df).write_parquet(filename)","title":"Caching computed data (work in progress)"},{"location":"getting_started/#storing-analysis-recipes","text":"The system for storing analysis recipes is extremely simple. The following will save the full analysis recipe for each channel in the data object. recipe_filename = os.path.join(output_dir.name, \"full_recipes.pkl\") data.save_recipes(recipe_filename) Beware that this operation will drop all \"debug information\" in certain steps in the recipe (generally, any steps that store a lot of data that's used not for performing the step but only for debugging). The benefit is that the recipes file is smaller, and faster to save and load. The cost is that debugging plots cannot be made after the recipes are reloaded. You probably save a recipe because you've already debugged its steps, so this is usually an acceptable tradeoff. If you don't like it, use the drop_debug=False argument. By default all steps in a recipe are saved, but you might not want or need this. For instance, in the examples given on this page, we computed two \"rough\" calibrations and a final one. The two rough ones are \"dead ends\" if you only care about the final energy. When you are saving a recipe for later use on future data sets in a real-time pipeline, you might want to store a minimal recipe--one that omits any steps not needed to reach a few critical fields. Perhaps you have no need to compute anything that doesn't directly lead to the final energy value. If you omit dead ends from the stored recipes, then not only is the file smaller, but the computation required to run the recipes is reduced. You might prefer instead: required_fields = {\"energy_5lagy_best\", \"pretrig_mean\"} trimmed_recipe_filename = os.path.join(output_dir.name, \"trimmed_recipes.pkl\") data.save_recipes(trimmed_recipe_filename, required_fields=required_fields) This version will save all the steps that produce the two specified required_fields , and all steps that the depend on. The two rough calibration steps, however, will be trimmed away as dead ends. (In this instance, having the pretrigger mean as a required field is superfluous. It's already required in the drift correction step, which is required before the step that takes drift-corrected optimally-filtered pulse height into energy.)","title":"Storing analysis recipes"},{"location":"getting_started/#re-using-a-recipe-on-the-same-or-new-data","text":"Once a mass2.Channels object is created from raw LJH files, an existing recipe can be loaded from a file and executed on the raw data. The result will be that the dataframe for each Channel will now contain the derived fields, including {\"energy_5lagy_best\", \"pretrig_mean\"} , as well as any fields computed along with them, or on the direct path to them. pn_pair = pulsedata.pulse_noise_ljh_pairs[\"bessy_20240727\"] data_replay = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder ).with_experiment_state_by_path() data_replay = data_replay.load_recipes(trimmed_recipe_filename) # Verify that the expected columns exist in the replay and have identical values df1 = data.ch0.df df2 = data_replay.ch0.df assert df2.equals(df1.select(df2.columns))","title":"Re-using a recipe (on the same or new data)"},{"location":"getting_started/#further-examples","text":"There are more examples of how one can use the Mass2 approach to data analysis in the examples/ directory, in the form of Marimo notebooks . Many examples in the top level of that directory have been designed for use on data found in the https://github.com/ggggggggg/pulsedata repository, so they should work for any user of Mass. Other examples rely on data that isn't public, but they can still be useful training materials for ways to use Mass2. Marimo is similar to Jupyter, but it has some nice advantages. It stores notebooks as pure Python (vs JSON for Jupyter). It uses a reactive execution model with an understanding of the data-flow dependency graph. This fixes a lot of the hidden-state and unexpected behavior that often come with Jupyter's linear-flow (top-to-bottom) execution model. Another really cool Marimo feature (though it's still a bit buggy) is that Matplotlib figure can be rendered and made interactive in the web browser (rather than having to explicitly set x/y limits and re-run a cell)--the result is closer to what you expect from an iPython session. Marimo is easy to start from the command line. The marimo shell command will start a marimo server running, which both serves web pages and runs the underlying Python code. Here are a few examples of how to start marimo: If you add pip scripts to your path (following the confusing documentation from Python or possible the helpful error message pip or uv prints out) you may: mass2-examples OR on windows mass2-examples.exe cd ~/where/I/stored/mass2 # Each of the following commands starts a local Marimo webserver, # then opens a brower tab pointing to it. They also... # ...display a chooser to select whatever example you like: marimo edit examples # ...display one specific example notebook, with (editable) code: marimo edit examples/bessy_20240727.py # ...display a notebook's results, but not the code that created it: marimo run examples/ebit_juy2024_from_off.py We strongly suggest exploring a couple of these notebooks as you launch your adventure with Mass2.","title":"Further examples"},{"location":"hci_lines_from_asd/","text":"Highly Charged Ion (HCI) Lines from NIST ASD Motivation We often find ourselves hard coding line center positions into mass, which is prone to errors and can be tedious when there are many lines of interest to insert. In addition, the line positions would need to be manually updated for any changes in established results. In the case of highly charged ions, such as those produced in an electron beam ion trap (EBIT), there is a vast number of potential lines coming from almost any charge state of almost any element. Luckily, these lines are well documented through the NIST Atomic Spectral Database (ASD). Here, we have parsed a NIST ASD SQL dump and converted it into an easily Python readable pickle file. The hci_lines.py module implements the NIST_ASD class, which loads that pickle file and contains useful functions for working with the ASD data. It also automatically adds in some of the more common HCI lines that we commonly use in our EBIT data analyses. Exploring the methods of class NIST_ASD The class NIST_ASD can be initialized without arguments if the user wants to use the default ASD pickle file. This file is located at mass/calibration/nist_asd.pickle. A custom pickle file can be used by passing in the pickleFilename argument during initialization. The methods of the NIST_ASD class are described below: Usage examples Next, we will demonstrate usage of these methods with the example of Ne, a commonly injected gas at the NIST EBIT. # mkdocs: render import mass2 import mass2.calibration.hci_lines import numpy as numpy import pylab as plt test_asd = mass2.calibration.hci_lines.NIST_ASD() availableElements = test_asd.getAvailableElements() assert 'Ne' in availableElements availableNeCharges = test_asd.getAvailableSpectralCharges(element='Ne') assert 10 in availableNeCharges subsetNe10Levels = test_asd.getAvailableLevels(element='Ne', spectralCharge=10, maxLevels=6, getUncertainty=False) assert '2p 2P* J=1/2' in list(subsetNe10Levels.keys()) exampleNeLevel = test_asd.getSingleLevel(element='Ne', spectralCharge=10, conf='2p', term='2P*', JVal='1/2', getUncertainty=False) print(availableElements[:10]) print(availableNeCharges) for k, v in subsetNe10Levels.items(): subsetNe10Levels[k] = round(v, 1) print(subsetNe10Levels) print(f'{exampleNeLevel:.1f}') [np.str_('Sn'), np.str_('Cu'), np.str_('Na'), np.str_('As'), np.str_('Zn'), np.str_('Ne'), np.str_('Ge'), np.str_('Ga'), np.str_('Rb'), np.str_('Se')] [9, 1, 2, 3, 4, 5, 6, 7, 8, 10] {'1s 2S J=1/2': 0.0, '2p 2P* J=1/2': 1021.5, '2s 2S J=1/2': 1021.5, '2p 2P* J=3/2': 1022.0, '3p 2P* J=1/2': 1210.8, '3s 2S J=1/2': 1210.8} 1021.5 Functions for generating SpectralLine objects from ASD data The module also contains some functions outside of the NIST_ASD class that are useful for integration with MASS. First, the add_hci_line function which, takes arguments that are relevant in HCI work, including as element , spectr_ch , energies , widths , and ratios . The function calls mass2.calibration.fluorescence_lines.addline , generates a line name with the given parameters, and populates the various fields. As an example, let us create a H-like Be line. Here, we assume a lorentzian width of 0.1 eV. # mkdocs: render test_element = 'Be' test_charge = 4 test_conf = '2p' test_term = '2P*' test_JVal = '3/2' test_level = f'{test_conf} {test_term} J={test_JVal}' test_energy = test_asd.getSingleLevel( element=test_element, spectralCharge=test_charge, conf=test_conf, term=test_term, JVal=test_JVal, getUncertainty=False) test_line = mass2.calibration.hci_lines.add_hci_line(element=test_element, spectr_ch=test_charge, line_identifier=test_level, energies=[test_energy], widths=[0.1], ratios=[1.0]) assert test_line.peak_energy == test_energy print(mass2.spectra[f'{test_element}{test_charge} {test_conf} {test_term} J={test_JVal}']) print(f'{test_line.peak_energy:.1f}') SpectralLine: Be4 2p 2P* J=3/2 163.3 The name format for grabbing the line from mass2.spectra is shown above. The transition is uniquely specified by the element, charge, configuration, term, and J value. Below, we show what this line looks like assuming a zero-width Gaussian component. # mkdocs: render test_line.plot() The module contains two other functions which are used to easily generate some lines from levels that are commonly observed at the NIST EBIT. These functions are add_H_like_lines_from_asd and add_He_like_lines_from_asd . As the names imply, these functions add H- and He-like lines to mass using the data in the ASD pickle. These functions require the asd and element arguments and also contain the optional maxLevels argument, which works similarly as the argument in the class methods. The module also automatically adds H- and He-like lines for the most commonly used elements, which includes 'N', 'O', 'Ne', and 'Ar'. Below, we check that common elements are being added as spectralLine objects and then add some of the lower order H- and He-like Ga lines. # mkdocs: render print([mass2.spectra['O7 1s.2p 1P* J=1'], round(mass2.spectra['O7 1s.2p 1P* J=1'].peak_energy,1)]) test_element = 'Ga' HLikeGaLines = mass2.calibration.hci_lines.add_H_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=6) HeLikeGaLines = mass2.calibration.hci_lines.add_He_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=7) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HLikeGaLines]) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HeLikeGaLines]) [SpectralLine: Ne10 2p 2P* J=3/2, np.float64(1022.0)] [SpectralLine: O7 1s.2p 1P* J=1, np.float64(574.0)] [[SpectralLine: Ga31 2p 2P* J=1/2, np.float64(9917.0)], [SpectralLine: Ga31 2s 2S J=1/2, np.float64(9918.0)], [SpectralLine: Ga31 2p 2P* J=3/2, np.float64(9960.3)], [SpectralLine: Ga31 3p 2P* J=1/2, np.float64(11767.7)], [SpectralLine: Ga31 3s 2S J=1/2, np.float64(11768.0)], [SpectralLine: Ga31 3d 2D J=3/2, np.float64(11780.5)]] [[SpectralLine: Ga30 1s.2s 3S J=1, np.float64(9535.6)], [SpectralLine: Ga30 1s.2p 3P* J=0, np.float64(9571.8)], [SpectralLine: Ga30 1s.2p 3P* J=1, np.float64(9574.4)], [SpectralLine: Ga30 1s.2s 1S J=0, np.float64(9574.6)], [SpectralLine: Ga30 1s.2p 3P* J=2, np.float64(9607.4)], [SpectralLine: Ga30 1s.2p 1P* J=1, np.float64(9628.2)], [SpectralLine: Ga30 1s.3s 3S J=1, np.float64(11304.6)]] HCI lines and models docstring info hci_lines.py Uses pickle file containing NIST ASD levels data to generate some commonly used HCI lines in mass. Meant to be a replacement for _highly_charged_ion_lines.py, which hard codes in line parameters. The pickle file can be gzip-compressed, provided the compressed filename ends with \".gz\". February 2020 Paul Szypryt NIST_ASD Class for working with a pickled atomic spectra database Source code in mass2/calibration/hci_lines.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class NIST_ASD : \"\"\"Class for working with a pickled atomic spectra database\"\"\" def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy __init__ ( pickleFilename = None ) Loads ASD pickle file (optionally gzipped) Parameters: pickleFilename ( str | None , default: None ) \u2013 ASD pickle file name, as str, or if none then mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH (default None) Source code in mass2/calibration/hci_lines.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) getAvailableElements () Returns a list of all available elements from the ASD pickle file Source code in mass2/calibration/hci_lines.py 52 53 54 55 def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) getAvailableLevels ( element , spectralCharge , requiredConf = None , requiredTerm = None , requiredJVal = None , maxLevels = None , units = 'eV' , getUncertainty = True ) For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters: element ( str ) \u2013 Elemental atomic symbol, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf ( str | None , default: None ) \u2013 if not None, limits results to those with conf == requiredConf , by default None requiredTerm ( str | None , default: None ) \u2013 if not None, limits results to those with term == requiredTerm , by default None requiredJVal ( str | None , default: None ) \u2013 if not None, limits results to those with a == requiredJVal , by default None maxLevels ( int | None , default: None ) \u2013 the maximum number of levels (sorted by energy) to return, by default None units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 whether to return uncertain values, by default True Returns: dict \u2013 A dictionary of energy level strings to energy levels. Source code in mass2/calibration/hci_lines.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict getAvailableSpectralCharges ( element ) For a given element, returns a list of all available charge states from the ASD pickle file Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' Returns: list [ int ] \u2013 Available charge states Source code in mass2/calibration/hci_lines.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) getSingleLevel ( element , spectralCharge , conf , term , JVal , units = 'eV' , getUncertainty = True ) Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf ( str ) \u2013 nuclear configuration, e.g. '2p' term ( str ) \u2013 nuclear term, e.g. '2P*' JVal ( str ) \u2013 total angular momentum J, e.g. '3/2' units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 includes uncertainties in list of levels, by default True Returns: float \u2013 description Source code in mass2/calibration/hci_lines.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy add_H_like_lines_from_asd ( asd , element , maxLevels = None ) Add all known H-like lines for a given element from the ASD database Source code in mass2/calibration/hci_lines.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def add_H_like_lines_from_asd ( asd : NIST_ASD , element : str , maxLevels : int | None = None ) -> list [ SpectralLine ]: \"\"\"Add all known H-like lines for a given element from the ASD database\"\"\" spectr_ch = xraydb . atomic_number ( element ) added_lines = [] if maxLevels is not None : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch , maxLevels = maxLevels + 1 ) else : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch ) for iLevel in list ( levelsDict . keys ()): lineEnergy = levelsDict [ iLevel ][ 0 ] if lineEnergy != 0.0 : iLine = add_hci_line ( element = element , spectr_ch = spectr_ch , line_identifier = iLevel , energies = [ lineEnergy ], widths = [ 0.1 ], ratios = [ 1.0 ] ) added_lines . append ( iLine ) return added_lines add_He_like_lines_from_asd ( asd , element , maxLevels = None ) Add all known He-like lines for a given element from the ASD database Source code in mass2/calibration/hci_lines.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def add_He_like_lines_from_asd ( asd : NIST_ASD , element : str , maxLevels : int | None = None ) -> list [ SpectralLine ]: \"\"\"Add all known He-like lines for a given element from the ASD database\"\"\" spectr_ch = xraydb . atomic_number ( element ) - 1 added_lines = [] if maxLevels is not None : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch , maxLevels = maxLevels + 1 ) else : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch ) for iLevel in list ( levelsDict . keys ()): lineEnergy = levelsDict [ iLevel ][ 0 ] if lineEnergy != 0.0 : iLine = add_hci_line ( element = element , spectr_ch = spectr_ch , line_identifier = iLevel , energies = [ lineEnergy ], widths = [ 0.1 ], ratios = [ 1.0 ] ) added_lines . append ( iLine ) return added_lines add_hci_line ( element , spectr_ch , line_identifier , energies , widths , ratios , nominal_peak_energy = None ) Add a single HCI line to the fluorescence_lines database Parameters: element ( str ) \u2013 The element whose line is being added, e.g. 'Ne' spectr_ch ( int ) \u2013 The charge state of the ion whose line is being added, e.g. 9 for H-like Ne line_identifier ( str ) \u2013 The line identifier, e.g. '1s2S1/2 - 2p2P3/2' energies ( ArrayLike ) \u2013 The energies of the components of the line, in eV widths ( ArrayLike ) \u2013 The Lorentzian FWHM widths of the components of the line, in eV ratios ( ArrayLike ) \u2013 The relative intensities of the components of the line nominal_peak_energy ( float | None , default: None ) \u2013 The nominal spectral peak in eV, by default None Returns: SpectralLine \u2013 The newly added SpectralLine object Source code in mass2/calibration/hci_lines.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def add_hci_line ( element : str , spectr_ch : int , line_identifier : str , energies : ArrayLike , widths : ArrayLike , ratios : ArrayLike , nominal_peak_energy : float | None = None , ) -> SpectralLine : \"\"\"Add a single HCI line to the fluorescence_lines database Parameters ---------- element : str The element whose line is being added, e.g. 'Ne' spectr_ch : int The charge state of the ion whose line is being added, e.g. 9 for H-like Ne line_identifier : str The line identifier, e.g. '1s2S1/2 - 2p2P3/2' energies : ArrayLike The energies of the components of the line, in eV widths : ArrayLike The Lorentzian FWHM widths of the components of the line, in eV ratios : ArrayLike The relative intensities of the components of the line nominal_peak_energy : float | None, optional The nominal spectral peak in eV, by default None Returns ------- SpectralLine The newly added SpectralLine object \"\"\" energies = np . asarray ( energies ) widths = np . asarray ( widths ) ratios = np . asarray ( ratios ) if nominal_peak_energy is None : nominal_peak_energy = np . dot ( energies , ratios ) / np . sum ( ratios ) linetype = f \" { int ( spectr_ch ) } { line_identifier } \" spectrum_class = fluorescence_lines . addline ( element = element , material = \"Highly Charged Ion\" , linetype = linetype , reference_short = \"NIST ASD\" , reference_plot_instrument_gaussian_fwhm = 0.5 , nominal_peak_energy = nominal_peak_energy , energies = energies , lorentzian_fwhm = widths , reference_amplitude = ratios , reference_amplitude_type = AmplitudeType . LORENTZIAN_PEAK_HEIGHT , ka12_energy_diff = None , ) return spectrum_class hci_models.py Some useful methods for initializing GenericLineModel and CompositeMLEModel objects applied to HCI lines. June 2020 Paul Szypryt add_bg_model ( generic_model , vary_slope = False ) Adds a LinearBackgroundModel to a generic lmfit model Parameters: generic_model ( GenericLineModel ) \u2013 object to which to add a linear background model vary_slope ( bool , default: False ) \u2013 allows a varying linear slope rather than just constant value, by default False Returns: GenericLineModel \u2013 The input model, with background componets added Source code in mass2/calibration/hci_models.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def add_bg_model ( generic_model : GenericLineModel , vary_slope : bool = False ) -> GenericLineModel : \"\"\"Adds a LinearBackgroundModel to a generic lmfit model Parameters ---------- generic_model : GenericLineModel object to which to add a linear background model vary_slope : bool, optional allows a varying linear slope rather than just constant value, by default False Returns ------- GenericLineModel The input model, with background componets added \"\"\" # composite_name = generic_model._name # bg_prefix = f\"{composite_name}_\".replace(\" \", \"_\").replace(\"J=\", \"\").replace(\"/\", \"_\").replace(\"*\", \"\").replace(\".\", \"\") raise NotImplementedError ( \"No LinearBackgroundModel still exists in mass2\" ) initialize_HLike_2P_model ( element , conf , has_linear_background = False , has_tails = False , vary_amp_ratio = False ) Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' conf ( str ) \u2013 nuclear configuration as str, e.g. '2p' or '3p' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False vary_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def initialize_HLike_2P_model ( element : str , conf : str , has_linear_background : bool = False , has_tails : bool = False , vary_amp_ratio : bool = False ) -> GenericLineModel : \"\"\"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' conf : str nuclear configuration as str, e.g. '2p' or '3p' has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional include low energy tail in the model, by default False vary_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns ------- GenericLineModel The new composite line \"\"\" # Set up line names and lmfit prefixes charge = int ( xraydb . atomic_number ( element )) line_name_1_2 = f \" { element }{ charge } { conf } 2P* J=1/2\" line_name_3_2 = f \" { element }{ charge } { conf } 2P* J=3/2\" prefix_1_2 = f \" { line_name_1_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) prefix_3_2 = f \" { line_name_3_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) # Initialize individual lines and models line_1_2 = spectra [ line_name_1_2 ] line_3_2 = spectra [ line_name_3_2 ] model_1_2 = line_1_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_1_2 ) model_3_2 = line_3_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_3_2 ) # Initialize composite model and set addition H-like constraints composite_name = f \" { element }{ charge } { conf } \" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = [ model_1_2 , model_3_2 ], has_linear_background = has_linear_background , peak_component_name = line_name_3_2 , ) amp_ratio_param_name = f \" { element }{ charge } _ { conf } _amp_ratio\" composite_model . set_param_hint ( name = amp_ratio_param_name , value = 0.5 , min = 0.0 , vary = vary_amp_ratio ) composite_model . set_param_hint ( f \" { prefix_1_2 } integral\" , expr = f \" { prefix_3_2 } integral * { amp_ratio_param_name } \" ) return composite_model initialize_HeLike_complex_model ( element , has_linear_background = False , has_tails = False , additional_line_names = []) Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P J=1, and 1s.2p 1P J=1 lines. Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the Lorentzian models, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False additional_line_names ( list , default: [] ) \u2013 additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns: GenericLineModel \u2013 A model of the given HCI complex. Source code in mass2/calibration/hci_models.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def initialize_HeLike_complex_model ( element : str , has_linear_background : bool = False , has_tails : bool = False , additional_line_names : list = [] ) -> GenericLineModel : \"\"\"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P* J=1, and 1s.2p 1P* J=1 lines. Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background : bool, optional include a single linear background on top of the Lorentzian models, by default False has_tails : bool, optional include low energy tail in the model, by default False additional_line_names : list, optional additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns ------- GenericLineModel A model of the given HCI complex. \"\"\" # Set up line names charge = int ( xraydb . atomic_number ( element ) - 1 ) line_name_1s2s_3S = f \" { element }{ charge } 1s.2s 3S J=1\" line_name_1s2p_3P = f \" { element }{ charge } 1s.2p 3P* J=1\" line_name_1s2p_1P = f \" { element }{ charge } 1s.2p 1P* J=1\" line_names = np . hstack ([[ line_name_1s2s_3S , line_name_1s2p_3P , line_name_1s2p_1P ], additional_line_names ]) # Set up lines and models based on line_names # individual_lines = [spectra[i_line_name]() for i_line_name in line_names] individual_models = [ initialize_hci_line_model ( i_line_name , has_linear_background = False , has_tails = has_tails ) for i_line_name in line_names ] # Set up composite model composite_name = f \" { element }{ charge } 1s2s_2p Complex\" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = individual_models , has_linear_background = has_linear_background , peak_component_name = line_name_1s2p_1P , ) return composite_model initialize_hci_composite_model ( composite_name , individual_models , has_linear_background = False , peak_component_name = None ) Initializes composite lmfit model from the sum of input models Parameters: composite_name ( str ) \u2013 name given to composite line model individual_models ( list [ GenericLineModel ] ) \u2013 Models to sum into a composite has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of group of lorentzians, by default False peak_component_name ( str | None , default: None ) \u2013 designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def initialize_hci_composite_model ( composite_name : str , individual_models : list [ GenericLineModel ], has_linear_background : bool = False , peak_component_name : str | None = None , ) -> GenericLineModel : \"\"\"Initializes composite lmfit model from the sum of input models Parameters ---------- composite_name : str name given to composite line model individual_models : list[GenericLineModel] Models to sum into a composite has_linear_background : bool, optional include a single linear background on top of group of lorentzians, by default False peak_component_name : str | None, optional designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns ------- GenericLineModel The new composite line \"\"\" composite_model : GenericLineModel = np . sum ( individual_models ) composite_model . name = composite_name if has_linear_background : composite_model = add_bg_model ( composite_model ) # Workaround for energy calibration using composite models, pick 1st GenericLineModel component line_model_components = [ i_comp for i_comp in composite_model . components if isinstance ( i_comp , mass2 . calibration . line_models . GenericLineModel ) ] if peak_component_name is None : peak_component_name = line_model_components [ 0 ] . _name peak_component_index = [ i_comp . _name for i_comp in line_model_components ] . index ( peak_component_name ) peak_component = line_model_components [ peak_component_index ] composite_model . peak_prefix = peak_component . prefix composite_model . peak_energy = peak_component . spect . peak_energy # Set up some constraints relative to peak_component num_line_components = len ( line_model_components ) line_component_prefixes = [ iComp . prefix for iComp in line_model_components ] line_component_energies = [ iComp . spect . peak_energy for iComp in line_model_components ] for i in np . arange ( num_line_components ): if i != peak_component_index : # Single fwhm across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } fwhm\" , expr = f \" { composite_model . peak_prefix } fwhm\" ) # Single dph_de across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } dph_de\" , expr = f \" { composite_model . peak_prefix } dph_de\" ) # Fixed energy separation based on database values separation = line_component_energies [ i ] - composite_model . peak_energy hint = f \"( { separation } * { composite_model . peak_prefix } dph_de) + { composite_model . peak_prefix } peak_ph\" composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } peak_ph\" , expr = hint ) composite_model . shortname = composite_name return composite_model initialize_hci_line_model ( line_name , has_linear_background = False , has_tails = False ) Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters: line_name ( str ) \u2013 name of line to use in mass2.spectra has_linear_background ( bool , default: False ) \u2013 include linear background in the model, by default False has_tails ( bool , default: False ) \u2013 include low-energy tail in the model, by default False Returns: GenericLineModel \u2013 New HCI line. Source code in mass2/calibration/hci_models.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def initialize_hci_line_model ( line_name : str , has_linear_background : bool = False , has_tails : bool = False ) -> GenericLineModel : \"\"\"Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters ---------- line_name : str name of line to use in mass2.spectra has_linear_background : bool, optional include linear background in the model, by default False has_tails : bool, optional include low-energy tail in the model, by default False Returns ------- GenericLineModel New HCI line. \"\"\" line = spectra [ line_name ] prefix = f \" { line_name } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) line_model = line . model ( has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix ) line_model . shortname = line_name return line_model models ( has_linear_background = False , has_tails = False , vary_Hlike_amp_ratio = False , additional_Helike_complex_lines = []) Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters: has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines ( list , default: [] ) \u2013 additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns: _type_ \u2013 Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. Source code in mass2/calibration/hci_models.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def models ( has_linear_background : bool = False , has_tails : bool = False , vary_Hlike_amp_ratio : bool = False , additional_Helike_complex_lines : list = [], ) -> dict : \"\"\"Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters ---------- has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines : list, optional additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns ------- _type_ Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. \"\"\" models_dict = {} # Make some common H-like 2P* models conf_Hlike_2P_dict = {} conf_Hlike_2P_dict [ \"N\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"O\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ne\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ar\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] for i_element in list ( conf_Hlike_2P_dict . keys ()): for i_conf in conf_Hlike_2P_dict [ i_element ]: Hlike_model = initialize_HLike_2P_model ( i_element , i_conf , has_linear_background = has_linear_background , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio , ) models_dict [ Hlike_model . _name ] = Hlike_model # Make some common He-like 1s2s,2p complex and higher order 1p* models # He-like lines Helike_complex_elements = [ \"N\" , \"O\" , \"Ne\" , \"Ar\" ] for i_element in Helike_complex_elements : Helike_model = initialize_HeLike_complex_model ( i_element , has_linear_background = has_linear_background , has_tails = has_tails , additional_line_names = additional_Helike_complex_lines , ) models_dict [ Helike_model . _name ] = Helike_model # 1s.np 1P* lines for n>=3 conf_Helike_1P_dict = {} conf_Helike_1P_dict [ \"N\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"O\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ne\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ar\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] for i_element in list ( conf_Helike_1P_dict . keys ()): i_charge = int ( xraydb . atomic_number ( i_element ) - 1 ) for i_conf in conf_Helike_1P_dict [ i_element ]: Helike_line_name = f \" { i_element }{ i_charge } { i_conf } 1P* J=1\" Helike_model = initialize_hci_line_model ( Helike_line_name , has_linear_background = has_linear_background , has_tails = has_tails ) models_dict [ Helike_model . _name ] = Helike_model # Some more complicated cases # 500 eV region of H-/He-like N N6_1s3p_model = initialize_hci_line_model ( \"N6 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) N7_2p_model = initialize_HLike_2P_model ( \"N\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) N_500eV_model = initialize_hci_composite_model ( \"N 500eV Region\" , [ N6_1s3p_model , N7_2p_model ], has_linear_background = has_linear_background , peak_component_name = \"N7 2p 2P* J=3/2\" , ) models_dict [ N_500eV_model . _name ] = N_500eV_model # 660 eV region of H-/He-like O O8_2p_model = initialize_HLike_2P_model ( \"O\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) O7_1s3p_model = initialize_hci_line_model ( \"O7 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) O_660eV_model = initialize_hci_composite_model ( \"O 660eV Region\" , [ O8_2p_model , O7_1s3p_model ], has_linear_background = has_linear_background , peak_component_name = \"O8 2p 2P* J=3/2\" , ) models_dict [ O_660eV_model . _name ] = O_660eV_model return models_dict","title":"Highly-charge ion lines"},{"location":"hci_lines_from_asd/#highly-charged-ion-hci-lines-from-nist-asd","text":"","title":"Highly Charged Ion (HCI) Lines from NIST ASD"},{"location":"hci_lines_from_asd/#motivation","text":"We often find ourselves hard coding line center positions into mass, which is prone to errors and can be tedious when there are many lines of interest to insert. In addition, the line positions would need to be manually updated for any changes in established results. In the case of highly charged ions, such as those produced in an electron beam ion trap (EBIT), there is a vast number of potential lines coming from almost any charge state of almost any element. Luckily, these lines are well documented through the NIST Atomic Spectral Database (ASD). Here, we have parsed a NIST ASD SQL dump and converted it into an easily Python readable pickle file. The hci_lines.py module implements the NIST_ASD class, which loads that pickle file and contains useful functions for working with the ASD data. It also automatically adds in some of the more common HCI lines that we commonly use in our EBIT data analyses.","title":"Motivation"},{"location":"hci_lines_from_asd/#exploring-the-methods-of-class-nist_asd","text":"The class NIST_ASD can be initialized without arguments if the user wants to use the default ASD pickle file. This file is located at mass/calibration/nist_asd.pickle. A custom pickle file can be used by passing in the pickleFilename argument during initialization. The methods of the NIST_ASD class are described below:","title":"Exploring the methods of class NIST_ASD"},{"location":"hci_lines_from_asd/#usage-examples","text":"Next, we will demonstrate usage of these methods with the example of Ne, a commonly injected gas at the NIST EBIT. # mkdocs: render import mass2 import mass2.calibration.hci_lines import numpy as numpy import pylab as plt test_asd = mass2.calibration.hci_lines.NIST_ASD() availableElements = test_asd.getAvailableElements() assert 'Ne' in availableElements availableNeCharges = test_asd.getAvailableSpectralCharges(element='Ne') assert 10 in availableNeCharges subsetNe10Levels = test_asd.getAvailableLevels(element='Ne', spectralCharge=10, maxLevels=6, getUncertainty=False) assert '2p 2P* J=1/2' in list(subsetNe10Levels.keys()) exampleNeLevel = test_asd.getSingleLevel(element='Ne', spectralCharge=10, conf='2p', term='2P*', JVal='1/2', getUncertainty=False) print(availableElements[:10]) print(availableNeCharges) for k, v in subsetNe10Levels.items(): subsetNe10Levels[k] = round(v, 1) print(subsetNe10Levels) print(f'{exampleNeLevel:.1f}') [np.str_('Sn'), np.str_('Cu'), np.str_('Na'), np.str_('As'), np.str_('Zn'), np.str_('Ne'), np.str_('Ge'), np.str_('Ga'), np.str_('Rb'), np.str_('Se')] [9, 1, 2, 3, 4, 5, 6, 7, 8, 10] {'1s 2S J=1/2': 0.0, '2p 2P* J=1/2': 1021.5, '2s 2S J=1/2': 1021.5, '2p 2P* J=3/2': 1022.0, '3p 2P* J=1/2': 1210.8, '3s 2S J=1/2': 1210.8} 1021.5","title":"Usage examples"},{"location":"hci_lines_from_asd/#functions-for-generating-spectralline-objects-from-asd-data","text":"The module also contains some functions outside of the NIST_ASD class that are useful for integration with MASS. First, the add_hci_line function which, takes arguments that are relevant in HCI work, including as element , spectr_ch , energies , widths , and ratios . The function calls mass2.calibration.fluorescence_lines.addline , generates a line name with the given parameters, and populates the various fields. As an example, let us create a H-like Be line. Here, we assume a lorentzian width of 0.1 eV. # mkdocs: render test_element = 'Be' test_charge = 4 test_conf = '2p' test_term = '2P*' test_JVal = '3/2' test_level = f'{test_conf} {test_term} J={test_JVal}' test_energy = test_asd.getSingleLevel( element=test_element, spectralCharge=test_charge, conf=test_conf, term=test_term, JVal=test_JVal, getUncertainty=False) test_line = mass2.calibration.hci_lines.add_hci_line(element=test_element, spectr_ch=test_charge, line_identifier=test_level, energies=[test_energy], widths=[0.1], ratios=[1.0]) assert test_line.peak_energy == test_energy print(mass2.spectra[f'{test_element}{test_charge} {test_conf} {test_term} J={test_JVal}']) print(f'{test_line.peak_energy:.1f}') SpectralLine: Be4 2p 2P* J=3/2 163.3 The name format for grabbing the line from mass2.spectra is shown above. The transition is uniquely specified by the element, charge, configuration, term, and J value. Below, we show what this line looks like assuming a zero-width Gaussian component. # mkdocs: render test_line.plot() The module contains two other functions which are used to easily generate some lines from levels that are commonly observed at the NIST EBIT. These functions are add_H_like_lines_from_asd and add_He_like_lines_from_asd . As the names imply, these functions add H- and He-like lines to mass using the data in the ASD pickle. These functions require the asd and element arguments and also contain the optional maxLevels argument, which works similarly as the argument in the class methods. The module also automatically adds H- and He-like lines for the most commonly used elements, which includes 'N', 'O', 'Ne', and 'Ar'. Below, we check that common elements are being added as spectralLine objects and then add some of the lower order H- and He-like Ga lines. # mkdocs: render print([mass2.spectra['O7 1s.2p 1P* J=1'], round(mass2.spectra['O7 1s.2p 1P* J=1'].peak_energy,1)]) test_element = 'Ga' HLikeGaLines = mass2.calibration.hci_lines.add_H_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=6) HeLikeGaLines = mass2.calibration.hci_lines.add_He_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=7) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HLikeGaLines]) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HeLikeGaLines]) [SpectralLine: Ne10 2p 2P* J=3/2, np.float64(1022.0)] [SpectralLine: O7 1s.2p 1P* J=1, np.float64(574.0)] [[SpectralLine: Ga31 2p 2P* J=1/2, np.float64(9917.0)], [SpectralLine: Ga31 2s 2S J=1/2, np.float64(9918.0)], [SpectralLine: Ga31 2p 2P* J=3/2, np.float64(9960.3)], [SpectralLine: Ga31 3p 2P* J=1/2, np.float64(11767.7)], [SpectralLine: Ga31 3s 2S J=1/2, np.float64(11768.0)], [SpectralLine: Ga31 3d 2D J=3/2, np.float64(11780.5)]] [[SpectralLine: Ga30 1s.2s 3S J=1, np.float64(9535.6)], [SpectralLine: Ga30 1s.2p 3P* J=0, np.float64(9571.8)], [SpectralLine: Ga30 1s.2p 3P* J=1, np.float64(9574.4)], [SpectralLine: Ga30 1s.2s 1S J=0, np.float64(9574.6)], [SpectralLine: Ga30 1s.2p 3P* J=2, np.float64(9607.4)], [SpectralLine: Ga30 1s.2p 1P* J=1, np.float64(9628.2)], [SpectralLine: Ga30 1s.3s 3S J=1, np.float64(11304.6)]]","title":"Functions for generating SpectralLine objects from ASD data"},{"location":"hci_lines_from_asd/#hci-lines-and-models-docstring-info","text":"hci_lines.py Uses pickle file containing NIST ASD levels data to generate some commonly used HCI lines in mass. Meant to be a replacement for _highly_charged_ion_lines.py, which hard codes in line parameters. The pickle file can be gzip-compressed, provided the compressed filename ends with \".gz\". February 2020 Paul Szypryt","title":"HCI lines and models docstring info"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD","text":"Class for working with a pickled atomic spectra database Source code in mass2/calibration/hci_lines.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class NIST_ASD : \"\"\"Class for working with a pickled atomic spectra database\"\"\" def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy","title":"NIST_ASD"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.__init__","text":"Loads ASD pickle file (optionally gzipped) Parameters: pickleFilename ( str | None , default: None ) \u2013 ASD pickle file name, as str, or if none then mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH (default None) Source code in mass2/calibration/hci_lines.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle )","title":"__init__"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getAvailableElements","text":"Returns a list of all available elements from the ASD pickle file Source code in mass2/calibration/hci_lines.py 52 53 54 55 def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ())","title":"getAvailableElements"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getAvailableLevels","text":"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters: element ( str ) \u2013 Elemental atomic symbol, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf ( str | None , default: None ) \u2013 if not None, limits results to those with conf == requiredConf , by default None requiredTerm ( str | None , default: None ) \u2013 if not None, limits results to those with term == requiredTerm , by default None requiredJVal ( str | None , default: None ) \u2013 if not None, limits results to those with a == requiredJVal , by default None maxLevels ( int | None , default: None ) \u2013 the maximum number of levels (sorted by energy) to return, by default None units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 whether to return uncertain values, by default True Returns: dict \u2013 A dictionary of energy level strings to energy levels. Source code in mass2/calibration/hci_lines.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict","title":"getAvailableLevels"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getAvailableSpectralCharges","text":"For a given element, returns a list of all available charge states from the ASD pickle file Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' Returns: list [ int ] \u2013 Available charge states Source code in mass2/calibration/hci_lines.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ())","title":"getAvailableSpectralCharges"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getSingleLevel","text":"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf ( str ) \u2013 nuclear configuration, e.g. '2p' term ( str ) \u2013 nuclear term, e.g. '2P*' JVal ( str ) \u2013 total angular momentum J, e.g. '3/2' units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 includes uncertainties in list of levels, by default True Returns: float \u2013 description Source code in mass2/calibration/hci_lines.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy","title":"getSingleLevel"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.add_H_like_lines_from_asd","text":"Add all known H-like lines for a given element from the ASD database Source code in mass2/calibration/hci_lines.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def add_H_like_lines_from_asd ( asd : NIST_ASD , element : str , maxLevels : int | None = None ) -> list [ SpectralLine ]: \"\"\"Add all known H-like lines for a given element from the ASD database\"\"\" spectr_ch = xraydb . atomic_number ( element ) added_lines = [] if maxLevels is not None : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch , maxLevels = maxLevels + 1 ) else : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch ) for iLevel in list ( levelsDict . keys ()): lineEnergy = levelsDict [ iLevel ][ 0 ] if lineEnergy != 0.0 : iLine = add_hci_line ( element = element , spectr_ch = spectr_ch , line_identifier = iLevel , energies = [ lineEnergy ], widths = [ 0.1 ], ratios = [ 1.0 ] ) added_lines . append ( iLine ) return added_lines","title":"add_H_like_lines_from_asd"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.add_He_like_lines_from_asd","text":"Add all known He-like lines for a given element from the ASD database Source code in mass2/calibration/hci_lines.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def add_He_like_lines_from_asd ( asd : NIST_ASD , element : str , maxLevels : int | None = None ) -> list [ SpectralLine ]: \"\"\"Add all known He-like lines for a given element from the ASD database\"\"\" spectr_ch = xraydb . atomic_number ( element ) - 1 added_lines = [] if maxLevels is not None : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch , maxLevels = maxLevels + 1 ) else : levelsDict = asd . getAvailableLevels ( element , spectralCharge = spectr_ch ) for iLevel in list ( levelsDict . keys ()): lineEnergy = levelsDict [ iLevel ][ 0 ] if lineEnergy != 0.0 : iLine = add_hci_line ( element = element , spectr_ch = spectr_ch , line_identifier = iLevel , energies = [ lineEnergy ], widths = [ 0.1 ], ratios = [ 1.0 ] ) added_lines . append ( iLine ) return added_lines","title":"add_He_like_lines_from_asd"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.add_hci_line","text":"Add a single HCI line to the fluorescence_lines database Parameters: element ( str ) \u2013 The element whose line is being added, e.g. 'Ne' spectr_ch ( int ) \u2013 The charge state of the ion whose line is being added, e.g. 9 for H-like Ne line_identifier ( str ) \u2013 The line identifier, e.g. '1s2S1/2 - 2p2P3/2' energies ( ArrayLike ) \u2013 The energies of the components of the line, in eV widths ( ArrayLike ) \u2013 The Lorentzian FWHM widths of the components of the line, in eV ratios ( ArrayLike ) \u2013 The relative intensities of the components of the line nominal_peak_energy ( float | None , default: None ) \u2013 The nominal spectral peak in eV, by default None Returns: SpectralLine \u2013 The newly added SpectralLine object Source code in mass2/calibration/hci_lines.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def add_hci_line ( element : str , spectr_ch : int , line_identifier : str , energies : ArrayLike , widths : ArrayLike , ratios : ArrayLike , nominal_peak_energy : float | None = None , ) -> SpectralLine : \"\"\"Add a single HCI line to the fluorescence_lines database Parameters ---------- element : str The element whose line is being added, e.g. 'Ne' spectr_ch : int The charge state of the ion whose line is being added, e.g. 9 for H-like Ne line_identifier : str The line identifier, e.g. '1s2S1/2 - 2p2P3/2' energies : ArrayLike The energies of the components of the line, in eV widths : ArrayLike The Lorentzian FWHM widths of the components of the line, in eV ratios : ArrayLike The relative intensities of the components of the line nominal_peak_energy : float | None, optional The nominal spectral peak in eV, by default None Returns ------- SpectralLine The newly added SpectralLine object \"\"\" energies = np . asarray ( energies ) widths = np . asarray ( widths ) ratios = np . asarray ( ratios ) if nominal_peak_energy is None : nominal_peak_energy = np . dot ( energies , ratios ) / np . sum ( ratios ) linetype = f \" { int ( spectr_ch ) } { line_identifier } \" spectrum_class = fluorescence_lines . addline ( element = element , material = \"Highly Charged Ion\" , linetype = linetype , reference_short = \"NIST ASD\" , reference_plot_instrument_gaussian_fwhm = 0.5 , nominal_peak_energy = nominal_peak_energy , energies = energies , lorentzian_fwhm = widths , reference_amplitude = ratios , reference_amplitude_type = AmplitudeType . LORENTZIAN_PEAK_HEIGHT , ka12_energy_diff = None , ) return spectrum_class hci_models.py Some useful methods for initializing GenericLineModel and CompositeMLEModel objects applied to HCI lines. June 2020 Paul Szypryt","title":"add_hci_line"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.add_bg_model","text":"Adds a LinearBackgroundModel to a generic lmfit model Parameters: generic_model ( GenericLineModel ) \u2013 object to which to add a linear background model vary_slope ( bool , default: False ) \u2013 allows a varying linear slope rather than just constant value, by default False Returns: GenericLineModel \u2013 The input model, with background componets added Source code in mass2/calibration/hci_models.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def add_bg_model ( generic_model : GenericLineModel , vary_slope : bool = False ) -> GenericLineModel : \"\"\"Adds a LinearBackgroundModel to a generic lmfit model Parameters ---------- generic_model : GenericLineModel object to which to add a linear background model vary_slope : bool, optional allows a varying linear slope rather than just constant value, by default False Returns ------- GenericLineModel The input model, with background componets added \"\"\" # composite_name = generic_model._name # bg_prefix = f\"{composite_name}_\".replace(\" \", \"_\").replace(\"J=\", \"\").replace(\"/\", \"_\").replace(\"*\", \"\").replace(\".\", \"\") raise NotImplementedError ( \"No LinearBackgroundModel still exists in mass2\" )","title":"add_bg_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_HLike_2P_model","text":"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' conf ( str ) \u2013 nuclear configuration as str, e.g. '2p' or '3p' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False vary_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def initialize_HLike_2P_model ( element : str , conf : str , has_linear_background : bool = False , has_tails : bool = False , vary_amp_ratio : bool = False ) -> GenericLineModel : \"\"\"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' conf : str nuclear configuration as str, e.g. '2p' or '3p' has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional include low energy tail in the model, by default False vary_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns ------- GenericLineModel The new composite line \"\"\" # Set up line names and lmfit prefixes charge = int ( xraydb . atomic_number ( element )) line_name_1_2 = f \" { element }{ charge } { conf } 2P* J=1/2\" line_name_3_2 = f \" { element }{ charge } { conf } 2P* J=3/2\" prefix_1_2 = f \" { line_name_1_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) prefix_3_2 = f \" { line_name_3_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) # Initialize individual lines and models line_1_2 = spectra [ line_name_1_2 ] line_3_2 = spectra [ line_name_3_2 ] model_1_2 = line_1_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_1_2 ) model_3_2 = line_3_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_3_2 ) # Initialize composite model and set addition H-like constraints composite_name = f \" { element }{ charge } { conf } \" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = [ model_1_2 , model_3_2 ], has_linear_background = has_linear_background , peak_component_name = line_name_3_2 , ) amp_ratio_param_name = f \" { element }{ charge } _ { conf } _amp_ratio\" composite_model . set_param_hint ( name = amp_ratio_param_name , value = 0.5 , min = 0.0 , vary = vary_amp_ratio ) composite_model . set_param_hint ( f \" { prefix_1_2 } integral\" , expr = f \" { prefix_3_2 } integral * { amp_ratio_param_name } \" ) return composite_model","title":"initialize_HLike_2P_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_HeLike_complex_model","text":"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P J=1, and 1s.2p 1P J=1 lines. Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the Lorentzian models, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False additional_line_names ( list , default: [] ) \u2013 additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns: GenericLineModel \u2013 A model of the given HCI complex. Source code in mass2/calibration/hci_models.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def initialize_HeLike_complex_model ( element : str , has_linear_background : bool = False , has_tails : bool = False , additional_line_names : list = [] ) -> GenericLineModel : \"\"\"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P* J=1, and 1s.2p 1P* J=1 lines. Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background : bool, optional include a single linear background on top of the Lorentzian models, by default False has_tails : bool, optional include low energy tail in the model, by default False additional_line_names : list, optional additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns ------- GenericLineModel A model of the given HCI complex. \"\"\" # Set up line names charge = int ( xraydb . atomic_number ( element ) - 1 ) line_name_1s2s_3S = f \" { element }{ charge } 1s.2s 3S J=1\" line_name_1s2p_3P = f \" { element }{ charge } 1s.2p 3P* J=1\" line_name_1s2p_1P = f \" { element }{ charge } 1s.2p 1P* J=1\" line_names = np . hstack ([[ line_name_1s2s_3S , line_name_1s2p_3P , line_name_1s2p_1P ], additional_line_names ]) # Set up lines and models based on line_names # individual_lines = [spectra[i_line_name]() for i_line_name in line_names] individual_models = [ initialize_hci_line_model ( i_line_name , has_linear_background = False , has_tails = has_tails ) for i_line_name in line_names ] # Set up composite model composite_name = f \" { element }{ charge } 1s2s_2p Complex\" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = individual_models , has_linear_background = has_linear_background , peak_component_name = line_name_1s2p_1P , ) return composite_model","title":"initialize_HeLike_complex_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_hci_composite_model","text":"Initializes composite lmfit model from the sum of input models Parameters: composite_name ( str ) \u2013 name given to composite line model individual_models ( list [ GenericLineModel ] ) \u2013 Models to sum into a composite has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of group of lorentzians, by default False peak_component_name ( str | None , default: None ) \u2013 designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def initialize_hci_composite_model ( composite_name : str , individual_models : list [ GenericLineModel ], has_linear_background : bool = False , peak_component_name : str | None = None , ) -> GenericLineModel : \"\"\"Initializes composite lmfit model from the sum of input models Parameters ---------- composite_name : str name given to composite line model individual_models : list[GenericLineModel] Models to sum into a composite has_linear_background : bool, optional include a single linear background on top of group of lorentzians, by default False peak_component_name : str | None, optional designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns ------- GenericLineModel The new composite line \"\"\" composite_model : GenericLineModel = np . sum ( individual_models ) composite_model . name = composite_name if has_linear_background : composite_model = add_bg_model ( composite_model ) # Workaround for energy calibration using composite models, pick 1st GenericLineModel component line_model_components = [ i_comp for i_comp in composite_model . components if isinstance ( i_comp , mass2 . calibration . line_models . GenericLineModel ) ] if peak_component_name is None : peak_component_name = line_model_components [ 0 ] . _name peak_component_index = [ i_comp . _name for i_comp in line_model_components ] . index ( peak_component_name ) peak_component = line_model_components [ peak_component_index ] composite_model . peak_prefix = peak_component . prefix composite_model . peak_energy = peak_component . spect . peak_energy # Set up some constraints relative to peak_component num_line_components = len ( line_model_components ) line_component_prefixes = [ iComp . prefix for iComp in line_model_components ] line_component_energies = [ iComp . spect . peak_energy for iComp in line_model_components ] for i in np . arange ( num_line_components ): if i != peak_component_index : # Single fwhm across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } fwhm\" , expr = f \" { composite_model . peak_prefix } fwhm\" ) # Single dph_de across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } dph_de\" , expr = f \" { composite_model . peak_prefix } dph_de\" ) # Fixed energy separation based on database values separation = line_component_energies [ i ] - composite_model . peak_energy hint = f \"( { separation } * { composite_model . peak_prefix } dph_de) + { composite_model . peak_prefix } peak_ph\" composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } peak_ph\" , expr = hint ) composite_model . shortname = composite_name return composite_model","title":"initialize_hci_composite_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_hci_line_model","text":"Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters: line_name ( str ) \u2013 name of line to use in mass2.spectra has_linear_background ( bool , default: False ) \u2013 include linear background in the model, by default False has_tails ( bool , default: False ) \u2013 include low-energy tail in the model, by default False Returns: GenericLineModel \u2013 New HCI line. Source code in mass2/calibration/hci_models.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def initialize_hci_line_model ( line_name : str , has_linear_background : bool = False , has_tails : bool = False ) -> GenericLineModel : \"\"\"Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters ---------- line_name : str name of line to use in mass2.spectra has_linear_background : bool, optional include linear background in the model, by default False has_tails : bool, optional include low-energy tail in the model, by default False Returns ------- GenericLineModel New HCI line. \"\"\" line = spectra [ line_name ] prefix = f \" { line_name } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) line_model = line . model ( has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix ) line_model . shortname = line_name return line_model","title":"initialize_hci_line_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.models","text":"Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters: has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines ( list , default: [] ) \u2013 additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns: _type_ \u2013 Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. Source code in mass2/calibration/hci_models.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def models ( has_linear_background : bool = False , has_tails : bool = False , vary_Hlike_amp_ratio : bool = False , additional_Helike_complex_lines : list = [], ) -> dict : \"\"\"Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters ---------- has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines : list, optional additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns ------- _type_ Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. \"\"\" models_dict = {} # Make some common H-like 2P* models conf_Hlike_2P_dict = {} conf_Hlike_2P_dict [ \"N\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"O\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ne\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ar\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] for i_element in list ( conf_Hlike_2P_dict . keys ()): for i_conf in conf_Hlike_2P_dict [ i_element ]: Hlike_model = initialize_HLike_2P_model ( i_element , i_conf , has_linear_background = has_linear_background , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio , ) models_dict [ Hlike_model . _name ] = Hlike_model # Make some common He-like 1s2s,2p complex and higher order 1p* models # He-like lines Helike_complex_elements = [ \"N\" , \"O\" , \"Ne\" , \"Ar\" ] for i_element in Helike_complex_elements : Helike_model = initialize_HeLike_complex_model ( i_element , has_linear_background = has_linear_background , has_tails = has_tails , additional_line_names = additional_Helike_complex_lines , ) models_dict [ Helike_model . _name ] = Helike_model # 1s.np 1P* lines for n>=3 conf_Helike_1P_dict = {} conf_Helike_1P_dict [ \"N\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"O\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ne\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ar\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] for i_element in list ( conf_Helike_1P_dict . keys ()): i_charge = int ( xraydb . atomic_number ( i_element ) - 1 ) for i_conf in conf_Helike_1P_dict [ i_element ]: Helike_line_name = f \" { i_element }{ i_charge } { i_conf } 1P* J=1\" Helike_model = initialize_hci_line_model ( Helike_line_name , has_linear_background = has_linear_background , has_tails = has_tails ) models_dict [ Helike_model . _name ] = Helike_model # Some more complicated cases # 500 eV region of H-/He-like N N6_1s3p_model = initialize_hci_line_model ( \"N6 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) N7_2p_model = initialize_HLike_2P_model ( \"N\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) N_500eV_model = initialize_hci_composite_model ( \"N 500eV Region\" , [ N6_1s3p_model , N7_2p_model ], has_linear_background = has_linear_background , peak_component_name = \"N7 2p 2P* J=3/2\" , ) models_dict [ N_500eV_model . _name ] = N_500eV_model # 660 eV region of H-/He-like O O8_2p_model = initialize_HLike_2P_model ( \"O\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) O7_1s3p_model = initialize_hci_line_model ( \"O7 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) O_660eV_model = initialize_hci_composite_model ( \"O 660eV Region\" , [ O8_2p_model , O7_1s3p_model ], has_linear_background = has_linear_background , peak_component_name = \"O8 2p 2P* J=3/2\" , ) models_dict [ O_660eV_model . _name ] = O_660eV_model return models_dict","title":"models"},{"location":"line_fitting/","text":"Fitting spectral line models to data in MASS Joe Fowler, January 2020. Mass uses use the LMFIT package by wrapping it in the Mass2 class GenericLineModel and its subclasses. LMFit vs Scipy LMFIT has numerous advantages over the basic scipy.optimize module. Quoting from the LMFIT documentation, the user can: forget about the order of variables and refer to Parameters by meaningful names. place bounds on Parameters as attributes, without worrying about preserving the order of arrays for variables and boundaries. fix Parameters, without having to rewrite the objective function. place algebraic constraints on Parameters. The one disadvantage of the core LMFIT package for our purposes is that it minimizes the sum of squares of a vector instead of maximizing the Poisson likelihood. This is easily remedied, however, by replacing the usual computation of residuals with one that computes the square root of the Poisson likelihood contribution from each bin. Voil\u00e1! A maximum likelihood fitter for histograms. Advantages of LMFIT over our earlier, homemade approach to fitting line shapes also include: The interface for setting upper/lower bounds on parameters and for varying or fixing them is much more elegant, memorable, and simple than our homemade version. It ultimately wraps the scipy.optimize package and therefore inherits all of its advantages: a choice of over a dozen optimizers with highly technical documentation, some optimizers that aim for true global (not just local) optimization, and the countless expert-years that have been invested in perfecting it. LMFIT automatically computes numerous statistics of each fit including estimated uncertainties, correlations, and multiple quality-of-fit statistics (information criteria as well as chi-square) and offers user-friendly fit reports. See the MinimizerResult object. It's the work of Matt Newville, an x-ray scientist responsible for the excellent ifeffit and its successor Larch . Above all, its documentation is complete, already written, and maintained by not-us. Usage guide This overview is hardly complete, but we hope it can be a quick-start guide and also hint at how you can convert your own analysis work from the old to the new, preferred fitting methods. The underlying spectral line shape models Objects of the type SpectralLine encode the line shape of a fluorescence line, as a sum of Voigt or Lorentzian distributions. Because they inherit from scipy.stats.rv_continuous , they allow computation of cumulative distribution functions and the simulation of data drawn from the distribution. An example of the creation and usage is: # mkdocs: render import numpy as np import pylab as plt import mass2 import mass2.materials # Known lines are accessed by: line = mass2.spectra[\"MnKAlpha\"] rng = np.random.default_rng(1066) N = 100000 energies = line.rvs(size=N, instrument_gaussian_fwhm=2.2, rng=rng) # draw from the distribution plt.clf() sim, bin_edges, _ = plt.hist(energies, 120, range=[5865, 5925], histtype=\"step\", lw=2); binsize = bin_edges[1] - bin_edges[0] e = bin_edges[:-1] + 0.5*binsize plt.plot(e, line(e, instrument_gaussian_fwhm=2.2)*N*binsize, \"k\", lw=0.5) plt.xlabel(\"Energy (eV)\") plt.title(\"Mn K$\\\\alpha$ random deviates and theory curve\") The SpectralLine object is useful to you if you need to generate simulated data, or to plot a line shape, as shown above. The objects that perform line fitting use the SpectralLine object to hold line shape information. You don't need to create a SpectralLine object for fitting, though; it will be done automatically. How to use the LMFIT-based models for fitting The simplest case of line fitting requires only 3 steps: create a model instance from a SpectralLine , guess its parameters from the data, and perform a fit with this guess. Plotting is not done as part of the fit--you have to do that separately. # mkdocs: render model = line.model() params = model.guess(sim, bin_centers=e, dph_de=1) resultA = model.fit(sim, params, bin_centers=e) # Fit again but with dPH/dE held at 1. # dPH/dE will be a free parameter for the fit by default, largely due to the history of MnKAlpha fits being so critical during development. # This will not work for nearly monochromatic lines, however, as the resolution (fwhm) and scale (dph_de) are exactly degenerate. # In practice, most fits are done with dph_de fixed. params = resultA.params.copy() resultB = model.fit(sim, params, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) # There are two plotting methods. The first is an LMfit built-in; the other (\"mass-style\") puts the # fit parameters on the plot. resultB.plot() resultB.plotm() # The best-fit params are found in resultB.params # and a dictionary of their values is resultB.best_values. # The parameters given as an argument to fit are unchanged. You can print a nicely formatted fit report with the method fit_report() : print(resultB.fit_report()) [[Model]] GenericLineModel(MnKAlpha) [[Fit Statistics]] # fitting method = least_squares # function evals = 15 # data points = 120 # variables = 4 chi-square = 100.565947 reduced chi-square = 0.86694782 Akaike info crit = -13.2013653 Bayesian info crit = -2.05139830 R-squared = 0.99999953 [[Variables]] fwhm: 2.21558094 +/- 0.02687437 (1.21%) (init = 2.217155) peak_ph: 5898.79525 +/- 0.00789761 (0.00%) (init = 5898.794) dph_de: 1 (fixed) integral: 99986.5425 +/- 314.455266 (0.31%) (init = 99985.8) background: 5.0098e-16 +/- 0.80578112 (160842446370819488.00%) (init = 2.791565e-09) bg_slope: 0 (fixed) [[Correlations]] (unreported correlations are < 0.100) C(integral, background) = -0.3147 C(fwhm, peak_ph) = -0.1121 Fitting with exponential tails (to low or high energy) Notice when you report the fit (or check the contents of the params or resultB.params objects), there are no parameters referring to exponential tails of a Bortels response. That's because the default fitter assumes a Gaussian response. If you want tails, that's a constructor argument: # mkdocs: render model = line.model(has_tails=True) params = model.guess(sim, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) resultC = model.fit(sim, params, bin_centers=e) resultC.plot() # print(resultC.fit_report()) By default, the has_tails=True will set up a non-zero low-energy tail and allow it to vary, while the high-energy tail is set to zero amplitude and doesn't vary. Use these numbered examples if you want to fit for a high-energy tail (1), to fix the low-E tail at some non-zero level (2) or to turn off the low-E tail completely (3): # 1. To let the low-E and high-E tail both vary simultaneously params[\"tail_share_hi\"].set(.1, vary=True) params[\"tail_tau_hi\"].set(30, vary=True) # 2. To fix the sum of low-E and high-E tail at a 10% level, with low-E tau=30 eV, but # the share of the low vs high tail can vary params[\"tail_frac\"].set(.1, vary=False) params[\"tail_tau\"].set(30, vary=False) # 3. To turn off low-E tail params[\"tail_frac\"].set(.1, vary=True) params[\"tail_share_hi\"].set(1, vary=False) params[\"tail_tau\"].set(vary=False) Fitting with a quantum efficiency model If you want to multiply the line models by a model of the quantum efficiency, you can do that. You need a qemodel function or callable function object that takes an energy (scalar or vector) and returns the corresponding efficiency. For example, you can use the \"Raven1 2019\" QE model from mass2.materials . The filter-stack models are not terribly fast to run, so it's best to compute once, spline the results, and pass that spline as the qemodel to line.model(qemodel=qemodel) . # mkdocs: render raven_filters = mass2.materials.efficiency_models.filterstack_models[\"RAVEN1 2019\"] eknots = np.linspace(100, 20000, 1991) qevalues = raven_filters(eknots) qemodel = mass2.mathstat.interpolate.CubicSpline(eknots, qevalues) model = line.model(qemodel=qemodel) resultD = model.fit(sim, resultB.params, bin_centers=e) resultD.plotm() # print(resultD.fit_report()) fit_counts = resultD.params[\"integral\"].value localqe = qemodel(mass2.STANDARD_FEATURES[\"MnKAlpha\"]) fit_observed = fit_counts*localqe fit_err = resultD.params[\"integral\"].stderr count_err = fit_err*localqe print(\"Fit finds {:.0f}\u00b1{:.0f} counts before QE, or {:.0f}\u00b1{:.0f} observed. True value {:d}.\".format( round(fit_counts, -1), round(fit_err, -1), round(fit_observed, -1), round(count_err, -1), N)) Fit finds 168810\u00b1530 counts before QE, or 100020\u00b1320 observed. True value 100000. When you fit with a non-trivial QE model, all fit parameters that refer to intensities of signal or background refer to a sensor with an ideal QE=1. These parameters include: integral background bg_slope That is, the fit values must be multiplied by the local QE to give the number of observed signal counts, background counts per bin, or background slope. With or without a QE model, \"integral\" refers to the number of photons that would be seen across all energies (not just in the range being fit). Fitting a simple Gaussian, Lorentzian, or Voigt function # mkdocs: render import dataclasses from mass2.calibration.fluorescence_lines import SpectralLine e_ctr = 1000.0 Nsig = 10000 Nbg = 1000 sigma = 1.0 x_gauss = rng.standard_normal(Nsig)*sigma + e_ctr hwhm = 1.0 x_lorentz = rng.standard_cauchy(Nsig)*hwhm + e_ctr x_voigt = rng.standard_cauchy(Nsig)*hwhm + rng.standard_normal(Nsig)*sigma + e_ctr bg = rng.uniform(e_ctr-5, e_ctr+5, size=Nbg) # Gaussian fit c, b = np.histogram(np.hstack([x_gauss, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, 0, 0) line = dataclasses.replace(line, linetype=\"Gaussian\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultG = model.fit(c, params, bin_centers=bin_ctr) resultG.plotm() # print(resultG.fit_report()) # mkdocs: render # Lorentzian fit c, b = np.histogram(np.hstack([x_lorentz, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, 0) line = dataclasses.replace(line, linetype=\"Lorentzian\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultL = model.fit(c, params, bin_centers=bin_ctr) resultL.plotm() # print(resultL.fit_report()) # mkdocs: render # Voigt fit c, b = np.histogram(np.hstack([x_voigt, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, sigma) line = dataclasses.replace(line, linetype=\"Voigt\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultV = model.fit(c, params, bin_centers=bin_ctr) resultV.plotm() # print(resultV.fit_report()) More details of fitting fluorescence-line models Use p=model.guess(data, bin_centers=e, dph_de=dph_de) to create a heuristic for the starting parameters. Change starting values and toggle the vary attribute on parameters, as needed. For example: p[\"dph_de\"].set(1.0, vary=False) Use result=model.fit(data, p, bin_centers=e) to perform the fit and store the result. The result holds many attributes and methods (see MinimizerResult for full documentation). These include: result.params = the model's best-fit parameters object result.best_values = a dictionary of the best-fit parameter values result.best_fit = the model's y-values at the best-fit parameter values result.chisqr = the chi-squared statistic of the fit (here, -2log(L)) result.covar = the computed covariance result.fit_report() = return a pretty-printed string reporting on the fit result.plot_fit() = make a plot of the data and fit result.plot_residuals() = make a plot of the residuals (fit-data) result.plotm() = make a plot of the data, fit, and fit params with dataset filename in title result.plot() = make a plot of the data, fit, and residuals, generally plotm is preferred The tau values (scale lengths of exponential tails) are in eV units. The parameter \"integral\" refers to the integrated number of counts across all energies (whether inside or beyond the fitted energy range).","title":"Line fitting"},{"location":"line_fitting/#fitting-spectral-line-models-to-data-in-mass","text":"Joe Fowler, January 2020. Mass uses use the LMFIT package by wrapping it in the Mass2 class GenericLineModel and its subclasses.","title":"Fitting spectral line models to data in MASS"},{"location":"line_fitting/#lmfit-vs-scipy","text":"LMFIT has numerous advantages over the basic scipy.optimize module. Quoting from the LMFIT documentation, the user can: forget about the order of variables and refer to Parameters by meaningful names. place bounds on Parameters as attributes, without worrying about preserving the order of arrays for variables and boundaries. fix Parameters, without having to rewrite the objective function. place algebraic constraints on Parameters. The one disadvantage of the core LMFIT package for our purposes is that it minimizes the sum of squares of a vector instead of maximizing the Poisson likelihood. This is easily remedied, however, by replacing the usual computation of residuals with one that computes the square root of the Poisson likelihood contribution from each bin. Voil\u00e1! A maximum likelihood fitter for histograms. Advantages of LMFIT over our earlier, homemade approach to fitting line shapes also include: The interface for setting upper/lower bounds on parameters and for varying or fixing them is much more elegant, memorable, and simple than our homemade version. It ultimately wraps the scipy.optimize package and therefore inherits all of its advantages: a choice of over a dozen optimizers with highly technical documentation, some optimizers that aim for true global (not just local) optimization, and the countless expert-years that have been invested in perfecting it. LMFIT automatically computes numerous statistics of each fit including estimated uncertainties, correlations, and multiple quality-of-fit statistics (information criteria as well as chi-square) and offers user-friendly fit reports. See the MinimizerResult object. It's the work of Matt Newville, an x-ray scientist responsible for the excellent ifeffit and its successor Larch . Above all, its documentation is complete, already written, and maintained by not-us.","title":"LMFit vs Scipy"},{"location":"line_fitting/#usage-guide","text":"This overview is hardly complete, but we hope it can be a quick-start guide and also hint at how you can convert your own analysis work from the old to the new, preferred fitting methods.","title":"Usage guide"},{"location":"line_fitting/#the-underlying-spectral-line-shape-models","text":"Objects of the type SpectralLine encode the line shape of a fluorescence line, as a sum of Voigt or Lorentzian distributions. Because they inherit from scipy.stats.rv_continuous , they allow computation of cumulative distribution functions and the simulation of data drawn from the distribution. An example of the creation and usage is: # mkdocs: render import numpy as np import pylab as plt import mass2 import mass2.materials # Known lines are accessed by: line = mass2.spectra[\"MnKAlpha\"] rng = np.random.default_rng(1066) N = 100000 energies = line.rvs(size=N, instrument_gaussian_fwhm=2.2, rng=rng) # draw from the distribution plt.clf() sim, bin_edges, _ = plt.hist(energies, 120, range=[5865, 5925], histtype=\"step\", lw=2); binsize = bin_edges[1] - bin_edges[0] e = bin_edges[:-1] + 0.5*binsize plt.plot(e, line(e, instrument_gaussian_fwhm=2.2)*N*binsize, \"k\", lw=0.5) plt.xlabel(\"Energy (eV)\") plt.title(\"Mn K$\\\\alpha$ random deviates and theory curve\") The SpectralLine object is useful to you if you need to generate simulated data, or to plot a line shape, as shown above. The objects that perform line fitting use the SpectralLine object to hold line shape information. You don't need to create a SpectralLine object for fitting, though; it will be done automatically.","title":"The underlying spectral line shape models"},{"location":"line_fitting/#how-to-use-the-lmfit-based-models-for-fitting","text":"The simplest case of line fitting requires only 3 steps: create a model instance from a SpectralLine , guess its parameters from the data, and perform a fit with this guess. Plotting is not done as part of the fit--you have to do that separately. # mkdocs: render model = line.model() params = model.guess(sim, bin_centers=e, dph_de=1) resultA = model.fit(sim, params, bin_centers=e) # Fit again but with dPH/dE held at 1. # dPH/dE will be a free parameter for the fit by default, largely due to the history of MnKAlpha fits being so critical during development. # This will not work for nearly monochromatic lines, however, as the resolution (fwhm) and scale (dph_de) are exactly degenerate. # In practice, most fits are done with dph_de fixed. params = resultA.params.copy() resultB = model.fit(sim, params, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) # There are two plotting methods. The first is an LMfit built-in; the other (\"mass-style\") puts the # fit parameters on the plot. resultB.plot() resultB.plotm() # The best-fit params are found in resultB.params # and a dictionary of their values is resultB.best_values. # The parameters given as an argument to fit are unchanged. You can print a nicely formatted fit report with the method fit_report() : print(resultB.fit_report()) [[Model]] GenericLineModel(MnKAlpha) [[Fit Statistics]] # fitting method = least_squares # function evals = 15 # data points = 120 # variables = 4 chi-square = 100.565947 reduced chi-square = 0.86694782 Akaike info crit = -13.2013653 Bayesian info crit = -2.05139830 R-squared = 0.99999953 [[Variables]] fwhm: 2.21558094 +/- 0.02687437 (1.21%) (init = 2.217155) peak_ph: 5898.79525 +/- 0.00789761 (0.00%) (init = 5898.794) dph_de: 1 (fixed) integral: 99986.5425 +/- 314.455266 (0.31%) (init = 99985.8) background: 5.0098e-16 +/- 0.80578112 (160842446370819488.00%) (init = 2.791565e-09) bg_slope: 0 (fixed) [[Correlations]] (unreported correlations are < 0.100) C(integral, background) = -0.3147 C(fwhm, peak_ph) = -0.1121","title":"How to use the LMFIT-based models for fitting"},{"location":"line_fitting/#fitting-with-exponential-tails-to-low-or-high-energy","text":"Notice when you report the fit (or check the contents of the params or resultB.params objects), there are no parameters referring to exponential tails of a Bortels response. That's because the default fitter assumes a Gaussian response. If you want tails, that's a constructor argument: # mkdocs: render model = line.model(has_tails=True) params = model.guess(sim, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) resultC = model.fit(sim, params, bin_centers=e) resultC.plot() # print(resultC.fit_report()) By default, the has_tails=True will set up a non-zero low-energy tail and allow it to vary, while the high-energy tail is set to zero amplitude and doesn't vary. Use these numbered examples if you want to fit for a high-energy tail (1), to fix the low-E tail at some non-zero level (2) or to turn off the low-E tail completely (3): # 1. To let the low-E and high-E tail both vary simultaneously params[\"tail_share_hi\"].set(.1, vary=True) params[\"tail_tau_hi\"].set(30, vary=True) # 2. To fix the sum of low-E and high-E tail at a 10% level, with low-E tau=30 eV, but # the share of the low vs high tail can vary params[\"tail_frac\"].set(.1, vary=False) params[\"tail_tau\"].set(30, vary=False) # 3. To turn off low-E tail params[\"tail_frac\"].set(.1, vary=True) params[\"tail_share_hi\"].set(1, vary=False) params[\"tail_tau\"].set(vary=False)","title":"Fitting with exponential tails (to low or high energy)"},{"location":"line_fitting/#fitting-with-a-quantum-efficiency-model","text":"If you want to multiply the line models by a model of the quantum efficiency, you can do that. You need a qemodel function or callable function object that takes an energy (scalar or vector) and returns the corresponding efficiency. For example, you can use the \"Raven1 2019\" QE model from mass2.materials . The filter-stack models are not terribly fast to run, so it's best to compute once, spline the results, and pass that spline as the qemodel to line.model(qemodel=qemodel) . # mkdocs: render raven_filters = mass2.materials.efficiency_models.filterstack_models[\"RAVEN1 2019\"] eknots = np.linspace(100, 20000, 1991) qevalues = raven_filters(eknots) qemodel = mass2.mathstat.interpolate.CubicSpline(eknots, qevalues) model = line.model(qemodel=qemodel) resultD = model.fit(sim, resultB.params, bin_centers=e) resultD.plotm() # print(resultD.fit_report()) fit_counts = resultD.params[\"integral\"].value localqe = qemodel(mass2.STANDARD_FEATURES[\"MnKAlpha\"]) fit_observed = fit_counts*localqe fit_err = resultD.params[\"integral\"].stderr count_err = fit_err*localqe print(\"Fit finds {:.0f}\u00b1{:.0f} counts before QE, or {:.0f}\u00b1{:.0f} observed. True value {:d}.\".format( round(fit_counts, -1), round(fit_err, -1), round(fit_observed, -1), round(count_err, -1), N)) Fit finds 168810\u00b1530 counts before QE, or 100020\u00b1320 observed. True value 100000. When you fit with a non-trivial QE model, all fit parameters that refer to intensities of signal or background refer to a sensor with an ideal QE=1. These parameters include: integral background bg_slope That is, the fit values must be multiplied by the local QE to give the number of observed signal counts, background counts per bin, or background slope. With or without a QE model, \"integral\" refers to the number of photons that would be seen across all energies (not just in the range being fit).","title":"Fitting with a quantum efficiency model"},{"location":"line_fitting/#fitting-a-simple-gaussian-lorentzian-or-voigt-function","text":"# mkdocs: render import dataclasses from mass2.calibration.fluorescence_lines import SpectralLine e_ctr = 1000.0 Nsig = 10000 Nbg = 1000 sigma = 1.0 x_gauss = rng.standard_normal(Nsig)*sigma + e_ctr hwhm = 1.0 x_lorentz = rng.standard_cauchy(Nsig)*hwhm + e_ctr x_voigt = rng.standard_cauchy(Nsig)*hwhm + rng.standard_normal(Nsig)*sigma + e_ctr bg = rng.uniform(e_ctr-5, e_ctr+5, size=Nbg) # Gaussian fit c, b = np.histogram(np.hstack([x_gauss, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, 0, 0) line = dataclasses.replace(line, linetype=\"Gaussian\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultG = model.fit(c, params, bin_centers=bin_ctr) resultG.plotm() # print(resultG.fit_report()) # mkdocs: render # Lorentzian fit c, b = np.histogram(np.hstack([x_lorentz, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, 0) line = dataclasses.replace(line, linetype=\"Lorentzian\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultL = model.fit(c, params, bin_centers=bin_ctr) resultL.plotm() # print(resultL.fit_report()) # mkdocs: render # Voigt fit c, b = np.histogram(np.hstack([x_voigt, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, sigma) line = dataclasses.replace(line, linetype=\"Voigt\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultV = model.fit(c, params, bin_centers=bin_ctr) resultV.plotm() # print(resultV.fit_report())","title":"Fitting a simple Gaussian, Lorentzian, or Voigt function"},{"location":"line_fitting/#more-details-of-fitting-fluorescence-line-models","text":"Use p=model.guess(data, bin_centers=e, dph_de=dph_de) to create a heuristic for the starting parameters. Change starting values and toggle the vary attribute on parameters, as needed. For example: p[\"dph_de\"].set(1.0, vary=False) Use result=model.fit(data, p, bin_centers=e) to perform the fit and store the result. The result holds many attributes and methods (see MinimizerResult for full documentation). These include: result.params = the model's best-fit parameters object result.best_values = a dictionary of the best-fit parameter values result.best_fit = the model's y-values at the best-fit parameter values result.chisqr = the chi-squared statistic of the fit (here, -2log(L)) result.covar = the computed covariance result.fit_report() = return a pretty-printed string reporting on the fit result.plot_fit() = make a plot of the data and fit result.plot_residuals() = make a plot of the residuals (fit-data) result.plotm() = make a plot of the data, fit, and fit params with dataset filename in title result.plot() = make a plot of the data, fit, and residuals, generally plotm is preferred The tau values (scale lengths of exponential tails) are in eV units. The parameter \"integral\" refers to the integrated number of counts across all energies (whether inside or beyond the fitted energy range).","title":"More details of fitting fluorescence-line models"},{"location":"tests/","text":"Unit testing If you want to run the unit tests for mass2 go to your mass2 directory and do pytest . If you want to add tests to mass2 , just use simple, modern tests in the pytest . Put any new tests in a file somewhere inside tests with a name like test_myfeature.py , it must match the pattern test_*.py to be found by pytest. Use assertions, numpy.allclose , and similar tools to test that outcomes match expectations. On each commit to develop, the tests will be run automatically by GitHub Actions. See results of recent tests . Documentation We could use a lot of help with documentation. As of September 2, 2025, we used GitHub's AI chatbot to generate all 450+ missing doc strings. We think most of them look pretty reasonable, and many are excellent. But few use the desired numpy style for docstrings. Please consider upgrading any docstrings that touch on your work as you develop Mass2.","title":"Testing"},{"location":"tests/#unit-testing","text":"If you want to run the unit tests for mass2 go to your mass2 directory and do pytest . If you want to add tests to mass2 , just use simple, modern tests in the pytest . Put any new tests in a file somewhere inside tests with a name like test_myfeature.py , it must match the pattern test_*.py to be found by pytest. Use assertions, numpy.allclose , and similar tools to test that outcomes match expectations. On each commit to develop, the tests will be run automatically by GitHub Actions. See results of recent tests .","title":"Unit testing"},{"location":"tests/#documentation","text":"We could use a lot of help with documentation. As of September 2, 2025, we used GitHub's AI chatbot to generate all 450+ missing doc strings. We think most of them look pretty reasonable, and many are excellent. But few use the desired numpy style for docstrings. Please consider upgrading any docstrings that touch on your work as you develop Mass2.","title":"Documentation"},{"location":"xray_efficiency_models/","text":"Detector X-ray Efficiency Models This module requires the xraydb python package. It should be included in the mass2 installation. Otherwise, you should be able to install with pip install xraydb . Motivation For many analyses, it is important to estimate a x-ray spectrum as it would be seen from the source rather than as it would be measured with a set of detectors. This can be important, for example, when trying to determine line intensity ratios of two lines separated in energy space. Here, we attempt to model the effects that would cause the measured spectrum to be different from the true spectrum, such as energy dependent losses in transmission due to IR blocking filters and vacuum windows. Energy-dependent absorber efficiency can also be modeled. FilterStack class and subclass functions with premade efficiency models Here, we import the mass2.efficiency_models module and demonstrate the functionality with some of the premade efficiency models. Generally, these premade models are put in place for TES instruments with well known absorber and filter stack compositions. To demonstrate, we work with the 'EBIT 2018' model, which models the TES spectrometer setup at the NIST EBIT, as it was commissioned in 2018. This model includes a ~1um thick absorber, 3 ~100nm thick Al IR blocking filters, and LEX HT vacuum windows for both the TES and EBIT vacuums. We begin by importing efficiency_models and examining the EBIT efficiency model components. We can see that the model is made of many submodels (aka components) and that all the parameters have uncertainties. The EBIT system was particularly well characterized, so the uncertainties are fairly low. The presence of uncertainties requires some special handling in a few places, these docs will show some examples. # mkdocs: render import mass2 import mass2.materials # you have to explicitly import mass2.materials import numpy as np import pylab as plt from uncertainties import unumpy as unp # useful for working with arrays with uncertainties aka uarray from uncertainties import ufloat EBIT_model = mass2.materials.filterstack_models['EBIT 2018'] print(EBIT_model) <class 'mass2.materials.efficiency_models.FilterStack'>( Electroplated Au Absorber: <class 'mass2.materials.efficiency_models.Filter'>(Au 0.00186+/-0.00006 g/cm^2, fill_fraction=1.000+/-0, absorber=True) 50mK Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (3.04+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 3K Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.93+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 50K Filter: <class 'mass2.materials.efficiency_models.FilterStack'>( Al Film: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.77+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) Ni Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Ni 0.0134+/-0.0018 g/cm^2, fill_fraction=0.170+/-0.010, absorber=False) ) Luxel Window TES: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) Luxel Window EBIT: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) ) Next, we examine the function get_efficiency(xray_energies_eV) , which is an method of FilterStack . This can be called for the entire filter stack or for individual components in the filter stack. As an example, we look at the efficiency of the EBIT 2018 filter stack and the 50K filter component between 2ekV and 10 keV, at 1 keV steps. # mkdocs: render sparse_xray_energies_eV = np.arange(2000, 10000, 1000) stack_efficiency = EBIT_model.get_efficiency(sparse_xray_energies_eV) stack_efficiency_uncertain = EBIT_model.get_efficiency(sparse_xray_energies_eV, uncertain=True) # you have to opt into getting uncertainties out filter50K_efficiency = EBIT_model.components[3].get_efficiency(sparse_xray_energies_eV) print(\"stack efficiencies\") print([f\"{x}\" for x in stack_efficiency_uncertain]) # this is a hack to get uarrays to print with auto chosen number of sig figs print(stack_efficiency) # this is a hack to get uarrays to print with auto chosen number of sig figs print(unp.nominal_values(stack_efficiency)) # you can easily strip uncertainties, see uncertains package docs for more info print(\"filter50K efficiencies\") print(filter50K_efficiency) # if you want to remove the uncertainties, eg for plotting stack efficiencies ['0.335+/-0.008', '0.472+/-0.010', '0.456+/-0.010', '0.383+/-0.010', '0.307+/-0.009', '0.242+/-0.007', '0.191+/-0.006', '0.136+/-0.005'] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] filter50K efficiencies [0.77672107 0.81107679 0.8233861 0.84072724 0.86670307 0.89357999 0.9163624 0.83360284] Here, we use the function plot_efficiency(xray_energies_eV, ax) to plot the efficiencies. ax defaults to None, but it can be used to plot the efficiencies on a user provided axis. Just like get_efficiency , plot_efficiency works with FilterStack and its subclasses. Testing with energy range 100 to 20,000 eV, 1 eV steps. # mkdocs: render xray_energies_eV = np.arange(100,20000,10) EBIT_model.plot_efficiency(xray_energies_eV) # mkdocs: render EBIT_model.components[3].plot_efficiency(xray_energies_eV) Creating your own custom filter stack model using FilterStack objects Now we will explore creating custom FilterStack objects and building up your very own filter stack model. First, we will create a general FilterStack object, representing a stack of filters. We will then populate this object with filters, which take the form of the various FilterStack object subclasses, such as Film , or even other FilterStack objects to create more complicated filters with multiple components. The add argument can be used to add a premade FilterStack object as a component of a different FilterStack object. We will start by adding some simple Film objects to the filter stack. This class requires a the name and material arguments, and the optical depth can be specified by passing in either area_density_g_per_cm2 or thickness_nm (but not both). By default, most FilterStack objects use the bulk density of a material to calculate the optical depth when the thickness_nm is used, but a custom density can be specified with the density_g_per_cm3 argument. In addition, a meshed style filter can be modelled using the fill_fraction argument. Finally, most FilterStack subclasses can use the absorber argument (default False), which will cause the object to return absorption, instead of transmittance, as the efficiency. All numerical arguments can be passed with our without uncertainties. If you don't have at least one number with specified uncertainty in a particular Film, the code will add a \u00b1100% uncertainty on that component. This way, hopefully you will notice that your uncertainty is higher than you expect, and double check the inputs. Read up on the uncertainties package for more info about how it works. # mkdocs: render custom_model = mass2.materials.FilterStack(name='My Filter Stack') custom_model.add_filter(name='My Bi Absorber', material='Bi', thickness_nm=ufloat(4.0e3, .1e3), absorber=True) custom_model.add_filter(name='My Al 50mK Filter', material='Al', thickness_nm=ufloat(100.0, 10)) custom_model.add_filter(name='My Si 3K Filter', material='Si', thickness_nm=ufloat(500.0, 2)) custom_filter = mass2.materials.FilterStack(name='My meshed 50K Filter') custom_filter.add_filter(name='Al Film', material='Al', thickness_nm=ufloat(100.0, 10)) custom_filter.add_filter(name='Ni Mesh', material='Ni', thickness_nm=ufloat(10.0e3, .1e3), fill_fraction=ufloat(0.2, 0.01)) custom_model.add(custom_filter) custom_model.plot_efficiency(xray_energies_eV) There are also some premade filter classes for filters that commonly show up in our instrument filter stacks. At the moment, the FilterStack subclasses listed below are implemented: - AlFilmWithOxide - models a typical IR blocking filter with native oxide layers, which can be important for thin filters. - AlFilmWithPolymer - models a similar IR blocking filter, but with increased structural support from a polymer backing. - LEX_HT - models LEX_HT vacuum windows, which contain a polymer backed Al film and stainless steel mesh. Usage examples and efficiency curves of these classes are shown below. # mkdocs: render premade_filter_stack = mass2.materials.FilterStack(name='A Stack of Premade Filters') f1 = mass2.materials.AlFilmWithOxide(name='My Oxidized Al Filter', Al_thickness_nm=50.0) f2 = mass2.materials.AlFilmWithPolymer(name='My Polymer Backed Al Filter', Al_thickness_nm=100.0, polymer_thickness_nm=200.0) f3 = mass2.materials.LEX_HT(name=\"My LEX HT Filter\") premade_filter_stack.add(f1) premade_filter_stack.add(f2) premade_filter_stack.add(f3) low_xray_energies_eV = np.arange(100,3000,5) premade_filter_stack.plot_efficiency(low_xray_energies_eV)","title":"X-ray efficiency models"},{"location":"xray_efficiency_models/#detector-x-ray-efficiency-models","text":"This module requires the xraydb python package. It should be included in the mass2 installation. Otherwise, you should be able to install with pip install xraydb .","title":"Detector X-ray Efficiency Models"},{"location":"xray_efficiency_models/#motivation","text":"For many analyses, it is important to estimate a x-ray spectrum as it would be seen from the source rather than as it would be measured with a set of detectors. This can be important, for example, when trying to determine line intensity ratios of two lines separated in energy space. Here, we attempt to model the effects that would cause the measured spectrum to be different from the true spectrum, such as energy dependent losses in transmission due to IR blocking filters and vacuum windows. Energy-dependent absorber efficiency can also be modeled.","title":"Motivation"},{"location":"xray_efficiency_models/#filterstack-class-and-subclass-functions-with-premade-efficiency-models","text":"Here, we import the mass2.efficiency_models module and demonstrate the functionality with some of the premade efficiency models. Generally, these premade models are put in place for TES instruments with well known absorber and filter stack compositions. To demonstrate, we work with the 'EBIT 2018' model, which models the TES spectrometer setup at the NIST EBIT, as it was commissioned in 2018. This model includes a ~1um thick absorber, 3 ~100nm thick Al IR blocking filters, and LEX HT vacuum windows for both the TES and EBIT vacuums. We begin by importing efficiency_models and examining the EBIT efficiency model components. We can see that the model is made of many submodels (aka components) and that all the parameters have uncertainties. The EBIT system was particularly well characterized, so the uncertainties are fairly low. The presence of uncertainties requires some special handling in a few places, these docs will show some examples. # mkdocs: render import mass2 import mass2.materials # you have to explicitly import mass2.materials import numpy as np import pylab as plt from uncertainties import unumpy as unp # useful for working with arrays with uncertainties aka uarray from uncertainties import ufloat EBIT_model = mass2.materials.filterstack_models['EBIT 2018'] print(EBIT_model) <class 'mass2.materials.efficiency_models.FilterStack'>( Electroplated Au Absorber: <class 'mass2.materials.efficiency_models.Filter'>(Au 0.00186+/-0.00006 g/cm^2, fill_fraction=1.000+/-0, absorber=True) 50mK Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (3.04+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 3K Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.93+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 50K Filter: <class 'mass2.materials.efficiency_models.FilterStack'>( Al Film: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.77+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) Ni Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Ni 0.0134+/-0.0018 g/cm^2, fill_fraction=0.170+/-0.010, absorber=False) ) Luxel Window TES: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) Luxel Window EBIT: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) ) Next, we examine the function get_efficiency(xray_energies_eV) , which is an method of FilterStack . This can be called for the entire filter stack or for individual components in the filter stack. As an example, we look at the efficiency of the EBIT 2018 filter stack and the 50K filter component between 2ekV and 10 keV, at 1 keV steps. # mkdocs: render sparse_xray_energies_eV = np.arange(2000, 10000, 1000) stack_efficiency = EBIT_model.get_efficiency(sparse_xray_energies_eV) stack_efficiency_uncertain = EBIT_model.get_efficiency(sparse_xray_energies_eV, uncertain=True) # you have to opt into getting uncertainties out filter50K_efficiency = EBIT_model.components[3].get_efficiency(sparse_xray_energies_eV) print(\"stack efficiencies\") print([f\"{x}\" for x in stack_efficiency_uncertain]) # this is a hack to get uarrays to print with auto chosen number of sig figs print(stack_efficiency) # this is a hack to get uarrays to print with auto chosen number of sig figs print(unp.nominal_values(stack_efficiency)) # you can easily strip uncertainties, see uncertains package docs for more info print(\"filter50K efficiencies\") print(filter50K_efficiency) # if you want to remove the uncertainties, eg for plotting stack efficiencies ['0.335+/-0.008', '0.472+/-0.010', '0.456+/-0.010', '0.383+/-0.010', '0.307+/-0.009', '0.242+/-0.007', '0.191+/-0.006', '0.136+/-0.005'] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] filter50K efficiencies [0.77672107 0.81107679 0.8233861 0.84072724 0.86670307 0.89357999 0.9163624 0.83360284] Here, we use the function plot_efficiency(xray_energies_eV, ax) to plot the efficiencies. ax defaults to None, but it can be used to plot the efficiencies on a user provided axis. Just like get_efficiency , plot_efficiency works with FilterStack and its subclasses. Testing with energy range 100 to 20,000 eV, 1 eV steps. # mkdocs: render xray_energies_eV = np.arange(100,20000,10) EBIT_model.plot_efficiency(xray_energies_eV) # mkdocs: render EBIT_model.components[3].plot_efficiency(xray_energies_eV)","title":"FilterStack class and subclass functions with premade efficiency models"},{"location":"xray_efficiency_models/#creating-your-own-custom-filter-stack-model-using-filterstack-objects","text":"Now we will explore creating custom FilterStack objects and building up your very own filter stack model. First, we will create a general FilterStack object, representing a stack of filters. We will then populate this object with filters, which take the form of the various FilterStack object subclasses, such as Film , or even other FilterStack objects to create more complicated filters with multiple components. The add argument can be used to add a premade FilterStack object as a component of a different FilterStack object. We will start by adding some simple Film objects to the filter stack. This class requires a the name and material arguments, and the optical depth can be specified by passing in either area_density_g_per_cm2 or thickness_nm (but not both). By default, most FilterStack objects use the bulk density of a material to calculate the optical depth when the thickness_nm is used, but a custom density can be specified with the density_g_per_cm3 argument. In addition, a meshed style filter can be modelled using the fill_fraction argument. Finally, most FilterStack subclasses can use the absorber argument (default False), which will cause the object to return absorption, instead of transmittance, as the efficiency. All numerical arguments can be passed with our without uncertainties. If you don't have at least one number with specified uncertainty in a particular Film, the code will add a \u00b1100% uncertainty on that component. This way, hopefully you will notice that your uncertainty is higher than you expect, and double check the inputs. Read up on the uncertainties package for more info about how it works. # mkdocs: render custom_model = mass2.materials.FilterStack(name='My Filter Stack') custom_model.add_filter(name='My Bi Absorber', material='Bi', thickness_nm=ufloat(4.0e3, .1e3), absorber=True) custom_model.add_filter(name='My Al 50mK Filter', material='Al', thickness_nm=ufloat(100.0, 10)) custom_model.add_filter(name='My Si 3K Filter', material='Si', thickness_nm=ufloat(500.0, 2)) custom_filter = mass2.materials.FilterStack(name='My meshed 50K Filter') custom_filter.add_filter(name='Al Film', material='Al', thickness_nm=ufloat(100.0, 10)) custom_filter.add_filter(name='Ni Mesh', material='Ni', thickness_nm=ufloat(10.0e3, .1e3), fill_fraction=ufloat(0.2, 0.01)) custom_model.add(custom_filter) custom_model.plot_efficiency(xray_energies_eV) There are also some premade filter classes for filters that commonly show up in our instrument filter stacks. At the moment, the FilterStack subclasses listed below are implemented: - AlFilmWithOxide - models a typical IR blocking filter with native oxide layers, which can be important for thin filters. - AlFilmWithPolymer - models a similar IR blocking filter, but with increased structural support from a polymer backing. - LEX_HT - models LEX_HT vacuum windows, which contain a polymer backed Al film and stainless steel mesh. Usage examples and efficiency curves of these classes are shown below. # mkdocs: render premade_filter_stack = mass2.materials.FilterStack(name='A Stack of Premade Filters') f1 = mass2.materials.AlFilmWithOxide(name='My Oxidized Al Filter', Al_thickness_nm=50.0) f2 = mass2.materials.AlFilmWithPolymer(name='My Polymer Backed Al Filter', Al_thickness_nm=100.0, polymer_thickness_nm=200.0) f3 = mass2.materials.LEX_HT(name=\"My LEX HT Filter\") premade_filter_stack.add(f1) premade_filter_stack.add(f2) premade_filter_stack.add(f3) low_xray_energies_eV = np.arange(100,3000,5) premade_filter_stack.plot_efficiency(low_xray_energies_eV)","title":"Creating your own custom filter stack model using FilterStack objects"}]}