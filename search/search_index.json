{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mass 2: Microcalorimeter Analysis Software Suite Overview of Mass 2 Mass is a Python software suite designed to analyze pulse records from high-resolution, cryogenic microcalorimeters. We use Mass with pulse records from x-ray and gamma-ray spectrometers, performing a sequence of analysis, or \"calibration\" steps to extract a high-precision estimate of the energy from each record. With raw pulse records and Mass, you can: Analyze data from one or multiple detectors at one time. Analyze data from one or more raw pulse data files per detector. Analyze a fixed dataset taken in the past, or perform \"online\" analysis of a dataset still being acquired. Analyze data from time-division multiplexed (TDM) and microwave-multiplexed (\u00b5MUX) systems. Compute and apply \"optimal filters\" of various types. With or without raw pulse records, further analysis tasks that Mass helps with include: Choose and apply data cuts. Fix complex line shapes in an energy spectrum. Estimate and apply accurate functions for absolute-energy calibration. Win friends and influence people. Major concepts Mass2 is built around a few core technologies: Pola.rs , a high-performance modern dataframe library. Organizes the data structures and provides data I/O. Numpy . For numerical arrays and scientific computation. Scipy . Additional algorithms for scientific computation. LMFIT . Convenient non-linear curve fitting. A particularly important change is that Mass 2 leverages Pola.rs for data organization. Therefore, for a critical portion of the code, the larger open-source software community provides the documentation, bug fixes, and testing.","title":"Home"},{"location":"#mass-2-microcalorimeter-analysis-software-suite","text":"","title":"Mass 2: Microcalorimeter Analysis Software Suite"},{"location":"#overview-of-mass-2","text":"Mass is a Python software suite designed to analyze pulse records from high-resolution, cryogenic microcalorimeters. We use Mass with pulse records from x-ray and gamma-ray spectrometers, performing a sequence of analysis, or \"calibration\" steps to extract a high-precision estimate of the energy from each record. With raw pulse records and Mass, you can: Analyze data from one or multiple detectors at one time. Analyze data from one or more raw pulse data files per detector. Analyze a fixed dataset taken in the past, or perform \"online\" analysis of a dataset still being acquired. Analyze data from time-division multiplexed (TDM) and microwave-multiplexed (\u00b5MUX) systems. Compute and apply \"optimal filters\" of various types. With or without raw pulse records, further analysis tasks that Mass helps with include: Choose and apply data cuts. Fix complex line shapes in an energy spectrum. Estimate and apply accurate functions for absolute-energy calibration. Win friends and influence people.","title":"Overview of Mass 2"},{"location":"#major-concepts","text":"Mass2 is built around a few core technologies: Pola.rs , a high-performance modern dataframe library. Organizes the data structures and provides data I/O. Numpy . For numerical arrays and scientific computation. Scipy . Additional algorithms for scientific computation. LMFIT . Convenient non-linear curve fitting. A particularly important change is that Mass 2 leverages Pola.rs for data organization. Therefore, for a critical portion of the code, the larger open-source software community provides the documentation, bug fixes, and testing.","title":"Major concepts"},{"location":"LJHfiles/","text":"LJH Memorial File Format LJH Version 2.2.0 is the current LJH file format (since 2015). An LJH file contains segmented, time-series data that represent separate triggered pulses observed in a single cryogenic microcalorimeter. The LJH file format consists of a human-readable ASCII header followed by an arbitrary number of binary data records . Information in the header specifies the exact length in bytes of each data record. Header Information The human-readable ASCII header is the start of the LJH file. That means you can say less myfile_chan5.ljh at a unix terminal and get meaningful information about the file before the gibberish starts. Handy, right? The header is somewhat fragile (it might have been better written in YAML or TOML or even JSON, but we decided just to live with it). It consists of key-value pairs, with a format of Key: value , one pair per line. Header Notes: Lines begining with # are usually ignored. #End of Header marks the end of the header and the transition to the binary data. #End of Description has special meaning if System description of this File: has been read. Newlines are the newlines of the digitizing computer. The interpreting program must accept LF, CR, or CRLF. Capitalization must be matched. One space follows a colon. Any additional spaces are treated as part of the value. Programs that read LJH files ignore header keys that are unexpected or unneeded. #LJH Memorial File Format This line indicates that the file is based on format described here. Save File Format Version: 2.2.0 Software Version: DASTARD version 0.2.15 Software Git Hash: 85ab821 Data source: Abaco These lines uniquely identify the exact format, so the interpreting program can adapt. While the first line should be sufficient for this purpose, the second and third lines take in the possibility that a particular program may have a bug. The interpreting program may be aware of this bug and compensate. The Data source is meant for later human reference. Number of rows: 74 Number of columns: 1 Row number (from 0-73 inclusive): 12 Column number (from 0-0 inclusive): 0 Number of channels: 74 Channel name: chan12 Channel: 12 ChannelIndex (in dastard): 12 Dastard inserts this information to help downstream analysis tools understand the array being used when this file was acquired. Digitized Word Size in Bytes: 2 Each sample is stored in this many bytes. Location: LLNL Cryostat: C3PO Thermometer: GRT1 Temperature (Ohm or K): 0.1 Bridge range: 20.0E3 Magnetic field (A or Gauss): 0.75 Detector: SnTES#8 Sample: Orange peel Excitation/Source: none Operator: Leisure Larry Like the several lines above, most lines are comments for later human use and are not interpreted by general-purpose LJH readers. System description of this File: blah blah blah User description of this File: blah blah blah #End of Description This is a multiline comment. Once the Description of this File: line is read, all following lines are concantenated until #End of Description is read. Again, this is ignored by programs that read LJH files. Number of Digitizers: 1 Number of Active Channels: 2 The number of digitizers and channels present in the file are given so that space may be allocated for them by the interpreting program, if necessary. Timestamp offset (s): 3016738980.049000 The meaning of this and the means of interpreting it are dependent upon the particular programs creating and reading this file. It was a necessary offset in earlier versions of LJH, where we did not reserve enough bytes per record to record a full timestamp. In LJH 2.2, it serves as a simple zero-time (all records should be no earlier than this \"offset\"). Server Start Time: 18 Nov 2022, 15:47:34 MST First Record Time: 18 Nov 2022, 16:54:15 MST These times show when the server (Dastard, in this case) started running, and when the first record was written to this file. Timebase: 5.000000E-8 Number of samples per point: 1 Timebase gives the sampling period (in seconds). Number of samples per point is generally 1, but can be more in special cases where samples are averaged and downsampled before recording. Presamples: 256 Total Samples: 1024 Total samples is the actual record length in samples. The trigger point will be located at sample number Presamples. Binary Information If you read an LJH file until the characters #End of Header plus the following CR and/or LF, then the remainder of the file is the binary section. It consists of a sequence of data records. Each record starts with a 16-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 16+L*M . All values in the data record are little endian. The first 8-byte word is the subframe counter. It counts the number of subframe times read out since the server started. If the server has to resynchronize on the raw data, then the subframe counter will be incremented by an estimate to account for the time missed. For TDM data, the subframe rate is equal to the row rate, also known as the line rate . For \u00b5MUX data, subframes run at a multiple of the frame rate given by the Subframe divisions: value in the LJH header (typically 64). The second 8-byte word is the POSIX microsecond time, i.e., the time in microseconds since 1 January 1970 00:00 UT. (Warning: this will overflow in only 292,226 years if you interpret it as a signed number.) The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for TDM feedback or for \u00b5MUX data.) Earlier versions of the LJH standard Binary Information (for LJH version 2.1.0) Version 2.1.0 follows. This version was made obsolete in 2015 by version 2.2.0. Each record starts with a 6-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 6+L*M . All values in the data record are little endian. The first byte is a \"4 microsecond tick\". That is, it counts microseconds past the millisecond counter and records the count divided by 4. Beware that previous versions of LJH used the first byte to signify something about the type of data. Igor seems to ignore this byte, though, so I think we're okay to stuff timing information into it. The second byte used to signify a channel number N, which corresponds to the Nth channel described in the header. Channel number 255 is reserved for temperature readout with the DMM. Since 2010, this has always been meaningless. The next 4 bytes are an unsigned 32-bit number that is the value of a millisecond counter on the digitizing computer. The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for the TDM feedback.) Changelog Version 2.2.0 (6 Aug 2015) Changed the binary definition to include 8 bytes each record for pulse timing (microsecond precision) and frame number. Version 2.1.0 (23 Sep 2011) Used the first byte of each record to get 4 microsec timing resolution instead of 1 ms. Version 2.0.0 (27 Mar 2011) Defined inversion and offset more clearly Version 2.0.0 (5 Jan 2001) Changed definition of discrimination level Version 2.0.0 (24 May 2000) since most PCs have least significant byte first, the binary information has been changed to default Version 1.1.0 (8 May 2000) added a few more user and channel parameters as well as provisions for temperature monitoring Initial 1.0.0 (5 Aug 1999) definition by Larry J. Hiller","title":"LJH file format"},{"location":"LJHfiles/#ljh-memorial-file-format","text":"LJH Version 2.2.0 is the current LJH file format (since 2015). An LJH file contains segmented, time-series data that represent separate triggered pulses observed in a single cryogenic microcalorimeter. The LJH file format consists of a human-readable ASCII header followed by an arbitrary number of binary data records . Information in the header specifies the exact length in bytes of each data record.","title":"LJH Memorial File Format"},{"location":"LJHfiles/#header-information","text":"The human-readable ASCII header is the start of the LJH file. That means you can say less myfile_chan5.ljh at a unix terminal and get meaningful information about the file before the gibberish starts. Handy, right? The header is somewhat fragile (it might have been better written in YAML or TOML or even JSON, but we decided just to live with it). It consists of key-value pairs, with a format of Key: value , one pair per line.","title":"Header Information"},{"location":"LJHfiles/#header-notes","text":"Lines begining with # are usually ignored. #End of Header marks the end of the header and the transition to the binary data. #End of Description has special meaning if System description of this File: has been read. Newlines are the newlines of the digitizing computer. The interpreting program must accept LF, CR, or CRLF. Capitalization must be matched. One space follows a colon. Any additional spaces are treated as part of the value. Programs that read LJH files ignore header keys that are unexpected or unneeded. #LJH Memorial File Format This line indicates that the file is based on format described here. Save File Format Version: 2.2.0 Software Version: DASTARD version 0.2.15 Software Git Hash: 85ab821 Data source: Abaco These lines uniquely identify the exact format, so the interpreting program can adapt. While the first line should be sufficient for this purpose, the second and third lines take in the possibility that a particular program may have a bug. The interpreting program may be aware of this bug and compensate. The Data source is meant for later human reference. Number of rows: 74 Number of columns: 1 Row number (from 0-73 inclusive): 12 Column number (from 0-0 inclusive): 0 Number of channels: 74 Channel name: chan12 Channel: 12 ChannelIndex (in dastard): 12 Dastard inserts this information to help downstream analysis tools understand the array being used when this file was acquired. Digitized Word Size in Bytes: 2 Each sample is stored in this many bytes. Location: LLNL Cryostat: C3PO Thermometer: GRT1 Temperature (Ohm or K): 0.1 Bridge range: 20.0E3 Magnetic field (A or Gauss): 0.75 Detector: SnTES#8 Sample: Orange peel Excitation/Source: none Operator: Leisure Larry Like the several lines above, most lines are comments for later human use and are not interpreted by general-purpose LJH readers. System description of this File: blah blah blah User description of this File: blah blah blah #End of Description This is a multiline comment. Once the Description of this File: line is read, all following lines are concantenated until #End of Description is read. Again, this is ignored by programs that read LJH files. Number of Digitizers: 1 Number of Active Channels: 2 The number of digitizers and channels present in the file are given so that space may be allocated for them by the interpreting program, if necessary. Timestamp offset (s): 3016738980.049000 The meaning of this and the means of interpreting it are dependent upon the particular programs creating and reading this file. It was a necessary offset in earlier versions of LJH, where we did not reserve enough bytes per record to record a full timestamp. In LJH 2.2, it serves as a simple zero-time (all records should be no earlier than this \"offset\"). Server Start Time: 18 Nov 2022, 15:47:34 MST First Record Time: 18 Nov 2022, 16:54:15 MST These times show when the server (Dastard, in this case) started running, and when the first record was written to this file. Timebase: 5.000000E-8 Number of samples per point: 1 Timebase gives the sampling period (in seconds). Number of samples per point is generally 1, but can be more in special cases where samples are averaged and downsampled before recording. Presamples: 256 Total Samples: 1024 Total samples is the actual record length in samples. The trigger point will be located at sample number Presamples.","title":"Header Notes:"},{"location":"LJHfiles/#binary-information","text":"If you read an LJH file until the characters #End of Header plus the following CR and/or LF, then the remainder of the file is the binary section. It consists of a sequence of data records. Each record starts with a 16-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 16+L*M . All values in the data record are little endian. The first 8-byte word is the subframe counter. It counts the number of subframe times read out since the server started. If the server has to resynchronize on the raw data, then the subframe counter will be incremented by an estimate to account for the time missed. For TDM data, the subframe rate is equal to the row rate, also known as the line rate . For \u00b5MUX data, subframes run at a multiple of the frame rate given by the Subframe divisions: value in the LJH header (typically 64). The second 8-byte word is the POSIX microsecond time, i.e., the time in microseconds since 1 January 1970 00:00 UT. (Warning: this will overflow in only 292,226 years if you interpret it as a signed number.) The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for TDM feedback or for \u00b5MUX data.)","title":"Binary Information"},{"location":"LJHfiles/#earlier-versions-of-the-ljh-standard","text":"","title":"Earlier versions of the LJH standard"},{"location":"LJHfiles/#binary-information-for-ljh-version-210","text":"Version 2.1.0 follows. This version was made obsolete in 2015 by version 2.2.0. Each record starts with a 6-byte time marker. The record's waveform data consists of the next L*M bytes, where L is the number of samples ( Total Samples: value from the header) and M is the number of bytes per sample ( Digitized Word Size in Bytes: from the header). M is always 2 bytes per sample, in practice. The full record's length is 6+L*M . All values in the data record are little endian. The first byte is a \"4 microsecond tick\". That is, it counts microseconds past the millisecond counter and records the count divided by 4. Beware that previous versions of LJH used the first byte to signify something about the type of data. Igor seems to ignore this byte, though, so I think we're okay to stuff timing information into it. The second byte used to signify a channel number N, which corresponds to the Nth channel described in the header. Channel number 255 is reserved for temperature readout with the DMM. Since 2010, this has always been meaningless. The next 4 bytes are an unsigned 32-bit number that is the value of a millisecond counter on the digitizing computer. The next L words (of M bytes each) are the data record, as a signed or unsigned integer. (Typically, we use signed for the TDM error signal and unsigned for the TDM feedback.)","title":"Binary Information (for LJH version 2.1.0)"},{"location":"LJHfiles/#changelog","text":"Version 2.2.0 (6 Aug 2015) Changed the binary definition to include 8 bytes each record for pulse timing (microsecond precision) and frame number. Version 2.1.0 (23 Sep 2011) Used the first byte of each record to get 4 microsec timing resolution instead of 1 ms. Version 2.0.0 (27 Mar 2011) Defined inversion and offset more clearly Version 2.0.0 (5 Jan 2001) Changed definition of discrimination level Version 2.0.0 (24 May 2000) since most PCs have least significant byte first, the binary information has been changed to default Version 1.1.0 (8 May 2000) added a few more user and channel parameters as well as provisions for temperature monitoring Initial 1.0.0 (5 Aug 1999) definition by Larry J. Hiller","title":"Changelog"},{"location":"about/","text":"Authors MASS is the work of physicists from Quantum Sensors Division of the NIST Boulder Labs and the University of Colorado Physics Department , with substantial contributions from: Joe Fowler , project director Galen O'Neil , co-director Dan Becker Young-Il Joe Jamie Titus Joshua Ho Many collaborators, who have made many bug reports, bug fixes, and feature requests. Major Versions MASS version 2 was begin in August 2025. It is still changing rapidly, in alpha status. Find it at https://github.com/usnistgov/mass2 MASS version 1 was begun in November 2010. Bug-fix development continues. Find it at https://github.com/usnistgov/mass","title":"About"},{"location":"about/#authors","text":"MASS is the work of physicists from Quantum Sensors Division of the NIST Boulder Labs and the University of Colorado Physics Department , with substantial contributions from: Joe Fowler , project director Galen O'Neil , co-director Dan Becker Young-Il Joe Jamie Titus Joshua Ho Many collaborators, who have made many bug reports, bug fixes, and feature requests.","title":"Authors"},{"location":"about/#major-versions","text":"MASS version 2 was begin in August 2025. It is still changing rapidly, in alpha status. Find it at https://github.com/usnistgov/mass2 MASS version 1 was begun in November 2010. Bug-fix development continues. Find it at https://github.com/usnistgov/mass","title":"Major Versions"},{"location":"demo_plot/","text":"Demonstration of live plots Here's a cell with plots generated by the mkdocs site-generation process: # mkdocs: render import matplotlib.pyplot as plt import numpy as np xpoints = np.linspace(1.1, 10, 100) ypoints = np.log(xpoints) plt.plot(xpoints, ypoints, \".-\")","title":"Demo plot"},{"location":"demo_plot/#demonstration-of-live-plots","text":"Here's a cell with plots generated by the mkdocs site-generation process: # mkdocs: render import matplotlib.pyplot as plt import numpy as np xpoints = np.linspace(1.1, 10, 100) ypoints = np.log(xpoints) plt.plot(xpoints, ypoints, \".-\")","title":"Demonstration of live plots"},{"location":"docstrings/","text":"Automatic documentation generated from docstrings Channel dataclass Source code in mass2/core/channel.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 @dataclass ( frozen = True ) # noqa: PLR0904 class Channel : df : pl . DataFrame = field ( repr = False ) header : ChannelHeader = field ( repr = True ) npulses : int subframediv : int | None = None noise : NoiseChannel | None = field ( default = None , repr = False ) good_expr : pl . Expr = field ( default_factory = alwaysTrue ) df_history : list [ pl . DataFrame ] = field ( default_factory = list , repr = False ) steps : Recipe = field ( default_factory = Recipe . new_empty , repr = False ) steps_elapsed_s : list [ float ] = field ( default_factory = list ) transform_raw : Callable | None = None @property def shortname ( self ) -> str : return self . header . description def mo_stepplots ( self ) -> mo . ui . dropdown : desc_ind = { step . description : i for i , step in enumerate ( self . steps )} first_non_summarize_step = self . steps [ 0 ] for step in self . steps : if isinstance ( step , SummarizeStep ): continue first_non_summarize_step = step break mo_ui = mo . ui . dropdown ( desc_ind , value = first_non_summarize_step . description , label = f \"choose step for ch { self . header . ch_num } \" , ) def show () -> mo . Html : return self . _mo_stepplots_explicit ( mo_ui ) def step_ind () -> Any : return mo_ui . value mo_ui . show = show mo_ui . step_ind = step_ind return mo_ui def _mo_stepplots_explicit ( self , mo_ui : mo . ui . dropdown ) -> mo . Html : step_ind = mo_ui . value self . step_plot ( step_ind ) fig = plt . gcf () return mo . vstack ([ mo_ui , misc . show ( fig )]) def get_step ( self , index : int ) -> tuple [ RecipeStep , int ]: if index < 0 : # normalize the index to a positive index index = len ( self . steps ) + index step = self . steps [ index ] return step , index def step_plot ( self , step_ind : int , ** kwargs : Any ) -> plt . Axes : step , step_ind = self . get_step ( step_ind ) if step_ind + 1 == len ( self . df_history ): df_after = self . df else : df_after = self . df_history [ step_ind + 1 ] return step . dbg_plot ( df_after , ** kwargs ) def hist ( self , col : str , bin_edges : ArrayLike , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col )) . collect () values = df_small [ col ] bin_centers , counts = misc . hist_of_series ( values , bin_edges ) return bin_centers , counts def plot_hist ( self , col : str , bin_edges : ArrayLike , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis bin_centers , counts = self . hist ( col , bin_edges = bin_edges , use_good_expr = use_good_expr , use_expr = use_expr ) _ , step_size = misc . midpoints_and_step_size ( bin_edges ) plt . step ( bin_centers , counts , where = \"mid\" ) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } for { self . shortname } \" ) plt . tight_layout () return bin_centers , counts def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : str , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), skip_none : bool = True , ) -> tuple [ NDArray , dict [ str , NDArray ]]: \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group counts_dict : dict [ str , NDArray ] = {} for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] _ , step_size = misc . midpoints_and_step_size ( bin_edges ) bin_centers , counts = misc . hist_of_series ( values , bin_edges ) group_name_str = str ( group_name ) counts_dict [ group_name_str ] = counts plt . step ( bin_centers , counts , where = \"mid\" , label = group_name_str ) # Plot the histogram for the current group # if group_name == \"EBIT\": # ax.hist(values, bins=bin_edges, alpha=0.9, color=\"k\", label=group_name_str) # else: # ax.hist(values, bins=bin_edges, alpha=0.5, label=group_name_str) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () return bin_centers , counts_dict def plot_scatter ( self , x_col : str , y_col : str , color_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), use_good_expr : bool = True , skip_none : bool = True , ax : plt . Axes | None = None , ) -> None : if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # set current axis so I can use plt api if use_good_expr : filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = self . good_expr df_small = self . df . lazy () . filter ( filter_expr ) . select ( x_col , y_col , color_col ) . collect () for ( name ,), data in df_small . group_by ( color_col , maintain_order = True ): if name is None and skip_none and color_col is not None : continue plt . plot ( data . select ( x_col ) . to_series (), data . select ( y_col ) . to_series (), \".\" , label = name , ) plt . xlabel ( str ( x_col )) plt . ylabel ( str ( y_col )) title_str = f \"\"\" { self . header . description } use_expr= { str ( use_expr ) } good_expr= { str ( self . good_expr ) } \"\"\" plt . title ( title_str ) if color_col is not None : plt . legend ( title = color_col ) plt . tight_layout () def good_series ( self , col : str , use_expr : pl . Expr = pl . lit ( True )) -> pl . Series : return mass2 . misc . good_series ( self . df , col , self . good_expr , use_expr ) def rough_cal_combinatoric ( self , line_names : list [ str ], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : step = mass2 . core . RoughCalibrationStep . learn_combinatoric ( self , line_names , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal_combinatoric_height_info ( self , line_names : list [ str ], line_heights_allowed : list [ list [ int ]], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : step = mass2 . core . RoughCalibrationStep . learn_combinatoric_height_info ( self , line_names , line_heights_allowed , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal ( # noqa: PLR0917 self , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"Channel\" : step = mass2 . core . RoughCalibrationStep . learn_3peak ( self , line_names , uncalibrated_col , calibrated_col , use_expr , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , fwhm_pulse_height_units , n_extra_peaks , acceptable_rms_residual_e , ) return self . with_step ( step ) def with_step ( self , step : RecipeStep ) -> \"Channel\" : t_start = time . time () df2 = step . calc_from_df ( self . df ) elapsed_s = time . time () - t_start ch2 = dataclasses . replace ( self , df = df2 , good_expr = step . good_expr , df_history = self . df_history + [ self . df ], steps = self . steps . with_step ( step ), steps_elapsed_s = self . steps_elapsed_s + [ elapsed_s ], ) return ch2 def with_steps ( self , steps : Recipe ) -> \"Channel\" : ch2 = self for step in steps : ch2 = ch2 . with_step ( step ) return ch2 def with_good_expr ( self , good_expr : pl . Expr , replace : bool = False ) -> \"Channel\" : # the default value of self.good_expr is pl.lit(True) # and_(True) will just add visual noise when looking at good_expr and not affect behavior if not replace and good_expr is not True and not good_expr . meta . eq ( pl . lit ( True )): good_expr = good_expr . and_ ( self . good_expr ) return dataclasses . replace ( self , good_expr = good_expr ) def with_column_map_step ( self , input_col : str , output_col : str , f : Callable ) -> \"Channel\" : \"\"\"f should take a numpy array and return a numpy array with the same number of elements\"\"\" step = mass2 . core . recipe . ColumnAsNumpyMapStep ([ input_col ], [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), f = f ) return self . with_step ( step ) def with_good_expr_pretrig_rms_and_postpeak_deriv ( self , n_sigma_pretrig_rms : float = 20 , n_sigma_postpeak_deriv : float = 20 , replace : bool = False ) -> \"Channel\" : max_postpeak_deriv = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"postpeak_deriv\" ] . to_numpy (), nsigma = n_sigma_postpeak_deriv ) max_pretrig_rms = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"pretrig_rms\" ] . to_numpy (), nsigma = n_sigma_pretrig_rms ) good_expr = ( pl . col ( \"postpeak_deriv\" ) < max_postpeak_deriv ) . and_ ( pl . col ( \"pretrig_rms\" ) < max_pretrig_rms ) return self . with_good_expr ( good_expr , replace ) def with_range_around_median ( self , col : str , range_up : float , range_down : float ) -> \"Channel\" : med = np . median ( self . df [ col ] . to_numpy ()) return self . with_good_expr ( pl . col ( col ) . is_between ( med - range_down , med + range_up )) def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) @functools . cache def typical_peak_ind ( self , col : str = \"pulse\" ) -> int : raw = self . df . limit ( 100 )[ col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) return int ( np . median ( raw . argmax ( axis = 1 ))) def summarize_pulses ( self , col : str = \"pulse\" , pretrigger_ignore_samples : int = 0 , peak_index : int | None = None ) -> \"Channel\" : if peak_index is None : peak_index = self . typical_peak_ind ( col ) out_names = mass2 . core . pulse_algorithms . result_dtype . names # mypy (incorrectly) thinks `out_names` might be None, and `list(None)` is forbidden. Assertion makes it happy again. assert out_names is not None outputs = list ( out_names ) step = SummarizeStep ( inputs = [ col ], output = outputs , good_expr = self . good_expr , use_expr = pl . lit ( True ), frametime_s = self . header . frametime_s , peak_index = peak_index , pulse_col = col , pretrigger_ignore_samples = pretrigger_ignore_samples , n_presamples = self . header . n_presamples , transform_raw = self . transform_raw , ) return self . with_step ( step ) def correct_pretrig_mean_jumps ( self , uncorrected : str = \"pretrig_mean\" , corrected : str = \"ptm_jf\" , period : int = 4096 ) -> \"Channel\" : step = mass2 . core . recipe . PretrigMeanJumpFixStep ( inputs = [ uncorrected ], output = [ corrected ], good_expr = self . good_expr , use_expr = pl . lit ( True ), period = period , ) return self . with_step ( step ) def with_select_step ( self , col_expr_dict : dict [ str , pl . Expr ]) -> \"Channel\" : \"\"\" This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. \"\"\" extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in col_expr_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . SelectStep ( inputs = list ( inputs ), output = list ( col_expr_dict . keys ()), good_expr = self . good_expr , use_expr = pl . lit ( True ), col_expr_dict = col_expr_dict , ) return self . with_step ( step ) def with_categorize_step ( self , category_condition_dict : dict [ str , pl . Expr ], output_col : str = \"category\" ) -> \"Channel\" : # ensure the first condition is True, to be used as a fallback first_expr = next ( iter ( category_condition_dict . values ())) if not first_expr . meta . eq ( pl . lit ( True )): category_condition_dict = { \"fallback\" : pl . lit ( True ), ** category_condition_dict } extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in category_condition_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . CategorizeStep ( inputs = list ( inputs ), output = [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), category_condition_dict = category_condition_dict , ) return self . with_step ( step ) def filter5lag ( self , pulse_col : str = \"pulse\" , peak_y_col : str = \"5lagy\" , peak_x_col : str = \"5lagx\" , f_3db : float = 25e3 , use_expr : pl . Expr = pl . lit ( True ), time_constant_s_of_exp_to_be_orthogonal_to : float | None = None , ) -> \"Channel\" : avg_pulse = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( pulse_col ) . limit ( 2000 ) . collect () . to_series () . to_numpy () . mean ( axis = 0 ) ) avg_pulse -= avg_pulse [: self . header . n_presamples ] . mean () assert self . noise spectrum5lag = self . noise . spectrum ( trunc_front = 2 , trunc_back = 2 ) filter_maker = FilterMaker ( signal_model = avg_pulse , n_pretrigger = self . header . n_presamples , noise_psd = spectrum5lag . psd , noise_autocorr = spectrum5lag . autocorr_vec , sample_time_sec = self . header . frametime_s , ) if time_constant_s_of_exp_to_be_orthogonal_to is None : filter5lag = filter_maker . compute_5lag ( f_3db = f_3db ) else : filter5lag = filter_maker . compute_5lag_noexp ( f_3db = f_3db , exp_time_seconds = time_constant_s_of_exp_to_be_orthogonal_to ) step = Filter5LagStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter5lag , spectrum = spectrum5lag , filter_maker = filter_maker , transform_raw = self . transform_raw , ) return self . with_step ( step ) def good_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : good_df = self . df . lazy () . filter ( self . good_expr ) if use_expr is not True : good_df = good_df . filter ( use_expr ) return good_df . select ( cols ) . collect () def bad_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : bad_df = self . df . lazy () . filter ( self . good_expr . not_ ()) if use_expr is not True : bad_df = bad_df . filter ( use_expr ) return bad_df . select ( cols ) . collect () def good_serieses ( self , cols : list [ str ], use_expr : pl . Expr = pl . lit ( True )) -> list [ pl . Series ]: df2 = self . good_df ( cols , use_expr ) return [ df2 [ col ] for col in cols ] def driftcorrect ( self , indicator_col : str = \"pretrig_mean\" , uncorrected_col : str = \"5lagy\" , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : # by defining a seperate learn method that takes ch as an argument, # we can move all the code for the step outside of Channel step = DriftCorrectStep . learn ( ch = self , indicator_col = indicator_col , uncorrected_col = uncorrected_col , corrected_col = corrected_col , use_expr = use_expr , ) return self . with_step ( step ) def linefit ( # noqa: PLR0917 self , line : GenericLineModel | SpectralLine | str | float , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = self . header . description , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def step_summary ( self ) -> list [ tuple [ str , float ]]: return [( type ( a ) . __name__ , b ) for ( a , b ) in zip ( self . steps , self . steps_elapsed_s )] def __hash__ ( self ) -> int : # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other : object ) -> bool : # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results # only checks if the ids match, does not try to be equal if all contents are equal return id ( self ) == id ( other ) @classmethod def from_ljh ( cls , path : str | Path , noise_path : str | Path | None = None , keep_posix_usec : bool = False , transform_raw : Callable | None = None , ) -> \"Channel\" : if not noise_path : noise_channel = None else : noise_channel = NoiseChannel . from_ljh ( noise_path ) ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars ( keep_posix_usec ) header = ChannelHeader . from_ljh_header_df ( header_df ) channel = cls ( df , header = header , npulses = ljh . npulses , subframediv = ljh . subframediv , noise = noise_channel , transform_raw = transform_raw ) return channel @classmethod def from_off ( cls , off : OffFile ) -> \"Channel\" : assert off . _mmap is not None df = pl . from_numpy ( np . asarray ( off . _mmap )) df = ( df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) . select ( pl . exclude ( \"unixnano\" )) ) df_header = pl . DataFrame ( off . header ) df_header = df_header . with_columns ( pl . Series ( \"Filename\" , [ off . filename ])) header = ChannelHeader ( f \" { os . path . split ( off . filename )[ 1 ] } \" , off . header [ \"ChannelNumberMatchingName\" ], off . framePeriodSeconds , off . _mmap [ \"recordPreSamples\" ][ 0 ], off . _mmap [ \"recordSamples\" ][ 0 ], df_header , ) channel = cls ( df , header , off . nRecords , subframediv = off . subframediv ) return channel def with_experiment_state_df ( self , df_es : pl . DataFrame , force_timestamp_monotonic : bool = False ) -> \"Channel\" : if not self . df [ \"timestamp\" ] . is_sorted (): df = self . df . select ( pl . col ( \"timestamp\" ) . cum_max () . alias ( \"timestamp\" )) . with_columns ( self . df . select ( pl . exclude ( \"timestamp\" ))) # print(\"WARNING: in with_experiment_state_df, timestamp is not monotonic, forcing it to be\") # print(\"This is likely a BUG in DASTARD.\") else : df = self . df df2 = df . join_asof ( df_es , on = \"timestamp\" , strategy = \"backward\" ) return self . with_replacement_df ( df2 ) def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channel\" : df2 = ( self . df . with_columns ( subframecount = pl . col ( \"framecount\" ) * self . subframediv ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"backward\" , coalesce = False , suffix = \"_prev_ext_trig\" ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"forward\" , coalesce = False , suffix = \"_next_ext_trig\" ) ) return self . with_replacement_df ( df2 ) def with_replacement_df ( self , df2 : pl . DataFrame ) -> \"Channel\" : return dataclasses . replace ( self , df = df2 , ) def with_columns ( self , df2 : pl . DataFrame ) -> \"Channel\" : df3 = self . df . with_columns ( df2 ) return self . with_replacement_df ( df3 ) def multifit_quadratic_gain_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : step = MultiFitQuadraticGainStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def multifit_mass_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : step = MultiFitMassCalibrationStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def concat_df ( self , df : pl . DataFrame ) -> \"Channel\" : ch2 = Channel ( mass2 . core . misc . concat_dfs_with_concat_state ( self . df , df ), self . header , self . npulses , subframediv = self . subframediv , noise = self . noise , good_expr = self . good_expr , ) # we won't copy over df_history and steps. I don't think you should use this when those are filled in? return ch2 def concat_ch ( self , ch : \"Channel\" ) -> \"Channel\" : ch2 = self . concat_df ( ch . df ) return ch2 def phase_correct_mass_specific_lines ( self , indicator_col : str , uncorrected_col : str , line_names : Iterable [ str | float ], previous_cal_step_index : int , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : if corrected_col is None : corrected_col = uncorrected_col + \"_pc\" step = mass2 . core . phase_correct_steps . phase_correct_mass_specific_lines ( self , indicator_col , uncorrected_col , corrected_col , previous_cal_step_index , line_names , use_expr , ) return self . with_step ( step ) def as_bad ( self , error_type : type | None , error_msg : str , backtrace : str | None ) -> \"BadChannel\" : return BadChannel ( self , error_type , error_msg , backtrace ) def save_recipes ( self , filename : str ) -> dict [ int , Recipe ]: steps = { self . header . ch_num : self . steps } misc . pickle_object ( steps , filename ) return steps def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr_in : pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample : int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log : bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) contents , _ , _ = plt . hist ( y , 200 , range = limits , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" ) def fit_pulse ( self , index : int = 0 , col : str = \"pulse\" , verbose : bool = True ) -> LineModelResult : pulse = self . df [ col ][ index ] . to_numpy () result = mass2 . core . pulse_algorithms . fit_pulse_2exp_with_tail ( pulse , npre = self . header . n_presamples , dt = self . header . frametime_s ) if verbose : print ( f \"ch= { self } \" ) print ( f \"pulse index= { index } \" ) print ( result . fit_report ()) return result plot_hists ( col , bin_edges , group_by_col , axis = None , use_good_expr = True , use_expr = pl . lit ( True ), skip_none = True ) Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channel.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : str , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), skip_none : bool = True , ) -> tuple [ NDArray , dict [ str , NDArray ]]: \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group counts_dict : dict [ str , NDArray ] = {} for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] _ , step_size = misc . midpoints_and_step_size ( bin_edges ) bin_centers , counts = misc . hist_of_series ( values , bin_edges ) group_name_str = str ( group_name ) counts_dict [ group_name_str ] = counts plt . step ( bin_centers , counts , where = \"mid\" , label = group_name_str ) # Plot the histogram for the current group # if group_name == \"EBIT\": # ax.hist(values, bins=bin_edges, alpha=0.9, color=\"k\", label=group_name_str) # else: # ax.hist(values, bins=bin_edges, alpha=0.5, label=group_name_str) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () return bin_centers , counts_dict plot_summaries ( use_expr_in = None , downsample = None , log = False ) Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters: use_expr_in ( Expr | None , default: None ) \u2013 A polars expression to determine valid pulses, by default None. If None, use self.good_expr downsample ( int | None , default: None ) \u2013 Plot only every one of downsample pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log ( bool , default: False ) \u2013 Whether to make the histograms have a logarithmic y-scale, by default False. Source code in mass2/core/channel.py 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr_in : pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample : int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log : bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) contents , _ , _ = plt . hist ( y , 200 , range = limits , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" ) with_column_map_step ( input_col , output_col , f ) f should take a numpy array and return a numpy array with the same number of elements Source code in mass2/core/channel.py 359 360 361 362 def with_column_map_step ( self , input_col : str , output_col : str , f : Callable ) -> \"Channel\" : \"\"\"f should take a numpy array and return a numpy array with the same number of elements\"\"\" step = mass2 . core . recipe . ColumnAsNumpyMapStep ([ input_col ], [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), f = f ) return self . with_step ( step ) with_good_expr_below_nsigma_outlier_resistant ( col_nsigma_pairs , replace = False , use_prev_good_expr = True ) always sets lower limit at 0, don't use for values that can be negative Source code in mass2/core/channel.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) with_good_expr_nsigma_range_outlier_resistant ( col_nsigma_pairs , replace = False , use_prev_good_expr = True ) always sets lower limit at 0, don't use for values that can be negative Source code in mass2/core/channel.py 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) with_select_step ( col_expr_dict ) This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. Source code in mass2/core/channel.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def with_select_step ( self , col_expr_dict : dict [ str , pl . Expr ]) -> \"Channel\" : \"\"\" This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. \"\"\" extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in col_expr_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . SelectStep ( inputs = list ( inputs ), output = list ( col_expr_dict . keys ()), good_expr = self . good_expr , use_expr = pl . lit ( True ), col_expr_dict = col_expr_dict , ) return self . with_step ( step ) Channels dataclass Source code in mass2/core/channels.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 @dataclass ( frozen = True ) # noqa: PLR0904 class Channels : channels : dict [ int , Channel ] description : str bad_channels : dict [ int , BadChannel ] = field ( default_factory = dict ) @property def ch0 ( self ) -> Channel : assert len ( self . channels ) > 0 , \"channels must be non-empty\" return next ( iter ( self . channels . values ())) @functools . cache def dfg ( self , exclude : str = \"pulse\" ) -> pl . DataFrame : # return a dataframe containing good pulses from each channel, # exluding \"pulse\" by default # and including columns \"key\" (to be removed?) and \"ch_num\" # the more common call should be to wrap this in a convenient plotter dfs = [] for ch_num , channel in self . channels . items (): df = channel . df . select ( pl . exclude ( exclude )) . filter ( channel . good_expr ) # key_series = pl.Series(\"key\", dtype=pl.Int64).extend_constant(key, len(df)) assert ch_num == channel . header . ch_num ch_series = pl . Series ( \"ch_num\" , dtype = pl . Int64 ) . extend_constant ( channel . header . ch_num , len ( df )) dfs . append ( df . with_columns ( ch_series )) return pl . concat ( dfs ) def linefit ( # noqa: PLR0917 self , line : float | str | SpectralLine | GenericLineModel , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = mass2 . misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = f \" { len ( self . channels ) } channels, { self . description } \" , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def plot_hist ( self , col : str , bin_edges : ArrayLike , use_expr : pl . Expr = pl . lit ( True ), axis : plt . Axes | None = None ) -> None : df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () ax = mass2 . misc . plot_hist_of_series ( df_small [ col ], bin_edges , axis ) ax . set_title ( f \" { len ( self . channels ) } channels, { self . description } \" ) def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : bool , axis : plt . Axes | None = None , use_expr : pl . Expr | None = None , skip_none : bool = True , ) -> None : \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () def map ( self , f : Callable , allow_throw : bool = False ) -> \"Channels\" : new_channels = {} new_bad_channels = {} for key , channel in self . channels . items (): try : new_channels [ key ] = f ( channel ) except KeyboardInterrupt : raise except Exception as ex : error_type : type = type ( ex ) error_message : str = str ( ex ) backtrace : str = traceback . format_exc () if allow_throw : raise print ( f \" { key =} { channel =} failed this step\" ) print ( f \" { error_type =} \" ) print ( f \" { error_message =} \" ) new_bad_channels [ key ] = channel . as_bad ( error_type , error_message , backtrace ) new_bad_channels = mass2 . misc . merge_dicts_ordered_by_keys ( self . bad_channels , new_bad_channels ) return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def set_bad ( self , ch_num : int , msg : str , require_ch_num_exists : bool = True ) -> \"Channels\" : new_channels = {} new_bad_channels = {} if require_ch_num_exists : assert ch_num in self . channels . keys (), f \" { ch_num } can't be set bad because it does not exist\" for key , channel in self . channels . items (): if key == ch_num : new_bad_channels [ key ] = channel . as_bad ( None , msg , None ) else : new_channels [ key ] = channel return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def linefit_joblib ( self , line : str , col : str , prefer : str = \"threads\" , n_jobs : int = 4 ) -> LineModelResult : def work ( key : int ) -> LineModelResult : channel = self . channels [ key ] return channel . linefit ( line , col ) parallel = joblib . Parallel ( n_jobs = n_jobs , prefer = prefer ) # its not clear if threads are better.... what blocks the gil? results = parallel ( joblib . delayed ( work )( key ) for key in self . channels . keys ()) return results def __hash__ ( self ) -> int : # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other : Any ) -> bool : return id ( self ) == id ( other ) @classmethod def from_ljh_path_pairs ( cls , pulse_noise_pairs : list [ tuple [ str , str ]], description : str ) -> \"Channels\" : \"\"\" Create a :class:`Channels` instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of `(pulse_path, noise_path)` tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class:`Channel` per `(pulse_path, noise_path)` pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth:`Channel.from_ljh`. The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] \"\"\" channels : dict [ int , Channel ] = {} for pulse_path , noise_path in pulse_noise_pairs : channel = Channel . from_ljh ( pulse_path , noise_path ) assert channel . header . ch_num not in channels . keys () channels [ channel . header . ch_num ] = channel return cls ( channels , description ) @classmethod def from_off_paths ( cls , off_paths : Iterable [ str | Path ], description : str ) -> \"Channels\" : channels = {} for path in off_paths : ch = Channel . from_off ( mass2 . core . OffFile ( str ( path ))) channels [ ch . header . ch_num ] = ch return cls ( channels , description ) @classmethod def from_ljh_folder ( cls , pulse_folder : str , noise_folder : str | None = None , limit : int | None = None , exclude_ch_nums : list [ int ] | None = None , ) -> \"Channels\" : assert os . path . isdir ( pulse_folder ), f \" { pulse_folder =} { noise_folder =} \" if exclude_ch_nums is None : exclude_ch_nums = [] if noise_folder is None : paths = ljhutil . find_ljh_files ( pulse_folder , exclude_ch_nums = exclude_ch_nums ) if limit is not None : paths = paths [: limit ] pairs = [( path , \"\" ) for path in paths ] else : assert os . path . isdir ( noise_folder ), f \" { pulse_folder =} { noise_folder =} \" pairs = ljhutil . match_files_by_channel ( pulse_folder , noise_folder , limit = limit , exclude_ch_nums = exclude_ch_nums ) description = f \"from_ljh_folder { pulse_folder =} { noise_folder =} \" print ( f \" { description } \" ) print ( f \" from_ljh_folder has { len ( pairs ) } pairs\" ) data = cls . from_ljh_path_pairs ( pairs , description ) print ( f \" and the Channels obj has { len ( data . channels ) } pairs\" ) return data def get_an_ljh_path ( self ) -> Path : return pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) def get_path_in_output_folder ( self , filename : str | Path ) -> Path : ljh_path = self . get_an_ljh_path () base_name , _ = ljh_path . name . split ( \"_chan\" ) date , run_num = base_name . split ( \"_run\" ) # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") output_dir = ljh_path . parent . parent / f \" { run_num } moss_output\" output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir / filename def get_experiment_state_df ( self , experiment_state_path : str | Path | None = None ) -> pl . DataFrame : if experiment_state_path is None : ljh_path = self . get_an_ljh_path () experiment_state_path = ljhutil . experiment_state_path_from_ljh_path ( ljh_path ) df = pl . read_csv ( experiment_state_path , new_columns = [ \"unixnano\" , \"state_label\" ]) # _col0, _col1 = df.columns df_es = df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) # strip whitespace from state_label column sl_series = df . select ( pl . col ( \"state_label\" ) . str . strip_chars ()) . to_series () df_es = df_es . with_columns ( state_label = pl . Series ( values = sl_series , dtype = pl . Categorical )) return df_es def with_experiment_state_by_path ( self , experiment_state_path : str | None = None ) -> \"Channels\" : df_es = self . get_experiment_state_df ( experiment_state_path ) return self . with_experiment_state_df ( df_es ) def with_external_trigger_by_path ( self , path : str | None = None ) -> \"Channels\" : if path is None : raise NotImplementedError ( \"cannot infer external trigger path yet\" ) with open ( path , \"rb\" ) as _f : _header_line = _f . readline () # read the one header line before opening the binary data external_trigger_subframe_count = np . fromfile ( _f , \"int64\" ) df_ext = pl . DataFrame ({ \"subframecount\" : external_trigger_subframe_count , }) return self . with_external_trigger_df ( df_ext ) def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channels\" : def with_etrig_df ( channel : Channel ) -> Channel : return channel . with_external_trigger_df ( df_ext ) return self . map ( with_etrig_df ) def with_experiment_state_df ( self , df_es : pl . DataFrame ) -> \"Channels\" : # this is not as performant as making use_exprs for states # and using .set_sorted on the timestamp column ch2s = {} for ch_num , ch in self . channels . items (): ch2s [ ch_num ] = ch . with_experiment_state_df ( df_es ) return Channels ( ch2s , self . description ) def with_steps_dict ( self , steps_dict : dict [ int , Recipe ]) -> \"Channels\" : def load_recipes ( channel : Channel ) -> Channel : try : steps = steps_dict [ channel . header . ch_num ] except KeyError : raise Exception ( \"steps dict did not contain steps for this ch_num\" ) return channel . with_steps ( steps ) return self . map ( load_recipes ) def save_recipes ( self , filename : str , required_fields : str | Iterable [ str ] | None = None , drop_debug : bool = True ) -> dict [ int , Recipe ]: \"\"\"Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set `required_fields` to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters ---------- filename : str Filename to store recipe in, typically of the form \"*.pkl\" required_fields : str | Iterable[str] | None The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug: bool Whether to remove debugging-related data from each `RecipeStep`, if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns ------- dict Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. \"\"\" steps = {} for channum , ch in self . channels . items (): steps [ channum ] = ch . steps . trim_dead_ends ( required_fields = required_fields , drop_debug = drop_debug ) mass2 . misc . pickle_object ( steps , filename ) return steps def load_recipes ( self , filename : str ) -> \"Channels\" : steps = mass2 . misc . unpickle_object ( filename ) return self . with_steps_dict ( steps ) def parent_folder_path ( self ) -> pathlib . Path : parent_folder_path = pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) . parent . parent print ( f \" { parent_folder_path =} \" ) return parent_folder_path def concat_data ( self , other_data : \"Channels\" ) -> \"Channels\" : # sorting here to show intention, but I think set is sorted by insertion order as # an implementation detail so this may not do anything ch_nums = sorted ( list ( set ( self . channels . keys ()) . intersection ( other_data . channels . keys ()))) new_channels = {} for ch_num in ch_nums : ch = self . channels [ ch_num ] other_ch = other_data . channels [ ch_num ] combined_df = mass2 . core . misc . concat_dfs_with_concat_state ( ch . df , other_ch . df ) new_ch = ch . with_replacement_df ( combined_df ) new_channels [ ch_num ] = new_ch return mass2 . Channels ( new_channels , self . description + other_data . description ) @classmethod def from_df ( cls , df_in : pl . DataFrame , frametime_s : float , n_presamples : int , n_samples : int , description : str = \"from Channels.channels_from_df\" , ) -> \"Channels\" : # requres a column named \"ch_num\" containing the channel number keys_df : dict [ tuple , pl . DataFrame ] = df_in . partition_by ( by = [ \"ch_num\" ], as_dict = True ) dfs : dict [ int , pl . DataFrame ] = { keys [ 0 ]: df for ( keys , df ) in keys_df . items ()} channels : dict [ int , Channel ] = {} for ch_num , df in dfs . items (): channels [ ch_num ] = Channel ( df , header = ChannelHeader ( description = \"from df\" , ch_num = ch_num , frametime_s = frametime_s , n_presamples = n_presamples , n_samples = n_samples , df = df , ), npulses = len ( df ), ) return Channels ( channels , description ) from_ljh_path_pairs ( pulse_noise_pairs , description ) classmethod Create a :class: Channels instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of (pulse_path, noise_path) tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class: Channel per (pulse_path, noise_path) pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth: Channel.from_ljh . The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] Source code in mass2/core/channels.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 @classmethod def from_ljh_path_pairs ( cls , pulse_noise_pairs : list [ tuple [ str , str ]], description : str ) -> \"Channels\" : \"\"\" Create a :class:`Channels` instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of `(pulse_path, noise_path)` tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class:`Channel` per `(pulse_path, noise_path)` pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth:`Channel.from_ljh`. The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] \"\"\" channels : dict [ int , Channel ] = {} for pulse_path , noise_path in pulse_noise_pairs : channel = Channel . from_ljh ( pulse_path , noise_path ) assert channel . header . ch_num not in channels . keys () channels [ channel . header . ch_num ] = channel return cls ( channels , description ) plot_hists ( col , bin_edges , group_by_col , axis = None , use_expr = None , skip_none = True ) Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channels.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : bool , axis : plt . Axes | None = None , use_expr : pl . Expr | None = None , skip_none : bool = True , ) -> None : \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () save_recipes ( filename , required_fields = None , drop_debug = True ) Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set required_fields to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters: filename ( str ) \u2013 Filename to store recipe in, typically of the form \"*.pkl\" required_fields ( str | Iterable [ str ] | None , default: None ) \u2013 The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug ( bool , default: True ) \u2013 Whether to remove debugging-related data from each RecipeStep , if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns: dict \u2013 Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. Source code in mass2/core/channels.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def save_recipes ( self , filename : str , required_fields : str | Iterable [ str ] | None = None , drop_debug : bool = True ) -> dict [ int , Recipe ]: \"\"\"Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set `required_fields` to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters ---------- filename : str Filename to store recipe in, typically of the form \"*.pkl\" required_fields : str | Iterable[str] | None The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug: bool Whether to remove debugging-related data from each `RecipeStep`, if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns ------- dict Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. \"\"\" steps = {} for channum , ch in self . channels . items (): steps [ channum ] = ch . steps . trim_dead_ends ( required_fields = required_fields , drop_debug = drop_debug ) mass2 . misc . pickle_object ( steps , filename ) return steps mass2.core.analysis_algorithms - main algorithms used in data analysis Designed to abstract certain key algorithms out of the class MicrocalDataSet and be able to run them fast. Created on Jun 9, 2014 @author: fowlerj HistogramSmoother Object that can repeatedly smooth histograms with the same bin count and width to the same Gaussian width. By pre-computing the smoothing kernel for that histogram, we can smooth multiple histograms with the same geometry. Source code in mass2/core/analysis_algorithms.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 class HistogramSmoother : \"\"\"Object that can repeatedly smooth histograms with the same bin count and width to the same Gaussian width. By pre-computing the smoothing kernel for that histogram, we can smooth multiple histograms with the same geometry. \"\"\" def __init__ ( self , smooth_sigma : float , limits : ArrayLike ): \"\"\"Give the smoothing Gaussian's width as <smooth_sigma> and the [lower,upper] histogram limits as <limits>.\"\"\" self . limits = tuple ( np . asarray ( limits , dtype = float )) self . smooth_sigma = smooth_sigma # Choose a reasonable # of bins, at least 1024 and a power of 2 stepsize = 0.4 * smooth_sigma dlimits = self . limits [ 1 ] - self . limits [ 0 ] nbins_guess = int ( dlimits / stepsize + 0.5 ) min_nbins = 1024 max_nbins = 32768 # 32k bins, 2**15 # Clamp nbins_guess to at least min_nbins clamped_nbins = np . clip ( nbins_guess , min_nbins , max_nbins ) nbins_forced_to_power_of_2 = int ( 2 ** np . ceil ( np . log2 ( clamped_nbins ))) if nbins_forced_to_power_of_2 == max_nbins : print ( f \"Warning: HistogramSmoother (for drift correct) Limiting histogram bins to { max_nbins } (requested { nbins_guess } )\" ) self . nbins = nbins_forced_to_power_of_2 self . stepsize = dlimits / self . nbins # Compute the Fourier-space smoothing kernel kernel = np . exp ( - 0.5 * ( np . arange ( self . nbins ) * self . stepsize / self . smooth_sigma ) ** 2 ) kernel [ 1 :] += kernel [ - 1 : 0 : - 1 ] # Handle the negative frequencies kernel /= kernel . sum () self . kernel_ft = np . fft . rfft ( kernel ) def __call__ ( self , values : ArrayLike ) -> NDArray : \"\"\"Return a smoothed histogram of the data vector <values>\"\"\" contents , _ = np . histogram ( values , self . nbins , self . limits ) ftc = np . fft . rfft ( contents ) csmooth = np . fft . irfft ( self . kernel_ft * ftc ) csmooth [ csmooth < 0 ] = 0 return csmooth __call__ ( values ) Return a smoothed histogram of the data vector Source code in mass2/core/analysis_algorithms.py 215 216 217 218 219 220 221 def __call__ ( self , values : ArrayLike ) -> NDArray : \"\"\"Return a smoothed histogram of the data vector <values>\"\"\" contents , _ = np . histogram ( values , self . nbins , self . limits ) ftc = np . fft . rfft ( contents ) csmooth = np . fft . irfft ( self . kernel_ft * ftc ) csmooth [ csmooth < 0 ] = 0 return csmooth __init__ ( smooth_sigma , limits ) Give the smoothing Gaussian's width as and the [lower,upper] histogram limits as . Source code in mass2/core/analysis_algorithms.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def __init__ ( self , smooth_sigma : float , limits : ArrayLike ): \"\"\"Give the smoothing Gaussian's width as <smooth_sigma> and the [lower,upper] histogram limits as <limits>.\"\"\" self . limits = tuple ( np . asarray ( limits , dtype = float )) self . smooth_sigma = smooth_sigma # Choose a reasonable # of bins, at least 1024 and a power of 2 stepsize = 0.4 * smooth_sigma dlimits = self . limits [ 1 ] - self . limits [ 0 ] nbins_guess = int ( dlimits / stepsize + 0.5 ) min_nbins = 1024 max_nbins = 32768 # 32k bins, 2**15 # Clamp nbins_guess to at least min_nbins clamped_nbins = np . clip ( nbins_guess , min_nbins , max_nbins ) nbins_forced_to_power_of_2 = int ( 2 ** np . ceil ( np . log2 ( clamped_nbins ))) if nbins_forced_to_power_of_2 == max_nbins : print ( f \"Warning: HistogramSmoother (for drift correct) Limiting histogram bins to { max_nbins } (requested { nbins_guess } )\" ) self . nbins = nbins_forced_to_power_of_2 self . stepsize = dlimits / self . nbins # Compute the Fourier-space smoothing kernel kernel = np . exp ( - 0.5 * ( np . arange ( self . nbins ) * self . stepsize / self . smooth_sigma ) ** 2 ) kernel [ 1 :] += kernel [ - 1 : 0 : - 1 ] # Handle the negative frequencies kernel /= kernel . sum () self . kernel_ft = np . fft . rfft ( kernel ) compute_max_deriv ( pulse_data , ignore_leading , spike_reject = True , kernel = None ) Computes the maximum derivative in timeseries . can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in . Args: pulse_data: ignore_leading: spike_reject: (default True) kernel: the linear filter against which the signals will be convolved (CONVOLED, not correlated, so reverse the filter as needed). If None, then the default kernel of [+.2 +.1 0 -.1 -.2] will be used. If \"SG\", then the cubic 5-point Savitzky-Golay filter will be used (see below). Otherwise, kernel needs to be a (short) array which will be converted to a 1xN 2-dimensional np.ndarray. (default None) Returns: An np.ndarray, dimension 1: the value of the maximum derivative (units of per sample). When kernel==\"SG\", then we estimate the derivative by Savitzky-Golay filtering (with 1 point before/3 points after the point in question and fitting polynomial of order 3). Find the right general area by first doing a simple difference. Source code in mass2/core/analysis_algorithms.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def compute_max_deriv ( pulse_data : ArrayLike , ignore_leading : int , spike_reject : bool = True , kernel : ArrayLike | str | None = None ) -> NDArray : \"\"\"Computes the maximum derivative in timeseries <pulse_data>. <pulse_data> can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in <pulse_data>. Args: pulse_data: ignore_leading: spike_reject: (default True) kernel: the linear filter against which the signals will be convolved (CONVOLED, not correlated, so reverse the filter as needed). If None, then the default kernel of [+.2 +.1 0 -.1 -.2] will be used. If \"SG\", then the cubic 5-point Savitzky-Golay filter will be used (see below). Otherwise, kernel needs to be a (short) array which will be converted to a 1xN 2-dimensional np.ndarray. (default None) Returns: An np.ndarray, dimension 1: the value of the maximum derivative (units of <pulse_data units> per sample). When kernel==\"SG\", then we estimate the derivative by Savitzky-Golay filtering (with 1 point before/3 points after the point in question and fitting polynomial of order 3). Find the right general area by first doing a simple difference. \"\"\" # If pulse_data is a 1D array, turn it into 2 pulse_data = np . asarray ( pulse_data ) ndim = len ( pulse_data . shape ) if ndim > 2 or ndim < 1 : raise ValueError ( \"input pulse_data should be a 1d or 2d array.\" ) if ndim == 1 : pulse_data . shape = ( 1 , pulse_data . shape [ 0 ]) pulse_view = pulse_data [:, ignore_leading :] NPulse = pulse_view . shape [ 0 ] NSamp = pulse_view . shape [ 1 ] # The default filter: filter_coef = np . array ([ + 0.2 , + 0.1 , 0 , - 0.1 , - 0.2 ]) if kernel == \"SG\" : # This filter is the Savitzky-Golay filter of n_L=1, n_R=3 and M=3, to use the # language of Numerical Recipes 3rd edition. It amounts to least-squares fitting # of an M=3rd order polynomial to the five points [-1,+3] and # finding the slope of the polynomial at 0. # Note that we reverse the order of coefficients because convolution will re-reverse filter_coef = np . array ([ - 0.45238 , - 0.02381 , 0.28571 , 0.30952 , - 0.11905 ])[:: - 1 ] elif kernel is not None : filter_coef = np . array ( kernel ) . ravel () f0 , f1 , f2 , f3 , f4 = filter_coef max_deriv = np . zeros ( NPulse , dtype = np . float64 ) if spike_reject : for i in range ( NPulse ): pulses = pulse_view [ i ] t0 = f4 * pulses [ 0 ] + f3 * pulses [ 1 ] + f2 * pulses [ 2 ] + f1 * pulses [ 3 ] + f0 * pulses [ 4 ] t1 = f4 * pulses [ 1 ] + f3 * pulses [ 2 ] + f2 * pulses [ 3 ] + f1 * pulses [ 4 ] + f0 * pulses [ 5 ] t2 = f4 * pulses [ 2 ] + f3 * pulses [ 3 ] + f2 * pulses [ 4 ] + f1 * pulses [ 5 ] + f0 * pulses [ 6 ] t_max_deriv = t2 if t2 < t0 else t0 for j in range ( 7 , NSamp ): t3 = f4 * pulses [ j - 4 ] + f3 * pulses [ j - 3 ] + f2 * pulses [ j - 2 ] + f1 * pulses [ j - 1 ] + f0 * pulses [ j ] t4 = t3 if t3 < t1 else t1 t_max_deriv = max ( t4 , t_max_deriv ) t0 , t1 , t2 = t1 , t2 , t3 max_deriv [ i ] = t_max_deriv else : for i in range ( NPulse ): pulses = pulse_view [ i ] t0 = f4 * pulses [ 0 ] + f3 * pulses [ 1 ] + f2 * pulses [ 2 ] + f1 * pulses [ 3 ] + f0 * pulses [ 4 ] t_max_deriv = t0 for j in range ( 5 , NSamp ): t0 = f4 * pulses [ j - 4 ] + f3 * pulses [ j - 3 ] + f2 * pulses [ j - 2 ] + f1 * pulses [ j - 1 ] + f0 * pulses [ j ] t_max_deriv = max ( t0 , t_max_deriv ) max_deriv [ i ] = t_max_deriv return np . asarray ( max_deriv , dtype = np . float32 ) correct_flux_jumps ( vals , mask , flux_quant ) Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected Source code in mass2/core/analysis_algorithms.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def correct_flux_jumps ( vals : ArrayLike , mask : ArrayLike , flux_quant : float ) -> NDArray : \"\"\"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected \"\"\" return unwrap_n ( vals , flux_quant , mask ) correct_flux_jumps_original ( vals , mask , flux_quant ) Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected Source code in mass2/core/analysis_algorithms.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def correct_flux_jumps_original ( vals : ArrayLike , mask : ArrayLike , flux_quant : float ) -> NDArray : \"\"\"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected \"\"\" # The naive thing is to simply replace each value with its value mod # the flux quantum. But of the baseline value turns out to fluctuate # about an integer number of flux quanta, this will introduce new # jumps. I don't know the best way to handle this in general. For now, # if there are still jumps after the mod, I add 1/4 of a flux quanta # before modding, then mod, then subtract the 1/4 flux quantum and then # *add* a single flux quantum so that the values never go negative. # # To determine whether there are \"still jumps after the mod\" I look at the # difference between the largest and smallest values for \"good\" pulses. If # you don't exclude \"bad\" pulses, this check can be tricked in cases where # the pretrigger section contains a (sufficiently large) tail. vals = np . asarray ( vals ) mask = np . asarray ( mask ) if ( np . amax ( vals ) - np . amin ( vals )) >= flux_quant : corrected = vals % flux_quant if ( np . amax ( corrected [ mask ]) - np . amin ( corrected [ mask ])) > 0.75 * flux_quant : corrected = ( vals + flux_quant / 4 ) % ( flux_quant ) corrected = corrected - flux_quant / 4 + flux_quant corrected -= corrected [ 0 ] - vals [ 0 ] return corrected else : return vals drift_correct ( indicator , uncorrected , limit = None ) Compute a drift correction that minimizes the spectral entropy. Args: indicator: The \"x-axis\", which indicates the size of the correction. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. limit: The upper limit of uncorrected values over which entropy is computed (default None). Generally indicator will be the pretrigger mean of the pulses, but you can experiment with other choices. The entropy will be computed on corrected values only in the range [0, limit], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then in will be compute as 25% larger than the 99%ile point of uncorrected. The model is that the filtered pulse height PH should be scaled by (1 + a*PTM) where a is an arbitrary parameter computed here, and PTM is the difference between each record's pretrigger mean and the median value of all pretrigger means. (Or replace \"pretrigger mean\" with whatever quantity you passed in as .) Source code in mass2/core/analysis_algorithms.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def drift_correct ( indicator : ArrayLike , uncorrected : ArrayLike , limit : float | None = None ) -> tuple [ float , dict ]: \"\"\"Compute a drift correction that minimizes the spectral entropy. Args: indicator: The \"x-axis\", which indicates the size of the correction. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. limit: The upper limit of uncorrected values over which entropy is computed (default None). Generally indicator will be the pretrigger mean of the pulses, but you can experiment with other choices. The entropy will be computed on corrected values only in the range [0, limit], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then in will be compute as 25% larger than the 99%ile point of uncorrected. The model is that the filtered pulse height PH should be scaled by (1 + a*PTM) where a is an arbitrary parameter computed here, and PTM is the difference between each record's pretrigger mean and the median value of all pretrigger means. (Or replace \"pretrigger mean\" with whatever quantity you passed in as <indicator>.) \"\"\" uncorrected = np . asarray ( uncorrected ) indicator = np . array ( indicator ) # make a copy ptm_offset = np . median ( indicator ) indicator -= ptm_offset if limit is None : pct99 = np . percentile ( uncorrected , 99 ) limit = 1.25 * pct99 smoother = HistogramSmoother ( 0.5 , [ 0 , limit ]) assert smoother . nbins < 1e6 , \"will be crazy slow, should not be possible\" def entropy ( param : NDArray , indicator : NDArray , uncorrected : NDArray , smoother : HistogramSmoother ) -> float : corrected = uncorrected * ( 1 + indicator * param ) hsmooth = smoother ( corrected ) w = hsmooth > 0 return - ( np . log ( hsmooth [ w ]) * hsmooth [ w ]) . sum () drift_corr_param = sp . optimize . brent ( entropy , ( indicator , uncorrected , smoother ), brack = [ 0 , 0.001 ]) drift_correct_info = { \"type\" : \"ptmean_gain\" , \"slope\" : drift_corr_param , \"median_pretrig_mean\" : ptm_offset } return drift_corr_param , drift_correct_info estimateRiseTime ( pulse_data , timebase , nPretrig ) Computes the rise time of timeseries , where the time steps are . can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in . If nPretrig >= 4, then the samples pulse_data[:nPretrig] are averaged to estimate the baseline. Otherwise, the minimum of pulse_data is assumed to be the baseline. Specifically, take the first and last of the rising points in the range of 10% to 90% of the peak value, interpolate a line between the two, and use its slope to find the time to rise from 0 to the peak. Args: pulse_data: An np.ndarray of dimension 1 (a single pulse record) or 2 (an array with each row being a pulse record). timebase: The sampling time. nPretrig: The number of samples that are recorded before the trigger. Returns: An ndarray of dimension 1, giving the rise times. Source code in mass2/core/analysis_algorithms.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @njit def estimateRiseTime ( pulse_data : ArrayLike , timebase : float , nPretrig : int ) -> NDArray : \"\"\"Computes the rise time of timeseries <pulse_data>, where the time steps are <timebase>. <pulse_data> can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in <pulse_data>. If nPretrig >= 4, then the samples pulse_data[:nPretrig] are averaged to estimate the baseline. Otherwise, the minimum of pulse_data is assumed to be the baseline. Specifically, take the first and last of the rising points in the range of 10% to 90% of the peak value, interpolate a line between the two, and use its slope to find the time to rise from 0 to the peak. Args: pulse_data: An np.ndarray of dimension 1 (a single pulse record) or 2 (an array with each row being a pulse record). timebase: The sampling time. nPretrig: The number of samples that are recorded before the trigger. Returns: An ndarray of dimension 1, giving the rise times. \"\"\" MINTHRESH , MAXTHRESH = 0.1 , 0.9 # If pulse_data is a 1D array, turn it into 2 pulse_data = np . asarray ( pulse_data ) ndim = len ( pulse_data . shape ) if ndim > 2 or ndim < 1 : raise ValueError ( \"input pulse_data should be a 1d or 2d array.\" ) if ndim == 1 : pulse_data . shape = ( 1 , pulse_data . shape [ 0 ]) # The following requires a lot of numpy foo to read. Sorry! if nPretrig >= 4 : baseline_value = pulse_data [:, 0 : nPretrig ] . mean ( axis = 1 ) else : baseline_value = pulse_data . min ( axis = 1 ) nPretrig = 0 value_at_peak = pulse_data . max ( axis = 1 ) - baseline_value idx_last_pk = pulse_data . argmax ( axis = 1 ) . max () npulses = pulse_data . shape [ 0 ] try : rising_data = ( pulse_data [:, nPretrig : idx_last_pk + 1 ] - baseline_value [:, np . newaxis ]) / value_at_peak [:, np . newaxis ] # Find the last and first indices at which the data are in (0.1, 0.9] times the # peak value. Then make sure last is at least 1 past first. last_idx = ( rising_data > MAXTHRESH ) . argmax ( axis = 1 ) - 1 first_idx = ( rising_data > MINTHRESH ) . argmax ( axis = 1 ) last_idx [ last_idx < first_idx ] = first_idx [ last_idx < first_idx ] + 1 last_idx [ last_idx == rising_data . shape [ 1 ]] = rising_data . shape [ 1 ] - 1 pulsenum = np . arange ( npulses ) y_diff = np . asarray ( rising_data [ pulsenum , last_idx ] - rising_data [ pulsenum , first_idx ], dtype = float ) y_diff [ y_diff < timebase ] = timebase time_diff = timebase * ( last_idx - first_idx ) rise_time = time_diff / y_diff rise_time [ y_diff <= 0 ] = - 9.9e-6 return rise_time except ValueError : return - 9.9e-6 + np . zeros ( npulses , dtype = float ) filter_signal_lowpass ( sig , fs , fcut ) Tophat lowpass filter using an FFT Args: sig - the signal to be filtered fs - the sampling frequency of the signal fcut - the frequency at which to cutoff the signal Returns: the filtered signal Source code in mass2/core/analysis_algorithms.py 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 @njit def filter_signal_lowpass ( sig : NDArray , fs : float , fcut : float ) -> NDArray : \"\"\"Tophat lowpass filter using an FFT Args: sig - the signal to be filtered fs - the sampling frequency of the signal fcut - the frequency at which to cutoff the signal Returns: the filtered signal \"\"\" N = sig . shape [ 0 ] SIG = np . fft . fft ( sig ) freqs = ( fs / N ) * np . concatenate (( np . arange ( 0 , N / 2 + 1 ), np . arange ( N / 2 - 1 , 0 , - 1 ))) filt = np . zeros_like ( SIG ) filt [ freqs < fcut ] = 1.0 sig_filt = np . fft . ifft ( SIG * filt ) return sig_filt make_smooth_histogram ( values , smooth_sigma , limit , upper_limit = None ) Convert a vector of arbitrary info a smoothed histogram by histogramming it and smoothing. This is a convenience function using the HistogramSmoother class. Args: values: The vector of data to be histogrammed. smooth_sigma: The smoothing Gaussian's width (FWHM) limit, upper_limit: The histogram limits are [limit,upper_limit] or [0,limit] if upper_limit is None. Returns: The smoothed histogram as an array. Source code in mass2/core/analysis_algorithms.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 @njit def make_smooth_histogram ( values : ArrayLike , smooth_sigma : float , limit : float , upper_limit : float | None = None ) -> NDArray : \"\"\"Convert a vector of arbitrary <values> info a smoothed histogram by histogramming it and smoothing. This is a convenience function using the HistogramSmoother class. Args: values: The vector of data to be histogrammed. smooth_sigma: The smoothing Gaussian's width (FWHM) limit, upper_limit: The histogram limits are [limit,upper_limit] or [0,limit] if upper_limit is None. Returns: The smoothed histogram as an array. \"\"\" if upper_limit is None : limit , upper_limit = 0 , limit return HistogramSmoother ( smooth_sigma , [ limit , upper_limit ])( values ) nearest_arrivals ( reference_times , other_times ) Find the external trigger time immediately before and after each pulse timestamp Args: pulse_timestamps - 1d array of pulse timestamps whose nearest neighbors need to be found. external_trigger_timestamps - 1d array of possible nearest neighbors. Returns: (before_times, after_times) before_times is an ndarray of the same size as pulse_timestamps. before_times[i] contains the difference between the closest lesser time contained in external_trigger_timestamps and pulse_timestamps[i] or inf if there was no earlier time in other_times Note that before_times is always a positive number even though the time difference it represents is negative. after_times is an ndarray of the same size as pulse_timestamps. after_times[i] contains the difference between pulse_timestamps[i] and the closest greater time contained in other_times or a inf number if there was no later time in external_trigger_timestamps. Source code in mass2/core/analysis_algorithms.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 @njit def nearest_arrivals ( reference_times : ArrayLike , other_times : ArrayLike ) -> tuple [ NDArray , NDArray ]: \"\"\"Find the external trigger time immediately before and after each pulse timestamp Args: pulse_timestamps - 1d array of pulse timestamps whose nearest neighbors need to be found. external_trigger_timestamps - 1d array of possible nearest neighbors. Returns: (before_times, after_times) before_times is an ndarray of the same size as pulse_timestamps. before_times[i] contains the difference between the closest lesser time contained in external_trigger_timestamps and pulse_timestamps[i] or inf if there was no earlier time in other_times Note that before_times is always a positive number even though the time difference it represents is negative. after_times is an ndarray of the same size as pulse_timestamps. after_times[i] contains the difference between pulse_timestamps[i] and the closest greater time contained in other_times or a inf number if there was no later time in external_trigger_timestamps. \"\"\" other_times = np . asarray ( other_times ) nearest_after_index = np . searchsorted ( other_times , reference_times ) # because both sets of arrival times should be sorted, there are faster algorithms than searchsorted # for example: https://github.com/kwgoodman/bottleneck/issues/47 # we could use one if performance becomes an issue last_index = np . searchsorted ( nearest_after_index , other_times . size , side = \"left\" ) first_index = np . searchsorted ( nearest_after_index , 1 ) nearest_before_index = np . copy ( nearest_after_index ) nearest_before_index [: first_index ] = 1 nearest_before_index -= 1 before_times = reference_times - other_times [ nearest_before_index ] before_times [: first_index ] = np . inf nearest_after_index [ last_index :] = other_times . size - 1 after_times = other_times [ nearest_after_index ] - reference_times after_times [ last_index :] = np . inf return before_times , after_times time_drift_correct ( time , uncorrected , w , sec_per_degree = 2000 , pulses_per_degree = 2000 , max_degrees = 20 , ndeg = None , limit = None ) Compute a time-based drift correction that minimizes the spectral entropy. Args: time: The \"time-axis\". Correction will be a low-order polynomial in this. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. w: the kernel width for the Laplace KDE density estimator sec_per_degree: assign as many as one polynomial degree per this many seconds pulses_per_degree: assign as many as one polynomial degree per this many pulses max_degrees: never use more than this many degrees of Legendre polynomial. n_deg: If not None, use this many degrees, regardless of the values of sec_per_degree, pulses_per_degree, and max_degress. In this case, never downsample. limit: The [lower,upper] limit of uncorrected values over which entropy is computed (default None). The entropy will be computed on corrected values only in the range [limit[0], limit[1]], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then it will be computed as 25%% larger than the 99%%ile point of uncorrected. Possible improvements in the future: * Use Numba to speed up. * Allow the parameters to be function arguments with defaults: photons per degree of freedom, seconds per degree of freedom, and max degrees of freedom. * Figure out how to span the available time with more than one set of legendre polynomials, so that we can have more than 20 d.o.f. eventually, for long runs. Source code in mass2/core/analysis_algorithms.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 def time_drift_correct ( # noqa: PLR0914 time : ArrayLike , uncorrected : ArrayLike , w : float , sec_per_degree : float = 2000 , pulses_per_degree : int = 2000 , max_degrees : int = 20 , ndeg : int | None = None , limit : tuple [ float , float ] | None = None , ) -> dict [ str , Any ]: \"\"\"Compute a time-based drift correction that minimizes the spectral entropy. Args: time: The \"time-axis\". Correction will be a low-order polynomial in this. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. w: the kernel width for the Laplace KDE density estimator sec_per_degree: assign as many as one polynomial degree per this many seconds pulses_per_degree: assign as many as one polynomial degree per this many pulses max_degrees: never use more than this many degrees of Legendre polynomial. n_deg: If not None, use this many degrees, regardless of the values of sec_per_degree, pulses_per_degree, and max_degress. In this case, never downsample. limit: The [lower,upper] limit of uncorrected values over which entropy is computed (default None). The entropy will be computed on corrected values only in the range [limit[0], limit[1]], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then it will be computed as 25%% larger than the 99%%ile point of uncorrected. Possible improvements in the future: * Use Numba to speed up. * Allow the parameters to be function arguments with defaults: photons per degree of freedom, seconds per degree of freedom, and max degrees of freedom. * Figure out how to span the available time with more than one set of legendre polynomials, so that we can have more than 20 d.o.f. eventually, for long runs. \"\"\" time = np . asarray ( time ) uncorrected = np . asarray ( uncorrected ) if limit is None : pct99 = np . percentile ( uncorrected , 99 ) limit = ( 0 , 1.25 * pct99 ) use = np . logical_and ( uncorrected > limit [ 0 ], uncorrected < limit [ 1 ]) time = np . asarray ( time [ use ]) uncorrected = np . asarray ( uncorrected [ use ]) tmin , tmax = np . min ( time ), np . max ( time ) def normalize ( t : NDArray ) -> NDArray : return ( t - tmin ) / ( tmax - tmin ) * 2 - 1 info = { \"tmin\" : tmin , \"tmax\" : tmax , \"normalize\" : normalize , } dtime = tmax - tmin N = len ( time ) if ndeg is None : ndeg = int ( np . minimum ( dtime / sec_per_degree , N / pulses_per_degree )) ndeg = min ( ndeg , max_degrees ) ndeg = max ( ndeg , 1 ) phot_per_degree = N / float ( ndeg ) if phot_per_degree >= 2 * pulses_per_degree : downsample = int ( phot_per_degree / pulses_per_degree ) time = time [:: downsample ] uncorrected = uncorrected [:: downsample ] N = len ( time ) else : downsample = 1 else : downsample = 1 LOG . info ( \"Using %2d degrees for %6d photons (after %d downsample)\" , ndeg , N , downsample ) LOG . info ( \"That's %6.1f photons per degree, and %6.1f seconds per degree.\" , N / float ( ndeg ), dtime / ndeg ) def model1 ( pi : NDArray , i : int , param : NDArray , basis : NDArray ) -> NDArray : pcopy = np . array ( param ) pcopy [ i ] = pi return 1 + np . dot ( basis . T , pcopy ) def cost1 ( pi : NDArray , i : int , param : NDArray , y : NDArray , w : float , basis : NDArray ) -> float : return laplace_entropy ( y * model1 ( pi , i , param , basis ), w = w ) param = np . zeros ( ndeg , dtype = float ) xnorm = np . asarray ( normalize ( time ), dtype = float ) basis = np . vstack ([ sp . special . legendre ( i + 1 )( xnorm ) for i in range ( ndeg )]) fc = 0 model : Callable = np . poly1d ([ 0 ]) info [ \"coefficients\" ] = np . zeros ( ndeg , dtype = float ) for i in range ( ndeg ): result , _fval , _iter , funcalls = sp . optimize . brent ( cost1 , ( i , param , uncorrected , w , basis ), [ - 0.001 , 0.001 ], tol = 1e-5 , full_output = True ) param [ i ] = result fc += funcalls model += sp . special . legendre ( i + 1 ) * result info [ \"coefficients\" ][ i ] = result info [ \"funccalls\" ] = fc xk = np . linspace ( - 1 , 1 , 1 + 2 * ndeg ) model2 = CubicSpline ( xk , model ( xk )) H1 = laplace_entropy ( uncorrected , w = w ) H2 = laplace_entropy ( uncorrected * ( 1 + model ( xnorm )), w = w ) H3 = laplace_entropy ( uncorrected * ( 1 + model2 ( xnorm )), w = w ) if H2 <= 0 or H2 - H1 > 0.0 : model = np . poly1d ([ 0 ]) elif H3 <= 0 or H3 - H2 > 0.00001 : model = model2 info [ \"entropies\" ] = ( H1 , H2 , H3 ) info [ \"model\" ] = model return info unwrap_n ( data , period , mask , n = 3 ) Unwrap data that has been restricted to a given period. The algorithm iterates through each data point and compares it to the average of the previous n data points. It then offsets the data point by the multiple of the period that will minimize the difference from that n-point running average. For the first n data points, there are not enough preceding points to average n of them, so the algorithm will average fewer points. This code was written by Thomas Baker; integrated into MASS by Dan Becker. Sped up 300x by @njit. Parameters: data ( array of data values ) \u2013 period ( the range over which the data loops ) \u2013 n ( how many preceding points to average , default: 3 ) \u2013 mask ( ArrayLike ) \u2013 Source code in mass2/core/analysis_algorithms.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 @njit def unwrap_n ( data : NDArray [ np . uint16 ], period : float , mask : ArrayLike , n : int = 3 ) -> NDArray : \"\"\"Unwrap data that has been restricted to a given period. The algorithm iterates through each data point and compares it to the average of the previous n data points. It then offsets the data point by the multiple of the period that will minimize the difference from that n-point running average. For the first n data points, there are not enough preceding points to average n of them, so the algorithm will average fewer points. This code was written by Thomas Baker; integrated into MASS by Dan Becker. Sped up 300x by @njit. Parameters ---------- data : array of data values period : the range over which the data loops n : how many preceding points to average mask: mask indentifying \"good\" pulses \"\"\" mask = np . asarray ( mask ) udata = np . copy ( data ) # make a copy for output if n <= 0 : return udata # Iterate through each data point and offset it by # an amount that will minimize the difference from the # rolling average nprior = 0 firstgoodidx = np . argmax ( mask ) priorvalues = np . full ( n , udata [ firstgoodidx ]) for i in range ( len ( data )): # Take the average of the previous n data points (only those with mask[i]==True). # Offset the data point by the most reasonable multiple of period (make this point closest to the running average). if mask [ i ]: avg = np . mean ( priorvalues ) if nprior == 0 : avg = float ( priorvalues [ 0 ]) elif nprior < n : avg = np . mean ( priorvalues [: nprior ]) udata [ i ] -= np . round (( udata [ i ] - avg ) / period ) * period if mask [ i ]: priorvalues [ nprior % n ] = udata [ i ] nprior += 1 return udata ColumnAsNumpyMapStep dataclass Bases: RecipeStep This step is meant for interactive exploration, it takes a column and applies a function to it, and makes a new column with the result. It makes it easy to test functions on a column without having to write a whole new step class, while maintaining the benefit of being able to use the step in a Recipe chain, like replaying steps on another channel. example usage: def my_function(x): ... return x * 2 step = ColumnAsNumpyMapStep(inputs=[\"my_column\"], output=[\"my_new_column\"], f=my_function) ch2 = ch.with_step(step) Source code in mass2/core/recipe.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 @dataclass ( frozen = True ) class ColumnAsNumpyMapStep ( RecipeStep ): \"\"\" This step is meant for interactive exploration, it takes a column and applies a function to it, and makes a new column with the result. It makes it easy to test functions on a column without having to write a whole new step class, while maintaining the benefit of being able to use the step in a Recipe chain, like replaying steps on another channel. example usage: >>> def my_function(x): ... return x * 2 >>> step = ColumnAsNumpyMapStep(inputs=[\"my_column\"], output=[\"my_new_column\"], f=my_function) >>> ch2 = ch.with_step(step) \"\"\" f : Callable [[ np . ndarray ], np . ndarray ] def __post_init__ ( self ) -> None : assert len ( self . inputs ) == 1 , \"ColumnMapStep expects exactly one input\" assert len ( self . output ) == 1 , \"ColumnMapStep expects exactly one output\" if not callable ( self . f ): raise ValueError ( f \"f must be a callable, got { self . f } \" ) def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : output_col = self . output [ 0 ] output_segments = [] for df_iter in df . select ( self . inputs ) . iter_slices (): series1 = df_iter [ self . inputs [ 0 ]] # Have to apply the function differently when series elements are arrays vs scalars if series1 . dtype . base_type () is pl . Array : output_numpy = np . array ([ self . f ( v . to_numpy ()) for v in series1 ]) else : output_numpy = self . f ( series1 . to_numpy ()) this_output_segment = pl . Series ( output_col , output_numpy ) output_segments . append ( this_output_segment ) combined = pl . concat ( output_segments ) # Put into a DataFrame with one column df2 = pl . DataFrame ({ output_col : combined }) . with_columns ( df ) return df2 Recipe dataclass Bases: Sequence [ RecipeStep ] Source code in mass2/core/recipe.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 @dataclass ( frozen = True ) class Recipe ( Sequence [ RecipeStep ]): steps : list [ RecipeStep ] # TODO: leaves many optimizations on the table, but is very simple # 1. we could calculate filt_value_5lag and filt_phase_5lag at the same time # 2. we could calculate intermediate quantities optionally and not materialize all of them def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"return a dataframe with all the newly calculated info\" for step in self . steps : df = step . calc_from_df ( df ) . with_columns ( df ) return df @classmethod def new_empty ( cls ) -> \"Recipe\" : return cls ([]) @overload def __getitem__ ( self , key : int ) -> RecipeStep : ... @overload def __getitem__ ( self , key : slice ) -> Sequence [ RecipeStep ]: ... def __getitem__ ( self , key : int | slice ) -> RecipeStep | Sequence [ RecipeStep ]: return self . steps [ key ] def __len__ ( self ) -> int : return len ( self . steps ) def with_step ( self , step : RecipeStep ) -> \"Recipe\" : # return a new Recipe with the step added, no mutation! return Recipe ( self . steps + [ step ]) def trim_dead_ends ( self , required_fields : Iterable [ str ] | str | None , drop_debug : bool = True ) -> \"Recipe\" : \"\"\"Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with `drop_debug=True`). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in `required_fields`. The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the `required_fields` (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters ---------- required_fields : Iterable[str] | str | None Steps will be preserved if any of their outputs are among `required_fields`, or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug : bool Whether to run `step.drop_debug()` to remove debugging information from the preserved steps. Returns ------- Recipe A copy of `self`, except that any steps not required to compute any of `required_fields` are omitted. \"\"\" if isinstance ( required_fields , str ): required_fields = [ required_fields ] nsteps = len ( self ) required = np . zeros ( nsteps , dtype = bool ) # The easiest approach is to traverse the steps from last to first to build our list of required # fields, because necessarily no later step can produce the inputs needed by an earlier step. if required_fields is None : required [:] = True else : all_fields_out : set [ str ] = set ( required_fields ) for istep in range ( nsteps - 1 , - 1 , - 1 ): step = self [ istep ] for field in step . output : if field in all_fields_out : required [ istep ] = True all_fields_out . update ( step . inputs ) break if not np . any ( required ): # If this error ever because a problem, where user _acutally_ wants an empty series of steps # to be a non-err, then add argument `error_on_empty_output=True` to this method. raise ValueError ( \"trim_dead_ends found no steps to be preserved\" ) steps = [] for i in range ( nsteps ): if required [ i ]: if drop_debug : steps . append ( self [ i ] . drop_debug ()) else : steps . append ( self [ i ]) return Recipe ( steps ) calc_from_df ( df ) return a dataframe with all the newly calculated info Source code in mass2/core/recipe.py 189 190 191 192 193 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"return a dataframe with all the newly calculated info\" for step in self . steps : df = step . calc_from_df ( df ) . with_columns ( df ) return df trim_dead_ends ( required_fields , drop_debug = True ) Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with drop_debug=True ). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in required_fields . The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the required_fields (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters: required_fields ( Iterable [ str ] | str | None ) \u2013 Steps will be preserved if any of their outputs are among required_fields , or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug ( bool , default: True ) \u2013 Whether to run step.drop_debug() to remove debugging information from the preserved steps. Returns: Recipe \u2013 A copy of self , except that any steps not required to compute any of required_fields are omitted. Source code in mass2/core/recipe.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def trim_dead_ends ( self , required_fields : Iterable [ str ] | str | None , drop_debug : bool = True ) -> \"Recipe\" : \"\"\"Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with `drop_debug=True`). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in `required_fields`. The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the `required_fields` (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters ---------- required_fields : Iterable[str] | str | None Steps will be preserved if any of their outputs are among `required_fields`, or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug : bool Whether to run `step.drop_debug()` to remove debugging information from the preserved steps. Returns ------- Recipe A copy of `self`, except that any steps not required to compute any of `required_fields` are omitted. \"\"\" if isinstance ( required_fields , str ): required_fields = [ required_fields ] nsteps = len ( self ) required = np . zeros ( nsteps , dtype = bool ) # The easiest approach is to traverse the steps from last to first to build our list of required # fields, because necessarily no later step can produce the inputs needed by an earlier step. if required_fields is None : required [:] = True else : all_fields_out : set [ str ] = set ( required_fields ) for istep in range ( nsteps - 1 , - 1 , - 1 ): step = self [ istep ] for field in step . output : if field in all_fields_out : required [ istep ] = True all_fields_out . update ( step . inputs ) break if not np . any ( required ): # If this error ever because a problem, where user _acutally_ wants an empty series of steps # to be a non-err, then add argument `error_on_empty_output=True` to this method. raise ValueError ( \"trim_dead_ends found no steps to be preserved\" ) steps = [] for i in range ( nsteps ): if required [ i ]: if drop_debug : steps . append ( self [ i ] . drop_debug ()) else : steps . append ( self [ i ]) return Recipe ( steps ) RecipeStep dataclass Source code in mass2/core/recipe.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @dataclass ( frozen = True ) class RecipeStep : inputs : list [ str ] output : list [ str ] good_expr : pl . Expr use_expr : pl . Expr @property def name ( self ) -> str : return str ( type ( self )) @property def description ( self ) -> str : return f \" { type ( self ) . __name__ } inputs= { self . inputs } outputs= { self . output } \" def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : # TODO: should this be an abstract method? return df . filter ( self . good_expr ) def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : # this is a no-op, subclasses can override this to plot something plt . figure () plt . text ( 0.0 , 0.5 , f \"No plot defined for: { self . description } \" ) return plt . gca () def drop_debug ( self ) -> \"RecipeStep\" : \"Return self, or a copy of it with debug information removed\" return self drop_debug () Return self, or a copy of it with debug information removed Source code in mass2/core/recipe.py 35 36 37 def drop_debug ( self ) -> \"RecipeStep\" : \"Return self, or a copy of it with debug information removed\" return self SelectStep dataclass Bases: RecipeStep This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. Source code in mass2/core/recipe.py 167 168 169 170 171 172 173 174 175 176 177 178 @dataclass ( frozen = True ) class SelectStep ( RecipeStep ): \"\"\" This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. \"\"\" col_expr_dict : dict [ str , pl . Expr ] def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : df2 = df . select ( ** self . col_expr_dict ) . with_columns ( df ) return df2 Classes to create time-domain and Fourier-domain optimal filters. Filter dataclass Bases: ABC A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level. Returns: Filter \u2013 A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted variance due to noise and the resulting predicted_v_over_dv , the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: nominal_peak . Source code in mass2/core/optimal_filtering.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 @dataclass ( frozen = True ) class Filter ( ABC ): \"\"\"A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level. Returns ------- Filter A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted `variance` due to noise and the resulting `predicted_v_over_dv`, the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: `nominal_peak`. \"\"\" values : np . ndarray nominal_peak : float variance : float predicted_v_over_dv : float dt_values : np . ndarray | None const_values : np . ndarray | None signal_model : np . ndarray | None dt_model : np . ndarray | None convolution_lags : int = 1 fmax : float | None = None f_3db : float | None = None cut_pre : int = 0 cut_post : int = 0 @property @abstractmethod def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property @abstractmethod def _filter_type ( self ) -> str : return \"illegal: this is supposed to be an abstract base class\" def plot ( self , axis : plt . Axes | None = None , ** kwargs : Any ) -> None : \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout () def report ( self , std_energy : float = 5898.8 ) -> None : \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" ) @abstractmethod def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: pass is_arrival_time_safe abstractmethod property Is this an arrival-time-safe filter? plot ( axis = None , ** kwargs ) Make a plot of the filter Parameters: axis ( Axes , default: None ) \u2013 A pre-existing axis to plot on, by default None Source code in mass2/core/optimal_filtering.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def plot ( self , axis : plt . Axes | None = None , ** kwargs : Any ) -> None : \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout () report ( std_energy = 5898.8 ) Report on estimated V/dV for the filter. Parameters: std_energy ( float , default: 5898.8 ) \u2013 Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 Source code in mass2/core/optimal_filtering.py 299 300 301 302 303 304 305 306 307 308 309 310 311 def report ( self , std_energy : float = 5898.8 ) -> None : \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" ) Filter5Lag dataclass Bases: Filter Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015. Returns: Filter5Lag \u2013 An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 @dataclass ( frozen = True ) class Filter5Lag ( Filter ): \"\"\"Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015. Returns ------- Filter5Lag An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ) -> None : assert self . convolution_lags == 5 @property def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property def _filter_type ( self ) -> str : return \"5lag\" # These parameters fit a parabola to any 5 evenly-spaced points FIVELAG_FITTER = ( np . array ( ( ( - 6 , 24 , 34 , 24 , - 6 ), ( - 14 , - 7 , 0 , 7 , 14 ), ( 10 , - 5 , - 10 , - 5 , 10 ), ), dtype = float , ) / 70.0 ) def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x is_arrival_time_safe property Is this an arrival-time-safe filter? filter_records ( x ) Filter one microcalorimeter record or an array of records. Parameters: x ( ArrayLike ) \u2013 A 1-d array, a single pulse record, or a 2-d array, where x[i, :] is pulse record number i . Returns: tuple [ ndarray , ndarray ] \u2013 The optimally filtered value, or an array (one per row) if the input is a 2-d array. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises: AssertionError \u2013 If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x FilterATS dataclass Bases: Filter Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015. Returns: FilterATS \u2013 An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 @dataclass ( frozen = True ) class FilterATS ( Filter ): \"\"\"Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015. Returns ------- FilterATS An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ) -> None : assert self . convolution_lags == 1 assert self . dt_values is not None @property def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return True @property def _filter_type ( self ) -> str : return \"ats\" def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values ) is_arrival_time_safe property Is this an arrival-time-safe filter? filter_records ( x ) Filter one microcalorimeter record or an array of records. Parameters: x ( ArrayLike ) \u2013 A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns: tuple [ ndarray , ndarray ] \u2013 The optimally filtered value, or an array (one per row) if the input is a 2-d array. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises: AssertionError \u2013 If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values ) FilterMaker dataclass An object capable of creating optimal filter based on a single signal and noise set. Parameters: signal_model ( ArrayLike ) \u2013 The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the peak value of this filter (that is, peak value relative to the baseline level). n_pretrigger ( int ) \u2013 The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only n_pretrigger samples at the start of a record. noise_autocorr ( Optional [ ArrayLike ] , default: None ) \u2013 The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of avg_signal . noise_psd ( Optional [ ArrayLike ] , default: None ) \u2013 The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of avg_signal , and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method compute_fourier() will not work. whitener ( Optional [ ToeplitzWhitener ] , default: None ) \u2013 An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes noise_autocorr if both are given. sample_time_sec ( float , default: 0.0 ) \u2013 The time step between samples in avg_signal and noise_autocorr (in seconds). This must be given if fmax or f_3db are ever to be used. peak ( float , default: 0.0 ) \u2013 The peak amplitude of the standard signal Notes If both noise_autocorr and whitener are None, then methods compute_5lag and compute_ats will both fail, as they require a time-domain characterization of the noise. The units of noise_autocorr are the square of the units used in signal_model and/or peak . The units of whitener are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. The units of noise_psd are square signal units, per Hertz. Returns: FilterMaker \u2013 An object that can make a variety of optimal filters, assuming a single signal and noise analysis. Source code in mass2/core/optimal_filtering.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 @dataclass ( frozen = True ) class FilterMaker : \"\"\"An object capable of creating optimal filter based on a single signal and noise set. Parameters --------- signal_model : ArrayLike The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the *peak value* of this filter (that is, peak value relative to the baseline level). n_pretrigger : int The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only `n_pretrigger` samples at the start of a record. noise_autocorr : Optional[ArrayLike] The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of `avg_signal`. noise_psd : Optional[ArrayLike] The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of `avg_signal`, and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method `compute_fourier()` will not work. whitener : Optional[ToeplitzWhitener] An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes `noise_autocorr` if both are given. sample_time_sec : float The time step between samples in `avg_signal` and `noise_autocorr` (in seconds). This must be given if `fmax` or `f_3db` are ever to be used. peak : float The peak amplitude of the standard signal Notes ----- * If both `noise_autocorr` and `whitener` are None, then methods `compute_5lag` and `compute_ats` will both fail, as they require a time-domain characterization of the noise. * The units of `noise_autocorr` are the square of the units used in `signal_model` and/or `peak`. The units of `whitener` are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. * The units of `noise_psd` are square signal units, per Hertz. Returns ------- FilterMaker An object that can make a variety of optimal filters, assuming a single signal and noise analysis. \"\"\" signal_model : NDArray n_pretrigger : int noise_autocorr : NDArray | None = None noise_psd : NDArray | None = None dt_model : NDArray | None = None whitener : ToeplitzWhitener | None = None sample_time_sec : float = 0.0 peak : float = 0.0 def compute_constrained_5lag ( self , constraints : ArrayLike | None = None , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 , ) -> Filter : \"\"\"Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- constraints: ndarray, optional The vector or vectors to which the filter should be orthogonal. If a 2d array, each _row_ is a constraint, and the number of columns should be equal to the len(self.signal_model) minus `(cut_pre+cut_post)`. fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) shorten = 2 # for 5-lag convolution truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" pulse_model = np . vstack (( truncated_signal , np . ones_like ( truncated_signal ))) if constraints is not None : pulse_model = np . vstack (( pulse_model , constraints )) assert pulse_model . shape [ 1 ] == n noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_model = np . vstack ([ TS ( r ) for r in pulse_model ]) A = pulse_model . dot ( Rinv_model . T ) all_filters = np . linalg . solve ( A , Rinv_model ) filt_noconst = all_filters [ 0 ] band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post ) def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" return self . compute_constrained_5lag ( None , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) def compute_5lag_noexp ( self , exp_time_seconds : float , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- exp_time_seconds: float Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" assert exp_time_seconds > 0 n = len ( self . signal_model ) - 4 - ( cut_pre + cut_post ) log_per_sample = self . sample_time_sec / exp_time_seconds constraint = np . exp ( - np . arange ( n ) * log_per_sample ) return self . compute_constrained_5lag ( constraint , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , ) def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post ) def _compute_autocorr ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> np . ndarray : \"\"\"Return the noise autocorrelation, if any, cut down by the requested number of values at the start and end. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- np.ndarray The noise autocorrelation of the appropriate length. Or a length-0 array if not known. \"\"\" # If there's an autocorrelation, cut it down to length. if self . noise_autocorr is None : return np . array ([], dtype = float ) N = len ( np . asarray ( self . signal_model )) return np . asarray ( self . noise_autocorr )[: N - ( cut_pre + cut_post )] def _normalize_signal ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> tuple [ np . ndarray , float , np . ndarray ]: \"\"\"Compute the normalized signal, peak value, and first-order arrival-time model. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- tuple[np.ndarray, float, np.ndarray] (sig, pk, dsig), where `sig` is the nominal signal model (normalized to have unit amplitude), `pk` is the peak values of the nominal signal, and `dsig` is the delta between `sig` that differ by one sample in arrival time. The `dsig` will be an empty array if no arrival-time model is known. Raises ------ ValueError If negative numbers of samples are to be cut, or the entire record is to be cut. \"\"\" avg_signal = np . array ( self . signal_model ) ns = len ( avg_signal ) pre_avg = avg_signal [ cut_pre : self . n_pretrigger - 1 ] . mean () if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) # Unless passed in, find the signal's peak value. This is normally peak=(max-pretrigger). # If signal is negative-going, however, then peak=(pretrigger-min). if self . peak > 0.0 : peak_signal = self . peak else : a = avg_signal [ cut_pre : ns - cut_post ] . min () b = avg_signal [ cut_pre : ns - cut_post ] . max () is_negative = pre_avg - a > b - pre_avg if is_negative : peak_signal = a - pre_avg else : peak_signal = b - pre_avg # avg_signal: normalize to have unit peak avg_signal -= pre_avg rescale = 1 / np . max ( avg_signal ) avg_signal *= rescale avg_signal [: self . n_pretrigger ] = 0.0 avg_signal = avg_signal [ cut_pre : ns - cut_post ] if self . dt_model is None : dt_model = np . array ([], dtype = float ) else : dt_model = self . dt_model * rescale dt_model = dt_model [ cut_pre : ns - cut_post ] return avg_signal , peak_signal , dt_model @staticmethod def _normalize_5lag_filter ( f : np . ndarray , avg_signal : np . ndarray ) -> None : \"\"\"Rescale 5-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) <= len ( avg_signal ) - 4 conv = np . zeros ( 5 , dtype = float ) for i in range ( 5 ): conv [ i ] = np . dot ( f , avg_signal [ i : i + len ( f )]) x = np . linspace ( - 2 , 2 , 5 ) fit = np . polyfit ( x , conv , 2 ) fit_ctr = - 0.5 * fit [ 1 ] / fit [ 0 ] fit_peak = np . polyval ( fit , fit_ctr ) f *= 1.0 / fit_peak @staticmethod def _normalize_filter ( f : np . ndarray , avg_signal : np . ndarray ) -> None : \"\"\"Rescale single-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) == len ( avg_signal ) f *= 1 / np . dot ( f , avg_signal ) compute_5lag ( fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" return self . compute_constrained_5lag ( None , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) compute_5lag_noexp ( exp_time_seconds , fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: exp_time_seconds ( float ) \u2013 Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 def compute_5lag_noexp ( self , exp_time_seconds : float , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- exp_time_seconds: float Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" assert exp_time_seconds > 0 n = len ( self . signal_model ) - 4 - ( cut_pre + cut_post ) log_per_sample = self . sample_time_sec / exp_time_seconds constraint = np . exp ( - np . arange ( n ) * log_per_sample ) return self . compute_constrained_5lag ( constraint , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) compute_ats ( fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 An arrival-time-safe optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post ) compute_constrained_5lag ( constraints = None , fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: constraints ( ArrayLike | None , default: None ) \u2013 The vector or vectors to which the filter should be orthogonal. If a 2d array, each row is a constraint, and the number of columns should be equal to the len(self.signal_model) minus (cut_pre+cut_post) . fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def compute_constrained_5lag ( self , constraints : ArrayLike | None = None , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 , ) -> Filter : \"\"\"Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- constraints: ndarray, optional The vector or vectors to which the filter should be orthogonal. If a 2d array, each _row_ is a constraint, and the number of columns should be equal to the len(self.signal_model) minus `(cut_pre+cut_post)`. fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) shorten = 2 # for 5-lag convolution truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" pulse_model = np . vstack (( truncated_signal , np . ones_like ( truncated_signal ))) if constraints is not None : pulse_model = np . vstack (( pulse_model , constraints )) assert pulse_model . shape [ 1 ] == n noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_model = np . vstack ([ TS ( r ) for r in pulse_model ]) A = pulse_model . dot ( Rinv_model . T ) all_filters = np . linalg . solve ( A , Rinv_model ) filt_noconst = all_filters [ 0 ] band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post ) compute_fourier ( fmax = None , f_3db = None , cut_pre = 0 , cut_post = 0 ) Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter, computed in the Fourier domain. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , ) ToeplitzWhitener dataclass An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: tw.whiten(v) returns Wv; it is equivalent to tw(v) tw.solveWT(v) returns inv(W')*v tw.applyWT(v) returns W'v tw.solveW(v) returns inv(W)*v Arguments theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients Returns: ToeplitzWhitener \u2013 Object that can perform approximate, time-invariant noise whitening. Raises: ValueError \u2013 If the operative methods are passed an array of dimension higher than 2. Source code in mass2/core/optimal_filtering.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 @dataclass ( frozen = True ) class ToeplitzWhitener : \"\"\"An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: * `tw.whiten(v)` returns Wv; it is equivalent to `tw(v)` * `tw.solveWT(v)` returns inv(W')*v * `tw.applyWT(v)` returns W'v * `tw.solveW(v)` returns inv(W)*v Arguments --------- theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients Returns ------- ToeplitzWhitener Object that can perform approximate, time-invariant noise whitening. Raises ------ ValueError If the operative methods are passed an array of dimension higher than 2. \"\"\" theta : np . ndarray phi : np . ndarray @property def p ( self ) -> int : return len ( self . phi ) - 1 @property def q ( self ) -> int : return len ( self . theta ) - 1 def whiten ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v ) def __call__ ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y def solveW ( self , v : ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y def solveWT ( self , v : ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :] def applyWT ( self , v : ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :] def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR ) W ( N ) Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. Source code in mass2/core/optimal_filtering.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR ) __call__ ( v ) Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __call__ ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y applyWT ( v ) Return vector (or matrix of column vectors) W'v Source code in mass2/core/optimal_filtering.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def applyWT ( self , v : ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :] solveW ( v ) Return unwhitened vector (or matrix of column vectors) inv(W)*v Source code in mass2/core/optimal_filtering.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def solveW ( self , v : ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y solveWT ( v ) Return vector (or matrix of column vectors) inv(W')*v Source code in mass2/core/optimal_filtering.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def solveWT ( self , v : ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :] whiten ( v ) Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 66 67 68 def whiten ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v ) band_limit ( modelmatrix , sample_time_sec , fmax , f_3db ) Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input modelmatrix in-place. No effect if both fmax and f_3db are None . Parameters: modelmatrix ( ndarray ) \u2013 The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec ( float ) \u2013 The sampling period, normally in seconds. fmax ( Optional [ float ] ) \u2013 The hard maximum frequency (units are inverse of sample_time_sec units, or Hz) f_3db ( Optional [ float ] ) \u2013 The 1-pole low-pass filter's 3 dB point (same units as fmax ) Source code in mass2/core/optimal_filtering.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def band_limit ( modelmatrix : np . ndarray , sample_time_sec : float , fmax : float | None , f_3db : float | None ) -> None : \"\"\"Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input `modelmatrix` in-place. No effect if both `fmax` and `f_3db` are `None`. Parameters ---------- modelmatrix : np.ndarray The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec : float The sampling period, normally in seconds. fmax : Optional[float] The hard maximum frequency (units are inverse of `sample_time_sec` units, or Hz) f_3db : Optional[float] The 1-pole low-pass filter's 3 dB point (same units as `fmax`) \"\"\" if fmax is None and f_3db is None : return # Handle the 2D case by calling this function once per column. assert len ( modelmatrix . shape ) <= 2 if len ( modelmatrix . shape ) == 2 : for i in range ( modelmatrix . shape [ 1 ]): band_limit ( modelmatrix [:, i ], sample_time_sec , fmax , f_3db ) return vector = modelmatrix filt_length = len ( vector ) sig_ft = np . fft . rfft ( vector ) freq = np . fft . fftfreq ( filt_length , d = sample_time_sec ) freq = np . abs ( freq [: len ( sig_ft )]) if fmax is not None : sig_ft [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft /= 1.0 + ( 1.0 * freq / f_3db ) ** 2 # n=filt_length is needed when filt_length is ODD vector [:] = np . fft . irfft ( sig_ft , n = filt_length ) bracketR ( q , noise ) Return the dot product (q^T R q) for vector and matrix R constructed from the vector by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). Source code in mass2/core/optimal_filtering.py 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 def bracketR ( q : NDArray , noise : NDArray ) -> float : \"\"\"Return the dot product (q^T R q) for vector <q> and matrix R constructed from the vector <noise> by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). \"\"\" if len ( noise ) < len ( q ): raise ValueError ( f \"Vector q (length { len ( q ) } ) cannot be longer than the noise (length { len ( noise ) } )\" ) n = len ( q ) r = np . zeros ( 2 * n - 1 , dtype = float ) r [ n - 1 :] = noise [: n ] r [ n - 1 :: - 1 ] = noise [: n ] dot = 0.0 for i in range ( n ): dot += q [ i ] * r [ n - i - 1 : 2 * n - i - 1 ] . dot ( q ) return dot HCI lines","title":"Docstrings"},{"location":"docstrings/#automatic-documentation-generated-from-docstrings","text":"","title":"Automatic documentation generated from docstrings"},{"location":"docstrings/#mass2.core.channel.Channel","text":"Source code in mass2/core/channel.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 @dataclass ( frozen = True ) # noqa: PLR0904 class Channel : df : pl . DataFrame = field ( repr = False ) header : ChannelHeader = field ( repr = True ) npulses : int subframediv : int | None = None noise : NoiseChannel | None = field ( default = None , repr = False ) good_expr : pl . Expr = field ( default_factory = alwaysTrue ) df_history : list [ pl . DataFrame ] = field ( default_factory = list , repr = False ) steps : Recipe = field ( default_factory = Recipe . new_empty , repr = False ) steps_elapsed_s : list [ float ] = field ( default_factory = list ) transform_raw : Callable | None = None @property def shortname ( self ) -> str : return self . header . description def mo_stepplots ( self ) -> mo . ui . dropdown : desc_ind = { step . description : i for i , step in enumerate ( self . steps )} first_non_summarize_step = self . steps [ 0 ] for step in self . steps : if isinstance ( step , SummarizeStep ): continue first_non_summarize_step = step break mo_ui = mo . ui . dropdown ( desc_ind , value = first_non_summarize_step . description , label = f \"choose step for ch { self . header . ch_num } \" , ) def show () -> mo . Html : return self . _mo_stepplots_explicit ( mo_ui ) def step_ind () -> Any : return mo_ui . value mo_ui . show = show mo_ui . step_ind = step_ind return mo_ui def _mo_stepplots_explicit ( self , mo_ui : mo . ui . dropdown ) -> mo . Html : step_ind = mo_ui . value self . step_plot ( step_ind ) fig = plt . gcf () return mo . vstack ([ mo_ui , misc . show ( fig )]) def get_step ( self , index : int ) -> tuple [ RecipeStep , int ]: if index < 0 : # normalize the index to a positive index index = len ( self . steps ) + index step = self . steps [ index ] return step , index def step_plot ( self , step_ind : int , ** kwargs : Any ) -> plt . Axes : step , step_ind = self . get_step ( step_ind ) if step_ind + 1 == len ( self . df_history ): df_after = self . df else : df_after = self . df_history [ step_ind + 1 ] return step . dbg_plot ( df_after , ** kwargs ) def hist ( self , col : str , bin_edges : ArrayLike , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col )) . collect () values = df_small [ col ] bin_centers , counts = misc . hist_of_series ( values , bin_edges ) return bin_centers , counts def plot_hist ( self , col : str , bin_edges : ArrayLike , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), ) -> tuple [ NDArray , NDArray ]: if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis bin_centers , counts = self . hist ( col , bin_edges = bin_edges , use_good_expr = use_good_expr , use_expr = use_expr ) _ , step_size = misc . midpoints_and_step_size ( bin_edges ) plt . step ( bin_centers , counts , where = \"mid\" ) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } for { self . shortname } \" ) plt . tight_layout () return bin_centers , counts def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : str , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), skip_none : bool = True , ) -> tuple [ NDArray , dict [ str , NDArray ]]: \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group counts_dict : dict [ str , NDArray ] = {} for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] _ , step_size = misc . midpoints_and_step_size ( bin_edges ) bin_centers , counts = misc . hist_of_series ( values , bin_edges ) group_name_str = str ( group_name ) counts_dict [ group_name_str ] = counts plt . step ( bin_centers , counts , where = \"mid\" , label = group_name_str ) # Plot the histogram for the current group # if group_name == \"EBIT\": # ax.hist(values, bins=bin_edges, alpha=0.9, color=\"k\", label=group_name_str) # else: # ax.hist(values, bins=bin_edges, alpha=0.5, label=group_name_str) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () return bin_centers , counts_dict def plot_scatter ( self , x_col : str , y_col : str , color_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), use_good_expr : bool = True , skip_none : bool = True , ax : plt . Axes | None = None , ) -> None : if ax is None : plt . figure () ax = plt . gca () plt . sca ( ax ) # set current axis so I can use plt api if use_good_expr : filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = self . good_expr df_small = self . df . lazy () . filter ( filter_expr ) . select ( x_col , y_col , color_col ) . collect () for ( name ,), data in df_small . group_by ( color_col , maintain_order = True ): if name is None and skip_none and color_col is not None : continue plt . plot ( data . select ( x_col ) . to_series (), data . select ( y_col ) . to_series (), \".\" , label = name , ) plt . xlabel ( str ( x_col )) plt . ylabel ( str ( y_col )) title_str = f \"\"\" { self . header . description } use_expr= { str ( use_expr ) } good_expr= { str ( self . good_expr ) } \"\"\" plt . title ( title_str ) if color_col is not None : plt . legend ( title = color_col ) plt . tight_layout () def good_series ( self , col : str , use_expr : pl . Expr = pl . lit ( True )) -> pl . Series : return mass2 . misc . good_series ( self . df , col , self . good_expr , use_expr ) def rough_cal_combinatoric ( self , line_names : list [ str ], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : step = mass2 . core . RoughCalibrationStep . learn_combinatoric ( self , line_names , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal_combinatoric_height_info ( self , line_names : list [ str ], line_heights_allowed : list [ list [ int ]], uncalibrated_col : str , calibrated_col : str , ph_smoothing_fwhm : float , n_extra : int = 3 , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : step = mass2 . core . RoughCalibrationStep . learn_combinatoric_height_info ( self , line_names , line_heights_allowed , uncalibrated_col = uncalibrated_col , calibrated_col = calibrated_col , ph_smoothing_fwhm = ph_smoothing_fwhm , n_extra = n_extra , use_expr = use_expr , ) return self . with_step ( step ) def rough_cal ( # noqa: PLR0917 self , line_names : list [ str | float ], uncalibrated_col : str = \"filtValue\" , calibrated_col : str | None = None , use_expr : pl . Expr = field ( default_factory = alwaysTrue ), max_fractional_energy_error_3rd_assignment : float = 0.1 , min_gain_fraction_at_ph_30k : float = 0.25 , fwhm_pulse_height_units : float = 75 , n_extra_peaks : int = 10 , acceptable_rms_residual_e : float = 10 , ) -> \"Channel\" : step = mass2 . core . RoughCalibrationStep . learn_3peak ( self , line_names , uncalibrated_col , calibrated_col , use_expr , max_fractional_energy_error_3rd_assignment , min_gain_fraction_at_ph_30k , fwhm_pulse_height_units , n_extra_peaks , acceptable_rms_residual_e , ) return self . with_step ( step ) def with_step ( self , step : RecipeStep ) -> \"Channel\" : t_start = time . time () df2 = step . calc_from_df ( self . df ) elapsed_s = time . time () - t_start ch2 = dataclasses . replace ( self , df = df2 , good_expr = step . good_expr , df_history = self . df_history + [ self . df ], steps = self . steps . with_step ( step ), steps_elapsed_s = self . steps_elapsed_s + [ elapsed_s ], ) return ch2 def with_steps ( self , steps : Recipe ) -> \"Channel\" : ch2 = self for step in steps : ch2 = ch2 . with_step ( step ) return ch2 def with_good_expr ( self , good_expr : pl . Expr , replace : bool = False ) -> \"Channel\" : # the default value of self.good_expr is pl.lit(True) # and_(True) will just add visual noise when looking at good_expr and not affect behavior if not replace and good_expr is not True and not good_expr . meta . eq ( pl . lit ( True )): good_expr = good_expr . and_ ( self . good_expr ) return dataclasses . replace ( self , good_expr = good_expr ) def with_column_map_step ( self , input_col : str , output_col : str , f : Callable ) -> \"Channel\" : \"\"\"f should take a numpy array and return a numpy array with the same number of elements\"\"\" step = mass2 . core . recipe . ColumnAsNumpyMapStep ([ input_col ], [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), f = f ) return self . with_step ( step ) def with_good_expr_pretrig_rms_and_postpeak_deriv ( self , n_sigma_pretrig_rms : float = 20 , n_sigma_postpeak_deriv : float = 20 , replace : bool = False ) -> \"Channel\" : max_postpeak_deriv = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"postpeak_deriv\" ] . to_numpy (), nsigma = n_sigma_postpeak_deriv ) max_pretrig_rms = misc . outlier_resistant_nsigma_above_mid ( self . df [ \"pretrig_rms\" ] . to_numpy (), nsigma = n_sigma_pretrig_rms ) good_expr = ( pl . col ( \"postpeak_deriv\" ) < max_postpeak_deriv ) . and_ ( pl . col ( \"pretrig_rms\" ) < max_pretrig_rms ) return self . with_good_expr ( good_expr , replace ) def with_range_around_median ( self , col : str , range_up : float , range_down : float ) -> \"Channel\" : med = np . median ( self . df [ col ] . to_numpy ()) return self . with_good_expr ( pl . col ( col ) . is_between ( med - range_down , med + range_up )) def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace ) @functools . cache def typical_peak_ind ( self , col : str = \"pulse\" ) -> int : raw = self . df . limit ( 100 )[ col ] . to_numpy () if self . transform_raw is not None : raw = self . transform_raw ( raw ) return int ( np . median ( raw . argmax ( axis = 1 ))) def summarize_pulses ( self , col : str = \"pulse\" , pretrigger_ignore_samples : int = 0 , peak_index : int | None = None ) -> \"Channel\" : if peak_index is None : peak_index = self . typical_peak_ind ( col ) out_names = mass2 . core . pulse_algorithms . result_dtype . names # mypy (incorrectly) thinks `out_names` might be None, and `list(None)` is forbidden. Assertion makes it happy again. assert out_names is not None outputs = list ( out_names ) step = SummarizeStep ( inputs = [ col ], output = outputs , good_expr = self . good_expr , use_expr = pl . lit ( True ), frametime_s = self . header . frametime_s , peak_index = peak_index , pulse_col = col , pretrigger_ignore_samples = pretrigger_ignore_samples , n_presamples = self . header . n_presamples , transform_raw = self . transform_raw , ) return self . with_step ( step ) def correct_pretrig_mean_jumps ( self , uncorrected : str = \"pretrig_mean\" , corrected : str = \"ptm_jf\" , period : int = 4096 ) -> \"Channel\" : step = mass2 . core . recipe . PretrigMeanJumpFixStep ( inputs = [ uncorrected ], output = [ corrected ], good_expr = self . good_expr , use_expr = pl . lit ( True ), period = period , ) return self . with_step ( step ) def with_select_step ( self , col_expr_dict : dict [ str , pl . Expr ]) -> \"Channel\" : \"\"\" This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. \"\"\" extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in col_expr_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . SelectStep ( inputs = list ( inputs ), output = list ( col_expr_dict . keys ()), good_expr = self . good_expr , use_expr = pl . lit ( True ), col_expr_dict = col_expr_dict , ) return self . with_step ( step ) def with_categorize_step ( self , category_condition_dict : dict [ str , pl . Expr ], output_col : str = \"category\" ) -> \"Channel\" : # ensure the first condition is True, to be used as a fallback first_expr = next ( iter ( category_condition_dict . values ())) if not first_expr . meta . eq ( pl . lit ( True )): category_condition_dict = { \"fallback\" : pl . lit ( True ), ** category_condition_dict } extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in category_condition_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . CategorizeStep ( inputs = list ( inputs ), output = [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), category_condition_dict = category_condition_dict , ) return self . with_step ( step ) def filter5lag ( self , pulse_col : str = \"pulse\" , peak_y_col : str = \"5lagy\" , peak_x_col : str = \"5lagx\" , f_3db : float = 25e3 , use_expr : pl . Expr = pl . lit ( True ), time_constant_s_of_exp_to_be_orthogonal_to : float | None = None , ) -> \"Channel\" : avg_pulse = ( self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( pulse_col ) . limit ( 2000 ) . collect () . to_series () . to_numpy () . mean ( axis = 0 ) ) avg_pulse -= avg_pulse [: self . header . n_presamples ] . mean () assert self . noise spectrum5lag = self . noise . spectrum ( trunc_front = 2 , trunc_back = 2 ) filter_maker = FilterMaker ( signal_model = avg_pulse , n_pretrigger = self . header . n_presamples , noise_psd = spectrum5lag . psd , noise_autocorr = spectrum5lag . autocorr_vec , sample_time_sec = self . header . frametime_s , ) if time_constant_s_of_exp_to_be_orthogonal_to is None : filter5lag = filter_maker . compute_5lag ( f_3db = f_3db ) else : filter5lag = filter_maker . compute_5lag_noexp ( f_3db = f_3db , exp_time_seconds = time_constant_s_of_exp_to_be_orthogonal_to ) step = Filter5LagStep ( inputs = [ \"pulse\" ], output = [ peak_x_col , peak_y_col ], good_expr = self . good_expr , use_expr = use_expr , filter = filter5lag , spectrum = spectrum5lag , filter_maker = filter_maker , transform_raw = self . transform_raw , ) return self . with_step ( step ) def good_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : good_df = self . df . lazy () . filter ( self . good_expr ) if use_expr is not True : good_df = good_df . filter ( use_expr ) return good_df . select ( cols ) . collect () def bad_df ( self , cols : list [ str ] | pl . Expr = pl . all (), use_expr : pl . Expr = pl . lit ( True )) -> pl . DataFrame : bad_df = self . df . lazy () . filter ( self . good_expr . not_ ()) if use_expr is not True : bad_df = bad_df . filter ( use_expr ) return bad_df . select ( cols ) . collect () def good_serieses ( self , cols : list [ str ], use_expr : pl . Expr = pl . lit ( True )) -> list [ pl . Series ]: df2 = self . good_df ( cols , use_expr ) return [ df2 [ col ] for col in cols ] def driftcorrect ( self , indicator_col : str = \"pretrig_mean\" , uncorrected_col : str = \"5lagy\" , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : # by defining a seperate learn method that takes ch as an argument, # we can move all the code for the step outside of Channel step = DriftCorrectStep . learn ( ch = self , indicator_col = indicator_col , uncorrected_col = uncorrected_col , corrected_col = corrected_col , use_expr = use_expr , ) return self . with_step ( step ) def linefit ( # noqa: PLR0917 self , line : GenericLineModel | SpectralLine | str | float , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . df . lazy () . filter ( self . good_expr ) . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = self . header . description , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def step_summary ( self ) -> list [ tuple [ str , float ]]: return [( type ( a ) . __name__ , b ) for ( a , b ) in zip ( self . steps , self . steps_elapsed_s )] def __hash__ ( self ) -> int : # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other : object ) -> bool : # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results # only checks if the ids match, does not try to be equal if all contents are equal return id ( self ) == id ( other ) @classmethod def from_ljh ( cls , path : str | Path , noise_path : str | Path | None = None , keep_posix_usec : bool = False , transform_raw : Callable | None = None , ) -> \"Channel\" : if not noise_path : noise_channel = None else : noise_channel = NoiseChannel . from_ljh ( noise_path ) ljh = mass2 . LJHFile . open ( path ) df , header_df = ljh . to_polars ( keep_posix_usec ) header = ChannelHeader . from_ljh_header_df ( header_df ) channel = cls ( df , header = header , npulses = ljh . npulses , subframediv = ljh . subframediv , noise = noise_channel , transform_raw = transform_raw ) return channel @classmethod def from_off ( cls , off : OffFile ) -> \"Channel\" : assert off . _mmap is not None df = pl . from_numpy ( np . asarray ( off . _mmap )) df = ( df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) . with_columns ( df ) . select ( pl . exclude ( \"unixnano\" )) ) df_header = pl . DataFrame ( off . header ) df_header = df_header . with_columns ( pl . Series ( \"Filename\" , [ off . filename ])) header = ChannelHeader ( f \" { os . path . split ( off . filename )[ 1 ] } \" , off . header [ \"ChannelNumberMatchingName\" ], off . framePeriodSeconds , off . _mmap [ \"recordPreSamples\" ][ 0 ], off . _mmap [ \"recordSamples\" ][ 0 ], df_header , ) channel = cls ( df , header , off . nRecords , subframediv = off . subframediv ) return channel def with_experiment_state_df ( self , df_es : pl . DataFrame , force_timestamp_monotonic : bool = False ) -> \"Channel\" : if not self . df [ \"timestamp\" ] . is_sorted (): df = self . df . select ( pl . col ( \"timestamp\" ) . cum_max () . alias ( \"timestamp\" )) . with_columns ( self . df . select ( pl . exclude ( \"timestamp\" ))) # print(\"WARNING: in with_experiment_state_df, timestamp is not monotonic, forcing it to be\") # print(\"This is likely a BUG in DASTARD.\") else : df = self . df df2 = df . join_asof ( df_es , on = \"timestamp\" , strategy = \"backward\" ) return self . with_replacement_df ( df2 ) def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channel\" : df2 = ( self . df . with_columns ( subframecount = pl . col ( \"framecount\" ) * self . subframediv ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"backward\" , coalesce = False , suffix = \"_prev_ext_trig\" ) . join_asof ( df_ext , on = \"subframecount\" , strategy = \"forward\" , coalesce = False , suffix = \"_next_ext_trig\" ) ) return self . with_replacement_df ( df2 ) def with_replacement_df ( self , df2 : pl . DataFrame ) -> \"Channel\" : return dataclasses . replace ( self , df = df2 , ) def with_columns ( self , df2 : pl . DataFrame ) -> \"Channel\" : df3 = self . df . with_columns ( df2 ) return self . with_replacement_df ( df3 ) def multifit_quadratic_gain_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : step = MultiFitQuadraticGainStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def multifit_mass_cal ( self , multifit : MultiFit , previous_cal_step_index : int , calibrated_col : str , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : step = MultiFitMassCalibrationStep . learn ( self , multifit_spec = multifit , previous_cal_step_index = previous_cal_step_index , calibrated_col = calibrated_col , use_expr = use_expr , ) return self . with_step ( step ) def concat_df ( self , df : pl . DataFrame ) -> \"Channel\" : ch2 = Channel ( mass2 . core . misc . concat_dfs_with_concat_state ( self . df , df ), self . header , self . npulses , subframediv = self . subframediv , noise = self . noise , good_expr = self . good_expr , ) # we won't copy over df_history and steps. I don't think you should use this when those are filled in? return ch2 def concat_ch ( self , ch : \"Channel\" ) -> \"Channel\" : ch2 = self . concat_df ( ch . df ) return ch2 def phase_correct_mass_specific_lines ( self , indicator_col : str , uncorrected_col : str , line_names : Iterable [ str | float ], previous_cal_step_index : int , corrected_col : str | None = None , use_expr : pl . Expr = pl . lit ( True ), ) -> \"Channel\" : if corrected_col is None : corrected_col = uncorrected_col + \"_pc\" step = mass2 . core . phase_correct_steps . phase_correct_mass_specific_lines ( self , indicator_col , uncorrected_col , corrected_col , previous_cal_step_index , line_names , use_expr , ) return self . with_step ( step ) def as_bad ( self , error_type : type | None , error_msg : str , backtrace : str | None ) -> \"BadChannel\" : return BadChannel ( self , error_type , error_msg , backtrace ) def save_recipes ( self , filename : str ) -> dict [ int , Recipe ]: steps = { self . header . ch_num : self . steps } misc . pickle_object ( steps , filename ) return steps def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr_in : pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample : int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log : bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) contents , _ , _ = plt . hist ( y , 200 , range = limits , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" ) def fit_pulse ( self , index : int = 0 , col : str = \"pulse\" , verbose : bool = True ) -> LineModelResult : pulse = self . df [ col ][ index ] . to_numpy () result = mass2 . core . pulse_algorithms . fit_pulse_2exp_with_tail ( pulse , npre = self . header . n_presamples , dt = self . header . frametime_s ) if verbose : print ( f \"ch= { self } \" ) print ( f \"pulse index= { index } \" ) print ( result . fit_report ()) return result","title":"Channel"},{"location":"docstrings/#mass2.core.channel.Channel.plot_hists","text":"Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channel.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : str , axis : plt . Axes | None = None , use_good_expr : bool = True , use_expr : pl . Expr = pl . lit ( True ), skip_none : bool = True , ) -> tuple [ NDArray , dict [ str , NDArray ]]: \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_good_expr and self . good_expr is not True : # True doesn't implement .and_, haven't found a exper literal equivalent that does # so we special case True filter_expr = self . good_expr . and_ ( use_expr ) else : filter_expr = use_expr # Group by the specified column and filter using good_expr df_small = ( self . df . lazy () . filter ( filter_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group counts_dict : dict [ str , NDArray ] = {} for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] _ , step_size = misc . midpoints_and_step_size ( bin_edges ) bin_centers , counts = misc . hist_of_series ( values , bin_edges ) group_name_str = str ( group_name ) counts_dict [ group_name_str ] = counts plt . step ( bin_centers , counts , where = \"mid\" , label = group_name_str ) # Plot the histogram for the current group # if group_name == \"EBIT\": # ax.hist(values, bins=bin_edges, alpha=0.9, color=\"k\", label=group_name_str) # else: # ax.hist(values, bins=bin_edges, alpha=0.5, label=group_name_str) # bin_centers, counts = misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( f \"Counts per { step_size : .02f } unit bin\" ) ax . set_title ( f \"Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () return bin_centers , counts_dict","title":"plot_hists"},{"location":"docstrings/#mass2.core.channel.Channel.plot_summaries","text":"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters: use_expr_in ( Expr | None , default: None ) \u2013 A polars expression to determine valid pulses, by default None. If None, use self.good_expr downsample ( int | None , default: None ) \u2013 Plot only every one of downsample pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log ( bool , default: False ) \u2013 Whether to make the histograms have a logarithmic y-scale, by default False. Source code in mass2/core/channel.py 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 def plot_summaries ( self , use_expr_in : pl . Expr | None = None , downsample : int | None = None , log : bool = False ) -> None : \"\"\"Plot a summary of the data set, including time series and histograms of key pulse properties. Parameters ---------- use_expr_in : pl.Expr | None, optional A polars expression to determine valid pulses, by default None. If None, use `self.good_expr` downsample : int | None, optional Plot only every one of `downsample` pulses in the scatter plots, by default None. If None, choose the smallest value so that no more than 10000 points appear log : bool, optional Whether to make the histograms have a logarithmic y-scale, by default False. \"\"\" plt . figure () tpi_microsec = ( self . typical_peak_ind () - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s ) plottables = ( ( \"pulse_rms\" , \"Pulse RMS\" , \"#dd00ff\" , None ), ( \"pulse_average\" , \"Pulse Avg\" , \"purple\" , None ), ( \"peak_value\" , \"Peak value\" , \"blue\" , None ), ( \"pretrig_rms\" , \"Pretrig RMS\" , \"green\" , [ 0 , 4000 ]), ( \"pretrig_mean\" , \"Pretrig Mean\" , \"#00ff26\" , None ), ( \"postpeak_deriv\" , \"Max PostPk deriv\" , \"gold\" , [ 0 , 200 ]), ( \"rise_time_\u00b5s\" , \"Rise time (\u00b5s)\" , \"orange\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ( \"peak_time_\u00b5s\" , \"Peak time (\u00b5s)\" , \"red\" , [ - 0.3 * tpi_microsec , 2 * tpi_microsec ]), ) use_expr = self . good_expr if use_expr_in is None else use_expr_in if downsample is None : downsample = self . npulses // 10000 downsample = max ( downsample , 1 ) df = self . df . lazy () . gather_every ( downsample ) df = df . with_columns ( (( pl . col ( \"peak_index\" ) - self . header . n_presamples ) * ( 1e6 * self . header . frametime_s )) . alias ( \"peak_time_\u00b5s\" ) ) df = df . with_columns (( pl . col ( \"rise_time\" ) * 1e6 ) . alias ( \"rise_time_\u00b5s\" )) existing_columns = df . collect_schema () . names () preserve = [ p [ 0 ] for p in plottables if p [ 0 ] in existing_columns ] preserve . append ( \"timestamp\" ) df2 = df . filter ( use_expr ) . select ( preserve ) . collect () # Plot timeseries relative to 0 = the last 00 UT during or before the run. timestamp = df2 [ \"timestamp\" ] . to_numpy () last_midnight = timestamp [ - 1 ] . astype ( \"datetime64[D]\" ) hour_rel = ( timestamp - last_midnight ) . astype ( float ) / 3600e6 for i , ( column_name , label , color , limits ) in enumerate ( plottables ): if column_name not in df2 : continue y = df2 [ column_name ] . to_numpy () # Time series scatter plots (left-hand panels) plt . subplot ( len ( plottables ), 2 , 1 + i * 2 ) plt . ylabel ( label ) plt . plot ( hour_rel , y , \".\" , ms = 1 , color = color ) if i == len ( plottables ) - 1 : plt . xlabel ( \"Time since last UT midnight (hours)\" ) # Histogram (right-hand panels) plt . subplot ( len ( plottables ), 2 , 2 + i * 2 ) contents , _ , _ = plt . hist ( y , 200 , range = limits , log = log , histtype = \"stepfilled\" , fc = color , alpha = 0.5 ) if log : plt . ylim ( ymin = contents . min ()) print ( f \"Plotting { len ( y ) } out of { self . npulses } data points\" )","title":"plot_summaries"},{"location":"docstrings/#mass2.core.channel.Channel.with_column_map_step","text":"f should take a numpy array and return a numpy array with the same number of elements Source code in mass2/core/channel.py 359 360 361 362 def with_column_map_step ( self , input_col : str , output_col : str , f : Callable ) -> \"Channel\" : \"\"\"f should take a numpy array and return a numpy array with the same number of elements\"\"\" step = mass2 . core . recipe . ColumnAsNumpyMapStep ([ input_col ], [ output_col ], good_expr = self . good_expr , use_expr = pl . lit ( True ), f = f ) return self . with_step ( step )","title":"with_column_map_step"},{"location":"docstrings/#mass2.core.channel.Channel.with_good_expr_below_nsigma_outlier_resistant","text":"always sets lower limit at 0, don't use for values that can be negative Source code in mass2/core/channel.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 def with_good_expr_below_nsigma_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): max_for_col = misc . outlier_resistant_nsigma_above_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( 0 , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace )","title":"with_good_expr_below_nsigma_outlier_resistant"},{"location":"docstrings/#mass2.core.channel.Channel.with_good_expr_nsigma_range_outlier_resistant","text":"always sets lower limit at 0, don't use for values that can be negative Source code in mass2/core/channel.py 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def with_good_expr_nsigma_range_outlier_resistant ( self , col_nsigma_pairs : Iterable [ tuple [ str , float ]], replace : bool = False , use_prev_good_expr : bool = True ) -> \"Channel\" : \"\"\" always sets lower limit at 0, don't use for values that can be negative \"\"\" if use_prev_good_expr : df = self . df . lazy () . select ( pl . exclude ( \"pulse\" )) . filter ( self . good_expr ) . collect () else : df = self . df for i , ( col , nsigma ) in enumerate ( col_nsigma_pairs ): min_for_col , max_for_col = misc . outlier_resistant_nsigma_range_from_mid ( df [ col ] . to_numpy (), nsigma = nsigma ) this_iter_good_expr = pl . col ( col ) . is_between ( min_for_col , max_for_col ) if i == 0 : good_expr = this_iter_good_expr else : good_expr = good_expr . and_ ( this_iter_good_expr ) return self . with_good_expr ( good_expr , replace )","title":"with_good_expr_nsigma_range_outlier_resistant"},{"location":"docstrings/#mass2.core.channel.Channel.with_select_step","text":"This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. Source code in mass2/core/channel.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def with_select_step ( self , col_expr_dict : dict [ str , pl . Expr ]) -> \"Channel\" : \"\"\" This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. \"\"\" extract = mass2 . misc . extract_column_names_from_polars_expr inputs : set [ str ] = set () for expr in col_expr_dict . values (): inputs . update ( extract ( expr )) step = mass2 . core . recipe . SelectStep ( inputs = list ( inputs ), output = list ( col_expr_dict . keys ()), good_expr = self . good_expr , use_expr = pl . lit ( True ), col_expr_dict = col_expr_dict , ) return self . with_step ( step )","title":"with_select_step"},{"location":"docstrings/#mass2.core.channels.Channels","text":"Source code in mass2/core/channels.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 @dataclass ( frozen = True ) # noqa: PLR0904 class Channels : channels : dict [ int , Channel ] description : str bad_channels : dict [ int , BadChannel ] = field ( default_factory = dict ) @property def ch0 ( self ) -> Channel : assert len ( self . channels ) > 0 , \"channels must be non-empty\" return next ( iter ( self . channels . values ())) @functools . cache def dfg ( self , exclude : str = \"pulse\" ) -> pl . DataFrame : # return a dataframe containing good pulses from each channel, # exluding \"pulse\" by default # and including columns \"key\" (to be removed?) and \"ch_num\" # the more common call should be to wrap this in a convenient plotter dfs = [] for ch_num , channel in self . channels . items (): df = channel . df . select ( pl . exclude ( exclude )) . filter ( channel . good_expr ) # key_series = pl.Series(\"key\", dtype=pl.Int64).extend_constant(key, len(df)) assert ch_num == channel . header . ch_num ch_series = pl . Series ( \"ch_num\" , dtype = pl . Int64 ) . extend_constant ( channel . header . ch_num , len ( df )) dfs . append ( df . with_columns ( ch_series )) return pl . concat ( dfs ) def linefit ( # noqa: PLR0917 self , line : float | str | SpectralLine | GenericLineModel , col : str , use_expr : pl . Expr = pl . lit ( True ), has_linear_background : bool = False , has_tails : bool = False , dlo : float = 50 , dhi : float = 50 , binsize : float = 0.5 , params_update : lmfit . Parameters = lmfit . Parameters (), ) -> LineModelResult : model = mass2 . calibration . algorithms . get_model ( line , has_linear_background = has_linear_background , has_tails = has_tails ) pe = model . spect . peak_energy _bin_edges = np . arange ( pe - dlo , pe + dhi , binsize ) df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () bin_centers , counts = mass2 . misc . hist_of_series ( df_small [ col ], _bin_edges ) params = model . guess ( counts , bin_centers = bin_centers , dph_de = 1 ) params [ \"dph_de\" ] . set ( 1.0 , vary = False ) print ( f \"before update { params =} \" ) params = params . update ( params_update ) print ( f \"after update { params =} \" ) result = model . fit ( counts , params , bin_centers = bin_centers , minimum_bins_per_fwhm = 3 ) result . set_label_hints ( binsize = bin_centers [ 1 ] - bin_centers [ 0 ], ds_shortname = f \" { len ( self . channels ) } channels, { self . description } \" , unit_str = \"eV\" , attr_str = col , states_hint = f \" { use_expr =} \" , cut_hint = \"\" , ) return result def plot_hist ( self , col : str , bin_edges : ArrayLike , use_expr : pl . Expr = pl . lit ( True ), axis : plt . Axes | None = None ) -> None : df_small = self . dfg () . lazy () . filter ( use_expr ) . select ( col ) . collect () ax = mass2 . misc . plot_hist_of_series ( df_small [ col ], bin_edges , axis ) ax . set_title ( f \" { len ( self . channels ) } channels, { self . description } \" ) def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : bool , axis : plt . Axes | None = None , use_expr : pl . Expr | None = None , skip_none : bool = True , ) -> None : \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout () def map ( self , f : Callable , allow_throw : bool = False ) -> \"Channels\" : new_channels = {} new_bad_channels = {} for key , channel in self . channels . items (): try : new_channels [ key ] = f ( channel ) except KeyboardInterrupt : raise except Exception as ex : error_type : type = type ( ex ) error_message : str = str ( ex ) backtrace : str = traceback . format_exc () if allow_throw : raise print ( f \" { key =} { channel =} failed this step\" ) print ( f \" { error_type =} \" ) print ( f \" { error_message =} \" ) new_bad_channels [ key ] = channel . as_bad ( error_type , error_message , backtrace ) new_bad_channels = mass2 . misc . merge_dicts_ordered_by_keys ( self . bad_channels , new_bad_channels ) return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def set_bad ( self , ch_num : int , msg : str , require_ch_num_exists : bool = True ) -> \"Channels\" : new_channels = {} new_bad_channels = {} if require_ch_num_exists : assert ch_num in self . channels . keys (), f \" { ch_num } can't be set bad because it does not exist\" for key , channel in self . channels . items (): if key == ch_num : new_bad_channels [ key ] = channel . as_bad ( None , msg , None ) else : new_channels [ key ] = channel return Channels ( new_channels , self . description , bad_channels = new_bad_channels ) def linefit_joblib ( self , line : str , col : str , prefer : str = \"threads\" , n_jobs : int = 4 ) -> LineModelResult : def work ( key : int ) -> LineModelResult : channel = self . channels [ key ] return channel . linefit ( line , col ) parallel = joblib . Parallel ( n_jobs = n_jobs , prefer = prefer ) # its not clear if threads are better.... what blocks the gil? results = parallel ( joblib . delayed ( work )( key ) for key in self . channels . keys ()) return results def __hash__ ( self ) -> int : # needed to make functools.cache work # if self or self.anything is mutated, assumptions will be broken # and we may get nonsense results return hash ( id ( self )) def __eq__ ( self , other : Any ) -> bool : return id ( self ) == id ( other ) @classmethod def from_ljh_path_pairs ( cls , pulse_noise_pairs : list [ tuple [ str , str ]], description : str ) -> \"Channels\" : \"\"\" Create a :class:`Channels` instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of `(pulse_path, noise_path)` tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class:`Channel` per `(pulse_path, noise_path)` pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth:`Channel.from_ljh`. The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] \"\"\" channels : dict [ int , Channel ] = {} for pulse_path , noise_path in pulse_noise_pairs : channel = Channel . from_ljh ( pulse_path , noise_path ) assert channel . header . ch_num not in channels . keys () channels [ channel . header . ch_num ] = channel return cls ( channels , description ) @classmethod def from_off_paths ( cls , off_paths : Iterable [ str | Path ], description : str ) -> \"Channels\" : channels = {} for path in off_paths : ch = Channel . from_off ( mass2 . core . OffFile ( str ( path ))) channels [ ch . header . ch_num ] = ch return cls ( channels , description ) @classmethod def from_ljh_folder ( cls , pulse_folder : str , noise_folder : str | None = None , limit : int | None = None , exclude_ch_nums : list [ int ] | None = None , ) -> \"Channels\" : assert os . path . isdir ( pulse_folder ), f \" { pulse_folder =} { noise_folder =} \" if exclude_ch_nums is None : exclude_ch_nums = [] if noise_folder is None : paths = ljhutil . find_ljh_files ( pulse_folder , exclude_ch_nums = exclude_ch_nums ) if limit is not None : paths = paths [: limit ] pairs = [( path , \"\" ) for path in paths ] else : assert os . path . isdir ( noise_folder ), f \" { pulse_folder =} { noise_folder =} \" pairs = ljhutil . match_files_by_channel ( pulse_folder , noise_folder , limit = limit , exclude_ch_nums = exclude_ch_nums ) description = f \"from_ljh_folder { pulse_folder =} { noise_folder =} \" print ( f \" { description } \" ) print ( f \" from_ljh_folder has { len ( pairs ) } pairs\" ) data = cls . from_ljh_path_pairs ( pairs , description ) print ( f \" and the Channels obj has { len ( data . channels ) } pairs\" ) return data def get_an_ljh_path ( self ) -> Path : return pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) def get_path_in_output_folder ( self , filename : str | Path ) -> Path : ljh_path = self . get_an_ljh_path () base_name , _ = ljh_path . name . split ( \"_chan\" ) date , run_num = base_name . split ( \"_run\" ) # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") output_dir = ljh_path . parent . parent / f \" { run_num } moss_output\" output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir / filename def get_experiment_state_df ( self , experiment_state_path : str | Path | None = None ) -> pl . DataFrame : if experiment_state_path is None : ljh_path = self . get_an_ljh_path () experiment_state_path = ljhutil . experiment_state_path_from_ljh_path ( ljh_path ) df = pl . read_csv ( experiment_state_path , new_columns = [ \"unixnano\" , \"state_label\" ]) # _col0, _col1 = df.columns df_es = df . select ( pl . from_epoch ( \"unixnano\" , time_unit = \"ns\" ) . dt . cast_time_unit ( \"us\" ) . alias ( \"timestamp\" )) # strip whitespace from state_label column sl_series = df . select ( pl . col ( \"state_label\" ) . str . strip_chars ()) . to_series () df_es = df_es . with_columns ( state_label = pl . Series ( values = sl_series , dtype = pl . Categorical )) return df_es def with_experiment_state_by_path ( self , experiment_state_path : str | None = None ) -> \"Channels\" : df_es = self . get_experiment_state_df ( experiment_state_path ) return self . with_experiment_state_df ( df_es ) def with_external_trigger_by_path ( self , path : str | None = None ) -> \"Channels\" : if path is None : raise NotImplementedError ( \"cannot infer external trigger path yet\" ) with open ( path , \"rb\" ) as _f : _header_line = _f . readline () # read the one header line before opening the binary data external_trigger_subframe_count = np . fromfile ( _f , \"int64\" ) df_ext = pl . DataFrame ({ \"subframecount\" : external_trigger_subframe_count , }) return self . with_external_trigger_df ( df_ext ) def with_external_trigger_df ( self , df_ext : pl . DataFrame ) -> \"Channels\" : def with_etrig_df ( channel : Channel ) -> Channel : return channel . with_external_trigger_df ( df_ext ) return self . map ( with_etrig_df ) def with_experiment_state_df ( self , df_es : pl . DataFrame ) -> \"Channels\" : # this is not as performant as making use_exprs for states # and using .set_sorted on the timestamp column ch2s = {} for ch_num , ch in self . channels . items (): ch2s [ ch_num ] = ch . with_experiment_state_df ( df_es ) return Channels ( ch2s , self . description ) def with_steps_dict ( self , steps_dict : dict [ int , Recipe ]) -> \"Channels\" : def load_recipes ( channel : Channel ) -> Channel : try : steps = steps_dict [ channel . header . ch_num ] except KeyError : raise Exception ( \"steps dict did not contain steps for this ch_num\" ) return channel . with_steps ( steps ) return self . map ( load_recipes ) def save_recipes ( self , filename : str , required_fields : str | Iterable [ str ] | None = None , drop_debug : bool = True ) -> dict [ int , Recipe ]: \"\"\"Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set `required_fields` to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters ---------- filename : str Filename to store recipe in, typically of the form \"*.pkl\" required_fields : str | Iterable[str] | None The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug: bool Whether to remove debugging-related data from each `RecipeStep`, if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns ------- dict Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. \"\"\" steps = {} for channum , ch in self . channels . items (): steps [ channum ] = ch . steps . trim_dead_ends ( required_fields = required_fields , drop_debug = drop_debug ) mass2 . misc . pickle_object ( steps , filename ) return steps def load_recipes ( self , filename : str ) -> \"Channels\" : steps = mass2 . misc . unpickle_object ( filename ) return self . with_steps_dict ( steps ) def parent_folder_path ( self ) -> pathlib . Path : parent_folder_path = pathlib . Path ( self . ch0 . header . df [ \"Filename\" ][ 0 ]) . parent . parent print ( f \" { parent_folder_path =} \" ) return parent_folder_path def concat_data ( self , other_data : \"Channels\" ) -> \"Channels\" : # sorting here to show intention, but I think set is sorted by insertion order as # an implementation detail so this may not do anything ch_nums = sorted ( list ( set ( self . channels . keys ()) . intersection ( other_data . channels . keys ()))) new_channels = {} for ch_num in ch_nums : ch = self . channels [ ch_num ] other_ch = other_data . channels [ ch_num ] combined_df = mass2 . core . misc . concat_dfs_with_concat_state ( ch . df , other_ch . df ) new_ch = ch . with_replacement_df ( combined_df ) new_channels [ ch_num ] = new_ch return mass2 . Channels ( new_channels , self . description + other_data . description ) @classmethod def from_df ( cls , df_in : pl . DataFrame , frametime_s : float , n_presamples : int , n_samples : int , description : str = \"from Channels.channels_from_df\" , ) -> \"Channels\" : # requres a column named \"ch_num\" containing the channel number keys_df : dict [ tuple , pl . DataFrame ] = df_in . partition_by ( by = [ \"ch_num\" ], as_dict = True ) dfs : dict [ int , pl . DataFrame ] = { keys [ 0 ]: df for ( keys , df ) in keys_df . items ()} channels : dict [ int , Channel ] = {} for ch_num , df in dfs . items (): channels [ ch_num ] = Channel ( df , header = ChannelHeader ( description = \"from df\" , ch_num = ch_num , frametime_s = frametime_s , n_presamples = n_presamples , n_samples = n_samples , df = df , ), npulses = len ( df ), ) return Channels ( channels , description )","title":"Channels"},{"location":"docstrings/#mass2.core.channels.Channels.from_ljh_path_pairs","text":"Create a :class: Channels instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of (pulse_path, noise_path) tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class: Channel per (pulse_path, noise_path) pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth: Channel.from_ljh . The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] Source code in mass2/core/channels.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 @classmethod def from_ljh_path_pairs ( cls , pulse_noise_pairs : list [ tuple [ str , str ]], description : str ) -> \"Channels\" : \"\"\" Create a :class:`Channels` instance from pairs of LJH files. Args: pulse_noise_pairs (List[Tuple[str, str]]): A list of `(pulse_path, noise_path)` tuples, where each entry contains the file path to a pulse LJH file and its corresponding noise LJH file. description (str): A human-readable description for the resulting Channels object. Returns: Channels: A Channels object with one :class:`Channel` per `(pulse_path, noise_path)` pair. Raises: AssertionError: If two input files correspond to the same channel number. Notes: Each channel is created via :meth:`Channel.from_ljh`. The channel number is taken from the LJH file header and used as the key in the returned Channels mapping. Examples: >>> pairs = [ ... (\"datadir/run0000_ch0000.ljh\", \"datadir/run0001_ch0000.ljh\"), ... (\"datadir/run0000_ch0001.ljh\", \"datadir/run0001_ch0001.ljh\"), ... ] >>> channels = Channels.from_ljh_path_pairs(pairs, description=\"Test run\") >>> list(channels.keys()) [0, 1] \"\"\" channels : dict [ int , Channel ] = {} for pulse_path , noise_path in pulse_noise_pairs : channel = Channel . from_ljh ( pulse_path , noise_path ) assert channel . header . ch_num not in channels . keys () channels [ channel . header . ch_num ] = channel return cls ( channels , description )","title":"from_ljh_path_pairs"},{"location":"docstrings/#mass2.core.channels.Channels.plot_hists","text":"Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. Source code in mass2/core/channels.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def plot_hists ( self , col : str , bin_edges : ArrayLike , group_by_col : bool , axis : plt . Axes | None = None , use_expr : pl . Expr | None = None , skip_none : bool = True , ) -> None : \"\"\" Plots histograms for the given column, grouped by the specified column. Parameters: - col (str): The column name to plot. - bin_edges (array-like): The edges of the bins for the histogram. - group_by_col (str): The column name to group by. This is required. - axis (matplotlib.Axes, optional): The axis to plot on. If None, a new figure is created. \"\"\" if axis is None : _ , ax = plt . subplots () # Create a new figure if no axis is provided else : ax = axis if use_expr is None : df_small = ( self . dfg () . lazy () . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) else : df_small = ( self . dfg () . lazy () . filter ( use_expr ) . select ( col , group_by_col )) . collect () . sort ( group_by_col , descending = False ) # Plot a histogram for each group for ( group_name ,), group_data in df_small . group_by ( group_by_col , maintain_order = True ): if group_name is None and skip_none : continue # Get the data for the column to plot values = group_data [ col ] # Plot the histogram for the current group if group_name == \"EBIT\" : ax . hist ( values , bins = bin_edges , alpha = 0.9 , color = \"k\" , label = str ( group_name )) else : ax . hist ( values , bins = bin_edges , alpha = 0.5 , label = str ( group_name )) # bin_centers, counts = mass2.misc.hist_of_series(values, bin_edges) # plt.plot(bin_centers, counts, label=group_name) # Customize the plot ax . set_xlabel ( str ( col )) ax . set_ylabel ( \"Frequency\" ) ax . set_title ( f \"Coadded Histogram of { col } grouped by { group_by_col } \" ) # Add a legend to label the groups ax . legend ( title = group_by_col ) plt . tight_layout ()","title":"plot_hists"},{"location":"docstrings/#mass2.core.channels.Channels.save_recipes","text":"Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set required_fields to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters: filename ( str ) \u2013 Filename to store recipe in, typically of the form \"*.pkl\" required_fields ( str | Iterable [ str ] | None , default: None ) \u2013 The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug ( bool , default: True ) \u2013 Whether to remove debugging-related data from each RecipeStep , if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns: dict \u2013 Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. Source code in mass2/core/channels.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def save_recipes ( self , filename : str , required_fields : str | Iterable [ str ] | None = None , drop_debug : bool = True ) -> dict [ int , Recipe ]: \"\"\"Pickle a dictionary (one entry per channel) of Recipe objects. If you want to save a \"recipe\", a minimal series of steps required to reproduce the required field(s), then set `required_fields` to be a list/tuple/set of DataFrame column names (or a single column name) whose production from raw data should be possible. Parameters ---------- filename : str Filename to store recipe in, typically of the form \"*.pkl\" required_fields : str | Iterable[str] | None The field (str) or fields (Iterable[str]) that the recipe should be able to generate from a raw LJH file. Drop all steps that do not lead (directly or indireactly) to producing this field or these fields. If None, then preserve all steps (default None). drop_debug: bool Whether to remove debugging-related data from each `RecipeStep`, if the subclass supports this (via the `RecipeStep.drop_debug() method). Returns ------- dict Dictionary with keys=channel numbers, values=the (possibly trimmed and debug-dropped) Recipe objects. \"\"\" steps = {} for channum , ch in self . channels . items (): steps [ channum ] = ch . steps . trim_dead_ends ( required_fields = required_fields , drop_debug = drop_debug ) mass2 . misc . pickle_object ( steps , filename ) return steps mass2.core.analysis_algorithms - main algorithms used in data analysis Designed to abstract certain key algorithms out of the class MicrocalDataSet and be able to run them fast. Created on Jun 9, 2014 @author: fowlerj","title":"save_recipes"},{"location":"docstrings/#mass2.core.analysis_algorithms.HistogramSmoother","text":"Object that can repeatedly smooth histograms with the same bin count and width to the same Gaussian width. By pre-computing the smoothing kernel for that histogram, we can smooth multiple histograms with the same geometry. Source code in mass2/core/analysis_algorithms.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 class HistogramSmoother : \"\"\"Object that can repeatedly smooth histograms with the same bin count and width to the same Gaussian width. By pre-computing the smoothing kernel for that histogram, we can smooth multiple histograms with the same geometry. \"\"\" def __init__ ( self , smooth_sigma : float , limits : ArrayLike ): \"\"\"Give the smoothing Gaussian's width as <smooth_sigma> and the [lower,upper] histogram limits as <limits>.\"\"\" self . limits = tuple ( np . asarray ( limits , dtype = float )) self . smooth_sigma = smooth_sigma # Choose a reasonable # of bins, at least 1024 and a power of 2 stepsize = 0.4 * smooth_sigma dlimits = self . limits [ 1 ] - self . limits [ 0 ] nbins_guess = int ( dlimits / stepsize + 0.5 ) min_nbins = 1024 max_nbins = 32768 # 32k bins, 2**15 # Clamp nbins_guess to at least min_nbins clamped_nbins = np . clip ( nbins_guess , min_nbins , max_nbins ) nbins_forced_to_power_of_2 = int ( 2 ** np . ceil ( np . log2 ( clamped_nbins ))) if nbins_forced_to_power_of_2 == max_nbins : print ( f \"Warning: HistogramSmoother (for drift correct) Limiting histogram bins to { max_nbins } (requested { nbins_guess } )\" ) self . nbins = nbins_forced_to_power_of_2 self . stepsize = dlimits / self . nbins # Compute the Fourier-space smoothing kernel kernel = np . exp ( - 0.5 * ( np . arange ( self . nbins ) * self . stepsize / self . smooth_sigma ) ** 2 ) kernel [ 1 :] += kernel [ - 1 : 0 : - 1 ] # Handle the negative frequencies kernel /= kernel . sum () self . kernel_ft = np . fft . rfft ( kernel ) def __call__ ( self , values : ArrayLike ) -> NDArray : \"\"\"Return a smoothed histogram of the data vector <values>\"\"\" contents , _ = np . histogram ( values , self . nbins , self . limits ) ftc = np . fft . rfft ( contents ) csmooth = np . fft . irfft ( self . kernel_ft * ftc ) csmooth [ csmooth < 0 ] = 0 return csmooth","title":"HistogramSmoother"},{"location":"docstrings/#mass2.core.analysis_algorithms.HistogramSmoother.__call__","text":"Return a smoothed histogram of the data vector Source code in mass2/core/analysis_algorithms.py 215 216 217 218 219 220 221 def __call__ ( self , values : ArrayLike ) -> NDArray : \"\"\"Return a smoothed histogram of the data vector <values>\"\"\" contents , _ = np . histogram ( values , self . nbins , self . limits ) ftc = np . fft . rfft ( contents ) csmooth = np . fft . irfft ( self . kernel_ft * ftc ) csmooth [ csmooth < 0 ] = 0 return csmooth","title":"__call__"},{"location":"docstrings/#mass2.core.analysis_algorithms.HistogramSmoother.__init__","text":"Give the smoothing Gaussian's width as and the [lower,upper] histogram limits as . Source code in mass2/core/analysis_algorithms.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def __init__ ( self , smooth_sigma : float , limits : ArrayLike ): \"\"\"Give the smoothing Gaussian's width as <smooth_sigma> and the [lower,upper] histogram limits as <limits>.\"\"\" self . limits = tuple ( np . asarray ( limits , dtype = float )) self . smooth_sigma = smooth_sigma # Choose a reasonable # of bins, at least 1024 and a power of 2 stepsize = 0.4 * smooth_sigma dlimits = self . limits [ 1 ] - self . limits [ 0 ] nbins_guess = int ( dlimits / stepsize + 0.5 ) min_nbins = 1024 max_nbins = 32768 # 32k bins, 2**15 # Clamp nbins_guess to at least min_nbins clamped_nbins = np . clip ( nbins_guess , min_nbins , max_nbins ) nbins_forced_to_power_of_2 = int ( 2 ** np . ceil ( np . log2 ( clamped_nbins ))) if nbins_forced_to_power_of_2 == max_nbins : print ( f \"Warning: HistogramSmoother (for drift correct) Limiting histogram bins to { max_nbins } (requested { nbins_guess } )\" ) self . nbins = nbins_forced_to_power_of_2 self . stepsize = dlimits / self . nbins # Compute the Fourier-space smoothing kernel kernel = np . exp ( - 0.5 * ( np . arange ( self . nbins ) * self . stepsize / self . smooth_sigma ) ** 2 ) kernel [ 1 :] += kernel [ - 1 : 0 : - 1 ] # Handle the negative frequencies kernel /= kernel . sum () self . kernel_ft = np . fft . rfft ( kernel )","title":"__init__"},{"location":"docstrings/#mass2.core.analysis_algorithms.compute_max_deriv","text":"Computes the maximum derivative in timeseries . can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in . Args: pulse_data: ignore_leading: spike_reject: (default True) kernel: the linear filter against which the signals will be convolved (CONVOLED, not correlated, so reverse the filter as needed). If None, then the default kernel of [+.2 +.1 0 -.1 -.2] will be used. If \"SG\", then the cubic 5-point Savitzky-Golay filter will be used (see below). Otherwise, kernel needs to be a (short) array which will be converted to a 1xN 2-dimensional np.ndarray. (default None) Returns: An np.ndarray, dimension 1: the value of the maximum derivative (units of per sample). When kernel==\"SG\", then we estimate the derivative by Savitzky-Golay filtering (with 1 point before/3 points after the point in question and fitting polynomial of order 3). Find the right general area by first doing a simple difference. Source code in mass2/core/analysis_algorithms.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def compute_max_deriv ( pulse_data : ArrayLike , ignore_leading : int , spike_reject : bool = True , kernel : ArrayLike | str | None = None ) -> NDArray : \"\"\"Computes the maximum derivative in timeseries <pulse_data>. <pulse_data> can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in <pulse_data>. Args: pulse_data: ignore_leading: spike_reject: (default True) kernel: the linear filter against which the signals will be convolved (CONVOLED, not correlated, so reverse the filter as needed). If None, then the default kernel of [+.2 +.1 0 -.1 -.2] will be used. If \"SG\", then the cubic 5-point Savitzky-Golay filter will be used (see below). Otherwise, kernel needs to be a (short) array which will be converted to a 1xN 2-dimensional np.ndarray. (default None) Returns: An np.ndarray, dimension 1: the value of the maximum derivative (units of <pulse_data units> per sample). When kernel==\"SG\", then we estimate the derivative by Savitzky-Golay filtering (with 1 point before/3 points after the point in question and fitting polynomial of order 3). Find the right general area by first doing a simple difference. \"\"\" # If pulse_data is a 1D array, turn it into 2 pulse_data = np . asarray ( pulse_data ) ndim = len ( pulse_data . shape ) if ndim > 2 or ndim < 1 : raise ValueError ( \"input pulse_data should be a 1d or 2d array.\" ) if ndim == 1 : pulse_data . shape = ( 1 , pulse_data . shape [ 0 ]) pulse_view = pulse_data [:, ignore_leading :] NPulse = pulse_view . shape [ 0 ] NSamp = pulse_view . shape [ 1 ] # The default filter: filter_coef = np . array ([ + 0.2 , + 0.1 , 0 , - 0.1 , - 0.2 ]) if kernel == \"SG\" : # This filter is the Savitzky-Golay filter of n_L=1, n_R=3 and M=3, to use the # language of Numerical Recipes 3rd edition. It amounts to least-squares fitting # of an M=3rd order polynomial to the five points [-1,+3] and # finding the slope of the polynomial at 0. # Note that we reverse the order of coefficients because convolution will re-reverse filter_coef = np . array ([ - 0.45238 , - 0.02381 , 0.28571 , 0.30952 , - 0.11905 ])[:: - 1 ] elif kernel is not None : filter_coef = np . array ( kernel ) . ravel () f0 , f1 , f2 , f3 , f4 = filter_coef max_deriv = np . zeros ( NPulse , dtype = np . float64 ) if spike_reject : for i in range ( NPulse ): pulses = pulse_view [ i ] t0 = f4 * pulses [ 0 ] + f3 * pulses [ 1 ] + f2 * pulses [ 2 ] + f1 * pulses [ 3 ] + f0 * pulses [ 4 ] t1 = f4 * pulses [ 1 ] + f3 * pulses [ 2 ] + f2 * pulses [ 3 ] + f1 * pulses [ 4 ] + f0 * pulses [ 5 ] t2 = f4 * pulses [ 2 ] + f3 * pulses [ 3 ] + f2 * pulses [ 4 ] + f1 * pulses [ 5 ] + f0 * pulses [ 6 ] t_max_deriv = t2 if t2 < t0 else t0 for j in range ( 7 , NSamp ): t3 = f4 * pulses [ j - 4 ] + f3 * pulses [ j - 3 ] + f2 * pulses [ j - 2 ] + f1 * pulses [ j - 1 ] + f0 * pulses [ j ] t4 = t3 if t3 < t1 else t1 t_max_deriv = max ( t4 , t_max_deriv ) t0 , t1 , t2 = t1 , t2 , t3 max_deriv [ i ] = t_max_deriv else : for i in range ( NPulse ): pulses = pulse_view [ i ] t0 = f4 * pulses [ 0 ] + f3 * pulses [ 1 ] + f2 * pulses [ 2 ] + f1 * pulses [ 3 ] + f0 * pulses [ 4 ] t_max_deriv = t0 for j in range ( 5 , NSamp ): t0 = f4 * pulses [ j - 4 ] + f3 * pulses [ j - 3 ] + f2 * pulses [ j - 2 ] + f1 * pulses [ j - 1 ] + f0 * pulses [ j ] t_max_deriv = max ( t0 , t_max_deriv ) max_deriv [ i ] = t_max_deriv return np . asarray ( max_deriv , dtype = np . float32 )","title":"compute_max_deriv"},{"location":"docstrings/#mass2.core.analysis_algorithms.correct_flux_jumps","text":"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected Source code in mass2/core/analysis_algorithms.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def correct_flux_jumps ( vals : ArrayLike , mask : ArrayLike , flux_quant : float ) -> NDArray : \"\"\"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected \"\"\" return unwrap_n ( vals , flux_quant , mask )","title":"correct_flux_jumps"},{"location":"docstrings/#mass2.core.analysis_algorithms.correct_flux_jumps_original","text":"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected Source code in mass2/core/analysis_algorithms.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def correct_flux_jumps_original ( vals : ArrayLike , mask : ArrayLike , flux_quant : float ) -> NDArray : \"\"\"Remove 'flux' jumps' from pretrigger mean. When using umux readout, if a pulse is recorded that has a very fast rising edge (e.g. a cosmic ray), the readout system will \"slip\" an integer number of flux quanta. This means that the baseline level returned to after the pulse will different from the pretrigger value by an integer number of flux quanta. This causes that pretrigger mean summary quantity to jump around in a way that causes trouble for the rest of MASS. This function attempts to correct these jumps. Arguments: vals -- array of values to correct mask -- mask indentifying \"good\" pulses flux_quant -- size of 1 flux quanta Returns: Array with values corrected \"\"\" # The naive thing is to simply replace each value with its value mod # the flux quantum. But of the baseline value turns out to fluctuate # about an integer number of flux quanta, this will introduce new # jumps. I don't know the best way to handle this in general. For now, # if there are still jumps after the mod, I add 1/4 of a flux quanta # before modding, then mod, then subtract the 1/4 flux quantum and then # *add* a single flux quantum so that the values never go negative. # # To determine whether there are \"still jumps after the mod\" I look at the # difference between the largest and smallest values for \"good\" pulses. If # you don't exclude \"bad\" pulses, this check can be tricked in cases where # the pretrigger section contains a (sufficiently large) tail. vals = np . asarray ( vals ) mask = np . asarray ( mask ) if ( np . amax ( vals ) - np . amin ( vals )) >= flux_quant : corrected = vals % flux_quant if ( np . amax ( corrected [ mask ]) - np . amin ( corrected [ mask ])) > 0.75 * flux_quant : corrected = ( vals + flux_quant / 4 ) % ( flux_quant ) corrected = corrected - flux_quant / 4 + flux_quant corrected -= corrected [ 0 ] - vals [ 0 ] return corrected else : return vals","title":"correct_flux_jumps_original"},{"location":"docstrings/#mass2.core.analysis_algorithms.drift_correct","text":"Compute a drift correction that minimizes the spectral entropy. Args: indicator: The \"x-axis\", which indicates the size of the correction. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. limit: The upper limit of uncorrected values over which entropy is computed (default None). Generally indicator will be the pretrigger mean of the pulses, but you can experiment with other choices. The entropy will be computed on corrected values only in the range [0, limit], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then in will be compute as 25% larger than the 99%ile point of uncorrected. The model is that the filtered pulse height PH should be scaled by (1 + a*PTM) where a is an arbitrary parameter computed here, and PTM is the difference between each record's pretrigger mean and the median value of all pretrigger means. (Or replace \"pretrigger mean\" with whatever quantity you passed in as .) Source code in mass2/core/analysis_algorithms.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def drift_correct ( indicator : ArrayLike , uncorrected : ArrayLike , limit : float | None = None ) -> tuple [ float , dict ]: \"\"\"Compute a drift correction that minimizes the spectral entropy. Args: indicator: The \"x-axis\", which indicates the size of the correction. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. limit: The upper limit of uncorrected values over which entropy is computed (default None). Generally indicator will be the pretrigger mean of the pulses, but you can experiment with other choices. The entropy will be computed on corrected values only in the range [0, limit], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then in will be compute as 25% larger than the 99%ile point of uncorrected. The model is that the filtered pulse height PH should be scaled by (1 + a*PTM) where a is an arbitrary parameter computed here, and PTM is the difference between each record's pretrigger mean and the median value of all pretrigger means. (Or replace \"pretrigger mean\" with whatever quantity you passed in as <indicator>.) \"\"\" uncorrected = np . asarray ( uncorrected ) indicator = np . array ( indicator ) # make a copy ptm_offset = np . median ( indicator ) indicator -= ptm_offset if limit is None : pct99 = np . percentile ( uncorrected , 99 ) limit = 1.25 * pct99 smoother = HistogramSmoother ( 0.5 , [ 0 , limit ]) assert smoother . nbins < 1e6 , \"will be crazy slow, should not be possible\" def entropy ( param : NDArray , indicator : NDArray , uncorrected : NDArray , smoother : HistogramSmoother ) -> float : corrected = uncorrected * ( 1 + indicator * param ) hsmooth = smoother ( corrected ) w = hsmooth > 0 return - ( np . log ( hsmooth [ w ]) * hsmooth [ w ]) . sum () drift_corr_param = sp . optimize . brent ( entropy , ( indicator , uncorrected , smoother ), brack = [ 0 , 0.001 ]) drift_correct_info = { \"type\" : \"ptmean_gain\" , \"slope\" : drift_corr_param , \"median_pretrig_mean\" : ptm_offset } return drift_corr_param , drift_correct_info","title":"drift_correct"},{"location":"docstrings/#mass2.core.analysis_algorithms.estimateRiseTime","text":"Computes the rise time of timeseries , where the time steps are . can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in . If nPretrig >= 4, then the samples pulse_data[:nPretrig] are averaged to estimate the baseline. Otherwise, the minimum of pulse_data is assumed to be the baseline. Specifically, take the first and last of the rising points in the range of 10% to 90% of the peak value, interpolate a line between the two, and use its slope to find the time to rise from 0 to the peak. Args: pulse_data: An np.ndarray of dimension 1 (a single pulse record) or 2 (an array with each row being a pulse record). timebase: The sampling time. nPretrig: The number of samples that are recorded before the trigger. Returns: An ndarray of dimension 1, giving the rise times. Source code in mass2/core/analysis_algorithms.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @njit def estimateRiseTime ( pulse_data : ArrayLike , timebase : float , nPretrig : int ) -> NDArray : \"\"\"Computes the rise time of timeseries <pulse_data>, where the time steps are <timebase>. <pulse_data> can be a 2D array where each row is a different pulse record, in which case the return value will be an array last long as the number of rows in <pulse_data>. If nPretrig >= 4, then the samples pulse_data[:nPretrig] are averaged to estimate the baseline. Otherwise, the minimum of pulse_data is assumed to be the baseline. Specifically, take the first and last of the rising points in the range of 10% to 90% of the peak value, interpolate a line between the two, and use its slope to find the time to rise from 0 to the peak. Args: pulse_data: An np.ndarray of dimension 1 (a single pulse record) or 2 (an array with each row being a pulse record). timebase: The sampling time. nPretrig: The number of samples that are recorded before the trigger. Returns: An ndarray of dimension 1, giving the rise times. \"\"\" MINTHRESH , MAXTHRESH = 0.1 , 0.9 # If pulse_data is a 1D array, turn it into 2 pulse_data = np . asarray ( pulse_data ) ndim = len ( pulse_data . shape ) if ndim > 2 or ndim < 1 : raise ValueError ( \"input pulse_data should be a 1d or 2d array.\" ) if ndim == 1 : pulse_data . shape = ( 1 , pulse_data . shape [ 0 ]) # The following requires a lot of numpy foo to read. Sorry! if nPretrig >= 4 : baseline_value = pulse_data [:, 0 : nPretrig ] . mean ( axis = 1 ) else : baseline_value = pulse_data . min ( axis = 1 ) nPretrig = 0 value_at_peak = pulse_data . max ( axis = 1 ) - baseline_value idx_last_pk = pulse_data . argmax ( axis = 1 ) . max () npulses = pulse_data . shape [ 0 ] try : rising_data = ( pulse_data [:, nPretrig : idx_last_pk + 1 ] - baseline_value [:, np . newaxis ]) / value_at_peak [:, np . newaxis ] # Find the last and first indices at which the data are in (0.1, 0.9] times the # peak value. Then make sure last is at least 1 past first. last_idx = ( rising_data > MAXTHRESH ) . argmax ( axis = 1 ) - 1 first_idx = ( rising_data > MINTHRESH ) . argmax ( axis = 1 ) last_idx [ last_idx < first_idx ] = first_idx [ last_idx < first_idx ] + 1 last_idx [ last_idx == rising_data . shape [ 1 ]] = rising_data . shape [ 1 ] - 1 pulsenum = np . arange ( npulses ) y_diff = np . asarray ( rising_data [ pulsenum , last_idx ] - rising_data [ pulsenum , first_idx ], dtype = float ) y_diff [ y_diff < timebase ] = timebase time_diff = timebase * ( last_idx - first_idx ) rise_time = time_diff / y_diff rise_time [ y_diff <= 0 ] = - 9.9e-6 return rise_time except ValueError : return - 9.9e-6 + np . zeros ( npulses , dtype = float )","title":"estimateRiseTime"},{"location":"docstrings/#mass2.core.analysis_algorithms.filter_signal_lowpass","text":"Tophat lowpass filter using an FFT Args: sig - the signal to be filtered fs - the sampling frequency of the signal fcut - the frequency at which to cutoff the signal Returns: the filtered signal Source code in mass2/core/analysis_algorithms.py 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 @njit def filter_signal_lowpass ( sig : NDArray , fs : float , fcut : float ) -> NDArray : \"\"\"Tophat lowpass filter using an FFT Args: sig - the signal to be filtered fs - the sampling frequency of the signal fcut - the frequency at which to cutoff the signal Returns: the filtered signal \"\"\" N = sig . shape [ 0 ] SIG = np . fft . fft ( sig ) freqs = ( fs / N ) * np . concatenate (( np . arange ( 0 , N / 2 + 1 ), np . arange ( N / 2 - 1 , 0 , - 1 ))) filt = np . zeros_like ( SIG ) filt [ freqs < fcut ] = 1.0 sig_filt = np . fft . ifft ( SIG * filt ) return sig_filt","title":"filter_signal_lowpass"},{"location":"docstrings/#mass2.core.analysis_algorithms.make_smooth_histogram","text":"Convert a vector of arbitrary info a smoothed histogram by histogramming it and smoothing. This is a convenience function using the HistogramSmoother class. Args: values: The vector of data to be histogrammed. smooth_sigma: The smoothing Gaussian's width (FWHM) limit, upper_limit: The histogram limits are [limit,upper_limit] or [0,limit] if upper_limit is None. Returns: The smoothed histogram as an array. Source code in mass2/core/analysis_algorithms.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 @njit def make_smooth_histogram ( values : ArrayLike , smooth_sigma : float , limit : float , upper_limit : float | None = None ) -> NDArray : \"\"\"Convert a vector of arbitrary <values> info a smoothed histogram by histogramming it and smoothing. This is a convenience function using the HistogramSmoother class. Args: values: The vector of data to be histogrammed. smooth_sigma: The smoothing Gaussian's width (FWHM) limit, upper_limit: The histogram limits are [limit,upper_limit] or [0,limit] if upper_limit is None. Returns: The smoothed histogram as an array. \"\"\" if upper_limit is None : limit , upper_limit = 0 , limit return HistogramSmoother ( smooth_sigma , [ limit , upper_limit ])( values )","title":"make_smooth_histogram"},{"location":"docstrings/#mass2.core.analysis_algorithms.nearest_arrivals","text":"Find the external trigger time immediately before and after each pulse timestamp Args: pulse_timestamps - 1d array of pulse timestamps whose nearest neighbors need to be found. external_trigger_timestamps - 1d array of possible nearest neighbors. Returns: (before_times, after_times) before_times is an ndarray of the same size as pulse_timestamps. before_times[i] contains the difference between the closest lesser time contained in external_trigger_timestamps and pulse_timestamps[i] or inf if there was no earlier time in other_times Note that before_times is always a positive number even though the time difference it represents is negative. after_times is an ndarray of the same size as pulse_timestamps. after_times[i] contains the difference between pulse_timestamps[i] and the closest greater time contained in other_times or a inf number if there was no later time in external_trigger_timestamps. Source code in mass2/core/analysis_algorithms.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 @njit def nearest_arrivals ( reference_times : ArrayLike , other_times : ArrayLike ) -> tuple [ NDArray , NDArray ]: \"\"\"Find the external trigger time immediately before and after each pulse timestamp Args: pulse_timestamps - 1d array of pulse timestamps whose nearest neighbors need to be found. external_trigger_timestamps - 1d array of possible nearest neighbors. Returns: (before_times, after_times) before_times is an ndarray of the same size as pulse_timestamps. before_times[i] contains the difference between the closest lesser time contained in external_trigger_timestamps and pulse_timestamps[i] or inf if there was no earlier time in other_times Note that before_times is always a positive number even though the time difference it represents is negative. after_times is an ndarray of the same size as pulse_timestamps. after_times[i] contains the difference between pulse_timestamps[i] and the closest greater time contained in other_times or a inf number if there was no later time in external_trigger_timestamps. \"\"\" other_times = np . asarray ( other_times ) nearest_after_index = np . searchsorted ( other_times , reference_times ) # because both sets of arrival times should be sorted, there are faster algorithms than searchsorted # for example: https://github.com/kwgoodman/bottleneck/issues/47 # we could use one if performance becomes an issue last_index = np . searchsorted ( nearest_after_index , other_times . size , side = \"left\" ) first_index = np . searchsorted ( nearest_after_index , 1 ) nearest_before_index = np . copy ( nearest_after_index ) nearest_before_index [: first_index ] = 1 nearest_before_index -= 1 before_times = reference_times - other_times [ nearest_before_index ] before_times [: first_index ] = np . inf nearest_after_index [ last_index :] = other_times . size - 1 after_times = other_times [ nearest_after_index ] - reference_times after_times [ last_index :] = np . inf return before_times , after_times","title":"nearest_arrivals"},{"location":"docstrings/#mass2.core.analysis_algorithms.time_drift_correct","text":"Compute a time-based drift correction that minimizes the spectral entropy. Args: time: The \"time-axis\". Correction will be a low-order polynomial in this. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. w: the kernel width for the Laplace KDE density estimator sec_per_degree: assign as many as one polynomial degree per this many seconds pulses_per_degree: assign as many as one polynomial degree per this many pulses max_degrees: never use more than this many degrees of Legendre polynomial. n_deg: If not None, use this many degrees, regardless of the values of sec_per_degree, pulses_per_degree, and max_degress. In this case, never downsample. limit: The [lower,upper] limit of uncorrected values over which entropy is computed (default None). The entropy will be computed on corrected values only in the range [limit[0], limit[1]], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then it will be computed as 25%% larger than the 99%%ile point of uncorrected. Possible improvements in the future: * Use Numba to speed up. * Allow the parameters to be function arguments with defaults: photons per degree of freedom, seconds per degree of freedom, and max degrees of freedom. * Figure out how to span the available time with more than one set of legendre polynomials, so that we can have more than 20 d.o.f. eventually, for long runs. Source code in mass2/core/analysis_algorithms.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 def time_drift_correct ( # noqa: PLR0914 time : ArrayLike , uncorrected : ArrayLike , w : float , sec_per_degree : float = 2000 , pulses_per_degree : int = 2000 , max_degrees : int = 20 , ndeg : int | None = None , limit : tuple [ float , float ] | None = None , ) -> dict [ str , Any ]: \"\"\"Compute a time-based drift correction that minimizes the spectral entropy. Args: time: The \"time-axis\". Correction will be a low-order polynomial in this. uncorrected: A filtered pulse height vector. Same length as indicator. Assumed to have some gain that is linearly related to indicator. w: the kernel width for the Laplace KDE density estimator sec_per_degree: assign as many as one polynomial degree per this many seconds pulses_per_degree: assign as many as one polynomial degree per this many pulses max_degrees: never use more than this many degrees of Legendre polynomial. n_deg: If not None, use this many degrees, regardless of the values of sec_per_degree, pulses_per_degree, and max_degress. In this case, never downsample. limit: The [lower,upper] limit of uncorrected values over which entropy is computed (default None). The entropy will be computed on corrected values only in the range [limit[0], limit[1]], so limit should be set to a characteristic large value of uncorrected. If limit is None (the default), then it will be computed as 25%% larger than the 99%%ile point of uncorrected. Possible improvements in the future: * Use Numba to speed up. * Allow the parameters to be function arguments with defaults: photons per degree of freedom, seconds per degree of freedom, and max degrees of freedom. * Figure out how to span the available time with more than one set of legendre polynomials, so that we can have more than 20 d.o.f. eventually, for long runs. \"\"\" time = np . asarray ( time ) uncorrected = np . asarray ( uncorrected ) if limit is None : pct99 = np . percentile ( uncorrected , 99 ) limit = ( 0 , 1.25 * pct99 ) use = np . logical_and ( uncorrected > limit [ 0 ], uncorrected < limit [ 1 ]) time = np . asarray ( time [ use ]) uncorrected = np . asarray ( uncorrected [ use ]) tmin , tmax = np . min ( time ), np . max ( time ) def normalize ( t : NDArray ) -> NDArray : return ( t - tmin ) / ( tmax - tmin ) * 2 - 1 info = { \"tmin\" : tmin , \"tmax\" : tmax , \"normalize\" : normalize , } dtime = tmax - tmin N = len ( time ) if ndeg is None : ndeg = int ( np . minimum ( dtime / sec_per_degree , N / pulses_per_degree )) ndeg = min ( ndeg , max_degrees ) ndeg = max ( ndeg , 1 ) phot_per_degree = N / float ( ndeg ) if phot_per_degree >= 2 * pulses_per_degree : downsample = int ( phot_per_degree / pulses_per_degree ) time = time [:: downsample ] uncorrected = uncorrected [:: downsample ] N = len ( time ) else : downsample = 1 else : downsample = 1 LOG . info ( \"Using %2d degrees for %6d photons (after %d downsample)\" , ndeg , N , downsample ) LOG . info ( \"That's %6.1f photons per degree, and %6.1f seconds per degree.\" , N / float ( ndeg ), dtime / ndeg ) def model1 ( pi : NDArray , i : int , param : NDArray , basis : NDArray ) -> NDArray : pcopy = np . array ( param ) pcopy [ i ] = pi return 1 + np . dot ( basis . T , pcopy ) def cost1 ( pi : NDArray , i : int , param : NDArray , y : NDArray , w : float , basis : NDArray ) -> float : return laplace_entropy ( y * model1 ( pi , i , param , basis ), w = w ) param = np . zeros ( ndeg , dtype = float ) xnorm = np . asarray ( normalize ( time ), dtype = float ) basis = np . vstack ([ sp . special . legendre ( i + 1 )( xnorm ) for i in range ( ndeg )]) fc = 0 model : Callable = np . poly1d ([ 0 ]) info [ \"coefficients\" ] = np . zeros ( ndeg , dtype = float ) for i in range ( ndeg ): result , _fval , _iter , funcalls = sp . optimize . brent ( cost1 , ( i , param , uncorrected , w , basis ), [ - 0.001 , 0.001 ], tol = 1e-5 , full_output = True ) param [ i ] = result fc += funcalls model += sp . special . legendre ( i + 1 ) * result info [ \"coefficients\" ][ i ] = result info [ \"funccalls\" ] = fc xk = np . linspace ( - 1 , 1 , 1 + 2 * ndeg ) model2 = CubicSpline ( xk , model ( xk )) H1 = laplace_entropy ( uncorrected , w = w ) H2 = laplace_entropy ( uncorrected * ( 1 + model ( xnorm )), w = w ) H3 = laplace_entropy ( uncorrected * ( 1 + model2 ( xnorm )), w = w ) if H2 <= 0 or H2 - H1 > 0.0 : model = np . poly1d ([ 0 ]) elif H3 <= 0 or H3 - H2 > 0.00001 : model = model2 info [ \"entropies\" ] = ( H1 , H2 , H3 ) info [ \"model\" ] = model return info","title":"time_drift_correct"},{"location":"docstrings/#mass2.core.analysis_algorithms.unwrap_n","text":"Unwrap data that has been restricted to a given period. The algorithm iterates through each data point and compares it to the average of the previous n data points. It then offsets the data point by the multiple of the period that will minimize the difference from that n-point running average. For the first n data points, there are not enough preceding points to average n of them, so the algorithm will average fewer points. This code was written by Thomas Baker; integrated into MASS by Dan Becker. Sped up 300x by @njit. Parameters: data ( array of data values ) \u2013 period ( the range over which the data loops ) \u2013 n ( how many preceding points to average , default: 3 ) \u2013 mask ( ArrayLike ) \u2013 Source code in mass2/core/analysis_algorithms.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 @njit def unwrap_n ( data : NDArray [ np . uint16 ], period : float , mask : ArrayLike , n : int = 3 ) -> NDArray : \"\"\"Unwrap data that has been restricted to a given period. The algorithm iterates through each data point and compares it to the average of the previous n data points. It then offsets the data point by the multiple of the period that will minimize the difference from that n-point running average. For the first n data points, there are not enough preceding points to average n of them, so the algorithm will average fewer points. This code was written by Thomas Baker; integrated into MASS by Dan Becker. Sped up 300x by @njit. Parameters ---------- data : array of data values period : the range over which the data loops n : how many preceding points to average mask: mask indentifying \"good\" pulses \"\"\" mask = np . asarray ( mask ) udata = np . copy ( data ) # make a copy for output if n <= 0 : return udata # Iterate through each data point and offset it by # an amount that will minimize the difference from the # rolling average nprior = 0 firstgoodidx = np . argmax ( mask ) priorvalues = np . full ( n , udata [ firstgoodidx ]) for i in range ( len ( data )): # Take the average of the previous n data points (only those with mask[i]==True). # Offset the data point by the most reasonable multiple of period (make this point closest to the running average). if mask [ i ]: avg = np . mean ( priorvalues ) if nprior == 0 : avg = float ( priorvalues [ 0 ]) elif nprior < n : avg = np . mean ( priorvalues [: nprior ]) udata [ i ] -= np . round (( udata [ i ] - avg ) / period ) * period if mask [ i ]: priorvalues [ nprior % n ] = udata [ i ] nprior += 1 return udata","title":"unwrap_n"},{"location":"docstrings/#mass2.core.recipe.ColumnAsNumpyMapStep","text":"Bases: RecipeStep This step is meant for interactive exploration, it takes a column and applies a function to it, and makes a new column with the result. It makes it easy to test functions on a column without having to write a whole new step class, while maintaining the benefit of being able to use the step in a Recipe chain, like replaying steps on another channel. example usage: def my_function(x): ... return x * 2 step = ColumnAsNumpyMapStep(inputs=[\"my_column\"], output=[\"my_new_column\"], f=my_function) ch2 = ch.with_step(step) Source code in mass2/core/recipe.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 @dataclass ( frozen = True ) class ColumnAsNumpyMapStep ( RecipeStep ): \"\"\" This step is meant for interactive exploration, it takes a column and applies a function to it, and makes a new column with the result. It makes it easy to test functions on a column without having to write a whole new step class, while maintaining the benefit of being able to use the step in a Recipe chain, like replaying steps on another channel. example usage: >>> def my_function(x): ... return x * 2 >>> step = ColumnAsNumpyMapStep(inputs=[\"my_column\"], output=[\"my_new_column\"], f=my_function) >>> ch2 = ch.with_step(step) \"\"\" f : Callable [[ np . ndarray ], np . ndarray ] def __post_init__ ( self ) -> None : assert len ( self . inputs ) == 1 , \"ColumnMapStep expects exactly one input\" assert len ( self . output ) == 1 , \"ColumnMapStep expects exactly one output\" if not callable ( self . f ): raise ValueError ( f \"f must be a callable, got { self . f } \" ) def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : output_col = self . output [ 0 ] output_segments = [] for df_iter in df . select ( self . inputs ) . iter_slices (): series1 = df_iter [ self . inputs [ 0 ]] # Have to apply the function differently when series elements are arrays vs scalars if series1 . dtype . base_type () is pl . Array : output_numpy = np . array ([ self . f ( v . to_numpy ()) for v in series1 ]) else : output_numpy = self . f ( series1 . to_numpy ()) this_output_segment = pl . Series ( output_col , output_numpy ) output_segments . append ( this_output_segment ) combined = pl . concat ( output_segments ) # Put into a DataFrame with one column df2 = pl . DataFrame ({ output_col : combined }) . with_columns ( df ) return df2","title":"ColumnAsNumpyMapStep"},{"location":"docstrings/#mass2.core.recipe.Recipe","text":"Bases: Sequence [ RecipeStep ] Source code in mass2/core/recipe.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 @dataclass ( frozen = True ) class Recipe ( Sequence [ RecipeStep ]): steps : list [ RecipeStep ] # TODO: leaves many optimizations on the table, but is very simple # 1. we could calculate filt_value_5lag and filt_phase_5lag at the same time # 2. we could calculate intermediate quantities optionally and not materialize all of them def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"return a dataframe with all the newly calculated info\" for step in self . steps : df = step . calc_from_df ( df ) . with_columns ( df ) return df @classmethod def new_empty ( cls ) -> \"Recipe\" : return cls ([]) @overload def __getitem__ ( self , key : int ) -> RecipeStep : ... @overload def __getitem__ ( self , key : slice ) -> Sequence [ RecipeStep ]: ... def __getitem__ ( self , key : int | slice ) -> RecipeStep | Sequence [ RecipeStep ]: return self . steps [ key ] def __len__ ( self ) -> int : return len ( self . steps ) def with_step ( self , step : RecipeStep ) -> \"Recipe\" : # return a new Recipe with the step added, no mutation! return Recipe ( self . steps + [ step ]) def trim_dead_ends ( self , required_fields : Iterable [ str ] | str | None , drop_debug : bool = True ) -> \"Recipe\" : \"\"\"Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with `drop_debug=True`). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in `required_fields`. The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the `required_fields` (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters ---------- required_fields : Iterable[str] | str | None Steps will be preserved if any of their outputs are among `required_fields`, or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug : bool Whether to run `step.drop_debug()` to remove debugging information from the preserved steps. Returns ------- Recipe A copy of `self`, except that any steps not required to compute any of `required_fields` are omitted. \"\"\" if isinstance ( required_fields , str ): required_fields = [ required_fields ] nsteps = len ( self ) required = np . zeros ( nsteps , dtype = bool ) # The easiest approach is to traverse the steps from last to first to build our list of required # fields, because necessarily no later step can produce the inputs needed by an earlier step. if required_fields is None : required [:] = True else : all_fields_out : set [ str ] = set ( required_fields ) for istep in range ( nsteps - 1 , - 1 , - 1 ): step = self [ istep ] for field in step . output : if field in all_fields_out : required [ istep ] = True all_fields_out . update ( step . inputs ) break if not np . any ( required ): # If this error ever because a problem, where user _acutally_ wants an empty series of steps # to be a non-err, then add argument `error_on_empty_output=True` to this method. raise ValueError ( \"trim_dead_ends found no steps to be preserved\" ) steps = [] for i in range ( nsteps ): if required [ i ]: if drop_debug : steps . append ( self [ i ] . drop_debug ()) else : steps . append ( self [ i ]) return Recipe ( steps )","title":"Recipe"},{"location":"docstrings/#mass2.core.recipe.Recipe.calc_from_df","text":"return a dataframe with all the newly calculated info Source code in mass2/core/recipe.py 189 190 191 192 193 def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : \"return a dataframe with all the newly calculated info\" for step in self . steps : df = step . calc_from_df ( df ) . with_columns ( df ) return df","title":"calc_from_df"},{"location":"docstrings/#mass2.core.recipe.Recipe.trim_dead_ends","text":"Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with drop_debug=True ). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in required_fields . The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the required_fields (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters: required_fields ( Iterable [ str ] | str | None ) \u2013 Steps will be preserved if any of their outputs are among required_fields , or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug ( bool , default: True ) \u2013 Whether to run step.drop_debug() to remove debugging information from the preserved steps. Returns: Recipe \u2013 A copy of self , except that any steps not required to compute any of required_fields are omitted. Source code in mass2/core/recipe.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def trim_dead_ends ( self , required_fields : Iterable [ str ] | str | None , drop_debug : bool = True ) -> \"Recipe\" : \"\"\"Create a new Recipe object with all dead-end steps (and optionally also debug info) removed. The purpose is to replace the fully useful interactive Recipe with a trimmed-down object that can repeat the current steps as a \"recipe\" without having the extra information from which the recipe was first created. In one test, this method reduced the pickle file's size from 3.4 MB per channel to 30 kB per channel, or a 112x size reduction (with `drop_debug=True`). Dead-end steps are defined as any step that can be omitted without affecting the ability to compute any of the fields given in `required_fields`. The result of this method is to return a Recipe where any step is remove if it does not contribute to computing any of the `required_fields` (i.e., if it is a dead end). Examples of a dead end are typically steps used to prepare a tentative, intermediate calibration function. Parameters ---------- required_fields : Iterable[str] | str | None Steps will be preserved if any of their outputs are among `required_fields`, or if their outputs are found recursively among the inputs to any such steps. If a string, treat as a list of that one string. If None, preserve all steps. drop_debug : bool Whether to run `step.drop_debug()` to remove debugging information from the preserved steps. Returns ------- Recipe A copy of `self`, except that any steps not required to compute any of `required_fields` are omitted. \"\"\" if isinstance ( required_fields , str ): required_fields = [ required_fields ] nsteps = len ( self ) required = np . zeros ( nsteps , dtype = bool ) # The easiest approach is to traverse the steps from last to first to build our list of required # fields, because necessarily no later step can produce the inputs needed by an earlier step. if required_fields is None : required [:] = True else : all_fields_out : set [ str ] = set ( required_fields ) for istep in range ( nsteps - 1 , - 1 , - 1 ): step = self [ istep ] for field in step . output : if field in all_fields_out : required [ istep ] = True all_fields_out . update ( step . inputs ) break if not np . any ( required ): # If this error ever because a problem, where user _acutally_ wants an empty series of steps # to be a non-err, then add argument `error_on_empty_output=True` to this method. raise ValueError ( \"trim_dead_ends found no steps to be preserved\" ) steps = [] for i in range ( nsteps ): if required [ i ]: if drop_debug : steps . append ( self [ i ] . drop_debug ()) else : steps . append ( self [ i ]) return Recipe ( steps )","title":"trim_dead_ends"},{"location":"docstrings/#mass2.core.recipe.RecipeStep","text":"Source code in mass2/core/recipe.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @dataclass ( frozen = True ) class RecipeStep : inputs : list [ str ] output : list [ str ] good_expr : pl . Expr use_expr : pl . Expr @property def name ( self ) -> str : return str ( type ( self )) @property def description ( self ) -> str : return f \" { type ( self ) . __name__ } inputs= { self . inputs } outputs= { self . output } \" def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : # TODO: should this be an abstract method? return df . filter ( self . good_expr ) def dbg_plot ( self , df_after : pl . DataFrame , ** kwargs : Any ) -> plt . Axes : # this is a no-op, subclasses can override this to plot something plt . figure () plt . text ( 0.0 , 0.5 , f \"No plot defined for: { self . description } \" ) return plt . gca () def drop_debug ( self ) -> \"RecipeStep\" : \"Return self, or a copy of it with debug information removed\" return self","title":"RecipeStep"},{"location":"docstrings/#mass2.core.recipe.RecipeStep.drop_debug","text":"Return self, or a copy of it with debug information removed Source code in mass2/core/recipe.py 35 36 37 def drop_debug ( self ) -> \"RecipeStep\" : \"Return self, or a copy of it with debug information removed\" return self","title":"drop_debug"},{"location":"docstrings/#mass2.core.recipe.SelectStep","text":"Bases: RecipeStep This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. Source code in mass2/core/recipe.py 167 168 169 170 171 172 173 174 175 176 177 178 @dataclass ( frozen = True ) class SelectStep ( RecipeStep ): \"\"\" This step is meant for interactive exploration, it's basically like the df.select() method, but it's saved as a step. \"\"\" col_expr_dict : dict [ str , pl . Expr ] def calc_from_df ( self , df : pl . DataFrame ) -> pl . DataFrame : df2 = df . select ( ** self . col_expr_dict ) . with_columns ( df ) return df2 Classes to create time-domain and Fourier-domain optimal filters.","title":"SelectStep"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter","text":"Bases: ABC A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level. Returns: Filter \u2013 A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted variance due to noise and the resulting predicted_v_over_dv , the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: nominal_peak . Source code in mass2/core/optimal_filtering.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 @dataclass ( frozen = True ) class Filter ( ABC ): \"\"\"A single optimal filter, possibly with optimal estimators of the Delta-t and of the DC level. Returns ------- Filter A set of optimal filter values. These values are chosen with the following specifics: * one model of the pulses and of the noise, including pulse record length, * a first-order arrival-time detection filter is (optionally) computed * filtering model (1-lag, or other odd # of lags), * low-pass smoothing of the filter itself, * a fixed number of samples \"cut\" (given zero weight) at the start and/or end of records. The object also stores the pulse shape and (optionally) the delta-T shape used to generate it, and the low-pass filter's fmax or f_3db (cutoff or rolloff) frequency. It also stores the predicted `variance` due to noise and the resulting `predicted_v_over_dv`, the ratio of the filtered pulse height to the (FWHM) noise, in pulse height units. Both of these values assume pulses of the same size as that used to generate the filter: `nominal_peak`. \"\"\" values : np . ndarray nominal_peak : float variance : float predicted_v_over_dv : float dt_values : np . ndarray | None const_values : np . ndarray | None signal_model : np . ndarray | None dt_model : np . ndarray | None convolution_lags : int = 1 fmax : float | None = None f_3db : float | None = None cut_pre : int = 0 cut_post : int = 0 @property @abstractmethod def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property @abstractmethod def _filter_type ( self ) -> str : return \"illegal: this is supposed to be an abstract base class\" def plot ( self , axis : plt . Axes | None = None , ** kwargs : Any ) -> None : \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout () def report ( self , std_energy : float = 5898.8 ) -> None : \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" ) @abstractmethod def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: pass","title":"Filter"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.is_arrival_time_safe","text":"Is this an arrival-time-safe filter?","title":"is_arrival_time_safe"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.plot","text":"Make a plot of the filter Parameters: axis ( Axes , default: None ) \u2013 A pre-existing axis to plot on, by default None Source code in mass2/core/optimal_filtering.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def plot ( self , axis : plt . Axes | None = None , ** kwargs : Any ) -> None : \"\"\"Make a plot of the filter Parameters ---------- axis : plt.Axes, optional A pre-existing axis to plot on, by default None \"\"\" if axis is None : plt . clf () axis = plt . subplot ( 111 ) axis . plot ( self . values , label = \"mass 5lag filter\" , ** kwargs ) axis . grid () axis . set_title ( f \" { self . _filter_type =} V/dV= { self . predicted_v_over_dv : .2f } \" ) axis . set_ylabel ( \"filter value\" ) axis . set_xlabel ( \"Lag Time (s)\" ) plt . gcf () . tight_layout ()","title":"plot"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter.report","text":"Report on estimated V/dV for the filter. Parameters: std_energy ( float , default: 5898.8 ) \u2013 Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 Source code in mass2/core/optimal_filtering.py 299 300 301 302 303 304 305 306 307 308 309 310 311 def report ( self , std_energy : float = 5898.8 ) -> None : \"\"\"Report on estimated V/dV for the filter. Parameters ---------- std_energy : float, optional Energy (in eV) of a \"standard\" pulse. Resolution will be given in eV at this energy, assuming linear devices, by default 5898.8 \"\"\" var = self . variance v_dv = self . predicted_v_over_dv fwhm_eV = std_energy / v_dv print ( f \"v/ \\u03b4 v: { v_dv : .2f } , variance: { var : .2f } \\u03b4 E: { fwhm_eV : .2f } eV (FWHM), assuming standard E= { std_energy : .2f } eV\" )","title":"report"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag","text":"Bases: Filter Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015. Returns: Filter5Lag \u2013 An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 @dataclass ( frozen = True ) class Filter5Lag ( Filter ): \"\"\"Represent an optimal filter, specifically one intended for 5-lag convolution with data The traditional 5-lag filter used by default until 2015. Returns ------- Filter5Lag An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ) -> None : assert self . convolution_lags == 5 @property def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return False @property def _filter_type ( self ) -> str : return \"5lag\" # These parameters fit a parabola to any 5 evenly-spaced points FIVELAG_FITTER = ( np . array ( ( ( - 6 , 24 , 34 , 24 , - 6 ), ( - 14 , - 7 , 0 , 7 , 14 ), ( 10 , - 5 , - 10 , - 5 , 10 ), ), dtype = float , ) / 70.0 ) def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x","title":"Filter5Lag"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag.is_arrival_time_safe","text":"Is this an arrival-time-safe filter?","title":"is_arrival_time_safe"},{"location":"docstrings/#mass2.core.optimal_filtering.Filter5Lag.filter_records","text":"Filter one microcalorimeter record or an array of records. Parameters: x ( ArrayLike ) \u2013 A 1-d array, a single pulse record, or a 2-d array, where x[i, :] is pulse record number i . Returns: tuple [ ndarray , ndarray ] \u2013 The optimally filtered value, or an array (one per row) if the input is a 2-d array. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises: AssertionError \u2013 If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, where `x[i, :]` is pulse record number `i`. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) nrec , nsamp = x . shape nlags = self . convolution_lags assert nsamp == len ( self . values ) + nlags - 1 nrec = x . shape [ 0 ] conv = np . zeros (( nlags , nrec ), dtype = float ) for i in range ( nlags - 1 ): conv [ i , :] = np . dot ( x [:, i : i + 1 - nlags ], self . values ) conv [ nlags - 1 , :] = np . dot ( x [:, nlags - 1 :], self . values ) # Least-squares fit of 5 values to a parabola. # Order is row 0 = constant ... row 2 = quadratic coefficients. if nlags != 5 : raise NotImplementedError ( \"Currently require 5 lags to estimate peak x, y\" ) param = np . dot ( self . FIVELAG_FITTER , conv ) peak_x = - 0.5 * param [ 1 , :] / param [ 2 , :] peak_y = param [ 0 , :] - 0.25 * param [ 1 , :] ** 2 / param [ 2 , :] return peak_y , peak_x","title":"filter_records"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS","text":"Bases: Filter Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015. Returns: FilterATS \u2013 An optimal filter, for convolution with data (at 5 lags, obviously) Source code in mass2/core/optimal_filtering.py 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 @dataclass ( frozen = True ) class FilterATS ( Filter ): \"\"\"Represent an optimal filter according to the arrival-time-safe, single-lag design of 2015. Returns ------- FilterATS An optimal filter, for convolution with data (at 5 lags, obviously) \"\"\" def __post_init__ ( self ) -> None : assert self . convolution_lags == 1 assert self . dt_values is not None @property def is_arrival_time_safe ( self ) -> bool : \"\"\"Is this an arrival-time-safe filter?\"\"\" return True @property def _filter_type ( self ) -> str : return \"ats\" def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values )","title":"FilterATS"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS.is_arrival_time_safe","text":"Is this an arrival-time-safe filter?","title":"is_arrival_time_safe"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterATS.filter_records","text":"Filter one microcalorimeter record or an array of records. Parameters: x ( ArrayLike ) \u2013 A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns: tuple [ ndarray , ndarray ] \u2013 The optimally filtered value, or an array (one per row) if the input is a 2-d array. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises: AssertionError \u2013 If the input array is the wrong length Source code in mass2/core/optimal_filtering.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 def filter_records ( self , x : ArrayLike ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Filter one microcalorimeter record or an array of records. Parameters ---------- x : ArrayLike A 1-d array, a single pulse record, or a 2-d array, each row a pulse records. Returns ------- tuple[np.ndarray, np.ndarray] 1. The optimally filtered value, or an array (one per row) if the input is a 2-d array. 2. The phase, or arrival-time estimate in samples. Same shape as the filtered value. Raises ------ AssertionError If the input array is the wrong length \"\"\" x = np . asarray ( x ) if x . ndim == 1 : x = x . reshape (( 1 , len ( x ))) _ , nsamp = x . shape assert nsamp == len ( self . values ) return _filter_records_ats ( x , self . values , self . dt_values )","title":"filter_records"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker","text":"An object capable of creating optimal filter based on a single signal and noise set. Parameters: signal_model ( ArrayLike ) \u2013 The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the peak value of this filter (that is, peak value relative to the baseline level). n_pretrigger ( int ) \u2013 The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only n_pretrigger samples at the start of a record. noise_autocorr ( Optional [ ArrayLike ] , default: None ) \u2013 The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of avg_signal . noise_psd ( Optional [ ArrayLike ] , default: None ) \u2013 The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of avg_signal , and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method compute_fourier() will not work. whitener ( Optional [ ToeplitzWhitener ] , default: None ) \u2013 An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes noise_autocorr if both are given. sample_time_sec ( float , default: 0.0 ) \u2013 The time step between samples in avg_signal and noise_autocorr (in seconds). This must be given if fmax or f_3db are ever to be used. peak ( float , default: 0.0 ) \u2013 The peak amplitude of the standard signal Notes If both noise_autocorr and whitener are None, then methods compute_5lag and compute_ats will both fail, as they require a time-domain characterization of the noise. The units of noise_autocorr are the square of the units used in signal_model and/or peak . The units of whitener are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. The units of noise_psd are square signal units, per Hertz. Returns: FilterMaker \u2013 An object that can make a variety of optimal filters, assuming a single signal and noise analysis. Source code in mass2/core/optimal_filtering.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 @dataclass ( frozen = True ) class FilterMaker : \"\"\"An object capable of creating optimal filter based on a single signal and noise set. Parameters --------- signal_model : ArrayLike The average signal shape. Filters will be rescaled so that the output upon putting this signal into the filter equals the *peak value* of this filter (that is, peak value relative to the baseline level). n_pretrigger : int The number of leading samples in the average signal that are considered to be pre-trigger samples. The avg_signal in this section is replaced by its constant averaged value before creating filters. Also, one filter (filt_baseline_pretrig) is designed to infer the baseline using only `n_pretrigger` samples at the start of a record. noise_autocorr : Optional[ArrayLike] The autocorrelation function of the noise, where the lag spacing is assumed to be the same as the sample period of `avg_signal`. noise_psd : Optional[ArrayLike] The noise power spectral density. If not None, then it must be of length (2N+1), where N is the length of `avg_signal`, and its values are assumed to cover the non-negative frequencies from 0, 1/Delta, 2/Delta,.... up to the Nyquist frequency. If None, then method `compute_fourier()` will not work. whitener : Optional[ToeplitzWhitener] An optional function object which, when called, whitens a vector or the columns of a matrix. Supersedes `noise_autocorr` if both are given. sample_time_sec : float The time step between samples in `avg_signal` and `noise_autocorr` (in seconds). This must be given if `fmax` or `f_3db` are ever to be used. peak : float The peak amplitude of the standard signal Notes ----- * If both `noise_autocorr` and `whitener` are None, then methods `compute_5lag` and `compute_ats` will both fail, as they require a time-domain characterization of the noise. * The units of `noise_autocorr` are the square of the units used in `signal_model` and/or `peak`. The units of `whitener` are the inverse of the signal units. Any rescaling of the noise autocorrelation or whitener does not affect any filter values, but only the predicted signal/noise ratios. * The units of `noise_psd` are square signal units, per Hertz. Returns ------- FilterMaker An object that can make a variety of optimal filters, assuming a single signal and noise analysis. \"\"\" signal_model : NDArray n_pretrigger : int noise_autocorr : NDArray | None = None noise_psd : NDArray | None = None dt_model : NDArray | None = None whitener : ToeplitzWhitener | None = None sample_time_sec : float = 0.0 peak : float = 0.0 def compute_constrained_5lag ( self , constraints : ArrayLike | None = None , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 , ) -> Filter : \"\"\"Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- constraints: ndarray, optional The vector or vectors to which the filter should be orthogonal. If a 2d array, each _row_ is a constraint, and the number of columns should be equal to the len(self.signal_model) minus `(cut_pre+cut_post)`. fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) shorten = 2 # for 5-lag convolution truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" pulse_model = np . vstack (( truncated_signal , np . ones_like ( truncated_signal ))) if constraints is not None : pulse_model = np . vstack (( pulse_model , constraints )) assert pulse_model . shape [ 1 ] == n noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_model = np . vstack ([ TS ( r ) for r in pulse_model ]) A = pulse_model . dot ( Rinv_model . T ) all_filters = np . linalg . solve ( A , Rinv_model ) filt_noconst = all_filters [ 0 ] band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post ) def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" return self . compute_constrained_5lag ( None , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) def compute_5lag_noexp ( self , exp_time_seconds : float , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- exp_time_seconds: float Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" assert exp_time_seconds > 0 n = len ( self . signal_model ) - 4 - ( cut_pre + cut_post ) log_per_sample = self . sample_time_sec / exp_time_seconds constraint = np . exp ( - np . arange ( n ) * log_per_sample ) return self . compute_constrained_5lag ( constraint , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post ) def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , ) def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post ) def _compute_autocorr ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> np . ndarray : \"\"\"Return the noise autocorrelation, if any, cut down by the requested number of values at the start and end. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- np.ndarray The noise autocorrelation of the appropriate length. Or a length-0 array if not known. \"\"\" # If there's an autocorrelation, cut it down to length. if self . noise_autocorr is None : return np . array ([], dtype = float ) N = len ( np . asarray ( self . signal_model )) return np . asarray ( self . noise_autocorr )[: N - ( cut_pre + cut_post )] def _normalize_signal ( self , cut_pre : int = 0 , cut_post : int = 0 ) -> tuple [ np . ndarray , float , np . ndarray ]: \"\"\"Compute the normalized signal, peak value, and first-order arrival-time model. Parameters ---------- cut_pre : int, optional How many samples to remove from the start of the each pulse record, by default 0 cut_post : int, optional How many samples to remove from the end of the each pulse record, by default 0 Returns ------- tuple[np.ndarray, float, np.ndarray] (sig, pk, dsig), where `sig` is the nominal signal model (normalized to have unit amplitude), `pk` is the peak values of the nominal signal, and `dsig` is the delta between `sig` that differ by one sample in arrival time. The `dsig` will be an empty array if no arrival-time model is known. Raises ------ ValueError If negative numbers of samples are to be cut, or the entire record is to be cut. \"\"\" avg_signal = np . array ( self . signal_model ) ns = len ( avg_signal ) pre_avg = avg_signal [ cut_pre : self . n_pretrigger - 1 ] . mean () if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) # Unless passed in, find the signal's peak value. This is normally peak=(max-pretrigger). # If signal is negative-going, however, then peak=(pretrigger-min). if self . peak > 0.0 : peak_signal = self . peak else : a = avg_signal [ cut_pre : ns - cut_post ] . min () b = avg_signal [ cut_pre : ns - cut_post ] . max () is_negative = pre_avg - a > b - pre_avg if is_negative : peak_signal = a - pre_avg else : peak_signal = b - pre_avg # avg_signal: normalize to have unit peak avg_signal -= pre_avg rescale = 1 / np . max ( avg_signal ) avg_signal *= rescale avg_signal [: self . n_pretrigger ] = 0.0 avg_signal = avg_signal [ cut_pre : ns - cut_post ] if self . dt_model is None : dt_model = np . array ([], dtype = float ) else : dt_model = self . dt_model * rescale dt_model = dt_model [ cut_pre : ns - cut_post ] return avg_signal , peak_signal , dt_model @staticmethod def _normalize_5lag_filter ( f : np . ndarray , avg_signal : np . ndarray ) -> None : \"\"\"Rescale 5-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) <= len ( avg_signal ) - 4 conv = np . zeros ( 5 , dtype = float ) for i in range ( 5 ): conv [ i ] = np . dot ( f , avg_signal [ i : i + len ( f )]) x = np . linspace ( - 2 , 2 , 5 ) fit = np . polyfit ( x , conv , 2 ) fit_ctr = - 0.5 * fit [ 1 ] / fit [ 0 ] fit_peak = np . polyval ( fit , fit_ctr ) f *= 1.0 / fit_peak @staticmethod def _normalize_filter ( f : np . ndarray , avg_signal : np . ndarray ) -> None : \"\"\"Rescale single-lag filter `f` in-place so that it gives unit response to avg_signal Parameters ---------- f : np.ndarray Optimal filter values, which need to be renormalized avg_signal : np.ndarray The signal to which filter `f` should give unit response \"\"\" assert len ( f ) == len ( avg_signal ) f *= 1 / np . dot ( f , avg_signal )","title":"FilterMaker"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_5lag","text":"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 def compute_5lag ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" return self . compute_constrained_5lag ( None , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post )","title":"compute_5lag"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_5lag_noexp","text":"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: exp_time_seconds ( float ) \u2013 Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 def compute_5lag_noexp ( self , exp_time_seconds : float , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- exp_time_seconds: float Generate a filter orthogonal to decaying exponentials of this time constant (must be positive) fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" assert exp_time_seconds > 0 n = len ( self . signal_model ) - 4 - ( cut_pre + cut_post ) log_per_sample = self . sample_time_sec / exp_time_seconds constraint = np . exp ( - np . arange ( n ) * log_per_sample ) return self . compute_constrained_5lag ( constraint , fmax = fmax , f_3db = f_3db , cut_pre = cut_pre , cut_post = cut_post )","title":"compute_5lag_noexp"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_ats","text":"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 An arrival-time-safe optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 def compute_ats ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : # noqa: PLR0914 \"\"\"Compute a single \"arrival-time-safe\" filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter An arrival-time-safe optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate ATS filters\" ) if self . dt_model is None : raise ValueError ( \"FilterMaker must have dt_model to generate ATS filters\" ) if self . sample_time_sec is None and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , dt_model = self . _normalize_signal ( cut_pre , cut_post ) ns = len ( avg_signal ) assert ns == len ( dt_model ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if cut_pre + cut_post >= ns : raise ValueError ( f \"cut_pre+cut_post = { cut_pre + cut_post } but should be < { ns } \" ) MT = np . vstack (( avg_signal , dt_model , np . ones ( ns ))) if self . whitener is not None : WM = self . whitener ( MT . T ) A = np . dot ( WM . T , WM ) Ainv = np . linalg . inv ( A ) WtWM = self . whitener . applyWT ( WM ) filt = np . dot ( Ainv , WtWM . T ) else : assert len ( noise_autocorr ) >= ns noise_corr = noise_autocorr [: ns ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) RinvM = np . vstack ([ TS ( r ) for r in MT ]) . T A = np . dot ( MT , RinvM ) Ainv = np . linalg . inv ( A ) filt = np . dot ( Ainv , RinvM . T ) band_limit ( filt . T , self . sample_time_sec , fmax , f_3db ) if cut_pre > 0 or cut_post > 0 : nfilt = filt . shape [ 0 ] filt = np . hstack ([ np . zeros (( nfilt , cut_pre ), dtype = float ), filt , np . zeros (( nfilt , cut_post ), dtype = float )]) filt_noconst = filt [ 0 ] filt_dt = filt [ 1 ] filt_baseline = filt [ 2 ] variance = bracketR ( filt_noconst , noise_autocorr ) vdv = peak / ( np . log ( 2 ) * 8 * variance ) ** 0.5 return FilterATS ( filt_noconst , peak , variance , vdv , filt_dt , filt_baseline , avg_signal , dt_model , 1 , fmax , f_3db , cut_pre , cut_post )","title":"compute_ats"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_constrained_5lag","text":"Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of fmax and f_3db are allowed. Parameters: constraints ( ArrayLike | None , default: None ) \u2013 The vector or vectors to which the filter should be orthogonal. If a 2d array, each row is a constraint, and the number of columns should be equal to the len(self.signal_model) minus (cut_pre+cut_post) . fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def compute_constrained_5lag ( self , constraints : ArrayLike | None = None , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 , ) -> Filter : \"\"\"Compute a single constrained optimal filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- constraints: ndarray, optional The vector or vectors to which the filter should be orthogonal. If a 2d array, each _row_ is a constraint, and the number of columns should be equal to the len(self.signal_model) minus `(cut_pre+cut_post)`. fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" if self . sample_time_sec <= 0 and not ( fmax is None and f_3db is None ): raise ValueError ( \"FilterMaker must have a sample_time_sec if it's to be smoothed with fmax or f_3db\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) if self . noise_autocorr is None and self . whitener is None : raise ValueError ( \"FilterMaker must have noise_autocorr or whitener arguments to generate 5-lag filters\" ) noise_autocorr = self . _compute_autocorr ( cut_pre , cut_post ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) shorten = 2 # for 5-lag convolution truncated_signal = avg_signal [ shorten : - shorten ] n = len ( truncated_signal ) assert len ( noise_autocorr ) >= n , \"Noise autocorrelation vector is too short for signal size\" pulse_model = np . vstack (( truncated_signal , np . ones_like ( truncated_signal ))) if constraints is not None : pulse_model = np . vstack (( pulse_model , constraints )) assert pulse_model . shape [ 1 ] == n noise_corr = noise_autocorr [: n ] TS = ToeplitzSolver ( noise_corr , symmetric = True ) Rinv_model = np . vstack ([ TS ( r ) for r in pulse_model ]) A = pulse_model . dot ( Rinv_model . T ) all_filters = np . linalg . solve ( A , Rinv_model ) filt_noconst = all_filters [ 0 ] band_limit ( filt_noconst , self . sample_time_sec , fmax , f_3db ) self . _normalize_5lag_filter ( filt_noconst , avg_signal ) variance = bracketR ( filt_noconst , noise_corr ) # Set weights in the cut_pre and cut_post windows to 0 if cut_pre > 0 or cut_post > 0 : filt_noconst = np . hstack ([ np . zeros ( cut_pre ), filt_noconst , np . zeros ( cut_post )]) vdv = peak / ( 8 * np . log ( 2 ) * variance ) ** 0.5 return Filter5Lag ( filt_noconst , peak , variance , vdv , None , None , avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post )","title":"compute_constrained_5lag"},{"location":"docstrings/#mass2.core.optimal_filtering.FilterMaker.compute_fourier","text":"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of fmax and f_3db are allowed. Parameters: fmax ( Optional [ float ] , default: None ) \u2013 The strict maximum frequency to be passed in all filters, by default None f_3db ( Optional [ float ] , default: None ) \u2013 The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre ( int , default: 0 ) \u2013 The number of initial samples to be given zero weight, by default 0 cut_post ( int , default: 0 ) \u2013 The number of samples at the end of a record to be given zero weight, by default 0 Returns: Filter \u2013 A 5-lag optimal filter, computed in the Fourier domain. Raises: ValueError \u2013 Under various conditions where arguments are inconsistent with the data Source code in mass2/core/optimal_filtering.py 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 def compute_fourier ( self , fmax : float | None = None , f_3db : float | None = None , cut_pre : int = 0 , cut_post : int = 0 ) -> Filter : \"\"\"Compute a single Fourier-domain filter, with optional low-pass filtering, and with optional zero weights at the pre-trigger or post-trigger end of the filter. Fourier domain calculation implicitly assumes periodic boundary conditions. Either or both of `fmax` and `f_3db` are allowed. Parameters ---------- fmax : Optional[float], optional The strict maximum frequency to be passed in all filters, by default None f_3db : Optional[float], optional The 3 dB point for a one-pole low-pass filter to be applied to all filters, by default None cut_pre : int The number of initial samples to be given zero weight, by default 0 cut_post : int The number of samples at the end of a record to be given zero weight, by default 0 Returns ------- Filter A 5-lag optimal filter, computed in the Fourier domain. Raises ------ ValueError Under various conditions where arguments are inconsistent with the data \"\"\" # Make sure we have either a noise PSD or an autocorrelation or a whitener if self . noise_psd is None : raise ValueError ( \"FilterMaker must have noise_psd to generate a Fourier filter\" ) if cut_pre < 0 or cut_post < 0 : raise ValueError ( f \"(cut_pre,cut_post)=( { cut_pre } , { cut_post } ), but neither can be negative\" ) avg_signal , peak , _ = self . _normalize_signal ( cut_pre , cut_post ) noise_psd = np . asarray ( self . noise_psd ) # Terminology: the `avg_signal` vector will be \"shortened\" by `shorten` _on each end. # That's to permit 5-lag filtering (where we step the filter by \u00b12 lags either direction from 0 lag). # The `avg_signal` was already \"reduced\" in length by (cut_pre, cut_post), for a total # `reduction` of `2 * shorten + (cut_pre + cut_post)`. shorten = 2 # to use in 5-lag style reduction = 2 * shorten + ( cut_pre + cut_post ) truncated_avg_signal = avg_signal [ shorten : - shorten ] len_reduced_psd = len ( noise_psd ) - ( reduction + 1 ) // 2 window = 1.0 sig_ft = np . fft . rfft ( truncated_avg_signal * window ) if len ( sig_ft ) != len_reduced_psd : raise ValueError ( f \"signal real DFT and noise PSD are not the same length ( { len ( sig_ft ) } and { len_reduced_psd } )\" ) # Careful with PSD: \"shorten\" it by converting into a real space autocorrelation, # truncating the middle, and going back to Fourier space if reduction > 0 : noise_autocorr = np . fft . irfft ( noise_psd ) noise_autocorr = np . hstack (( noise_autocorr [: len_reduced_psd - 1 ], noise_autocorr [ - len_reduced_psd :])) noise_psd = np . abs ( np . fft . rfft ( noise_autocorr )) sig_ft_weighted = sig_ft / noise_psd # Band-limit if fmax is not None or f_3db is not None : f_nyquist = 0.5 / self . sample_time_sec freq = np . linspace ( 0 , f_nyquist , len_reduced_psd , dtype = float ) if fmax is not None : sig_ft_weighted [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft_weighted /= 1 + ( freq * 1.0 / f_3db ) ** 2 sig_ft_weighted [ 0 ] = 0.0 filt_fourier = np . fft . irfft ( sig_ft_weighted ) / window self . _normalize_5lag_filter ( filt_fourier , avg_signal ) # How we compute the uncertainty depends on whether there's a noise autocorrelation result if self . noise_autocorr is None : noise_ft_squared = ( len ( noise_psd ) - 1 ) / self . sample_time_sec * noise_psd kappa = ( np . abs ( sig_ft ) ** 2 / noise_ft_squared )[ 1 :] . sum () variance_fourier = 1.0 / kappa print ( kappa , noise_ft_squared ) else : ac = np . array ( self . noise_autocorr )[: len ( filt_fourier )] variance_fourier = bracketR ( filt_fourier , ac ) vdv = peak / ( 8 * np . log ( 2 ) * variance_fourier ) ** 0.5 return Filter5Lag ( filt_fourier , peak , variance_fourier , vdv , None , None , truncated_avg_signal , None , 1 + 2 * shorten , fmax , f_3db , cut_pre , cut_post , )","title":"compute_fourier"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener","text":"An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: tw.whiten(v) returns Wv; it is equivalent to tw(v) tw.solveWT(v) returns inv(W')*v tw.applyWT(v) returns W'v tw.solveW(v) returns inv(W)*v Arguments theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients Returns: ToeplitzWhitener \u2013 Object that can perform approximate, time-invariant noise whitening. Raises: ValueError \u2013 If the operative methods are passed an array of dimension higher than 2. Source code in mass2/core/optimal_filtering.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 @dataclass ( frozen = True ) class ToeplitzWhitener : \"\"\"An object that can perform approximate noise whitening. For an ARMA(p,q) noise model, mutliply by (or solve) the matrix W (or its transpose), where W is the Toeplitz approximation to the whitening matrix V. A whitening matrix V means that if R is the ARMA noise covariance matrix, then VRV' = I. While W only approximately satisfies this, it has some handy properties that make it a useful replacement. (In particular, it has the time-transpose property that if you zero-pad the beginning of vector v and shift the remaining elements, then the same is done to Wv.) The callable function object returns Wv or WM if called with vector v or matrix M. Other methods: * `tw.whiten(v)` returns Wv; it is equivalent to `tw(v)` * `tw.solveWT(v)` returns inv(W')*v * `tw.applyWT(v)` returns W'v * `tw.solveW(v)` returns inv(W)*v Arguments --------- theta : np.ndarray The moving-average (MA) process coefficients phi : np.ndarray The autoregressive (AR) process coefficients Returns ------- ToeplitzWhitener Object that can perform approximate, time-invariant noise whitening. Raises ------ ValueError If the operative methods are passed an array of dimension higher than 2. \"\"\" theta : np . ndarray phi : np . ndarray @property def p ( self ) -> int : return len ( self . phi ) - 1 @property def q ( self ) -> int : return len ( self . theta ) - 1 def whiten ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v ) def __call__ ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y def solveW ( self , v : ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y def solveWT ( self , v : ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :] def applyWT ( self , v : ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :] def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR )","title":"ToeplitzWhitener"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.W","text":"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. Source code in mass2/core/optimal_filtering.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def W ( self , N : int ) -> np . ndarray : \"\"\"Return the full, approximate whitening matrix. Normally the full W is large and slow to use. But it's here so you can easily test that W(len(v))*v == whiten(v), and similar. \"\"\" AR = np . zeros (( N , N ), dtype = float ) MA = np . zeros (( N , N ), dtype = float ) for i in range ( N ): for j in range ( max ( 0 , i - self . p ), i + 1 ): AR [ i , j ] = self . phi [ i - j ] for j in range ( max ( 0 , i - self . q ), i + 1 ): MA [ i , j ] = self . theta [ i - j ] return np . linalg . solve ( MA , AR )","title":"W"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.__call__","text":"Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __call__ ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be an array of dimension 1 or 2\" ) elif v . ndim == 2 : w = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): w [:, i ] = self ( v [:, i ]) return w # Multiply by the Toeplitz AR matrix to make the MA*w vector. N = len ( v ) y = self . phi [ 0 ] * v for i in range ( 1 , 1 + self . p ): y [ i :] += self . phi [ i ] * v [: - i ] # Second, solve the MA matrix (a banded, lower-triangular Toeplitz matrix with # q non-zero subdiagonals.) y [ 0 ] /= self . theta [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . q , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] for i in range ( self . q , N ): for j in range ( i - self . q , i ): y [ i ] -= y [ j ] * self . theta [ i - j ] y [ i ] /= self . theta [ 0 ] return y","title":"__call__"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.applyWT","text":"Return vector (or matrix of column vectors) W'v Source code in mass2/core/optimal_filtering.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def applyWT ( self , v : ArrayLike ) -> np . ndarray : \"\"\"Return vector (or matrix of column vectors) W'v\"\"\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . applyWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . theta [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . q + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . theta [ 1 : f ]) y [ i ] /= self . theta [ 0 ] return np . correlate ( y , self . phi , \"full\" )[ self . p :]","title":"applyWT"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.solveW","text":"Return unwhitened vector (or matrix of column vectors) inv(W)*v Source code in mass2/core/optimal_filtering.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def solveW ( self , v : ArrayLike ) -> np . ndarray : \"Return unwhitened vector (or matrix of column vectors) inv(W)*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveW ( v [:, i ]) return r # Multiply by the Toeplitz MA matrix to make the AR*w vector. N = len ( v ) y = self . theta [ 0 ] * v for i in range ( 1 , 1 + self . q ): y [ i :] += self . theta [ i ] * v [: - i ] # Second, solve the AR matrix (a banded, lower-triangular Toeplitz matrix with # p non-zero subdiagonals.) y [ 0 ] /= self . phi [ 0 ] if N == 1 : return y for i in range ( 1 , min ( self . p , N )): for j in range ( i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] for i in range ( self . p , N ): for j in range ( i - self . p , i ): y [ i ] -= y [ j ] * self . phi [ i - j ] y [ i ] /= self . phi [ 0 ] return y","title":"solveW"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.solveWT","text":"Return vector (or matrix of column vectors) inv(W')*v Source code in mass2/core/optimal_filtering.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def solveWT ( self , v : ArrayLike ) -> np . ndarray : \"Return vector (or matrix of column vectors) inv(W')*v\" v = np . asarray ( v ) if v . ndim > 3 : raise ValueError ( \"v must be dimension 1 or 2\" ) elif v . ndim == 2 : r = np . zeros_like ( v ) for i in range ( v . shape [ 1 ]): r [:, i ] = self . solveWT ( v [:, i ]) return r N = len ( v ) y = np . array ( v ) y [ N - 1 ] /= self . phi [ 0 ] for i in range ( N - 2 , - 1 , - 1 ): f = min ( self . p + 1 , N - i ) y [ i ] -= np . dot ( y [ i + 1 : i + f ], self . phi [ 1 : f ]) y [ i ] /= self . phi [ 0 ] return np . correlate ( y , self . theta , \"full\" )[ self . q :]","title":"solveWT"},{"location":"docstrings/#mass2.core.optimal_filtering.ToeplitzWhitener.whiten","text":"Return whitened vector (or matrix of column vectors) Wv Source code in mass2/core/optimal_filtering.py 66 67 68 def whiten ( self , v : ArrayLike ) -> np . ndarray : \"Return whitened vector (or matrix of column vectors) Wv\" return self ( v )","title":"whiten"},{"location":"docstrings/#mass2.core.optimal_filtering.band_limit","text":"Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input modelmatrix in-place. No effect if both fmax and f_3db are None . Parameters: modelmatrix ( ndarray ) \u2013 The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec ( float ) \u2013 The sampling period, normally in seconds. fmax ( Optional [ float ] ) \u2013 The hard maximum frequency (units are inverse of sample_time_sec units, or Hz) f_3db ( Optional [ float ] ) \u2013 The 1-pole low-pass filter's 3 dB point (same units as fmax ) Source code in mass2/core/optimal_filtering.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def band_limit ( modelmatrix : np . ndarray , sample_time_sec : float , fmax : float | None , f_3db : float | None ) -> None : \"\"\"Band-limit the column-vectors in a model matrix with a hard and/or 1-pole low-pass filter. Change the input `modelmatrix` in-place. No effect if both `fmax` and `f_3db` are `None`. Parameters ---------- modelmatrix : np.ndarray The 1D or 2D array to band-limit. (If a 2D array, columns are independently band-limited.) sample_time_sec : float The sampling period, normally in seconds. fmax : Optional[float] The hard maximum frequency (units are inverse of `sample_time_sec` units, or Hz) f_3db : Optional[float] The 1-pole low-pass filter's 3 dB point (same units as `fmax`) \"\"\" if fmax is None and f_3db is None : return # Handle the 2D case by calling this function once per column. assert len ( modelmatrix . shape ) <= 2 if len ( modelmatrix . shape ) == 2 : for i in range ( modelmatrix . shape [ 1 ]): band_limit ( modelmatrix [:, i ], sample_time_sec , fmax , f_3db ) return vector = modelmatrix filt_length = len ( vector ) sig_ft = np . fft . rfft ( vector ) freq = np . fft . fftfreq ( filt_length , d = sample_time_sec ) freq = np . abs ( freq [: len ( sig_ft )]) if fmax is not None : sig_ft [ freq > fmax ] = 0.0 if f_3db is not None : sig_ft /= 1.0 + ( 1.0 * freq / f_3db ) ** 2 # n=filt_length is needed when filt_length is ODD vector [:] = np . fft . irfft ( sig_ft , n = filt_length )","title":"band_limit"},{"location":"docstrings/#mass2.core.optimal_filtering.bracketR","text":"Return the dot product (q^T R q) for vector and matrix R constructed from the vector by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). Source code in mass2/core/optimal_filtering.py 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 def bracketR ( q : NDArray , noise : NDArray ) -> float : \"\"\"Return the dot product (q^T R q) for vector <q> and matrix R constructed from the vector <noise> by R_ij = noise_|i-j|. We don't want to construct the full matrix R because for records as long as 10,000 samples, the matrix will consist of 10^8 floats (800 MB of memory). \"\"\" if len ( noise ) < len ( q ): raise ValueError ( f \"Vector q (length { len ( q ) } ) cannot be longer than the noise (length { len ( noise ) } )\" ) n = len ( q ) r = np . zeros ( 2 * n - 1 , dtype = float ) r [ n - 1 :] = noise [: n ] r [ n - 1 :: - 1 ] = noise [: n ] dot = 0.0 for i in range ( n ): dot += q [ i ] * r [ n - i - 1 : 2 * n - i - 1 ] . dot ( q ) return dot","title":"bracketR"},{"location":"docstrings/#hci-lines","text":"","title":"HCI lines"},{"location":"filtering/","text":"Optimal filtering The FilterMaker interface The optimal filtering interface FilterMaker is based on Python dataclasses , a modern approach to Python objects based on having set of attributes fixed at creation time. When the dataclass is \"frozen\" (as in this case), it also does not allow changing the values of these attributes. Our intention with the new API is offer a range of objects that can perform optimal filtering, or create objects that do, but rejecting the proliferation of incompatible features that used to appear in slightly different flavors of filters. This API consists of two key objects: The Filter is a specific implementation of an optimal filter, designed to be used in one-lag or five-lag mode, and with fixed choices about low-pass filtering of the filter's values, or about giving zero weight to a number of initial or final samples in a record. Offers a filter_records(r) method to apply its optimal filter to one or more pulse records r . When r is a 2d array, each row corresponds to a pulse record. The FilterMaker contains a model of one channel's signal and noise. It is able to create various objects of the type Filter (or subtypes thereof). The user first creates a FilterMaker from the analyzed noise and signal, then uses it to generate an optimal filter with the desired properties. The filter will be an object whose type is a subclass of Filter , either Filter5Lag or FilterATS . That object has a method filter_records(...) . Usage looks like the following: import numpy as np import mass2 n = 500 Maxsignal = 1000.0 sigma_noise = 1.0 tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.core.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) filt_5lag = maker.compute_5lag() print(f\"Filter peak value: {filt_5lag.nominal_peak:.1f}\") print(f\"Filter rms value: {filt_5lag.variance**0.5:.4f}\") print(f\"Filter predicted V/dV (FWHM): {filt_5lag.predicted_v_over_dv:.4f}\") from numpy.testing import assert_allclose assert_allclose(filt_5lag.nominal_peak, 1000) assert_allclose(filt_5lag.variance**0.5, 0.1549, rtol=1e-3) assert_allclose(filt_5lag.predicted_v_over_dv, 2741.6517) # mkdocs: render # Here's what the filter looks like: a pulse minus a constant. filt_5lag.plot() This code produces a filter maker maker and an optimal filter filt_5lag and generates the following output: Filter peak value: 1000.0 Filter rms value: 0.1549 Filter predicted V/dV (FWHM): 2741.6517 A test of normalization and filter variance import numpy as np import mass2 def verify_close(x, y, rtol=1e-5, topic=None): if topic is not None: print(f\"Checking {topic:20s}: \", end=\"\") isclose = np.isclose(x, y, rtol=rtol) print(f\"x={x:.4e}, y={y:.4e} are close to each other? {isclose}\") assert isclose def test_mass_5lag_filters(Maxsignal=100.0, sigma_noise=1.0, n=500): tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.core.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) filt_5lag = maker.compute_5lag() # Check filter's normalization f = filt_5lag.values verify_close(Maxsignal, f.dot(truncated_signal), rtol=1e-5, topic = \"Filter normalization\") # Check filter's variance expected_dV = sigma_noise / n**0.5 * signal.max()/truncated_signal.std() verify_close(expected_dV, filt_5lag.variance**0.5, rtol=1e-5, topic=\"Expected variance\") # Check filter's V/dV calculation fwhm_sigma_ratio = np.sqrt(8*np.log(2)) expected_V_dV = Maxsignal / (expected_dV * fwhm_sigma_ratio) verify_close(expected_V_dV, filt_5lag.predicted_v_over_dv, rtol=1e-5, topic=\"Expected V/\\u03b4v\") print() test_mass_5lag_filters(100, 1.0, 500) test_mass_5lag_filters(400, 1.0, 500) test_mass_5lag_filters(100, 1.0, 1000) test_mass_5lag_filters(100, 2.0, 500) These four tests should yield the following output: Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=2.7417e+02, y=2.7417e+02 are close to each other? True Checking Filter normalization: x=4.0000e+02, y=4.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.0967e+03, y=1.0967e+03 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.0963e-01, y=1.0963e-01 are close to each other? True Checking Expected V/\u03b4v : x=3.8734e+02, y=3.8734e+02 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=3.0978e-01, y=3.0978e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.3708e+02, y=1.3708e+02 are close to each other? True","title":"Optimal filtering"},{"location":"filtering/#optimal-filtering","text":"","title":"Optimal filtering"},{"location":"filtering/#the-filtermaker-interface","text":"The optimal filtering interface FilterMaker is based on Python dataclasses , a modern approach to Python objects based on having set of attributes fixed at creation time. When the dataclass is \"frozen\" (as in this case), it also does not allow changing the values of these attributes. Our intention with the new API is offer a range of objects that can perform optimal filtering, or create objects that do, but rejecting the proliferation of incompatible features that used to appear in slightly different flavors of filters. This API consists of two key objects: The Filter is a specific implementation of an optimal filter, designed to be used in one-lag or five-lag mode, and with fixed choices about low-pass filtering of the filter's values, or about giving zero weight to a number of initial or final samples in a record. Offers a filter_records(r) method to apply its optimal filter to one or more pulse records r . When r is a 2d array, each row corresponds to a pulse record. The FilterMaker contains a model of one channel's signal and noise. It is able to create various objects of the type Filter (or subtypes thereof). The user first creates a FilterMaker from the analyzed noise and signal, then uses it to generate an optimal filter with the desired properties. The filter will be an object whose type is a subclass of Filter , either Filter5Lag or FilterATS . That object has a method filter_records(...) . Usage looks like the following: import numpy as np import mass2 n = 500 Maxsignal = 1000.0 sigma_noise = 1.0 tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.core.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) filt_5lag = maker.compute_5lag() print(f\"Filter peak value: {filt_5lag.nominal_peak:.1f}\") print(f\"Filter rms value: {filt_5lag.variance**0.5:.4f}\") print(f\"Filter predicted V/dV (FWHM): {filt_5lag.predicted_v_over_dv:.4f}\") from numpy.testing import assert_allclose assert_allclose(filt_5lag.nominal_peak, 1000) assert_allclose(filt_5lag.variance**0.5, 0.1549, rtol=1e-3) assert_allclose(filt_5lag.predicted_v_over_dv, 2741.6517) # mkdocs: render # Here's what the filter looks like: a pulse minus a constant. filt_5lag.plot() This code produces a filter maker maker and an optimal filter filt_5lag and generates the following output: Filter peak value: 1000.0 Filter rms value: 0.1549 Filter predicted V/dV (FWHM): 2741.6517","title":"The FilterMaker interface"},{"location":"filtering/#a-test-of-normalization-and-filter-variance","text":"import numpy as np import mass2 def verify_close(x, y, rtol=1e-5, topic=None): if topic is not None: print(f\"Checking {topic:20s}: \", end=\"\") isclose = np.isclose(x, y, rtol=rtol) print(f\"x={x:.4e}, y={y:.4e} are close to each other? {isclose}\") assert isclose def test_mass_5lag_filters(Maxsignal=100.0, sigma_noise=1.0, n=500): tau = [.05, .25] t = np.linspace(-1, 1, n+4) npre = (t < 0).sum() signal = (np.exp(-t/tau[1]) - np.exp(-t/tau[0]) ) signal[t <= 0] = 0 signal *= Maxsignal / signal.max() truncated_signal = signal[2:-2] noise_covar = np.zeros(n) noise_covar[0] = sigma_noise**2 maker = mass2.core.FilterMaker(signal, npre, noise_covar, peak=Maxsignal) filt_5lag = maker.compute_5lag() # Check filter's normalization f = filt_5lag.values verify_close(Maxsignal, f.dot(truncated_signal), rtol=1e-5, topic = \"Filter normalization\") # Check filter's variance expected_dV = sigma_noise / n**0.5 * signal.max()/truncated_signal.std() verify_close(expected_dV, filt_5lag.variance**0.5, rtol=1e-5, topic=\"Expected variance\") # Check filter's V/dV calculation fwhm_sigma_ratio = np.sqrt(8*np.log(2)) expected_V_dV = Maxsignal / (expected_dV * fwhm_sigma_ratio) verify_close(expected_V_dV, filt_5lag.predicted_v_over_dv, rtol=1e-5, topic=\"Expected V/\\u03b4v\") print() test_mass_5lag_filters(100, 1.0, 500) test_mass_5lag_filters(400, 1.0, 500) test_mass_5lag_filters(100, 1.0, 1000) test_mass_5lag_filters(100, 2.0, 500) These four tests should yield the following output: Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=2.7417e+02, y=2.7417e+02 are close to each other? True Checking Filter normalization: x=4.0000e+02, y=4.0000e+02 are close to each other? True Checking Expected variance : x=1.5489e-01, y=1.5489e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.0967e+03, y=1.0967e+03 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=1.0963e-01, y=1.0963e-01 are close to each other? True Checking Expected V/\u03b4v : x=3.8734e+02, y=3.8734e+02 are close to each other? True Checking Filter normalization: x=1.0000e+02, y=1.0000e+02 are close to each other? True Checking Expected variance : x=3.0978e-01, y=3.0978e-01 are close to each other? True Checking Expected V/\u03b4v : x=1.3708e+02, y=1.3708e+02 are close to each other? True","title":"A test of normalization and filter variance"},{"location":"fluorescence/","text":"Fluorescence Lines Mass includes numerous features to help you analyze and model the fluorescence emission of various elements. Mass can Approximate the shape of the fluorescence line emission for certain lines (particularly the K-alpha and K-beta lines of elements from Mg to Zn, or Z=12 to 30). Generate random deviates, drawn from these same energy distributions. Fit a measured spectrum to on of these energy distributions. Examples 1. Plot the distribution Objects of the SpectralLine class are callable, and return their PDF given the energy as an array or scalar argument. # mkdocs: render import mass2 import numpy as np import pylab as plt spectrum = mass2.spectra[\"MnKAlpha\"] plt.clf() axis=plt.gca() cm = plt.cm.magma for fwhm in (3,4,5,6,8,10): spectrum.plot(axis=axis,components=False,label=f\"FWHM: {fwhm} eV\", setylim=False, instrument_gaussian_fwhm=fwhm, color=cm(fwhm/10-0.2)); plt.legend(loc=\"upper left\") plt.title(\"Mn K$\\\\alpha$ distribution at various resolutions\") plt.xlabel(\"Energy (eV)\") 2. Generate random deviates from a fluorescence line shape Objects of the SpectralLine class roughly copy the API of the scipy type scipy.stats.rv_continuous and offer some of the methods, such as pdf , rvs .: # mkdocs: render energies0 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=0) energies3 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=3) energies6 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=6) plt.clf() erange=(5840, 5940) for E, color in zip((energies0, energies3, energies6), (\"r\", \"b\", \"purple\")): contents, bin_edges, _ = plt.hist(E, 200, range=erange, histtype=\"step\", color=color) plt.xlabel(\"Energy (eV)\") plt.ylabel(\"Counts per bin\") plt.xlim((erange[0], erange[1])) 3. Fit data to a fluorescence line model # mkdocs: render model = mass2.spectra[\"MnKAlpha\"].model() contents3, bins = np.histogram(energies3, 200, range=erange) bin_ctr = bins[:-1] + 0.5 * (bins[1] - bins[0]) guess_params = model.guess(contents3, bin_ctr, dph_de=1.0) result = model.fit(contents3, guess_params, bin_centers=bin_ctr) result.plotm() print(result.best_values) fwhm = result.params[\"fwhm\"] print(f\"Estimated resolution (FWHM) = {fwhm.value}\u00b1{fwhm.stderr}\")","title":"Atomic fluorescence"},{"location":"fluorescence/#fluorescence-lines","text":"Mass includes numerous features to help you analyze and model the fluorescence emission of various elements. Mass can Approximate the shape of the fluorescence line emission for certain lines (particularly the K-alpha and K-beta lines of elements from Mg to Zn, or Z=12 to 30). Generate random deviates, drawn from these same energy distributions. Fit a measured spectrum to on of these energy distributions.","title":"Fluorescence Lines"},{"location":"fluorescence/#examples","text":"","title":"Examples"},{"location":"fluorescence/#1-plot-the-distribution","text":"Objects of the SpectralLine class are callable, and return their PDF given the energy as an array or scalar argument. # mkdocs: render import mass2 import numpy as np import pylab as plt spectrum = mass2.spectra[\"MnKAlpha\"] plt.clf() axis=plt.gca() cm = plt.cm.magma for fwhm in (3,4,5,6,8,10): spectrum.plot(axis=axis,components=False,label=f\"FWHM: {fwhm} eV\", setylim=False, instrument_gaussian_fwhm=fwhm, color=cm(fwhm/10-0.2)); plt.legend(loc=\"upper left\") plt.title(\"Mn K$\\\\alpha$ distribution at various resolutions\") plt.xlabel(\"Energy (eV)\")","title":"1. Plot the distribution"},{"location":"fluorescence/#2-generate-random-deviates-from-a-fluorescence-line-shape","text":"Objects of the SpectralLine class roughly copy the API of the scipy type scipy.stats.rv_continuous and offer some of the methods, such as pdf , rvs .: # mkdocs: render energies0 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=0) energies3 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=3) energies6 = spectrum.rvs(size=20000, instrument_gaussian_fwhm=6) plt.clf() erange=(5840, 5940) for E, color in zip((energies0, energies3, energies6), (\"r\", \"b\", \"purple\")): contents, bin_edges, _ = plt.hist(E, 200, range=erange, histtype=\"step\", color=color) plt.xlabel(\"Energy (eV)\") plt.ylabel(\"Counts per bin\") plt.xlim((erange[0], erange[1]))","title":"2. Generate random deviates from a fluorescence line shape"},{"location":"fluorescence/#3-fit-data-to-a-fluorescence-line-model","text":"# mkdocs: render model = mass2.spectra[\"MnKAlpha\"].model() contents3, bins = np.histogram(energies3, 200, range=erange) bin_ctr = bins[:-1] + 0.5 * (bins[1] - bins[0]) guess_params = model.guess(contents3, bin_ctr, dph_de=1.0) result = model.fit(contents3, guess_params, bin_centers=bin_ctr) result.plotm() print(result.best_values) fwhm = result.params[\"fwhm\"] print(f\"Estimated resolution (FWHM) = {fwhm.value}\u00b1{fwhm.stderr}\")","title":"3. Fit data to a fluorescence line model"},{"location":"getting_started/","text":"Getting started with Mass2 The Microcalorimeter Analysis Software Suite, or Mass, is a library designed for the analysis of data generated by superconducting microcalorimeters, such as Transition-Edge Sensors (TESs) and Thermal Kinetic-Inductance Detectors (TKIDs). Key features and methods Mass2 represents microcalorimeter pulse data using a Pola.rs DataFrame. Each row represents a single pulse, and there may be an arbitrary number of columns representing \"per pulse quantities\". The raw pulse record may be stored in the DataFrame as well; Mass2 uses \"memmaps\" to limit memory usage in this case. The use of the Polars DataFrame enables many best-in-class methods for slicing and dicing your data. It abstracts away the input and output file formats, allowing Mass2 to easily handle data from many different file formats (LJH, OFF, .bin, and anything you can make a numpy array from). It enables export/reload from many formats as well. When in doubt, we suggest using the Apache Parquet file format to save data in a native data frame structure. Mass2 is designed to do analysis on arrays of microcalorimeters, so Mass2 provides APIs and algorithms that can perform equivalent operations on hundreds of distinct channels all at once. Mass2 creates and applies optimal filters. Mass2 creates and applies energy-calibration functions. Mass2 Fits known fluoresence-line shapes to measured spectra, and it can generate calibrations from the fits. Mass2 supports two modes of operation: From scratch, where you analyze detector data for the first time assuming no prior knowledge and From Recipes, where you \"replay\" an analysis Recipe created in the other mode. In Recipe mode, there is no learning from data, it simply does the same transformations to your DataFrame to produce a new DataFrame. This is intended for use in instruments that operate the same way day in and day out. Mass2 uses many programming best practices, including automated testing and doc tests. We also have adopted an immutable-first approach, where the majority of Python objects are immutable, like the Polars DataFrame. Data transformations are reprented by binding a new variable for example ch2 = ch.do_something() . This makes the code easier to read and reason about. Comparison of Mass versions 2 vs 1 Mass2 builds on many lessons learned developing Mass and adopts many newer practices. Mass was created starting in February 2011 with the goal of analyzing data from a 16-pixel array. It was, in many ways, an inflexible, custom-built Data Science library before Data Science was a common term. Pandas, a popular DataFrame library, existed in 2011 but was not widely known and had only one major contributor. Changes from Mass version 1 include: Mass2 takes a different view of data wrangling and bookkeeping than version 1, leveraging the power of the Pola.rs Dataframe for this purpose. Polars is a high-performance modern dataframe library. It organizes the data structures and provides data I/O. This change increases efficiency and performance for some activities. More importantly, using a Dataframe makes the arrays of per-pulse quantities both flexible (you can add fields of your own design on equal footing with standard quantities) and clearer (you can easily find a complete list of all such quantities, plus data types and statistics). Mass2 no longer uses HDF5 datasets to automatically cache analysis results alongside the raw data. This change removes a valuable, but often maddeningly imperfect feature. We think the cost worth paying. There are big benefits in clarity and avoiding both errors and hidden side effects. Mass2 give access to the native save/load features of Polars, making it easy to preserve and recover subsets or complete sets of analyzed data. The preferred file format is Apache Parquet , but several other choices like CSV are supported and can be used for interchange with frameworks other than Mass. Many data structures in Mass2 use Python's \"frozen dataclasses\" features, making them immutable. With immutable objects, it is much easier to avoid inconsistent state and sneaky bugs. They will take some getting used to at first, but (just as with the lack of HDF5 backing) we believe the benefits are worth the costs. Mass2 tries to expose all functionality as a \"bare function\" that takes arrays as argument, then also exposes that with a convenient ch.do_something() API; in Mass version 1, many algorithms were implemented as methods of a data class and thus impossible to use on their own. Some things have not changed. Mass still makes heavy use of the libraries: Numpy . For numerical arrays and scientific computation. Scipy . Additional algorithms for scientific computation. Matplotlib . For high-quality plotting. LMFIT . Convenient non-linear curve fitting. Mass also still: Lets you operate on many microcalorimeter data channels in parallel with a single function call. Works from full pulse records in LJH files, or pre-filtered pulses in OFF files. Generates standard pulse-summary quantities. Creates and applies optimal filters. Creates and applies energy-calibration functions. Fits known fluorescence-line shapes to measured spectra. This guide is based on the evolving API of Mass2, still under development. How to load pulse data Raw pulse data may be stored in the custom LJH format . An Optimally Filtered Format (\"OFF\") has also been used, in which the Dastard data acquisition program performs optimal filtering in real time. Mass2 can work with data of both types, though of course any process that requires access to the full raw pulse record will fail on an OFF file. Mass2 also has some capability to work with untriggered data stored in .bin or similar files. (Other formats could be added to Mass2 in the future, if necessary.) How to load full pulses from LJH files Mass2 can read LJH files, whether the modern version 2.2, or the earlier 2.1 or 2.0 versions. The older versions have much reduced time resolution and no ability to connect with external triggers; for the most part, they were superceded in late 2015 by the latest verison. The file version is automatically detected by Mass, and you should not have to think about it. For this getting started guide, we'll assume you have the pulsedata package installed. (This should happen automatically when you install Mass2.) It's a standard source of a small number of LJH and OFF example files. Often, you want to load all the data files in a directory, possibly with companion noise files from another. That looks like this: # mkdocs: render import numpy as np import pylab as plt import polars as pl import pulsedata import mass2 pn_pair = pulsedata.pulse_noise_ljh_pairs[\"bessy_20240727\"] data = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder ) The value returned, data , is a mass2.Channels object. At its heart is data.channels , a dictionary mapping from channel numbers to single-sensor objects of the type mass2.Channel . When you create the Channels , there are options to exclude certain channels with problems, or limit the result to some maximum number of channels. For these needs, you can use the arguments limit: int and/or exclude_ch_nums: Iterable[int] . For instance, the following will return a Channels that omits channel 4220 and limits the output to no more than 5 channels (in fact, there will be only 1, given the data source has only one channel other than number 4220): less_data = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder, limit=5, exclude_ch_nums=[4220] ) Advanced cases, for deep exploration If you want to create a single mass2.Channel object (not created as part of a mass2.Channels object): pfile = pn_pair.pulse_folder / \"20240727_run0002_chan4220.ljh\" nfile = pn_pair.noise_folder / \"20240727_run0000_chan4220.ljh\" # the noise file is optional ch = mass2.Channel.from_ljh(pfile, nfile) To open a single LJH file and study it as a pure file, you can use the internal class LJHFile , like this: # mkdocs: render ljh = mass2.LJHFile.open(pn_pair.pulse_folder/\"20240727_run0002_chan4220.ljh\") print(ljh.npulses, ljh.npresamples, ljh.nsamples) print(ljh.is_continuous) print(ljh.dtype) plt.clf() y = ljh.read_trace(30) plt.plot(y, \".r\") How to load pre-filtered pulses from OFF files OFF files do not record the entire microcalorimeter pulse, but only a summary of it. This means that no analysis steps that require the raw pulse can be performed. Also, many of the per-pulse summary quantities that exist in an OFF file have different names (and perhaps slightly different definitions) than the corresponding fields computed in a standard LJH-based analysis. Use of OFF files is not a primary goal of Mass2 in the initial release, but they are supported for legacy analysis. # Load one OFF file into an `OffFile` object' p = pn_pair.pulse_folder off = mass2.core.OffFile(p / \"20240727_run0002_chan4220.off\") # Load multiple OFF files info a `Channels` object import glob files = glob.glob(str(p / \"*_chan*.off\")) offdata = mass2.Channels.from_off_paths(files, description=\"OFF file demo\") # Here's the dataframe for a single channel's per-pulse data given an OFF input ch = offdata.ch0 print(ch.df) How to load additional information Most data sets come with experiment-state labels, and some also have external trigger timing. Both sets of information are universal across channels, in the sense that a single table of state labels or external trigger times is relevant to all channels in that data acquisition. Therefore, they are best loaded for all channels at once. Here is the first time we see a consequence of the immutability of the Channel and Channels classes: instead of adding information to an existing object, we have to create a new object, a copy of what came before but with added information. In this case, the added information will be new data columns in the dataframe of each channel. But to use the new, enhanced object, we have to re-assign the name data to the new object. Thus: # mkdocs: render # Load the *_experiment_state.txt file, assuming it has the usual name and path data = data.with_experiment_state_by_path() ch = data.ch0 print(ch.df.columns) states = ch.df[\"state_label\"] print(states.dtype) print(states.unique(), states.unique_counts()) assert states.unique()[-2] == \"SCAN3\" assert states.unique_counts()[-2] == 42433 The output reads: ['timestamp', 'pulse', 'subframecount', 'state_label'] Categorical shape: (6,) Series: 'state_label' [cat] [ null \"START\" \"CAL2\" \"PAUSE\" \"SCAN3\" \"SCAN4\" ] shape: (6,) Series: 'state_label' [u32] [ 4 4 11468 40854 42433 5237 ] Notice that Polars saves the state labels as a \"Categorical\" type . This type improves performance when the values are strings known to take on one of a limited number of possibilities (far fewer than the number of rows in a table). Here's how you would load external trigger, if this example data set had external triggers: # exttrig_path = p / \"20240727_run0002_external_trigger.bin\" # data = data.with_external_trigger_by_path(exttrig_path) What is in the Channel and Channels objects The most important part of Channels is a dictionary mapping channel numbers each to a Channel object. You can access a specific channel, or the \"first\", like this: ch = data.channels[4220] # or if you just want any rando channel, this returns the first channel in order of creation (normally, the one with the lowest channel #) arb_channel = data.ch0 Other features include a dictionary of channels declared \"bad\" print(f\"There are {len(data.bad_channels)} bad channels in the data at first.\") data_withbad = data.set_bad(4219, \"We think chan 4219 is no good at all.\") print(f\"There are {len(data_withbad.bad_channels)} bad channels if we capriciously declare one bad.\") Bad channels are used to prevent a few problematic channels from derailing a whole analysis, so when one channel has an error with an analysis step, it can be marked bad and the rest of the channels can proceed to the next analysis step. It is easy to join the data from successive experiments into a single concatenated object. This example just joins a second copy of the same data to itself, because the pulsedata repository doesn't happen to have more data from the same experiment. later_data = data data_merged = data.concat_data(later_data) The most important part of each Channel object is the underlying per-pulse dataframe. This code prints that DataFrame , then the facts specific to the sensor channel as a whole, first as an object, then a dataframe: print(ch.df) print(ch.header) print(ch.header.df) # Here are the pulse dataframe's column names, the contents of the subframecount column, and the min/mean/max of the timestamp columns print(ch.df.columns) print(ch.df[\"subframecount\"]) ts = ch.df[\"timestamp\"] print(ts.min(), ts.mean(), ts.max()) assert len(ts) == 100_000 The above commands produce the output: shape: (100_000, 4) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 timestamp \u2506 pulse \u2506 subframecount \u2506 state_label \u2502 \u2502 --- \u2506 --- \u2506 --- \u2506 --- \u2502 \u2502 datetime[\u03bcs] \u2506 array[u16, 500] \u2506 u64 \u2506 cat \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 2024-07-27 13:30:44.451920 \u2506 [6063, 6076, \u2026 6655] \u2506 1519640266112 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.472140 \u2506 [6071, 6065, \u2026 6670] \u2506 1519640589632 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.475788 \u2506 [6120, 6123, \u2026 6215] \u2506 1519640648000 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.497137 \u2506 [6074, 6065, \u2026 6660] \u2506 1519640944448 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.573915 \u2506 [6074, 6072, \u2026 6672] \u2506 1519642206080 \u2506 START \u2502 \u2502 \u2026 \u2506 \u2026 \u2506 \u2026 \u2506 \u2026 \u2502 \u2502 2024-07-27 16:11:55.982246 \u2506 [6065, 6063, \u2026 6278] \u2506 1674384771712 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:55.986849 \u2506 [6077, 6089, \u2026 6662] \u2506 1674384833216 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.054243 \u2506 [6058, 6048, \u2026 6637] \u2506 1674385923008 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.080575 \u2506 [6067, 6064, \u2026 6639] \u2506 1674386344320 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.220080 \u2506 [6062, 6055, \u2026 6483] \u2506 1674388576448 \u2506 PAUSE \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ChannelHeader(description='20240727_run0002_chan4219.ljh', ch_num=4219, frametime_s=4e-06, n_presamples=250, n_samples=500) shape: (1, 27) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Save File Format \u2506 Software Version \u2506 Software Git Hash \u2506 Data source \u2506 \u2026 \u2506 Pixel Name \u2506 Timebase \u2506 Filename \u2506 continuous \u2502 \u2502 Version \u2506 --- \u2506 --- \u2506 --- \u2506 \u2506 --- \u2506 --- \u2506 --- \u2506 --- \u2502 \u2502 --- \u2506 str \u2506 str \u2506 str \u2506 \u2506 str \u2506 f64 \u2506 str \u2506 bool \u2502 \u2502 str \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 2.2.1 \u2506 DASTARD version 0.3.2 \u2506 508fd92 \u2506 Abaco \u2506 \u2026 \u2506 \u2506 0.000004 \u2506 /Users/fowlerj/qsp/lib/ \u2506 false \u2502 \u2502 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 python3\u2026 \u2506 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ['timestamp', 'pulse', 'subframecount', 'state_label'] shape: (100_000,) Series: 'subframecount' [u64] [ 1519640266112 1519640589632 1519640648000 1519640944448 1519642206080 \u2026 1674384771712 1674384833216 1674385923008 1674386344320 1674388576448 ] 2024-07-27 13:30:44.451920 2024-07-27 14:43:53.618149 2024-07-27 16:11:56.220080 Each column (e.g., ch.df[\"timestamp\"] ) is a Polars \"series\" (type: pl.Series ). See the Polars documentation to learn about the enormous range of operations you can perform on dataframes and data series. The library is optimized for parallel operation and offers database-like queries to select and join frames. It offers the option of lazy queries, where a query is built up step-by-step then optimized before being executed. We cannot stress enough how thoroughly worth it it is for a Mass2 user to learn and take advantage of the Polars library to make any query, no matter how simple or complex. Polars is excellent. Analyzing pulse data One of the design goals for Mass2 was to enable pulse analysis in each of two distinct situations: Analysis of a data set from a new instrument, or a new configuration (e.g., a changed bath temperature or bias voltage), or an unfamiliar sample (especially a new calibration target). In this case, we don't yet know the appropriate optimal filter to use for each sensor, or how to convert measured pulse sizes into photon energies. We might also not even know the optimal set of analysis choices to make, such as whether arrival-time (\"phase\") correction is required. Analysis of new data from an established instrument in a known configuration with a known energy-calibration curve. In this case, we can apply an analysis \"recipe\" learned from the other case. It is assumed that this earlier recipe was learned from similar data, and that it will therefore produce good, if imperfect, results. The results should be good enough to use during real-time, or \"online\" data analysis to assess data quality, even if they have to be improved offline for a final result. This section concentrate on the first case, analysis of new data for which no established recipe exists. For the second case, see Re-using a recipe and Storing a recipe . Summarizing pulses The Channel.summarize_pulses() method returns a new Channel with a much enhanced dataframe that has nearly a dozen new columns, such as [\"pretrig_mean\", \"pulse_rms\", ...] . To use this method on all the Channel s in a Channels object, we make use of the handy Channels.map() method. It takes a single, callable argument f which must accept a Channel object and returns a modified version of it. We also often follow this up by a function that attaches a \"good expression\" to each channel. Here we'll consider pulses \"good\" when their pretrigger rms and their postpeak derivatives are no more than 8-sigma off the median. These two steps can be performed with map() like this: # mkdocs: render def summarize_and_cut(ch: mass2.Channel) -> mass2.Channel: return ( ch.summarize_pulses() .with_good_expr_pretrig_rms_and_postpeak_deriv(8, 8) ) data = data.map(summarize_and_cut) # Plot a distribution ch = data.ch0 prms = ch.df[\"pulse_rms\"] hist_range = range=np.percentile(prms, [0.5, 99.5]) bin_edges = np.linspace(hist_range[0], hist_range[1], 1000) ch.plot_hist(\"pulse_rms\", bin_edges) plt.xlabel(\"Pulse rms (arbs)\") plt.title(f\"The middle 99% of pulse rms values for channel {ch.header.ch_num}\") You may either re-use the same variable name, e.g., data=data.do_something() here, or use a new variable name such as data2 . Most operations only add to the fields in a data frame, without modifying or removing any existing fields, so preserving the old object is not usually useful. It can be useful to build up an analysis in stages, however, checkpointing with new variables names as you go. Many of the example notebooks use data , data2 , data3 as checkpoints. Computing and applying optimal filters To compute an optimal filter for each channel, one must analyze the noise (to learn its autocorrelation function) and create a model of pulses (typically, the model is an average pulse, possibly computed over a selected subset of the records). The simplest approach is to use the Channel.filter5lag method. It analyzes noise a standard way, and it computes an average of all pulses designated \"good\" in the previous step. If you wanted to restrict those pulses further, an optional use_expr argument lets you pass in a Polars expression to reject pulses outside a certain range of times, or experiment states, or to select based on a range of pulse_rms values. The good_expr is remembered through each step of the analsys, while the use_expr is used only for a single step of the analysis. We'll skip the \"use\" here and instead take the default choice of using all good pulses: # mkdocs: render def do_filter(ch: mass2.Channel) -> mass2.Channel: return ch.filter5lag(f_3db=10000) data = data.map(do_filter) The operation above (or the application of any 5-lag filter) will add new fields (\"5lagx\", \"5lagy\") to the channel data frame. It will also add a new, last \"step\" in each channel's analysis \"recipe\". See more in Saving your results below. Here's how you can look at the last step and make its built-in debug plot. For optimal filter construction, the debug plot is simply a plot of the filter created in the step. # mkdocs: render ch = data.ch0 step = ch.steps[-1] step.dbg_plot(ch.df) The step also contains both the new optimal filter and the FilterMaker used to create it. You can look at the latter to find the noise correlation and spectrum, and the signal model--all the ingredients needed to make optimal filters: # mkdocs: render maker = step.filter_maker plt.clf() plt.subplot(221) plt.plot(maker.noise_autocorr[:100], \".-b\") plt.plot(0, maker.noise_autocorr[0], \"ok\") plt.title(\"Noise autocorrelation\") plt.xlabel(f\"Lags (each lag = {maker.sample_time_sec*1e6:.2f} \u00b5s)\") plt.subplot(222) freq_khz = np.linspace(0, 0.5*1e-3/maker.sample_time_sec, len(maker.noise_psd)) plt.loglog(freq_khz[1:], maker.noise_psd[1:], \"-g\") plt.xlabel(\"Frequency (kHz)\") plt.title(\"Noise power spectral density\") plt.ylabel(\"Noise PSD (arbs$^2$ / Hz)\") plt.subplot(212) t_ms = (np.arange(ch.header.n_samples)-ch.header.n_presamples)*maker.sample_time_sec*1000 plt.plot(t_ms, maker.signal_model, \"r\") plt.title(\"Model pulse\") plt.xlabel(\"Time after trigger (ms)\") plt.ylabel(\"Signal (arbs)\") plt.tight_layout() Corrections and energy calibration We can apply the drift correction based on pretrigger mean to the filtered values, and also perform a rough calibration. That might look like: # mkdocs: render def dc_and_rough_cal2(ch: mass2.Channel) -> mass2.Channel: import polars as pl use_cal = (pl.col(\"state_label\") == \"CAL2\") use_dc = pl.lit(True) line_names = [\"CKAlpha\", \"NKAlpha\", \"OKAlpha\", \"FeLl\", \"FeLAlpha\", \"FeLBeta\", \"NiLAlpha\", \"NiLBeta\", \"CuLAlpha\", \"CuLBeta\", 980] return ( ch.rough_cal_combinatoric( line_names=line_names, uncalibrated_col=\"pulse_rms\", calibrated_col=\"energy_pulse_rms\", ph_smoothing_fwhm=6, use_expr=use_cal, ) .driftcorrect( indicator_col=\"pretrig_mean\", uncorrected_col=\"5lagy\", use_expr=use_dc) .rough_cal_combinatoric( line_names, uncalibrated_col=\"5lagy_dc\", calibrated_col=\"energy_5lagy_dc\", ph_smoothing_fwhm=6, use_expr=use_cal, ) ) data = data.map(dc_and_rough_cal2) ch = data.ch0 plt.clf() use = (pl.col(\"state_label\") == \"CAL2\") ax = plt.subplot(211) ch.plot_hist(\"energy_pulse_rms\", np.linspace(0, 1000, 1001), axis=ax, use_expr=use) plt.title(\"Rough-calibrated energy (not optimally filtered)\") plt.xlabel(\"Rough energy (eV)\") ax = plt.subplot(212) ch.plot_hist(\"5lagy_dc\", np.linspace(0, 2600, 1001), axis=ax, use_expr=use) plt.title(\"Uncalibrated, optimally filtered pulse heights\") plt.xlabel(\"Pulse heights (arbs)\") plt.tight_layout() Now, an improved calibration can be achieved with actual fits to the known calibration lines. (One subtle aspect of this improved calibration is that there must be an existing calibration from the same field into energy that you want to use for the final. In this case, we must have that second rough_cal_combinatoric above, because only that second one uses the field 5lagy_dc as its input. This allows the multifit calibration routine to know about the energy scale well enough to find the lines to fit. Furthermore, we have to track which step it was whose calibration we are improving upon. Here, that is the previous step, called -1 in python. The first rough calibration wouldn't meet our needs, because it is a calibration from pulse_rms into energy, a totally different mapping.) # mkdocs: render def do_multifit(ch: mass2.Channel) -> mass2.Channel: import mass2 import polars as pl STEP_WITH_ROUGH_CAL = -1 multifit = mass2.MultiFit( default_fit_width=80, default_use_expr=(pl.col(\"state_label\") == \"CAL2\"), default_bin_size=0.3, ) line_names = [\"CKAlpha\", \"NKAlpha\", \"OKAlpha\", \"FeLl\", \"FeLAlpha\", \"NiLAlpha\", \"CuLAlpha\", 980.0] dhigh = {\"CuLAlpha\": 12, \"NiLAlpha\": 12} dlow = {980.0: 20} for line in line_names: multifit = multifit.with_line(line, dlo=dlow.get(line, None), dhi=dhigh.get(line, None)) return ch.multifit_mass_cal(multifit, STEP_WITH_ROUGH_CAL, \"energy_5lagy_best\") data = data.map(do_multifit) ch = data.ch0 plt.clf() ax1 = plt.subplot(211) use_cal = (pl.col(\"state_label\") == \"CAL2\") edges = np.linspace(0, 1000, 1001) ch.plot_hist(\"energy_5lagy_best\", edges, axis=ax1, use_expr=use_cal) plt.title(\"Calibrated energy, state='CAL2'\") plt.xlabel(\"Pulse energy (eV)\") ax2 = plt.subplot(212, sharex=ax1) use_noncal = (pl.col(\"state_label\") == \"SCAN3\") ch.plot_hist(\"energy_5lagy_best\", edges, axis=ax2, use_expr=use_noncal) plt.title(\"Calibrated energy, state='SCAN3'\") plt.xlabel(\"Pulse energy (eV)\") plt.xlim([0, 1000]) plt.tight_layout() Saving results and the Recipe system Mass2 is designed to make storing results easy, both the the quantities produced in analysis and the steps taken to compute them. This section explains how and explores the saving of steps and of numerical results. Analysis \"steps\" have been mentioned before. The big idea is that each channel is associated with a Recipe containing all the operations and parameters used to create it, starting from the raw pulse records in an Raw Data file. (Raw Data files are never modified by Mass2.) This recipe consists of one or more steps to be applied in a fixed order. Each step takes for input fields that existed in the channel's dataframe before the step, and it creates one or more new fields that exist in the dataframe afterwards. Importantly, a recipe can be stored to disk, and applied in the future, either to the same data set, or to another. Warning: One catch is that each recipe is specific to one sensor, and they are tracked only by channel number. If the set of active sensors changes after the learning phase, any new sensors will lack a recipe. Worse, if a probe of the \u00b5MUX resonator frequencies leads to renumbering of the sensors, then we won't be able to find the right recipe for most (or all) sensors. Caching computed data (work in progress) Sometimes you run an analysis that you consider \"final\"; you want to keep the results only, and you expect (hope?) never to look at the raw LJH files again. We are still working out a standard system for caching computed data: how to name the files, how to combine them, etc. Here are two approaches that employ Parquet files. One stores a file per channel, and the other stores all data in a single file. Approach A: one file per channel Here we store each channel's dataframe in a separate file. Notice that we want to drop the column representing the raw pulse data, because the output would otherwise be far too large (and redundant). Probably it's fine to drop the subframe count, too, so we do that here. # mkdocs: render # For automated tests, we want the output in a temporary directory import tempfile, os output_dir = tempfile.TemporaryDirectory(prefix=\"mass2_getting_started\") print(f\"Test output lives in '{output_dir.name}'\") columns_to_drop = (\"pulse\", \"subframecount\") for ch_num, ch in data.channels.items(): filename = os.path.join(output_dir.name, f\"output_test_chan{ch_num}.parquet\") df = ch.df.drop(columns_to_drop) df.write_parquet(filename) You can pass options to write_parquet to control compression and other aspects of the file. If you prefer to write out only a specified subset of the fields in the dataframes, replace the .drop(columns_to_drop) in the code above with a .select(columns_to_keep) operation. All the methods that act on dataframes are possible: drop to remove specific columns select to keep specific columns limit to write only a maximum number of rows filter to select rows according to a Polars expression alias to rename columns and many others. TODO: we should work on a way to make it easier to load up these files and start analyzing where you left off. One might match up these files with an LJH file, to permit creation of a new Channel object. For now, this sort of data caching is limited to when you just need the data, not the full framework of Mass2. (But keep in mind, the Polars framework is really powerful, and there's a lot you could imagine doing with just the data.) Approach B: one file for all channels This is similar, except that we use some new Polars tricks: polars.DataFrame.with_columns() to append a new column containing this channel's number (we will need it later, because we're about to mix all channels into a new dataframe) polars.concat() to join rows from multiple dataframes columns_to_drop = (\"pulse\", \"subframecount\") all_df = [] for ch_num, ch in data.channels.items(): df = ch.df.drop(columns_to_drop).with_columns(ch_num=pl.lit(ch_num)) all_df.append(df) filename = os.path.join(output_dir.name, \"output_test_allchan.parquet\") pl.concat(all_df).write_parquet(filename) Storing analysis recipes The system for storing analysis recipes is extremely simple. The following will save the full analysis recipe for each channel in the data object. recipe_filename = os.path.join(output_dir.name, \"full_recipes.pkl\") data.save_recipes(recipe_filename) Beware that this operation will drop all \"debug information\" in certain steps in the recipe (generally, any steps that store a lot of data that's used not for performing the step but only for debugging). The benefit is that the recipes file is smaller, and faster to save and load. The cost is that debugging plots cannot be made after the recipes are reloaded. You probably save a recipe because you've already debugged its steps, so this is usually an acceptable tradeoff. If you don't like it, use the drop_debug=False argument. By default all steps in a recipe are saved, but you might not want or need this. For instance, in the examples given on this page, we computed two \"rough\" calibrations and a final one. The two rough ones are \"dead ends\" if you only care about the final energy. When you are saving a recipe for later use on future data sets in a real-time pipeline, you might want to store a minimal recipe--one that omits any steps not needed to reach a few critical fields. Perhaps you have no need to compute anything that doesn't directly lead to the final energy value. If you omit dead ends from the stored recipes, then not only is the file smaller, but the computation required to run the recipes is reduced. You might prefer instead: required_fields = {\"energy_5lagy_best\", \"pretrig_mean\"} trimmed_recipe_filename = os.path.join(output_dir.name, \"trimmed_recipes.pkl\") data.save_recipes(trimmed_recipe_filename, required_fields=required_fields) This version will save all the steps that produce the two specified required_fields , and all steps that the depend on. The two rough calibration steps, however, will be trimmed away as dead ends. (In this instance, having the pretrigger mean as a required field is superfluous. It's already required in the drift correction step, which is required before the step that takes drift-corrected optimally-filtered pulse height into energy.) Re-using a recipe (on the same or new data) Once a mass2.Channels object is created from raw LJH files, an existing recipe can be loaded from a file and executed on the raw data. The result will be that the dataframe for each Channel will now contain the derived fields, including {\"energy_5lagy_best\", \"pretrig_mean\"} , as well as any fields computed along with them, or on the direct path to them. pn_pair = pulsedata.pulse_noise_ljh_pairs[\"bessy_20240727\"] data_replay = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder ).with_experiment_state_by_path() data_replay = data_replay.load_recipes(trimmed_recipe_filename) # Verify that the expected columns exist in the replay and have identical values df1 = data.ch0.df df2 = data_replay.ch0.df assert df2.equals(df1.select(df2.columns)) Further examples There are more examples of how one can use the Mass2 approach to data analysis in the examples/ directory, in the form of Marimo notebooks . Many examples in the top level of that directory have been designed for use on data found in the https://github.com/ggggggggg/pulsedata repository, so they should work for any user of Mass. Other examples rely on data that isn't public, but they can still be useful training materials for ways to use Mass2. Marimo is similar to Jupyter, but it has some nice advantages. It stores notebooks as pure Python (vs JSON for Jupyter). It uses a reactive execution model with an understanding of the data-flow dependency graph. This fixes a lot of the hidden-state and unexpected behavior that often come with Jupyter's linear-flow (top-to-bottom) execution model. Another really cool Marimo feature (though it's still a bit buggy) is that Matplotlib figure can be rendered and made interactive in the web browser (rather than having to explicitly set x/y limits and re-run a cell)--the result is closer to what you expect from an iPython session. Marimo is easy to start from the command line. The marimo shell command will start a marimo server running, which both serves web pages and runs the underlying Python code. Here are a few examples of how to start marimo: If you add pip scripts to your path (following the confusing documentation from Python or possible the helpful error message pip or uv prints out) you may: mass2-examples OR on windows mass2-examples.exe cd ~/where/I/stored/mass2 # Each of the following commands starts a local Marimo webserver, # then opens a brower tab pointing to it. They also... # ...display a chooser to select whatever example you like: marimo edit examples # ...display one specific example notebook, with (editable) code: marimo edit examples/bessy_20240727.py # ...display a notebook's results, but not the code that created it: marimo run examples/ebit_juy2024_from_off.py We strongly suggest exploring a couple of these notebooks as you launch your adventure with Mass2.","title":"Getting started"},{"location":"getting_started/#getting-started-with-mass2","text":"The Microcalorimeter Analysis Software Suite, or Mass, is a library designed for the analysis of data generated by superconducting microcalorimeters, such as Transition-Edge Sensors (TESs) and Thermal Kinetic-Inductance Detectors (TKIDs).","title":"Getting started with Mass2"},{"location":"getting_started/#key-features-and-methods","text":"Mass2 represents microcalorimeter pulse data using a Pola.rs DataFrame. Each row represents a single pulse, and there may be an arbitrary number of columns representing \"per pulse quantities\". The raw pulse record may be stored in the DataFrame as well; Mass2 uses \"memmaps\" to limit memory usage in this case. The use of the Polars DataFrame enables many best-in-class methods for slicing and dicing your data. It abstracts away the input and output file formats, allowing Mass2 to easily handle data from many different file formats (LJH, OFF, .bin, and anything you can make a numpy array from). It enables export/reload from many formats as well. When in doubt, we suggest using the Apache Parquet file format to save data in a native data frame structure. Mass2 is designed to do analysis on arrays of microcalorimeters, so Mass2 provides APIs and algorithms that can perform equivalent operations on hundreds of distinct channels all at once. Mass2 creates and applies optimal filters. Mass2 creates and applies energy-calibration functions. Mass2 Fits known fluoresence-line shapes to measured spectra, and it can generate calibrations from the fits. Mass2 supports two modes of operation: From scratch, where you analyze detector data for the first time assuming no prior knowledge and From Recipes, where you \"replay\" an analysis Recipe created in the other mode. In Recipe mode, there is no learning from data, it simply does the same transformations to your DataFrame to produce a new DataFrame. This is intended for use in instruments that operate the same way day in and day out. Mass2 uses many programming best practices, including automated testing and doc tests. We also have adopted an immutable-first approach, where the majority of Python objects are immutable, like the Polars DataFrame. Data transformations are reprented by binding a new variable for example ch2 = ch.do_something() . This makes the code easier to read and reason about.","title":"Key features and methods"},{"location":"getting_started/#comparison-of-mass-versions-2-vs-1","text":"Mass2 builds on many lessons learned developing Mass and adopts many newer practices. Mass was created starting in February 2011 with the goal of analyzing data from a 16-pixel array. It was, in many ways, an inflexible, custom-built Data Science library before Data Science was a common term. Pandas, a popular DataFrame library, existed in 2011 but was not widely known and had only one major contributor. Changes from Mass version 1 include: Mass2 takes a different view of data wrangling and bookkeeping than version 1, leveraging the power of the Pola.rs Dataframe for this purpose. Polars is a high-performance modern dataframe library. It organizes the data structures and provides data I/O. This change increases efficiency and performance for some activities. More importantly, using a Dataframe makes the arrays of per-pulse quantities both flexible (you can add fields of your own design on equal footing with standard quantities) and clearer (you can easily find a complete list of all such quantities, plus data types and statistics). Mass2 no longer uses HDF5 datasets to automatically cache analysis results alongside the raw data. This change removes a valuable, but often maddeningly imperfect feature. We think the cost worth paying. There are big benefits in clarity and avoiding both errors and hidden side effects. Mass2 give access to the native save/load features of Polars, making it easy to preserve and recover subsets or complete sets of analyzed data. The preferred file format is Apache Parquet , but several other choices like CSV are supported and can be used for interchange with frameworks other than Mass. Many data structures in Mass2 use Python's \"frozen dataclasses\" features, making them immutable. With immutable objects, it is much easier to avoid inconsistent state and sneaky bugs. They will take some getting used to at first, but (just as with the lack of HDF5 backing) we believe the benefits are worth the costs. Mass2 tries to expose all functionality as a \"bare function\" that takes arrays as argument, then also exposes that with a convenient ch.do_something() API; in Mass version 1, many algorithms were implemented as methods of a data class and thus impossible to use on their own. Some things have not changed. Mass still makes heavy use of the libraries: Numpy . For numerical arrays and scientific computation. Scipy . Additional algorithms for scientific computation. Matplotlib . For high-quality plotting. LMFIT . Convenient non-linear curve fitting. Mass also still: Lets you operate on many microcalorimeter data channels in parallel with a single function call. Works from full pulse records in LJH files, or pre-filtered pulses in OFF files. Generates standard pulse-summary quantities. Creates and applies optimal filters. Creates and applies energy-calibration functions. Fits known fluorescence-line shapes to measured spectra. This guide is based on the evolving API of Mass2, still under development.","title":"Comparison of Mass versions 2 vs 1"},{"location":"getting_started/#how-to-load-pulse-data","text":"Raw pulse data may be stored in the custom LJH format . An Optimally Filtered Format (\"OFF\") has also been used, in which the Dastard data acquisition program performs optimal filtering in real time. Mass2 can work with data of both types, though of course any process that requires access to the full raw pulse record will fail on an OFF file. Mass2 also has some capability to work with untriggered data stored in .bin or similar files. (Other formats could be added to Mass2 in the future, if necessary.)","title":"How to load pulse data"},{"location":"getting_started/#how-to-load-full-pulses-from-ljh-files","text":"Mass2 can read LJH files, whether the modern version 2.2, or the earlier 2.1 or 2.0 versions. The older versions have much reduced time resolution and no ability to connect with external triggers; for the most part, they were superceded in late 2015 by the latest verison. The file version is automatically detected by Mass, and you should not have to think about it. For this getting started guide, we'll assume you have the pulsedata package installed. (This should happen automatically when you install Mass2.) It's a standard source of a small number of LJH and OFF example files. Often, you want to load all the data files in a directory, possibly with companion noise files from another. That looks like this: # mkdocs: render import numpy as np import pylab as plt import polars as pl import pulsedata import mass2 pn_pair = pulsedata.pulse_noise_ljh_pairs[\"bessy_20240727\"] data = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder ) The value returned, data , is a mass2.Channels object. At its heart is data.channels , a dictionary mapping from channel numbers to single-sensor objects of the type mass2.Channel . When you create the Channels , there are options to exclude certain channels with problems, or limit the result to some maximum number of channels. For these needs, you can use the arguments limit: int and/or exclude_ch_nums: Iterable[int] . For instance, the following will return a Channels that omits channel 4220 and limits the output to no more than 5 channels (in fact, there will be only 1, given the data source has only one channel other than number 4220): less_data = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder, limit=5, exclude_ch_nums=[4220] )","title":"How to load full pulses from LJH files"},{"location":"getting_started/#advanced-cases-for-deep-exploration","text":"If you want to create a single mass2.Channel object (not created as part of a mass2.Channels object): pfile = pn_pair.pulse_folder / \"20240727_run0002_chan4220.ljh\" nfile = pn_pair.noise_folder / \"20240727_run0000_chan4220.ljh\" # the noise file is optional ch = mass2.Channel.from_ljh(pfile, nfile) To open a single LJH file and study it as a pure file, you can use the internal class LJHFile , like this: # mkdocs: render ljh = mass2.LJHFile.open(pn_pair.pulse_folder/\"20240727_run0002_chan4220.ljh\") print(ljh.npulses, ljh.npresamples, ljh.nsamples) print(ljh.is_continuous) print(ljh.dtype) plt.clf() y = ljh.read_trace(30) plt.plot(y, \".r\")","title":"Advanced cases, for deep exploration"},{"location":"getting_started/#how-to-load-pre-filtered-pulses-from-off-files","text":"OFF files do not record the entire microcalorimeter pulse, but only a summary of it. This means that no analysis steps that require the raw pulse can be performed. Also, many of the per-pulse summary quantities that exist in an OFF file have different names (and perhaps slightly different definitions) than the corresponding fields computed in a standard LJH-based analysis. Use of OFF files is not a primary goal of Mass2 in the initial release, but they are supported for legacy analysis. # Load one OFF file into an `OffFile` object' p = pn_pair.pulse_folder off = mass2.core.OffFile(p / \"20240727_run0002_chan4220.off\") # Load multiple OFF files info a `Channels` object import glob files = glob.glob(str(p / \"*_chan*.off\")) offdata = mass2.Channels.from_off_paths(files, description=\"OFF file demo\") # Here's the dataframe for a single channel's per-pulse data given an OFF input ch = offdata.ch0 print(ch.df)","title":"How to load pre-filtered pulses from OFF files"},{"location":"getting_started/#how-to-load-additional-information","text":"Most data sets come with experiment-state labels, and some also have external trigger timing. Both sets of information are universal across channels, in the sense that a single table of state labels or external trigger times is relevant to all channels in that data acquisition. Therefore, they are best loaded for all channels at once. Here is the first time we see a consequence of the immutability of the Channel and Channels classes: instead of adding information to an existing object, we have to create a new object, a copy of what came before but with added information. In this case, the added information will be new data columns in the dataframe of each channel. But to use the new, enhanced object, we have to re-assign the name data to the new object. Thus: # mkdocs: render # Load the *_experiment_state.txt file, assuming it has the usual name and path data = data.with_experiment_state_by_path() ch = data.ch0 print(ch.df.columns) states = ch.df[\"state_label\"] print(states.dtype) print(states.unique(), states.unique_counts()) assert states.unique()[-2] == \"SCAN3\" assert states.unique_counts()[-2] == 42433 The output reads: ['timestamp', 'pulse', 'subframecount', 'state_label'] Categorical shape: (6,) Series: 'state_label' [cat] [ null \"START\" \"CAL2\" \"PAUSE\" \"SCAN3\" \"SCAN4\" ] shape: (6,) Series: 'state_label' [u32] [ 4 4 11468 40854 42433 5237 ] Notice that Polars saves the state labels as a \"Categorical\" type . This type improves performance when the values are strings known to take on one of a limited number of possibilities (far fewer than the number of rows in a table). Here's how you would load external trigger, if this example data set had external triggers: # exttrig_path = p / \"20240727_run0002_external_trigger.bin\" # data = data.with_external_trigger_by_path(exttrig_path)","title":"How to load additional information"},{"location":"getting_started/#what-is-in-the-channel-and-channels-objects","text":"The most important part of Channels is a dictionary mapping channel numbers each to a Channel object. You can access a specific channel, or the \"first\", like this: ch = data.channels[4220] # or if you just want any rando channel, this returns the first channel in order of creation (normally, the one with the lowest channel #) arb_channel = data.ch0 Other features include a dictionary of channels declared \"bad\" print(f\"There are {len(data.bad_channels)} bad channels in the data at first.\") data_withbad = data.set_bad(4219, \"We think chan 4219 is no good at all.\") print(f\"There are {len(data_withbad.bad_channels)} bad channels if we capriciously declare one bad.\") Bad channels are used to prevent a few problematic channels from derailing a whole analysis, so when one channel has an error with an analysis step, it can be marked bad and the rest of the channels can proceed to the next analysis step. It is easy to join the data from successive experiments into a single concatenated object. This example just joins a second copy of the same data to itself, because the pulsedata repository doesn't happen to have more data from the same experiment. later_data = data data_merged = data.concat_data(later_data) The most important part of each Channel object is the underlying per-pulse dataframe. This code prints that DataFrame , then the facts specific to the sensor channel as a whole, first as an object, then a dataframe: print(ch.df) print(ch.header) print(ch.header.df) # Here are the pulse dataframe's column names, the contents of the subframecount column, and the min/mean/max of the timestamp columns print(ch.df.columns) print(ch.df[\"subframecount\"]) ts = ch.df[\"timestamp\"] print(ts.min(), ts.mean(), ts.max()) assert len(ts) == 100_000 The above commands produce the output: shape: (100_000, 4) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 timestamp \u2506 pulse \u2506 subframecount \u2506 state_label \u2502 \u2502 --- \u2506 --- \u2506 --- \u2506 --- \u2502 \u2502 datetime[\u03bcs] \u2506 array[u16, 500] \u2506 u64 \u2506 cat \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 2024-07-27 13:30:44.451920 \u2506 [6063, 6076, \u2026 6655] \u2506 1519640266112 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.472140 \u2506 [6071, 6065, \u2026 6670] \u2506 1519640589632 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.475788 \u2506 [6120, 6123, \u2026 6215] \u2506 1519640648000 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.497137 \u2506 [6074, 6065, \u2026 6660] \u2506 1519640944448 \u2506 null \u2502 \u2502 2024-07-27 13:30:44.573915 \u2506 [6074, 6072, \u2026 6672] \u2506 1519642206080 \u2506 START \u2502 \u2502 \u2026 \u2506 \u2026 \u2506 \u2026 \u2506 \u2026 \u2502 \u2502 2024-07-27 16:11:55.982246 \u2506 [6065, 6063, \u2026 6278] \u2506 1674384771712 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:55.986849 \u2506 [6077, 6089, \u2026 6662] \u2506 1674384833216 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.054243 \u2506 [6058, 6048, \u2026 6637] \u2506 1674385923008 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.080575 \u2506 [6067, 6064, \u2026 6639] \u2506 1674386344320 \u2506 PAUSE \u2502 \u2502 2024-07-27 16:11:56.220080 \u2506 [6062, 6055, \u2026 6483] \u2506 1674388576448 \u2506 PAUSE \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ChannelHeader(description='20240727_run0002_chan4219.ljh', ch_num=4219, frametime_s=4e-06, n_presamples=250, n_samples=500) shape: (1, 27) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Save File Format \u2506 Software Version \u2506 Software Git Hash \u2506 Data source \u2506 \u2026 \u2506 Pixel Name \u2506 Timebase \u2506 Filename \u2506 continuous \u2502 \u2502 Version \u2506 --- \u2506 --- \u2506 --- \u2506 \u2506 --- \u2506 --- \u2506 --- \u2506 --- \u2502 \u2502 --- \u2506 str \u2506 str \u2506 str \u2506 \u2506 str \u2506 f64 \u2506 str \u2506 bool \u2502 \u2502 str \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 2.2.1 \u2506 DASTARD version 0.3.2 \u2506 508fd92 \u2506 Abaco \u2506 \u2026 \u2506 \u2506 0.000004 \u2506 /Users/fowlerj/qsp/lib/ \u2506 false \u2502 \u2502 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 \u2506 python3\u2026 \u2506 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ['timestamp', 'pulse', 'subframecount', 'state_label'] shape: (100_000,) Series: 'subframecount' [u64] [ 1519640266112 1519640589632 1519640648000 1519640944448 1519642206080 \u2026 1674384771712 1674384833216 1674385923008 1674386344320 1674388576448 ] 2024-07-27 13:30:44.451920 2024-07-27 14:43:53.618149 2024-07-27 16:11:56.220080 Each column (e.g., ch.df[\"timestamp\"] ) is a Polars \"series\" (type: pl.Series ). See the Polars documentation to learn about the enormous range of operations you can perform on dataframes and data series. The library is optimized for parallel operation and offers database-like queries to select and join frames. It offers the option of lazy queries, where a query is built up step-by-step then optimized before being executed. We cannot stress enough how thoroughly worth it it is for a Mass2 user to learn and take advantage of the Polars library to make any query, no matter how simple or complex. Polars is excellent.","title":"What is in the Channel and Channels objects"},{"location":"getting_started/#analyzing-pulse-data","text":"One of the design goals for Mass2 was to enable pulse analysis in each of two distinct situations: Analysis of a data set from a new instrument, or a new configuration (e.g., a changed bath temperature or bias voltage), or an unfamiliar sample (especially a new calibration target). In this case, we don't yet know the appropriate optimal filter to use for each sensor, or how to convert measured pulse sizes into photon energies. We might also not even know the optimal set of analysis choices to make, such as whether arrival-time (\"phase\") correction is required. Analysis of new data from an established instrument in a known configuration with a known energy-calibration curve. In this case, we can apply an analysis \"recipe\" learned from the other case. It is assumed that this earlier recipe was learned from similar data, and that it will therefore produce good, if imperfect, results. The results should be good enough to use during real-time, or \"online\" data analysis to assess data quality, even if they have to be improved offline for a final result. This section concentrate on the first case, analysis of new data for which no established recipe exists. For the second case, see Re-using a recipe and Storing a recipe .","title":"Analyzing pulse data"},{"location":"getting_started/#summarizing-pulses","text":"The Channel.summarize_pulses() method returns a new Channel with a much enhanced dataframe that has nearly a dozen new columns, such as [\"pretrig_mean\", \"pulse_rms\", ...] . To use this method on all the Channel s in a Channels object, we make use of the handy Channels.map() method. It takes a single, callable argument f which must accept a Channel object and returns a modified version of it. We also often follow this up by a function that attaches a \"good expression\" to each channel. Here we'll consider pulses \"good\" when their pretrigger rms and their postpeak derivatives are no more than 8-sigma off the median. These two steps can be performed with map() like this: # mkdocs: render def summarize_and_cut(ch: mass2.Channel) -> mass2.Channel: return ( ch.summarize_pulses() .with_good_expr_pretrig_rms_and_postpeak_deriv(8, 8) ) data = data.map(summarize_and_cut) # Plot a distribution ch = data.ch0 prms = ch.df[\"pulse_rms\"] hist_range = range=np.percentile(prms, [0.5, 99.5]) bin_edges = np.linspace(hist_range[0], hist_range[1], 1000) ch.plot_hist(\"pulse_rms\", bin_edges) plt.xlabel(\"Pulse rms (arbs)\") plt.title(f\"The middle 99% of pulse rms values for channel {ch.header.ch_num}\") You may either re-use the same variable name, e.g., data=data.do_something() here, or use a new variable name such as data2 . Most operations only add to the fields in a data frame, without modifying or removing any existing fields, so preserving the old object is not usually useful. It can be useful to build up an analysis in stages, however, checkpointing with new variables names as you go. Many of the example notebooks use data , data2 , data3 as checkpoints.","title":"Summarizing pulses"},{"location":"getting_started/#computing-and-applying-optimal-filters","text":"To compute an optimal filter for each channel, one must analyze the noise (to learn its autocorrelation function) and create a model of pulses (typically, the model is an average pulse, possibly computed over a selected subset of the records). The simplest approach is to use the Channel.filter5lag method. It analyzes noise a standard way, and it computes an average of all pulses designated \"good\" in the previous step. If you wanted to restrict those pulses further, an optional use_expr argument lets you pass in a Polars expression to reject pulses outside a certain range of times, or experiment states, or to select based on a range of pulse_rms values. The good_expr is remembered through each step of the analsys, while the use_expr is used only for a single step of the analysis. We'll skip the \"use\" here and instead take the default choice of using all good pulses: # mkdocs: render def do_filter(ch: mass2.Channel) -> mass2.Channel: return ch.filter5lag(f_3db=10000) data = data.map(do_filter) The operation above (or the application of any 5-lag filter) will add new fields (\"5lagx\", \"5lagy\") to the channel data frame. It will also add a new, last \"step\" in each channel's analysis \"recipe\". See more in Saving your results below. Here's how you can look at the last step and make its built-in debug plot. For optimal filter construction, the debug plot is simply a plot of the filter created in the step. # mkdocs: render ch = data.ch0 step = ch.steps[-1] step.dbg_plot(ch.df) The step also contains both the new optimal filter and the FilterMaker used to create it. You can look at the latter to find the noise correlation and spectrum, and the signal model--all the ingredients needed to make optimal filters: # mkdocs: render maker = step.filter_maker plt.clf() plt.subplot(221) plt.plot(maker.noise_autocorr[:100], \".-b\") plt.plot(0, maker.noise_autocorr[0], \"ok\") plt.title(\"Noise autocorrelation\") plt.xlabel(f\"Lags (each lag = {maker.sample_time_sec*1e6:.2f} \u00b5s)\") plt.subplot(222) freq_khz = np.linspace(0, 0.5*1e-3/maker.sample_time_sec, len(maker.noise_psd)) plt.loglog(freq_khz[1:], maker.noise_psd[1:], \"-g\") plt.xlabel(\"Frequency (kHz)\") plt.title(\"Noise power spectral density\") plt.ylabel(\"Noise PSD (arbs$^2$ / Hz)\") plt.subplot(212) t_ms = (np.arange(ch.header.n_samples)-ch.header.n_presamples)*maker.sample_time_sec*1000 plt.plot(t_ms, maker.signal_model, \"r\") plt.title(\"Model pulse\") plt.xlabel(\"Time after trigger (ms)\") plt.ylabel(\"Signal (arbs)\") plt.tight_layout()","title":"Computing and applying optimal filters"},{"location":"getting_started/#corrections-and-energy-calibration","text":"We can apply the drift correction based on pretrigger mean to the filtered values, and also perform a rough calibration. That might look like: # mkdocs: render def dc_and_rough_cal2(ch: mass2.Channel) -> mass2.Channel: import polars as pl use_cal = (pl.col(\"state_label\") == \"CAL2\") use_dc = pl.lit(True) line_names = [\"CKAlpha\", \"NKAlpha\", \"OKAlpha\", \"FeLl\", \"FeLAlpha\", \"FeLBeta\", \"NiLAlpha\", \"NiLBeta\", \"CuLAlpha\", \"CuLBeta\", 980] return ( ch.rough_cal_combinatoric( line_names=line_names, uncalibrated_col=\"pulse_rms\", calibrated_col=\"energy_pulse_rms\", ph_smoothing_fwhm=6, use_expr=use_cal, ) .driftcorrect( indicator_col=\"pretrig_mean\", uncorrected_col=\"5lagy\", use_expr=use_dc) .rough_cal_combinatoric( line_names, uncalibrated_col=\"5lagy_dc\", calibrated_col=\"energy_5lagy_dc\", ph_smoothing_fwhm=6, use_expr=use_cal, ) ) data = data.map(dc_and_rough_cal2) ch = data.ch0 plt.clf() use = (pl.col(\"state_label\") == \"CAL2\") ax = plt.subplot(211) ch.plot_hist(\"energy_pulse_rms\", np.linspace(0, 1000, 1001), axis=ax, use_expr=use) plt.title(\"Rough-calibrated energy (not optimally filtered)\") plt.xlabel(\"Rough energy (eV)\") ax = plt.subplot(212) ch.plot_hist(\"5lagy_dc\", np.linspace(0, 2600, 1001), axis=ax, use_expr=use) plt.title(\"Uncalibrated, optimally filtered pulse heights\") plt.xlabel(\"Pulse heights (arbs)\") plt.tight_layout() Now, an improved calibration can be achieved with actual fits to the known calibration lines. (One subtle aspect of this improved calibration is that there must be an existing calibration from the same field into energy that you want to use for the final. In this case, we must have that second rough_cal_combinatoric above, because only that second one uses the field 5lagy_dc as its input. This allows the multifit calibration routine to know about the energy scale well enough to find the lines to fit. Furthermore, we have to track which step it was whose calibration we are improving upon. Here, that is the previous step, called -1 in python. The first rough calibration wouldn't meet our needs, because it is a calibration from pulse_rms into energy, a totally different mapping.) # mkdocs: render def do_multifit(ch: mass2.Channel) -> mass2.Channel: import mass2 import polars as pl STEP_WITH_ROUGH_CAL = -1 multifit = mass2.MultiFit( default_fit_width=80, default_use_expr=(pl.col(\"state_label\") == \"CAL2\"), default_bin_size=0.3, ) line_names = [\"CKAlpha\", \"NKAlpha\", \"OKAlpha\", \"FeLl\", \"FeLAlpha\", \"NiLAlpha\", \"CuLAlpha\", 980.0] dhigh = {\"CuLAlpha\": 12, \"NiLAlpha\": 12} dlow = {980.0: 20} for line in line_names: multifit = multifit.with_line(line, dlo=dlow.get(line, None), dhi=dhigh.get(line, None)) return ch.multifit_mass_cal(multifit, STEP_WITH_ROUGH_CAL, \"energy_5lagy_best\") data = data.map(do_multifit) ch = data.ch0 plt.clf() ax1 = plt.subplot(211) use_cal = (pl.col(\"state_label\") == \"CAL2\") edges = np.linspace(0, 1000, 1001) ch.plot_hist(\"energy_5lagy_best\", edges, axis=ax1, use_expr=use_cal) plt.title(\"Calibrated energy, state='CAL2'\") plt.xlabel(\"Pulse energy (eV)\") ax2 = plt.subplot(212, sharex=ax1) use_noncal = (pl.col(\"state_label\") == \"SCAN3\") ch.plot_hist(\"energy_5lagy_best\", edges, axis=ax2, use_expr=use_noncal) plt.title(\"Calibrated energy, state='SCAN3'\") plt.xlabel(\"Pulse energy (eV)\") plt.xlim([0, 1000]) plt.tight_layout()","title":"Corrections and energy calibration"},{"location":"getting_started/#saving-results-and-the-recipe-system","text":"Mass2 is designed to make storing results easy, both the the quantities produced in analysis and the steps taken to compute them. This section explains how and explores the saving of steps and of numerical results. Analysis \"steps\" have been mentioned before. The big idea is that each channel is associated with a Recipe containing all the operations and parameters used to create it, starting from the raw pulse records in an Raw Data file. (Raw Data files are never modified by Mass2.) This recipe consists of one or more steps to be applied in a fixed order. Each step takes for input fields that existed in the channel's dataframe before the step, and it creates one or more new fields that exist in the dataframe afterwards. Importantly, a recipe can be stored to disk, and applied in the future, either to the same data set, or to another. Warning: One catch is that each recipe is specific to one sensor, and they are tracked only by channel number. If the set of active sensors changes after the learning phase, any new sensors will lack a recipe. Worse, if a probe of the \u00b5MUX resonator frequencies leads to renumbering of the sensors, then we won't be able to find the right recipe for most (or all) sensors.","title":"Saving results and the Recipe system"},{"location":"getting_started/#caching-computed-data-work-in-progress","text":"Sometimes you run an analysis that you consider \"final\"; you want to keep the results only, and you expect (hope?) never to look at the raw LJH files again. We are still working out a standard system for caching computed data: how to name the files, how to combine them, etc. Here are two approaches that employ Parquet files. One stores a file per channel, and the other stores all data in a single file. Approach A: one file per channel Here we store each channel's dataframe in a separate file. Notice that we want to drop the column representing the raw pulse data, because the output would otherwise be far too large (and redundant). Probably it's fine to drop the subframe count, too, so we do that here. # mkdocs: render # For automated tests, we want the output in a temporary directory import tempfile, os output_dir = tempfile.TemporaryDirectory(prefix=\"mass2_getting_started\") print(f\"Test output lives in '{output_dir.name}'\") columns_to_drop = (\"pulse\", \"subframecount\") for ch_num, ch in data.channels.items(): filename = os.path.join(output_dir.name, f\"output_test_chan{ch_num}.parquet\") df = ch.df.drop(columns_to_drop) df.write_parquet(filename) You can pass options to write_parquet to control compression and other aspects of the file. If you prefer to write out only a specified subset of the fields in the dataframes, replace the .drop(columns_to_drop) in the code above with a .select(columns_to_keep) operation. All the methods that act on dataframes are possible: drop to remove specific columns select to keep specific columns limit to write only a maximum number of rows filter to select rows according to a Polars expression alias to rename columns and many others. TODO: we should work on a way to make it easier to load up these files and start analyzing where you left off. One might match up these files with an LJH file, to permit creation of a new Channel object. For now, this sort of data caching is limited to when you just need the data, not the full framework of Mass2. (But keep in mind, the Polars framework is really powerful, and there's a lot you could imagine doing with just the data.) Approach B: one file for all channels This is similar, except that we use some new Polars tricks: polars.DataFrame.with_columns() to append a new column containing this channel's number (we will need it later, because we're about to mix all channels into a new dataframe) polars.concat() to join rows from multiple dataframes columns_to_drop = (\"pulse\", \"subframecount\") all_df = [] for ch_num, ch in data.channels.items(): df = ch.df.drop(columns_to_drop).with_columns(ch_num=pl.lit(ch_num)) all_df.append(df) filename = os.path.join(output_dir.name, \"output_test_allchan.parquet\") pl.concat(all_df).write_parquet(filename)","title":"Caching computed data (work in progress)"},{"location":"getting_started/#storing-analysis-recipes","text":"The system for storing analysis recipes is extremely simple. The following will save the full analysis recipe for each channel in the data object. recipe_filename = os.path.join(output_dir.name, \"full_recipes.pkl\") data.save_recipes(recipe_filename) Beware that this operation will drop all \"debug information\" in certain steps in the recipe (generally, any steps that store a lot of data that's used not for performing the step but only for debugging). The benefit is that the recipes file is smaller, and faster to save and load. The cost is that debugging plots cannot be made after the recipes are reloaded. You probably save a recipe because you've already debugged its steps, so this is usually an acceptable tradeoff. If you don't like it, use the drop_debug=False argument. By default all steps in a recipe are saved, but you might not want or need this. For instance, in the examples given on this page, we computed two \"rough\" calibrations and a final one. The two rough ones are \"dead ends\" if you only care about the final energy. When you are saving a recipe for later use on future data sets in a real-time pipeline, you might want to store a minimal recipe--one that omits any steps not needed to reach a few critical fields. Perhaps you have no need to compute anything that doesn't directly lead to the final energy value. If you omit dead ends from the stored recipes, then not only is the file smaller, but the computation required to run the recipes is reduced. You might prefer instead: required_fields = {\"energy_5lagy_best\", \"pretrig_mean\"} trimmed_recipe_filename = os.path.join(output_dir.name, \"trimmed_recipes.pkl\") data.save_recipes(trimmed_recipe_filename, required_fields=required_fields) This version will save all the steps that produce the two specified required_fields , and all steps that the depend on. The two rough calibration steps, however, will be trimmed away as dead ends. (In this instance, having the pretrigger mean as a required field is superfluous. It's already required in the drift correction step, which is required before the step that takes drift-corrected optimally-filtered pulse height into energy.)","title":"Storing analysis recipes"},{"location":"getting_started/#re-using-a-recipe-on-the-same-or-new-data","text":"Once a mass2.Channels object is created from raw LJH files, an existing recipe can be loaded from a file and executed on the raw data. The result will be that the dataframe for each Channel will now contain the derived fields, including {\"energy_5lagy_best\", \"pretrig_mean\"} , as well as any fields computed along with them, or on the direct path to them. pn_pair = pulsedata.pulse_noise_ljh_pairs[\"bessy_20240727\"] data_replay = mass2.Channels.from_ljh_folder( pulse_folder=pn_pair.pulse_folder, noise_folder=pn_pair.noise_folder ).with_experiment_state_by_path() data_replay = data_replay.load_recipes(trimmed_recipe_filename) # Verify that the expected columns exist in the replay and have identical values df1 = data.ch0.df df2 = data_replay.ch0.df assert df2.equals(df1.select(df2.columns))","title":"Re-using a recipe (on the same or new data)"},{"location":"getting_started/#further-examples","text":"There are more examples of how one can use the Mass2 approach to data analysis in the examples/ directory, in the form of Marimo notebooks . Many examples in the top level of that directory have been designed for use on data found in the https://github.com/ggggggggg/pulsedata repository, so they should work for any user of Mass. Other examples rely on data that isn't public, but they can still be useful training materials for ways to use Mass2. Marimo is similar to Jupyter, but it has some nice advantages. It stores notebooks as pure Python (vs JSON for Jupyter). It uses a reactive execution model with an understanding of the data-flow dependency graph. This fixes a lot of the hidden-state and unexpected behavior that often come with Jupyter's linear-flow (top-to-bottom) execution model. Another really cool Marimo feature (though it's still a bit buggy) is that Matplotlib figure can be rendered and made interactive in the web browser (rather than having to explicitly set x/y limits and re-run a cell)--the result is closer to what you expect from an iPython session. Marimo is easy to start from the command line. The marimo shell command will start a marimo server running, which both serves web pages and runs the underlying Python code. Here are a few examples of how to start marimo: If you add pip scripts to your path (following the confusing documentation from Python or possible the helpful error message pip or uv prints out) you may: mass2-examples OR on windows mass2-examples.exe cd ~/where/I/stored/mass2 # Each of the following commands starts a local Marimo webserver, # then opens a brower tab pointing to it. They also... # ...display a chooser to select whatever example you like: marimo edit examples # ...display one specific example notebook, with (editable) code: marimo edit examples/bessy_20240727.py # ...display a notebook's results, but not the code that created it: marimo run examples/ebit_juy2024_from_off.py We strongly suggest exploring a couple of these notebooks as you launch your adventure with Mass2.","title":"Further examples"},{"location":"hci_lines_from_asd/","text":"Highly Charged Ion (HCI) Lines from NIST ASD Motivation We often find ourselves hard coding line center positions into mass, which is prone to errors and can be tedious when there are many lines of interest to insert. In addition, the line positions would need to be manually updated for any changes in established results. In the case of highly charged ions, such as those produced in an electron beam ion trap (EBIT), there is a vast number of potential lines coming from almost any charge state of almost any element. Luckily, these lines are well documented through the NIST Atomic Spectral Database (ASD). Here, we have parsed a NIST ASD SQL dump and converted it into an easily Python readable pickle file. The hci_lines.py module implements the NIST_ASD class, which loads that pickle file and contains useful functions for working with the ASD data. It also automatically adds in some of the more common HCI lines that we commonly use in our EBIT data analyses. Exploring the methods of class NIST_ASD The class NIST_ASD can be initialized without arguments if the user wants to use the default ASD pickle file. This file is located at mass/calibration/nist_asd.pickle. A custom pickle file can be used by passing in the pickleFilename argument during initialization. The methods of the NIST_ASD class are described below: Usage examples Next, we will demonstrate usage of these methods with the example of Ne, a commonly injected gas at the NIST EBIT. # mkdocs: render import mass2 import mass2.calibration.hci_lines import numpy as numpy import pylab as plt test_asd = mass2.calibration.hci_lines.NIST_ASD() availableElements = test_asd.getAvailableElements() assert 'Ne' in availableElements availableNeCharges = test_asd.getAvailableSpectralCharges(element='Ne') assert 10 in availableNeCharges subsetNe10Levels = test_asd.getAvailableLevels(element='Ne', spectralCharge=10, maxLevels=6, getUncertainty=False) assert '2p 2P* J=1/2' in list(subsetNe10Levels.keys()) exampleNeLevel = test_asd.getSingleLevel(element='Ne', spectralCharge=10, conf='2p', term='2P*', JVal='1/2', getUncertainty=False) print(availableElements[:10]) print(availableNeCharges) for k, v in subsetNe10Levels.items(): subsetNe10Levels[k] = round(v, 1) print(subsetNe10Levels) print(f'{exampleNeLevel:.1f}') [np.str_('Sn'), np.str_('Cu'), np.str_('Na'), np.str_('As'), np.str_('Zn'), np.str_('Ne'), np.str_('Ge'), np.str_('Ga'), np.str_('Rb'), np.str_('Se')] [9, 1, 2, 3, 4, 5, 6, 7, 8, 10] {'1s 2S J=1/2': 0.0, '2p 2P* J=1/2': 1021.5, '2s 2S J=1/2': 1021.5, '2p 2P* J=3/2': 1022.0, '3p 2P* J=1/2': 1210.8, '3s 2S J=1/2': 1210.8} 1021.5 Functions for generating SpectralLine objects from ASD data The module also contains some functions outside of the NIST_ASD class that are useful for integration with MASS. First, the add_hci_line function which, takes arguments that are relevant in HCI work, including as element , spectr_ch , energies , widths , and ratios . The function calls mass2.calibration.fluorescence_lines.addline , generates a line name with the given parameters, and populates the various fields. As an example, let us create a H-like Be line. Here, we assume a lorentzian width of 0.1 eV. # mkdocs: render test_element = 'Be' test_charge = 4 test_conf = '2p' test_term = '2P*' test_JVal = '3/2' test_level = f'{test_conf} {test_term} J={test_JVal}' test_energy = test_asd.getSingleLevel( element=test_element, spectralCharge=test_charge, conf=test_conf, term=test_term, JVal=test_JVal, getUncertainty=False) test_line = mass2.calibration.hci_lines.add_hci_line(element=test_element, spectr_ch=test_charge, line_identifier=test_level, energies=[test_energy], widths=[0.1], ratios=[1.0]) assert test_line.peak_energy == test_energy print(mass2.spectra[f'{test_element}{test_charge} {test_conf} {test_term} J={test_JVal}']) print(f'{test_line.peak_energy:.1f}') SpectralLine: Be4 2p 2P* J=3/2 163.3 The name format for grabbing the line from mass2.spectra is shown above. The transition is uniquely specified by the element, charge, configuration, term, and J value. Below, we show what this line looks like assuming a zero-width Gaussian component. # mkdocs: render test_line.plot() The module contains two other functions which are used to easily generate some lines from levels that are commonly observed at the NIST EBIT. These functions are add_H_like_lines_from_asd and add_He_like_lines_from_asd . As the names imply, these functions add H- and He-like lines to mass using the data in the ASD pickle. These functions require the asd and element arguments and also contain the optional maxLevels argument, which works similarly as the argument in the class methods. The module also automatically adds H- and He-like lines for the most commonly used elements, which includes 'N', 'O', 'Ne', and 'Ar'. Below, we check that common elements are being added as spectralLine objects and then add some of the lower order H- and He-like Ga lines. # mkdocs: render print([mass2.spectra['O7 1s.2p 1P* J=1'], round(mass2.spectra['O7 1s.2p 1P* J=1'].peak_energy,1)]) test_element = 'Ga' HLikeGaLines = mass2.calibration.hci_lines.add_H_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=6) HeLikeGaLines = mass2.calibration.hci_lines.add_He_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=7) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HLikeGaLines]) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HeLikeGaLines]) [SpectralLine: Ne10 2p 2P* J=3/2, np.float64(1022.0)] [SpectralLine: O7 1s.2p 1P* J=1, np.float64(574.0)] [[SpectralLine: Ga31 2p 2P* J=1/2, np.float64(9917.0)], [SpectralLine: Ga31 2s 2S J=1/2, np.float64(9918.0)], [SpectralLine: Ga31 2p 2P* J=3/2, np.float64(9960.3)], [SpectralLine: Ga31 3p 2P* J=1/2, np.float64(11767.7)], [SpectralLine: Ga31 3s 2S J=1/2, np.float64(11768.0)], [SpectralLine: Ga31 3d 2D J=3/2, np.float64(11780.5)]] [[SpectralLine: Ga30 1s.2s 3S J=1, np.float64(9535.6)], [SpectralLine: Ga30 1s.2p 3P* J=0, np.float64(9571.8)], [SpectralLine: Ga30 1s.2p 3P* J=1, np.float64(9574.4)], [SpectralLine: Ga30 1s.2s 1S J=0, np.float64(9574.6)], [SpectralLine: Ga30 1s.2p 3P* J=2, np.float64(9607.4)], [SpectralLine: Ga30 1s.2p 1P* J=1, np.float64(9628.2)], [SpectralLine: Ga30 1s.3s 3S J=1, np.float64(11304.6)]] HCI lines and models docstring info hci_lines.py Uses pickle file containing NIST ASD levels data to generate some commonly used HCI lines in mass. Meant to be a replacement for _highly_charged_ion_lines.py, which hard codes in line parameters. The pickle file can be gzip-compressed, provided the compressed filename ends with \".gz\". February 2020 Paul Szypryt NIST_ASD Class for working with a pickled atomic spectra database Source code in mass2/calibration/hci_lines.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class NIST_ASD : \"\"\"Class for working with a pickled atomic spectra database\"\"\" def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy __init__ ( pickleFilename = None ) Loads ASD pickle file (optionally gzipped) Parameters: pickleFilename ( str | None , default: None ) \u2013 ASD pickle file name, as str, or if none then mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH (default None) Source code in mass2/calibration/hci_lines.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) getAvailableElements () Returns a list of all available elements from the ASD pickle file Source code in mass2/calibration/hci_lines.py 52 53 54 55 def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) getAvailableLevels ( element , spectralCharge , requiredConf = None , requiredTerm = None , requiredJVal = None , maxLevels = None , units = 'eV' , getUncertainty = True ) For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters: element ( str ) \u2013 Elemental atomic symbol, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf ( str | None , default: None ) \u2013 if not None, limits results to those with conf == requiredConf , by default None requiredTerm ( str | None , default: None ) \u2013 if not None, limits results to those with term == requiredTerm , by default None requiredJVal ( str | None , default: None ) \u2013 if not None, limits results to those with a == requiredJVal , by default None maxLevels ( int | None , default: None ) \u2013 the maximum number of levels (sorted by energy) to return, by default None units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 whether to return uncertain values, by default True Returns: dict \u2013 A dictionary of energy level strings to energy levels. Source code in mass2/calibration/hci_lines.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict getAvailableSpectralCharges ( element ) For a given element, returns a list of all available charge states from the ASD pickle file Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' Returns: list [ int ] \u2013 Available charge states Source code in mass2/calibration/hci_lines.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) getSingleLevel ( element , spectralCharge , conf , term , JVal , units = 'eV' , getUncertainty = True ) Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf ( str ) \u2013 nuclear configuration, e.g. '2p' term ( str ) \u2013 nuclear term, e.g. '2P*' JVal ( str ) \u2013 total angular momentum J, e.g. '3/2' units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 includes uncertainties in list of levels, by default True Returns: float \u2013 description Source code in mass2/calibration/hci_lines.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy hci_models.py Some useful methods for initializing GenericLineModel and CompositeMLEModel objects applied to HCI lines. June 2020 Paul Szypryt add_bg_model ( generic_model , vary_slope = False ) Adds a LinearBackgroundModel to a generic lmfit model Parameters: generic_model ( GenericLineModel ) \u2013 object to which to add a linear background model vary_slope ( bool , default: False ) \u2013 allows a varying linear slope rather than just constant value, by default False Returns: GenericLineModel \u2013 The input model, with background componets added Source code in mass2/calibration/hci_models.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def add_bg_model ( generic_model : GenericLineModel , vary_slope : bool = False ) -> GenericLineModel : \"\"\"Adds a LinearBackgroundModel to a generic lmfit model Parameters ---------- generic_model : GenericLineModel object to which to add a linear background model vary_slope : bool, optional allows a varying linear slope rather than just constant value, by default False Returns ------- GenericLineModel The input model, with background componets added \"\"\" # composite_name = generic_model._name # bg_prefix = f\"{composite_name}_\".replace(\" \", \"_\").replace(\"J=\", \"\").replace(\"/\", \"_\").replace(\"*\", \"\").replace(\".\", \"\") raise NotImplementedError ( \"No LinearBackgroundModel still exists in mass2\" ) initialize_HLike_2P_model ( element , conf , has_linear_background = False , has_tails = False , vary_amp_ratio = False ) Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' conf ( str ) \u2013 nuclear configuration as str, e.g. '2p' or '3p' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False vary_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def initialize_HLike_2P_model ( element : str , conf : str , has_linear_background : bool = False , has_tails : bool = False , vary_amp_ratio : bool = False ) -> GenericLineModel : \"\"\"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' conf : str nuclear configuration as str, e.g. '2p' or '3p' has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional include low energy tail in the model, by default False vary_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns ------- GenericLineModel The new composite line \"\"\" # Set up line names and lmfit prefixes charge = int ( xraydb . atomic_number ( element )) line_name_1_2 = f \" { element }{ charge } { conf } 2P* J=1/2\" line_name_3_2 = f \" { element }{ charge } { conf } 2P* J=3/2\" prefix_1_2 = f \" { line_name_1_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) prefix_3_2 = f \" { line_name_3_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) # Initialize individual lines and models line_1_2 = spectra [ line_name_1_2 ] line_3_2 = spectra [ line_name_3_2 ] model_1_2 = line_1_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_1_2 ) model_3_2 = line_3_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_3_2 ) # Initialize composite model and set addition H-like constraints composite_name = f \" { element }{ charge } { conf } \" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = [ model_1_2 , model_3_2 ], has_linear_background = has_linear_background , peak_component_name = line_name_3_2 , ) amp_ratio_param_name = f \" { element }{ charge } _ { conf } _amp_ratio\" composite_model . set_param_hint ( name = amp_ratio_param_name , value = 0.5 , min = 0.0 , vary = vary_amp_ratio ) composite_model . set_param_hint ( f \" { prefix_1_2 } integral\" , expr = f \" { prefix_3_2 } integral * { amp_ratio_param_name } \" ) return composite_model initialize_HeLike_complex_model ( element , has_linear_background = False , has_tails = False , additional_line_names = []) Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P J=1, and 1s.2p 1P J=1 lines. Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the Lorentzian models, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False additional_line_names ( list , default: [] ) \u2013 additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns: GenericLineModel \u2013 A model of the given HCI complex. Source code in mass2/calibration/hci_models.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def initialize_HeLike_complex_model ( element : str , has_linear_background : bool = False , has_tails : bool = False , additional_line_names : list = [] ) -> GenericLineModel : \"\"\"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P* J=1, and 1s.2p 1P* J=1 lines. Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background : bool, optional include a single linear background on top of the Lorentzian models, by default False has_tails : bool, optional include low energy tail in the model, by default False additional_line_names : list, optional additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns ------- GenericLineModel A model of the given HCI complex. \"\"\" # Set up line names charge = int ( xraydb . atomic_number ( element ) - 1 ) line_name_1s2s_3S = f \" { element }{ charge } 1s.2s 3S J=1\" line_name_1s2p_3P = f \" { element }{ charge } 1s.2p 3P* J=1\" line_name_1s2p_1P = f \" { element }{ charge } 1s.2p 1P* J=1\" line_names = np . hstack ([[ line_name_1s2s_3S , line_name_1s2p_3P , line_name_1s2p_1P ], additional_line_names ]) # Set up lines and models based on line_names # individual_lines = [spectra[i_line_name]() for i_line_name in line_names] individual_models = [ initialize_hci_line_model ( i_line_name , has_linear_background = False , has_tails = has_tails ) for i_line_name in line_names ] # Set up composite model composite_name = f \" { element }{ charge } 1s2s_2p Complex\" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = individual_models , has_linear_background = has_linear_background , peak_component_name = line_name_1s2p_1P , ) return composite_model initialize_hci_composite_model ( composite_name , individual_models , has_linear_background = False , peak_component_name = None ) Initializes composite lmfit model from the sum of input models Parameters: composite_name ( str ) \u2013 name given to composite line model individual_models ( list [ GenericLineModel ] ) \u2013 Models to sum into a composite has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of group of lorentzians, by default False peak_component_name ( str | None , default: None ) \u2013 designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def initialize_hci_composite_model ( composite_name : str , individual_models : list [ GenericLineModel ], has_linear_background : bool = False , peak_component_name : str | None = None , ) -> GenericLineModel : \"\"\"Initializes composite lmfit model from the sum of input models Parameters ---------- composite_name : str name given to composite line model individual_models : list[GenericLineModel] Models to sum into a composite has_linear_background : bool, optional include a single linear background on top of group of lorentzians, by default False peak_component_name : str | None, optional designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns ------- GenericLineModel The new composite line \"\"\" composite_model : GenericLineModel = np . sum ( individual_models ) composite_model . name = composite_name if has_linear_background : composite_model = add_bg_model ( composite_model ) # Workaround for energy calibration using composite models, pick 1st GenericLineModel component line_model_components = [ i_comp for i_comp in composite_model . components if isinstance ( i_comp , mass2 . calibration . line_models . GenericLineModel ) ] if peak_component_name is None : peak_component_name = line_model_components [ 0 ] . _name peak_component_index = [ i_comp . _name for i_comp in line_model_components ] . index ( peak_component_name ) peak_component = line_model_components [ peak_component_index ] composite_model . peak_prefix = peak_component . prefix composite_model . peak_energy = peak_component . spect . peak_energy # Set up some constraints relative to peak_component num_line_components = len ( line_model_components ) line_component_prefixes = [ iComp . prefix for iComp in line_model_components ] line_component_energies = [ iComp . spect . peak_energy for iComp in line_model_components ] for i in np . arange ( num_line_components ): if i != peak_component_index : # Single fwhm across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } fwhm\" , expr = f \" { composite_model . peak_prefix } fwhm\" ) # Single dph_de across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } dph_de\" , expr = f \" { composite_model . peak_prefix } dph_de\" ) # Fixed energy separation based on database values separation = line_component_energies [ i ] - composite_model . peak_energy hint = f \"( { separation } * { composite_model . peak_prefix } dph_de) + { composite_model . peak_prefix } peak_ph\" composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } peak_ph\" , expr = hint ) composite_model . shortname = composite_name return composite_model initialize_hci_line_model ( line_name , has_linear_background = False , has_tails = False ) Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters: line_name ( str ) \u2013 name of line to use in mass2.spectra has_linear_background ( bool , default: False ) \u2013 include linear background in the model, by default False has_tails ( bool , default: False ) \u2013 include low-energy tail in the model, by default False Returns: GenericLineModel \u2013 New HCI line. Source code in mass2/calibration/hci_models.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def initialize_hci_line_model ( line_name : str , has_linear_background : bool = False , has_tails : bool = False ) -> GenericLineModel : \"\"\"Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters ---------- line_name : str name of line to use in mass2.spectra has_linear_background : bool, optional include linear background in the model, by default False has_tails : bool, optional include low-energy tail in the model, by default False Returns ------- GenericLineModel New HCI line. \"\"\" line = spectra [ line_name ] prefix = f \" { line_name } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) line_model = line . model ( has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix ) line_model . shortname = line_name return line_model models ( has_linear_background = False , has_tails = False , vary_Hlike_amp_ratio = False , additional_Helike_complex_lines = []) Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters: has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines ( list , default: [] ) \u2013 additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns: _type_ \u2013 Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. Source code in mass2/calibration/hci_models.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def models ( has_linear_background : bool = False , has_tails : bool = False , vary_Hlike_amp_ratio : bool = False , additional_Helike_complex_lines : list = [], ) -> dict : \"\"\"Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters ---------- has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines : list, optional additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns ------- _type_ Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. \"\"\" models_dict = {} # Make some common H-like 2P* models conf_Hlike_2P_dict = {} conf_Hlike_2P_dict [ \"N\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"O\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ne\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ar\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] for i_element in list ( conf_Hlike_2P_dict . keys ()): for i_conf in conf_Hlike_2P_dict [ i_element ]: Hlike_model = initialize_HLike_2P_model ( i_element , i_conf , has_linear_background = has_linear_background , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio , ) models_dict [ Hlike_model . _name ] = Hlike_model # Make some common He-like 1s2s,2p complex and higher order 1p* models # He-like lines Helike_complex_elements = [ \"N\" , \"O\" , \"Ne\" , \"Ar\" ] for i_element in Helike_complex_elements : Helike_model = initialize_HeLike_complex_model ( i_element , has_linear_background = has_linear_background , has_tails = has_tails , additional_line_names = additional_Helike_complex_lines , ) models_dict [ Helike_model . _name ] = Helike_model # 1s.np 1P* lines for n>=3 conf_Helike_1P_dict = {} conf_Helike_1P_dict [ \"N\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"O\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ne\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ar\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] for i_element in list ( conf_Helike_1P_dict . keys ()): i_charge = int ( xraydb . atomic_number ( i_element ) - 1 ) for i_conf in conf_Helike_1P_dict [ i_element ]: Helike_line_name = f \" { i_element }{ i_charge } { i_conf } 1P* J=1\" Helike_model = initialize_hci_line_model ( Helike_line_name , has_linear_background = has_linear_background , has_tails = has_tails ) models_dict [ Helike_model . _name ] = Helike_model # Some more complicated cases # 500 eV region of H-/He-like N N6_1s3p_model = initialize_hci_line_model ( \"N6 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) N7_2p_model = initialize_HLike_2P_model ( \"N\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) N_500eV_model = initialize_hci_composite_model ( \"N 500eV Region\" , [ N6_1s3p_model , N7_2p_model ], has_linear_background = has_linear_background , peak_component_name = \"N7 2p 2P* J=3/2\" , ) models_dict [ N_500eV_model . _name ] = N_500eV_model # 660 eV region of H-/He-like O O8_2p_model = initialize_HLike_2P_model ( \"O\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) O7_1s3p_model = initialize_hci_line_model ( \"O7 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) O_660eV_model = initialize_hci_composite_model ( \"O 660eV Region\" , [ O8_2p_model , O7_1s3p_model ], has_linear_background = has_linear_background , peak_component_name = \"O8 2p 2P* J=3/2\" , ) models_dict [ O_660eV_model . _name ] = O_660eV_model return models_dict","title":"Highly-charge ion lines"},{"location":"hci_lines_from_asd/#highly-charged-ion-hci-lines-from-nist-asd","text":"","title":"Highly Charged Ion (HCI) Lines from NIST ASD"},{"location":"hci_lines_from_asd/#motivation","text":"We often find ourselves hard coding line center positions into mass, which is prone to errors and can be tedious when there are many lines of interest to insert. In addition, the line positions would need to be manually updated for any changes in established results. In the case of highly charged ions, such as those produced in an electron beam ion trap (EBIT), there is a vast number of potential lines coming from almost any charge state of almost any element. Luckily, these lines are well documented through the NIST Atomic Spectral Database (ASD). Here, we have parsed a NIST ASD SQL dump and converted it into an easily Python readable pickle file. The hci_lines.py module implements the NIST_ASD class, which loads that pickle file and contains useful functions for working with the ASD data. It also automatically adds in some of the more common HCI lines that we commonly use in our EBIT data analyses.","title":"Motivation"},{"location":"hci_lines_from_asd/#exploring-the-methods-of-class-nist_asd","text":"The class NIST_ASD can be initialized without arguments if the user wants to use the default ASD pickle file. This file is located at mass/calibration/nist_asd.pickle. A custom pickle file can be used by passing in the pickleFilename argument during initialization. The methods of the NIST_ASD class are described below:","title":"Exploring the methods of class NIST_ASD"},{"location":"hci_lines_from_asd/#usage-examples","text":"Next, we will demonstrate usage of these methods with the example of Ne, a commonly injected gas at the NIST EBIT. # mkdocs: render import mass2 import mass2.calibration.hci_lines import numpy as numpy import pylab as plt test_asd = mass2.calibration.hci_lines.NIST_ASD() availableElements = test_asd.getAvailableElements() assert 'Ne' in availableElements availableNeCharges = test_asd.getAvailableSpectralCharges(element='Ne') assert 10 in availableNeCharges subsetNe10Levels = test_asd.getAvailableLevels(element='Ne', spectralCharge=10, maxLevels=6, getUncertainty=False) assert '2p 2P* J=1/2' in list(subsetNe10Levels.keys()) exampleNeLevel = test_asd.getSingleLevel(element='Ne', spectralCharge=10, conf='2p', term='2P*', JVal='1/2', getUncertainty=False) print(availableElements[:10]) print(availableNeCharges) for k, v in subsetNe10Levels.items(): subsetNe10Levels[k] = round(v, 1) print(subsetNe10Levels) print(f'{exampleNeLevel:.1f}') [np.str_('Sn'), np.str_('Cu'), np.str_('Na'), np.str_('As'), np.str_('Zn'), np.str_('Ne'), np.str_('Ge'), np.str_('Ga'), np.str_('Rb'), np.str_('Se')] [9, 1, 2, 3, 4, 5, 6, 7, 8, 10] {'1s 2S J=1/2': 0.0, '2p 2P* J=1/2': 1021.5, '2s 2S J=1/2': 1021.5, '2p 2P* J=3/2': 1022.0, '3p 2P* J=1/2': 1210.8, '3s 2S J=1/2': 1210.8} 1021.5","title":"Usage examples"},{"location":"hci_lines_from_asd/#functions-for-generating-spectralline-objects-from-asd-data","text":"The module also contains some functions outside of the NIST_ASD class that are useful for integration with MASS. First, the add_hci_line function which, takes arguments that are relevant in HCI work, including as element , spectr_ch , energies , widths , and ratios . The function calls mass2.calibration.fluorescence_lines.addline , generates a line name with the given parameters, and populates the various fields. As an example, let us create a H-like Be line. Here, we assume a lorentzian width of 0.1 eV. # mkdocs: render test_element = 'Be' test_charge = 4 test_conf = '2p' test_term = '2P*' test_JVal = '3/2' test_level = f'{test_conf} {test_term} J={test_JVal}' test_energy = test_asd.getSingleLevel( element=test_element, spectralCharge=test_charge, conf=test_conf, term=test_term, JVal=test_JVal, getUncertainty=False) test_line = mass2.calibration.hci_lines.add_hci_line(element=test_element, spectr_ch=test_charge, line_identifier=test_level, energies=[test_energy], widths=[0.1], ratios=[1.0]) assert test_line.peak_energy == test_energy print(mass2.spectra[f'{test_element}{test_charge} {test_conf} {test_term} J={test_JVal}']) print(f'{test_line.peak_energy:.1f}') SpectralLine: Be4 2p 2P* J=3/2 163.3 The name format for grabbing the line from mass2.spectra is shown above. The transition is uniquely specified by the element, charge, configuration, term, and J value. Below, we show what this line looks like assuming a zero-width Gaussian component. # mkdocs: render test_line.plot() The module contains two other functions which are used to easily generate some lines from levels that are commonly observed at the NIST EBIT. These functions are add_H_like_lines_from_asd and add_He_like_lines_from_asd . As the names imply, these functions add H- and He-like lines to mass using the data in the ASD pickle. These functions require the asd and element arguments and also contain the optional maxLevels argument, which works similarly as the argument in the class methods. The module also automatically adds H- and He-like lines for the most commonly used elements, which includes 'N', 'O', 'Ne', and 'Ar'. Below, we check that common elements are being added as spectralLine objects and then add some of the lower order H- and He-like Ga lines. # mkdocs: render print([mass2.spectra['O7 1s.2p 1P* J=1'], round(mass2.spectra['O7 1s.2p 1P* J=1'].peak_energy,1)]) test_element = 'Ga' HLikeGaLines = mass2.calibration.hci_lines.add_H_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=6) HeLikeGaLines = mass2.calibration.hci_lines.add_He_like_lines_from_asd(asd=test_asd, element=test_element, maxLevels=7) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HLikeGaLines]) print([[iLine, round(iLine.peak_energy, 1)] for iLine in HeLikeGaLines]) [SpectralLine: Ne10 2p 2P* J=3/2, np.float64(1022.0)] [SpectralLine: O7 1s.2p 1P* J=1, np.float64(574.0)] [[SpectralLine: Ga31 2p 2P* J=1/2, np.float64(9917.0)], [SpectralLine: Ga31 2s 2S J=1/2, np.float64(9918.0)], [SpectralLine: Ga31 2p 2P* J=3/2, np.float64(9960.3)], [SpectralLine: Ga31 3p 2P* J=1/2, np.float64(11767.7)], [SpectralLine: Ga31 3s 2S J=1/2, np.float64(11768.0)], [SpectralLine: Ga31 3d 2D J=3/2, np.float64(11780.5)]] [[SpectralLine: Ga30 1s.2s 3S J=1, np.float64(9535.6)], [SpectralLine: Ga30 1s.2p 3P* J=0, np.float64(9571.8)], [SpectralLine: Ga30 1s.2p 3P* J=1, np.float64(9574.4)], [SpectralLine: Ga30 1s.2s 1S J=0, np.float64(9574.6)], [SpectralLine: Ga30 1s.2p 3P* J=2, np.float64(9607.4)], [SpectralLine: Ga30 1s.2p 1P* J=1, np.float64(9628.2)], [SpectralLine: Ga30 1s.3s 3S J=1, np.float64(11304.6)]]","title":"Functions for generating SpectralLine objects from ASD data"},{"location":"hci_lines_from_asd/#hci-lines-and-models-docstring-info","text":"hci_lines.py Uses pickle file containing NIST ASD levels data to generate some commonly used HCI lines in mass. Meant to be a replacement for _highly_charged_ion_lines.py, which hard codes in line parameters. The pickle file can be gzip-compressed, provided the compressed filename ends with \".gz\". February 2020 Paul Szypryt","title":"HCI lines and models docstring info"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD","text":"Class for working with a pickled atomic spectra database Source code in mass2/calibration/hci_lines.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class NIST_ASD : \"\"\"Class for working with a pickled atomic spectra database\"\"\" def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ()) def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ()) def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy","title":"NIST_ASD"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.__init__","text":"Loads ASD pickle file (optionally gzipped) Parameters: pickleFilename ( str | None , default: None ) \u2013 ASD pickle file name, as str, or if none then mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH (default None) Source code in mass2/calibration/hci_lines.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , pickleFilename : str | None = None ): \"\"\"Loads ASD pickle file (optionally gzipped) Parameters ---------- pickleFilename : str | None, optional ASD pickle file name, as str, or if none then `mass2.calibration.hci_lines.DEFAULT_PICKLE_PATH` (default None) \"\"\" if pickleFilename is None : pickleFilename = os . path . join ( os . path . split ( __file__ )[ 0 ], str ( DEFAULT_PICKLE_PATH )) if pickleFilename . endswith ( \".gz\" ): with gzip . GzipFile ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle ) else : with open ( pickleFilename , \"rb\" ) as handle : self . NIST_ASD_Dict = pickle . load ( handle )","title":"__init__"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getAvailableElements","text":"Returns a list of all available elements from the ASD pickle file Source code in mass2/calibration/hci_lines.py 52 53 54 55 def getAvailableElements ( self ) -> list [ str ]: \"\"\"Returns a list of all available elements from the ASD pickle file\"\"\" return list ( self . NIST_ASD_Dict . keys ())","title":"getAvailableElements"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getAvailableLevels","text":"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters: element ( str ) \u2013 Elemental atomic symbol, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf ( str | None , default: None ) \u2013 if not None, limits results to those with conf == requiredConf , by default None requiredTerm ( str | None , default: None ) \u2013 if not None, limits results to those with term == requiredTerm , by default None requiredJVal ( str | None , default: None ) \u2013 if not None, limits results to those with a == requiredJVal , by default None maxLevels ( int | None , default: None ) \u2013 the maximum number of levels (sorted by energy) to return, by default None units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 whether to return uncertain values, by default True Returns: dict \u2013 A dictionary of energy level strings to energy levels. Source code in mass2/calibration/hci_lines.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def getAvailableLevels ( self , element : str , spectralCharge : int , requiredConf : str | None = None , requiredTerm : str | None = None , requiredJVal : str | None = None , maxLevels : int | None = None , units : str = \"eV\" , getUncertainty : bool = True , ) -> dict : \"\"\"For a given element and spectral charge state, return a dict of all known levels from the ASD pickle file Parameters ---------- element : str Elemental atomic symbol, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne requiredConf : str | None, optional if not None, limits results to those with `conf == requiredConf`, by default None requiredTerm : str | None, optional if not None, limits results to those with `term == requiredTerm`, by default None requiredJVal : str | None, optional if not None, limits results to those with `a == requiredJVal`, by default None maxLevels : int | None, optional the maximum number of levels (sorted by energy) to return, by default None units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional whether to return uncertain values, by default True Returns ------- dict A dictionary of energy level strings to energy levels. \"\"\" if units not in { \"eV\" , \"cm-1\" }: raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) spectralCharge = int ( spectralCharge ) levelsDict : dict = {} numLevels = 0 for iLevel in list ( self . NIST_ASD_Dict [ element ][ spectralCharge ] . keys ()): try : # Check to see if we reached maximum number of levels to return if maxLevels is not None : if numLevels == maxLevels : return levelsDict # If required, check to see if level matches search conf, term, JVal includeTerm = False includeJVal = False conf , term , j_str = iLevel . split () JVal = j_str . split ( \"=\" )[ 1 ] includeConf = ( requiredConf is None ) or conf == requiredConf includeTerm = ( requiredTerm is None ) or term == requiredTerm includeJVal = ( requiredJVal is None ) or JVal == requiredJVal # Include levels that match, in either cm-1 or eV if includeConf and includeTerm and includeJVal : numLevels += 1 if units == \"cm-1\" : if getUncertainty : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] else : levelsDict [ iLevel ] = self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] elif units == \"eV\" : if getUncertainty : levelsDict [ iLevel ] = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ] ] else : levelsDict [ iLevel ] = INVCM_TO_EV * self . NIST_ASD_Dict [ element ][ spectralCharge ][ iLevel ][ 0 ] except ValueError : f \"Warning: cannot parse level: { iLevel } \" return levelsDict","title":"getAvailableLevels"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getAvailableSpectralCharges","text":"For a given element, returns a list of all available charge states from the ASD pickle file Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' Returns: list [ int ] \u2013 Available charge states Source code in mass2/calibration/hci_lines.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def getAvailableSpectralCharges ( self , element : str ) -> list [ int ]: \"\"\"For a given element, returns a list of all available charge states from the ASD pickle file Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' Returns ------- list[int] Available charge states \"\"\" return list ( self . NIST_ASD_Dict [ element ] . keys ())","title":"getAvailableSpectralCharges"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_lines.NIST_ASD.getSingleLevel","text":"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters: element ( str ) \u2013 atomic symbol of element, e.g. 'Ne' spectralCharge ( int ) \u2013 spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf ( str ) \u2013 nuclear configuration, e.g. '2p' term ( str ) \u2013 nuclear term, e.g. '2P*' JVal ( str ) \u2013 total angular momentum J, e.g. '3/2' units ( str , default: 'eV' ) \u2013 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty ( bool , default: True ) \u2013 includes uncertainties in list of levels, by default True Returns: float \u2013 description Source code in mass2/calibration/hci_lines.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def getSingleLevel ( self , element : str , spectralCharge : int , conf : str , term : str , JVal : str , units : str = \"eV\" , getUncertainty : bool = True ) -> float : \"\"\"Return the level data for a fully defined element, charge state, conf, term, and JVal. Parameters ---------- element : str atomic symbol of element, e.g. 'Ne' spectralCharge : int spectral charge state, e.g. 1 for neutral atoms, 10 for H-like Ne conf : str nuclear configuration, e.g. '2p' term : str nuclear term, e.g. '2P*' JVal : str total angular momentum J, e.g. '3/2' units : str, optional 'cm-1' or 'eV' for returned line position. If 'eV', converts from database 'cm-1' values, by default \"eV\" getUncertainty : bool, optional includes uncertainties in list of levels, by default True Returns ------- float _description_ \"\"\" levelString = f \" { conf } { term } J= { JVal } \" if units == \"cm-1\" : if getUncertainty : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] elif units == \"eV\" : if getUncertainty : levelEnergy = [ iValue * INVCM_TO_EV for iValue in self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ]] else : levelEnergy = self . NIST_ASD_Dict [ element ][ spectralCharge ][ levelString ][ 0 ] * INVCM_TO_EV else : raise ValueError ( \"Unit type not supported, please use eV or cm-1\" ) return levelEnergy hci_models.py Some useful methods for initializing GenericLineModel and CompositeMLEModel objects applied to HCI lines. June 2020 Paul Szypryt","title":"getSingleLevel"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.add_bg_model","text":"Adds a LinearBackgroundModel to a generic lmfit model Parameters: generic_model ( GenericLineModel ) \u2013 object to which to add a linear background model vary_slope ( bool , default: False ) \u2013 allows a varying linear slope rather than just constant value, by default False Returns: GenericLineModel \u2013 The input model, with background componets added Source code in mass2/calibration/hci_models.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def add_bg_model ( generic_model : GenericLineModel , vary_slope : bool = False ) -> GenericLineModel : \"\"\"Adds a LinearBackgroundModel to a generic lmfit model Parameters ---------- generic_model : GenericLineModel object to which to add a linear background model vary_slope : bool, optional allows a varying linear slope rather than just constant value, by default False Returns ------- GenericLineModel The input model, with background componets added \"\"\" # composite_name = generic_model._name # bg_prefix = f\"{composite_name}_\".replace(\" \", \"_\").replace(\"J=\", \"\").replace(\"/\", \"_\").replace(\"*\", \"\").replace(\".\", \"\") raise NotImplementedError ( \"No LinearBackgroundModel still exists in mass2\" )","title":"add_bg_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_HLike_2P_model","text":"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' conf ( str ) \u2013 nuclear configuration as str, e.g. '2p' or '3p' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False vary_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def initialize_HLike_2P_model ( element : str , conf : str , has_linear_background : bool = False , has_tails : bool = False , vary_amp_ratio : bool = False ) -> GenericLineModel : \"\"\"Initializes H-like 2P models consisting of J=1/2 and J=3/2 lines Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' conf : str nuclear configuration as str, e.g. '2p' or '3p' has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional include low energy tail in the model, by default False vary_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 states to vary away from 2, by default False Returns ------- GenericLineModel The new composite line \"\"\" # Set up line names and lmfit prefixes charge = int ( xraydb . atomic_number ( element )) line_name_1_2 = f \" { element }{ charge } { conf } 2P* J=1/2\" line_name_3_2 = f \" { element }{ charge } { conf } 2P* J=3/2\" prefix_1_2 = f \" { line_name_1_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) prefix_3_2 = f \" { line_name_3_2 } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) # Initialize individual lines and models line_1_2 = spectra [ line_name_1_2 ] line_3_2 = spectra [ line_name_3_2 ] model_1_2 = line_1_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_1_2 ) model_3_2 = line_3_2 . model ( has_linear_background = False , has_tails = has_tails , prefix = prefix_3_2 ) # Initialize composite model and set addition H-like constraints composite_name = f \" { element }{ charge } { conf } \" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = [ model_1_2 , model_3_2 ], has_linear_background = has_linear_background , peak_component_name = line_name_3_2 , ) amp_ratio_param_name = f \" { element }{ charge } _ { conf } _amp_ratio\" composite_model . set_param_hint ( name = amp_ratio_param_name , value = 0.5 , min = 0.0 , vary = vary_amp_ratio ) composite_model . set_param_hint ( f \" { prefix_1_2 } integral\" , expr = f \" { prefix_3_2 } integral * { amp_ratio_param_name } \" ) return composite_model","title":"initialize_HLike_2P_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_HeLike_complex_model","text":"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P J=1, and 1s.2p 1P J=1 lines. Parameters: element ( str ) \u2013 atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the Lorentzian models, by default False has_tails ( bool , default: False ) \u2013 include low energy tail in the model, by default False additional_line_names ( list , default: [] ) \u2013 additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns: GenericLineModel \u2013 A model of the given HCI complex. Source code in mass2/calibration/hci_models.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def initialize_HeLike_complex_model ( element : str , has_linear_background : bool = False , has_tails : bool = False , additional_line_names : list = [] ) -> GenericLineModel : \"\"\"Initializes 1s2s,2p He-like complexes for a given element. By default, uses only the 1s.2s 3S J=1, 1s.2p 3P* J=1, and 1s.2p 1P* J=1 lines. Parameters ---------- element : str atomic symbol as str, e.g. 'Ne' or 'Ar' has_linear_background : bool, optional include a single linear background on top of the Lorentzian models, by default False has_tails : bool, optional include low energy tail in the model, by default False additional_line_names : list, optional additional line names to include in model, e.g. low level Li/Be-like features, by default [] Returns ------- GenericLineModel A model of the given HCI complex. \"\"\" # Set up line names charge = int ( xraydb . atomic_number ( element ) - 1 ) line_name_1s2s_3S = f \" { element }{ charge } 1s.2s 3S J=1\" line_name_1s2p_3P = f \" { element }{ charge } 1s.2p 3P* J=1\" line_name_1s2p_1P = f \" { element }{ charge } 1s.2p 1P* J=1\" line_names = np . hstack ([[ line_name_1s2s_3S , line_name_1s2p_3P , line_name_1s2p_1P ], additional_line_names ]) # Set up lines and models based on line_names # individual_lines = [spectra[i_line_name]() for i_line_name in line_names] individual_models = [ initialize_hci_line_model ( i_line_name , has_linear_background = False , has_tails = has_tails ) for i_line_name in line_names ] # Set up composite model composite_name = f \" { element }{ charge } 1s2s_2p Complex\" composite_model = initialize_hci_composite_model ( composite_name = composite_name , individual_models = individual_models , has_linear_background = has_linear_background , peak_component_name = line_name_1s2p_1P , ) return composite_model","title":"initialize_HeLike_complex_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_hci_composite_model","text":"Initializes composite lmfit model from the sum of input models Parameters: composite_name ( str ) \u2013 name given to composite line model individual_models ( list [ GenericLineModel ] ) \u2013 Models to sum into a composite has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of group of lorentzians, by default False peak_component_name ( str | None , default: None ) \u2013 designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns: GenericLineModel \u2013 The new composite line Source code in mass2/calibration/hci_models.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def initialize_hci_composite_model ( composite_name : str , individual_models : list [ GenericLineModel ], has_linear_background : bool = False , peak_component_name : str | None = None , ) -> GenericLineModel : \"\"\"Initializes composite lmfit model from the sum of input models Parameters ---------- composite_name : str name given to composite line model individual_models : list[GenericLineModel] Models to sum into a composite has_linear_background : bool, optional include a single linear background on top of group of lorentzians, by default False peak_component_name : str | None, optional designate a component to be a peak for energy, all expressions are referenced to this component, by default None Returns ------- GenericLineModel The new composite line \"\"\" composite_model : GenericLineModel = np . sum ( individual_models ) composite_model . name = composite_name if has_linear_background : composite_model = add_bg_model ( composite_model ) # Workaround for energy calibration using composite models, pick 1st GenericLineModel component line_model_components = [ i_comp for i_comp in composite_model . components if isinstance ( i_comp , mass2 . calibration . line_models . GenericLineModel ) ] if peak_component_name is None : peak_component_name = line_model_components [ 0 ] . _name peak_component_index = [ i_comp . _name for i_comp in line_model_components ] . index ( peak_component_name ) peak_component = line_model_components [ peak_component_index ] composite_model . peak_prefix = peak_component . prefix composite_model . peak_energy = peak_component . spect . peak_energy # Set up some constraints relative to peak_component num_line_components = len ( line_model_components ) line_component_prefixes = [ iComp . prefix for iComp in line_model_components ] line_component_energies = [ iComp . spect . peak_energy for iComp in line_model_components ] for i in np . arange ( num_line_components ): if i != peak_component_index : # Single fwhm across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } fwhm\" , expr = f \" { composite_model . peak_prefix } fwhm\" ) # Single dph_de across model composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } dph_de\" , expr = f \" { composite_model . peak_prefix } dph_de\" ) # Fixed energy separation based on database values separation = line_component_energies [ i ] - composite_model . peak_energy hint = f \"( { separation } * { composite_model . peak_prefix } dph_de) + { composite_model . peak_prefix } peak_ph\" composite_model . set_param_hint ( f \" { line_component_prefixes [ i ] } peak_ph\" , expr = hint ) composite_model . shortname = composite_name return composite_model","title":"initialize_hci_composite_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.initialize_hci_line_model","text":"Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters: line_name ( str ) \u2013 name of line to use in mass2.spectra has_linear_background ( bool , default: False ) \u2013 include linear background in the model, by default False has_tails ( bool , default: False ) \u2013 include low-energy tail in the model, by default False Returns: GenericLineModel \u2013 New HCI line. Source code in mass2/calibration/hci_models.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def initialize_hci_line_model ( line_name : str , has_linear_background : bool = False , has_tails : bool = False ) -> GenericLineModel : \"\"\"Initializes a single lorentzian HCI line model. Reformats line_name to create a lmfit valid prefix. Parameters ---------- line_name : str name of line to use in mass2.spectra has_linear_background : bool, optional include linear background in the model, by default False has_tails : bool, optional include low-energy tail in the model, by default False Returns ------- GenericLineModel New HCI line. \"\"\" line = spectra [ line_name ] prefix = f \" { line_name } _\" . replace ( \" \" , \"_\" ) . replace ( \"J=\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"*\" , \"\" ) . replace ( \".\" , \"\" ) line_model = line . model ( has_linear_background = has_linear_background , has_tails = has_tails , prefix = prefix ) line_model . shortname = line_name return line_model","title":"initialize_hci_line_model"},{"location":"hci_lines_from_asd/#mass2.calibration.hci_models.models","text":"Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters: has_linear_background ( bool , default: False ) \u2013 include a single linear background on top of the 2 Lorentzians, by default False has_tails ( bool , default: False ) \u2013 nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio ( bool , default: False ) \u2013 allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines ( list , default: [] ) \u2013 additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns: _type_ \u2013 Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. Source code in mass2/calibration/hci_models.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def models ( has_linear_background : bool = False , has_tails : bool = False , vary_Hlike_amp_ratio : bool = False , additional_Helike_complex_lines : list = [], ) -> dict : \"\"\"Generates some commonly used HCI line models that can be used for energy calibration, etc. Parameters ---------- has_linear_background : bool, optional include a single linear background on top of the 2 Lorentzians, by default False has_tails : bool, optional nclude low-energy tail in the model, by default False vary_Hlike_amp_ratio : bool, optional allow the ratio of the J=3/2 to J=1/2 H-like states to vary, by default False additional_Helike_complex_lines : list, optional additional line names to include inHe-like complex model, e.g. low level Li/Be-like features, by default [] Returns ------- _type_ Dictionary of mass2.calibration.fluorescence_lines.SpectralLine objects, containing commonly used HCI lines. \"\"\" models_dict = {} # Make some common H-like 2P* models conf_Hlike_2P_dict = {} conf_Hlike_2P_dict [ \"N\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"O\" ] = [ \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ne\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] conf_Hlike_2P_dict [ \"Ar\" ] = [ \"2p\" , \"3p\" , \"4p\" , \"5p\" ] for i_element in list ( conf_Hlike_2P_dict . keys ()): for i_conf in conf_Hlike_2P_dict [ i_element ]: Hlike_model = initialize_HLike_2P_model ( i_element , i_conf , has_linear_background = has_linear_background , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio , ) models_dict [ Hlike_model . _name ] = Hlike_model # Make some common He-like 1s2s,2p complex and higher order 1p* models # He-like lines Helike_complex_elements = [ \"N\" , \"O\" , \"Ne\" , \"Ar\" ] for i_element in Helike_complex_elements : Helike_model = initialize_HeLike_complex_model ( i_element , has_linear_background = has_linear_background , has_tails = has_tails , additional_line_names = additional_Helike_complex_lines , ) models_dict [ Helike_model . _name ] = Helike_model # 1s.np 1P* lines for n>=3 conf_Helike_1P_dict = {} conf_Helike_1P_dict [ \"N\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"O\" ] = [ \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ne\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] conf_Helike_1P_dict [ \"Ar\" ] = [ \"1s.3p\" , \"1s.4p\" , \"1s.5p\" ] for i_element in list ( conf_Helike_1P_dict . keys ()): i_charge = int ( xraydb . atomic_number ( i_element ) - 1 ) for i_conf in conf_Helike_1P_dict [ i_element ]: Helike_line_name = f \" { i_element }{ i_charge } { i_conf } 1P* J=1\" Helike_model = initialize_hci_line_model ( Helike_line_name , has_linear_background = has_linear_background , has_tails = has_tails ) models_dict [ Helike_model . _name ] = Helike_model # Some more complicated cases # 500 eV region of H-/He-like N N6_1s3p_model = initialize_hci_line_model ( \"N6 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) N7_2p_model = initialize_HLike_2P_model ( \"N\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) N_500eV_model = initialize_hci_composite_model ( \"N 500eV Region\" , [ N6_1s3p_model , N7_2p_model ], has_linear_background = has_linear_background , peak_component_name = \"N7 2p 2P* J=3/2\" , ) models_dict [ N_500eV_model . _name ] = N_500eV_model # 660 eV region of H-/He-like O O8_2p_model = initialize_HLike_2P_model ( \"O\" , \"2p\" , has_linear_background = False , has_tails = has_tails , vary_amp_ratio = vary_Hlike_amp_ratio ) O7_1s3p_model = initialize_hci_line_model ( \"O7 1s.3p 1P* J=1\" , has_linear_background = False , has_tails = has_tails ) O_660eV_model = initialize_hci_composite_model ( \"O 660eV Region\" , [ O8_2p_model , O7_1s3p_model ], has_linear_background = has_linear_background , peak_component_name = \"O8 2p 2P* J=3/2\" , ) models_dict [ O_660eV_model . _name ] = O_660eV_model return models_dict","title":"models"},{"location":"line_fitting/","text":"Fitting spectral line models to data in MASS Joe Fowler, January 2020. Mass uses use the LMFIT package by wrapping it in the Mass2 class GenericLineModel and its subclasses. LMFit vs Scipy LMFIT has numerous advantages over the basic scipy.optimize module. Quoting from the LMFIT documentation, the user can: forget about the order of variables and refer to Parameters by meaningful names. place bounds on Parameters as attributes, without worrying about preserving the order of arrays for variables and boundaries. fix Parameters, without having to rewrite the objective function. place algebraic constraints on Parameters. The one disadvantage of the core LMFIT package for our purposes is that it minimizes the sum of squares of a vector instead of maximizing the Poisson likelihood. This is easily remedied, however, by replacing the usual computation of residuals with one that computes the square root of the Poisson likelihood contribution from each bin. Voil\u00e1! A maximum likelihood fitter for histograms. Advantages of LMFIT over our earlier, homemade approach to fitting line shapes also include: The interface for setting upper/lower bounds on parameters and for varying or fixing them is much more elegant, memorable, and simple than our homemade version. It ultimately wraps the scipy.optimize package and therefore inherits all of its advantages: a choice of over a dozen optimizers with highly technical documentation, some optimizers that aim for true global (not just local) optimization, and the countless expert-years that have been invested in perfecting it. LMFIT automatically computes numerous statistics of each fit including estimated uncertainties, correlations, and multiple quality-of-fit statistics (information criteria as well as chi-square) and offers user-friendly fit reports. See the MinimizerResult object. It's the work of Matt Newville, an x-ray scientist responsible for the excellent ifeffit and its successor Larch . Above all, its documentation is complete, already written, and maintained by not-us. Usage guide This overview is hardly complete, but we hope it can be a quick-start guide and also hint at how you can convert your own analysis work from the old to the new, preferred fitting methods. The underlying spectral line shape models Objects of the type SpectralLine encode the line shape of a fluorescence line, as a sum of Voigt or Lorentzian distributions. Because they inherit from scipy.stats.rv_continuous , they allow computation of cumulative distribution functions and the simulation of data drawn from the distribution. An example of the creation and usage is: # mkdocs: render import numpy as np import pylab as plt import mass2 import mass2.materials # Known lines are accessed by: line = mass2.spectra[\"MnKAlpha\"] rng = np.random.default_rng(1066) N = 100000 energies = line.rvs(size=N, instrument_gaussian_fwhm=2.2, rng=rng) # draw from the distribution plt.clf() sim, bin_edges, _ = plt.hist(energies, 120, range=[5865, 5925], histtype=\"step\", lw=2); binsize = bin_edges[1] - bin_edges[0] e = bin_edges[:-1] + 0.5*binsize plt.plot(e, line(e, instrument_gaussian_fwhm=2.2)*N*binsize, \"k\", lw=0.5) plt.xlabel(\"Energy (eV)\") plt.title(\"Mn K$\\\\alpha$ random deviates and theory curve\") The SpectralLine object is useful to you if you need to generate simulated data, or to plot a line shape, as shown above. The objects that perform line fitting use the SpectralLine object to hold line shape information. You don't need to create a SpectralLine object for fitting, though; it will be done automatically. How to use the LMFIT-based models for fitting The simplest case of line fitting requires only 3 steps: create a model instance from a SpectralLine , guess its parameters from the data, and perform a fit with this guess. Plotting is not done as part of the fit--you have to do that separately. # mkdocs: render model = line.model() params = model.guess(sim, bin_centers=e, dph_de=1) resultA = model.fit(sim, params, bin_centers=e) # Fit again but with dPH/dE held at 1. # dPH/dE will be a free parameter for the fit by default, largely due to the history of MnKAlpha fits being so critical during development. # This will not work for nearly monochromatic lines, however, as the resolution (fwhm) and scale (dph_de) are exactly degenerate. # In practice, most fits are done with dph_de fixed. params = resultA.params.copy() resultB = model.fit(sim, params, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) # There are two plotting methods. The first is an LMfit built-in; the other (\"mass-style\") puts the # fit parameters on the plot. resultB.plot() resultB.plotm() # The best-fit params are found in resultB.params # and a dictionary of their values is resultB.best_values. # The parameters given as an argument to fit are unchanged. You can print a nicely formatted fit report with the method fit_report() : print(resultB.fit_report()) [[Model]] GenericLineModel(MnKAlpha) [[Fit Statistics]] # fitting method = least_squares # function evals = 15 # data points = 120 # variables = 4 chi-square = 100.565947 reduced chi-square = 0.86694782 Akaike info crit = -13.2013653 Bayesian info crit = -2.05139830 R-squared = 0.99999953 [[Variables]] fwhm: 2.21558094 +/- 0.02687437 (1.21%) (init = 2.217155) peak_ph: 5898.79525 +/- 0.00789761 (0.00%) (init = 5898.794) dph_de: 1 (fixed) integral: 99986.5425 +/- 314.455266 (0.31%) (init = 99985.8) background: 5.0098e-16 +/- 0.80578112 (160842446370819488.00%) (init = 2.791565e-09) bg_slope: 0 (fixed) [[Correlations]] (unreported correlations are < 0.100) C(integral, background) = -0.3147 C(fwhm, peak_ph) = -0.1121 Fitting with exponential tails (to low or high energy) Notice when you report the fit (or check the contents of the params or resultB.params objects), there are no parameters referring to exponential tails of a Bortels response. That's because the default fitter assumes a Gaussian response. If you want tails, that's a constructor argument: # mkdocs: render model = line.model(has_tails=True) params = model.guess(sim, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) resultC = model.fit(sim, params, bin_centers=e) resultC.plot() # print(resultC.fit_report()) By default, the has_tails=True will set up a non-zero low-energy tail and allow it to vary, while the high-energy tail is set to zero amplitude and doesn't vary. Use these numbered examples if you want to fit for a high-energy tail (1), to fix the low-E tail at some non-zero level (2) or to turn off the low-E tail completely (3): # 1. To let the low-E and high-E tail both vary simultaneously params[\"tail_share_hi\"].set(.1, vary=True) params[\"tail_tau_hi\"].set(30, vary=True) # 2. To fix the sum of low-E and high-E tail at a 10% level, with low-E tau=30 eV, but # the share of the low vs high tail can vary params[\"tail_frac\"].set(.1, vary=False) params[\"tail_tau\"].set(30, vary=False) # 3. To turn off low-E tail params[\"tail_frac\"].set(.1, vary=True) params[\"tail_share_hi\"].set(1, vary=False) params[\"tail_tau\"].set(vary=False) Fitting with a quantum efficiency model If you want to multiply the line models by a model of the quantum efficiency, you can do that. You need a qemodel function or callable function object that takes an energy (scalar or vector) and returns the corresponding efficiency. For example, you can use the \"Raven1 2019\" QE model from mass2.materials . The filter-stack models are not terribly fast to run, so it's best to compute once, spline the results, and pass that spline as the qemodel to line.model(qemodel=qemodel) . # mkdocs: render raven_filters = mass2.materials.efficiency_models.filterstack_models[\"RAVEN1 2019\"] eknots = np.linspace(100, 20000, 1991) qevalues = raven_filters(eknots) qemodel = mass2.mathstat.interpolate.CubicSpline(eknots, qevalues) model = line.model(qemodel=qemodel) resultD = model.fit(sim, resultB.params, bin_centers=e) resultD.plotm() # print(resultD.fit_report()) fit_counts = resultD.params[\"integral\"].value localqe = qemodel(mass2.STANDARD_FEATURES[\"MnKAlpha\"]) fit_observed = fit_counts*localqe fit_err = resultD.params[\"integral\"].stderr count_err = fit_err*localqe print(\"Fit finds {:.0f}\u00b1{:.0f} counts before QE, or {:.0f}\u00b1{:.0f} observed. True value {:d}.\".format( round(fit_counts, -1), round(fit_err, -1), round(fit_observed, -1), round(count_err, -1), N)) Fit finds 168810\u00b1530 counts before QE, or 100020\u00b1320 observed. True value 100000. When you fit with a non-trivial QE model, all fit parameters that refer to intensities of signal or background refer to a sensor with an ideal QE=1. These parameters include: integral background bg_slope That is, the fit values must be multiplied by the local QE to give the number of observed signal counts, background counts per bin, or background slope. With or without a QE model, \"integral\" refers to the number of photons that would be seen across all energies (not just in the range being fit). Fitting a simple Gaussian, Lorentzian, or Voigt function # mkdocs: render import dataclasses from mass2.calibration.fluorescence_lines import SpectralLine e_ctr = 1000.0 Nsig = 10000 Nbg = 1000 sigma = 1.0 x_gauss = rng.standard_normal(Nsig)*sigma + e_ctr hwhm = 1.0 x_lorentz = rng.standard_cauchy(Nsig)*hwhm + e_ctr x_voigt = rng.standard_cauchy(Nsig)*hwhm + rng.standard_normal(Nsig)*sigma + e_ctr bg = rng.uniform(e_ctr-5, e_ctr+5, size=Nbg) # Gaussian fit c, b = np.histogram(np.hstack([x_gauss, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, 0, 0) line = dataclasses.replace(line, linetype=\"Gaussian\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultG = model.fit(c, params, bin_centers=bin_ctr) resultG.plotm() # print(resultG.fit_report()) # mkdocs: render # Lorentzian fit c, b = np.histogram(np.hstack([x_lorentz, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, 0) line = dataclasses.replace(line, linetype=\"Lorentzian\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultL = model.fit(c, params, bin_centers=bin_ctr) resultL.plotm() # print(resultL.fit_report()) # mkdocs: render # Voigt fit c, b = np.histogram(np.hstack([x_voigt, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, sigma) line = dataclasses.replace(line, linetype=\"Voigt\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultV = model.fit(c, params, bin_centers=bin_ctr) resultV.plotm() # print(resultV.fit_report()) More details of fitting fluorescence-line models Use p=model.guess(data, bin_centers=e, dph_de=dph_de) to create a heuristic for the starting parameters. Change starting values and toggle the vary attribute on parameters, as needed. For example: p[\"dph_de\"].set(1.0, vary=False) Use result=model.fit(data, p, bin_centers=e) to perform the fit and store the result. The result holds many attributes and methods (see MinimizerResult for full documentation). These include: result.params = the model's best-fit parameters object result.best_values = a dictionary of the best-fit parameter values result.best_fit = the model's y-values at the best-fit parameter values result.chisqr = the chi-squared statistic of the fit (here, -2log(L)) result.covar = the computed covariance result.fit_report() = return a pretty-printed string reporting on the fit result.plot_fit() = make a plot of the data and fit result.plot_residuals() = make a plot of the residuals (fit-data) result.plotm() = make a plot of the data, fit, and fit params with dataset filename in title result.plot() = make a plot of the data, fit, and residuals, generally plotm is preferred The tau values (scale lengths of exponential tails) are in eV units. The parameter \"integral\" refers to the integrated number of counts across all energies (whether inside or beyond the fitted energy range).","title":"Line fitting"},{"location":"line_fitting/#fitting-spectral-line-models-to-data-in-mass","text":"Joe Fowler, January 2020. Mass uses use the LMFIT package by wrapping it in the Mass2 class GenericLineModel and its subclasses.","title":"Fitting spectral line models to data in MASS"},{"location":"line_fitting/#lmfit-vs-scipy","text":"LMFIT has numerous advantages over the basic scipy.optimize module. Quoting from the LMFIT documentation, the user can: forget about the order of variables and refer to Parameters by meaningful names. place bounds on Parameters as attributes, without worrying about preserving the order of arrays for variables and boundaries. fix Parameters, without having to rewrite the objective function. place algebraic constraints on Parameters. The one disadvantage of the core LMFIT package for our purposes is that it minimizes the sum of squares of a vector instead of maximizing the Poisson likelihood. This is easily remedied, however, by replacing the usual computation of residuals with one that computes the square root of the Poisson likelihood contribution from each bin. Voil\u00e1! A maximum likelihood fitter for histograms. Advantages of LMFIT over our earlier, homemade approach to fitting line shapes also include: The interface for setting upper/lower bounds on parameters and for varying or fixing them is much more elegant, memorable, and simple than our homemade version. It ultimately wraps the scipy.optimize package and therefore inherits all of its advantages: a choice of over a dozen optimizers with highly technical documentation, some optimizers that aim for true global (not just local) optimization, and the countless expert-years that have been invested in perfecting it. LMFIT automatically computes numerous statistics of each fit including estimated uncertainties, correlations, and multiple quality-of-fit statistics (information criteria as well as chi-square) and offers user-friendly fit reports. See the MinimizerResult object. It's the work of Matt Newville, an x-ray scientist responsible for the excellent ifeffit and its successor Larch . Above all, its documentation is complete, already written, and maintained by not-us.","title":"LMFit vs Scipy"},{"location":"line_fitting/#usage-guide","text":"This overview is hardly complete, but we hope it can be a quick-start guide and also hint at how you can convert your own analysis work from the old to the new, preferred fitting methods.","title":"Usage guide"},{"location":"line_fitting/#the-underlying-spectral-line-shape-models","text":"Objects of the type SpectralLine encode the line shape of a fluorescence line, as a sum of Voigt or Lorentzian distributions. Because they inherit from scipy.stats.rv_continuous , they allow computation of cumulative distribution functions and the simulation of data drawn from the distribution. An example of the creation and usage is: # mkdocs: render import numpy as np import pylab as plt import mass2 import mass2.materials # Known lines are accessed by: line = mass2.spectra[\"MnKAlpha\"] rng = np.random.default_rng(1066) N = 100000 energies = line.rvs(size=N, instrument_gaussian_fwhm=2.2, rng=rng) # draw from the distribution plt.clf() sim, bin_edges, _ = plt.hist(energies, 120, range=[5865, 5925], histtype=\"step\", lw=2); binsize = bin_edges[1] - bin_edges[0] e = bin_edges[:-1] + 0.5*binsize plt.plot(e, line(e, instrument_gaussian_fwhm=2.2)*N*binsize, \"k\", lw=0.5) plt.xlabel(\"Energy (eV)\") plt.title(\"Mn K$\\\\alpha$ random deviates and theory curve\") The SpectralLine object is useful to you if you need to generate simulated data, or to plot a line shape, as shown above. The objects that perform line fitting use the SpectralLine object to hold line shape information. You don't need to create a SpectralLine object for fitting, though; it will be done automatically.","title":"The underlying spectral line shape models"},{"location":"line_fitting/#how-to-use-the-lmfit-based-models-for-fitting","text":"The simplest case of line fitting requires only 3 steps: create a model instance from a SpectralLine , guess its parameters from the data, and perform a fit with this guess. Plotting is not done as part of the fit--you have to do that separately. # mkdocs: render model = line.model() params = model.guess(sim, bin_centers=e, dph_de=1) resultA = model.fit(sim, params, bin_centers=e) # Fit again but with dPH/dE held at 1. # dPH/dE will be a free parameter for the fit by default, largely due to the history of MnKAlpha fits being so critical during development. # This will not work for nearly monochromatic lines, however, as the resolution (fwhm) and scale (dph_de) are exactly degenerate. # In practice, most fits are done with dph_de fixed. params = resultA.params.copy() resultB = model.fit(sim, params, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) # There are two plotting methods. The first is an LMfit built-in; the other (\"mass-style\") puts the # fit parameters on the plot. resultB.plot() resultB.plotm() # The best-fit params are found in resultB.params # and a dictionary of their values is resultB.best_values. # The parameters given as an argument to fit are unchanged. You can print a nicely formatted fit report with the method fit_report() : print(resultB.fit_report()) [[Model]] GenericLineModel(MnKAlpha) [[Fit Statistics]] # fitting method = least_squares # function evals = 15 # data points = 120 # variables = 4 chi-square = 100.565947 reduced chi-square = 0.86694782 Akaike info crit = -13.2013653 Bayesian info crit = -2.05139830 R-squared = 0.99999953 [[Variables]] fwhm: 2.21558094 +/- 0.02687437 (1.21%) (init = 2.217155) peak_ph: 5898.79525 +/- 0.00789761 (0.00%) (init = 5898.794) dph_de: 1 (fixed) integral: 99986.5425 +/- 314.455266 (0.31%) (init = 99985.8) background: 5.0098e-16 +/- 0.80578112 (160842446370819488.00%) (init = 2.791565e-09) bg_slope: 0 (fixed) [[Correlations]] (unreported correlations are < 0.100) C(integral, background) = -0.3147 C(fwhm, peak_ph) = -0.1121","title":"How to use the LMFIT-based models for fitting"},{"location":"line_fitting/#fitting-with-exponential-tails-to-low-or-high-energy","text":"Notice when you report the fit (or check the contents of the params or resultB.params objects), there are no parameters referring to exponential tails of a Bortels response. That's because the default fitter assumes a Gaussian response. If you want tails, that's a constructor argument: # mkdocs: render model = line.model(has_tails=True) params = model.guess(sim, bin_centers=e, dph_de=1) params[\"dph_de\"].set(1.0, vary=False) resultC = model.fit(sim, params, bin_centers=e) resultC.plot() # print(resultC.fit_report()) By default, the has_tails=True will set up a non-zero low-energy tail and allow it to vary, while the high-energy tail is set to zero amplitude and doesn't vary. Use these numbered examples if you want to fit for a high-energy tail (1), to fix the low-E tail at some non-zero level (2) or to turn off the low-E tail completely (3): # 1. To let the low-E and high-E tail both vary simultaneously params[\"tail_share_hi\"].set(.1, vary=True) params[\"tail_tau_hi\"].set(30, vary=True) # 2. To fix the sum of low-E and high-E tail at a 10% level, with low-E tau=30 eV, but # the share of the low vs high tail can vary params[\"tail_frac\"].set(.1, vary=False) params[\"tail_tau\"].set(30, vary=False) # 3. To turn off low-E tail params[\"tail_frac\"].set(.1, vary=True) params[\"tail_share_hi\"].set(1, vary=False) params[\"tail_tau\"].set(vary=False)","title":"Fitting with exponential tails (to low or high energy)"},{"location":"line_fitting/#fitting-with-a-quantum-efficiency-model","text":"If you want to multiply the line models by a model of the quantum efficiency, you can do that. You need a qemodel function or callable function object that takes an energy (scalar or vector) and returns the corresponding efficiency. For example, you can use the \"Raven1 2019\" QE model from mass2.materials . The filter-stack models are not terribly fast to run, so it's best to compute once, spline the results, and pass that spline as the qemodel to line.model(qemodel=qemodel) . # mkdocs: render raven_filters = mass2.materials.efficiency_models.filterstack_models[\"RAVEN1 2019\"] eknots = np.linspace(100, 20000, 1991) qevalues = raven_filters(eknots) qemodel = mass2.mathstat.interpolate.CubicSpline(eknots, qevalues) model = line.model(qemodel=qemodel) resultD = model.fit(sim, resultB.params, bin_centers=e) resultD.plotm() # print(resultD.fit_report()) fit_counts = resultD.params[\"integral\"].value localqe = qemodel(mass2.STANDARD_FEATURES[\"MnKAlpha\"]) fit_observed = fit_counts*localqe fit_err = resultD.params[\"integral\"].stderr count_err = fit_err*localqe print(\"Fit finds {:.0f}\u00b1{:.0f} counts before QE, or {:.0f}\u00b1{:.0f} observed. True value {:d}.\".format( round(fit_counts, -1), round(fit_err, -1), round(fit_observed, -1), round(count_err, -1), N)) Fit finds 168810\u00b1530 counts before QE, or 100020\u00b1320 observed. True value 100000. When you fit with a non-trivial QE model, all fit parameters that refer to intensities of signal or background refer to a sensor with an ideal QE=1. These parameters include: integral background bg_slope That is, the fit values must be multiplied by the local QE to give the number of observed signal counts, background counts per bin, or background slope. With or without a QE model, \"integral\" refers to the number of photons that would be seen across all energies (not just in the range being fit).","title":"Fitting with a quantum efficiency model"},{"location":"line_fitting/#fitting-a-simple-gaussian-lorentzian-or-voigt-function","text":"# mkdocs: render import dataclasses from mass2.calibration.fluorescence_lines import SpectralLine e_ctr = 1000.0 Nsig = 10000 Nbg = 1000 sigma = 1.0 x_gauss = rng.standard_normal(Nsig)*sigma + e_ctr hwhm = 1.0 x_lorentz = rng.standard_cauchy(Nsig)*hwhm + e_ctr x_voigt = rng.standard_cauchy(Nsig)*hwhm + rng.standard_normal(Nsig)*sigma + e_ctr bg = rng.uniform(e_ctr-5, e_ctr+5, size=Nbg) # Gaussian fit c, b = np.histogram(np.hstack([x_gauss, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, 0, 0) line = dataclasses.replace(line, linetype=\"Gaussian\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultG = model.fit(c, params, bin_centers=bin_ctr) resultG.plotm() # print(resultG.fit_report()) # mkdocs: render # Lorentzian fit c, b = np.histogram(np.hstack([x_lorentz, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, 0) line = dataclasses.replace(line, linetype=\"Lorentzian\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultL = model.fit(c, params, bin_centers=bin_ctr) resultL.plotm() # print(resultL.fit_report()) # mkdocs: render # Voigt fit c, b = np.histogram(np.hstack([x_voigt, bg]), 50, [e_ctr-5, e_ctr+5]) bin_ctr = b[:-1] + (b[1]-b[0]) * 0.5 line = SpectralLine.quick_monochromatic_line(\"testline\", e_ctr, hwhm*2, sigma) line = dataclasses.replace(line, linetype=\"Voigt\") model = line.model() params = model.guess(c, bin_centers=bin_ctr, dph_de=1) params[\"fwhm\"].set(2.3548*sigma) params[\"background\"].set(Nbg/len(c)) resultV = model.fit(c, params, bin_centers=bin_ctr) resultV.plotm() # print(resultV.fit_report())","title":"Fitting a simple Gaussian, Lorentzian, or Voigt function"},{"location":"line_fitting/#more-details-of-fitting-fluorescence-line-models","text":"Use p=model.guess(data, bin_centers=e, dph_de=dph_de) to create a heuristic for the starting parameters. Change starting values and toggle the vary attribute on parameters, as needed. For example: p[\"dph_de\"].set(1.0, vary=False) Use result=model.fit(data, p, bin_centers=e) to perform the fit and store the result. The result holds many attributes and methods (see MinimizerResult for full documentation). These include: result.params = the model's best-fit parameters object result.best_values = a dictionary of the best-fit parameter values result.best_fit = the model's y-values at the best-fit parameter values result.chisqr = the chi-squared statistic of the fit (here, -2log(L)) result.covar = the computed covariance result.fit_report() = return a pretty-printed string reporting on the fit result.plot_fit() = make a plot of the data and fit result.plot_residuals() = make a plot of the residuals (fit-data) result.plotm() = make a plot of the data, fit, and fit params with dataset filename in title result.plot() = make a plot of the data, fit, and residuals, generally plotm is preferred The tau values (scale lengths of exponential tails) are in eV units. The parameter \"integral\" refers to the integrated number of counts across all energies (whether inside or beyond the fitted energy range).","title":"More details of fitting fluorescence-line models"},{"location":"tests/","text":"Unit testing If you want to run the unit tests for mass2 go to your mass2 directory and do pytest . If you want to add tests to mass2 , you do not need to use the old unittest framework. Simpler, more modern tests in the pytest style are allowed (and preferred). Put any new tests in a file somewhere inside tests with a name like test_myfeature.py , it must match the pattern test_*.py to be found by pytest. On each commit to develop, the tests will be run automatically by GitHub Actions. See results of recent tests .","title":"Testing"},{"location":"tests/#unit-testing","text":"If you want to run the unit tests for mass2 go to your mass2 directory and do pytest . If you want to add tests to mass2 , you do not need to use the old unittest framework. Simpler, more modern tests in the pytest style are allowed (and preferred). Put any new tests in a file somewhere inside tests with a name like test_myfeature.py , it must match the pattern test_*.py to be found by pytest. On each commit to develop, the tests will be run automatically by GitHub Actions. See results of recent tests .","title":"Unit testing"},{"location":"xray_efficiency_models/","text":"Detector X-ray Efficiency Models This module requires the xraydb python package. It should be included in the mass2 installation. Otherwise, you should be able to install with pip install xraydb . Motivation For many analyses, it is important to estimate a x-ray spectrum as it would be seen from the source rather than as it would be measured with a set of detectors. This can be important, for example, when trying to determine line intensity ratios of two lines separated in energy space. Here, we attempt to model the effects that would cause the measured spectrum to be different from the true spectrum, such as energy dependent losses in transmission due to IR blocking filters and vacuum windows. Energy-dependent absorber efficiency can also be modeled. FilterStack class and subclass functions with premade efficiency models Here, we import the mass2.efficiency_models module and demonstrate the functionality with some of the premade efficiency models. Generally, these premade models are put in place for TES instruments with well known absorber and filter stack compositions. To demonstrate, we work with the 'EBIT 2018' model, which models the TES spectrometer setup at the NIST EBIT, as it was commissioned in 2018. This model includes a ~1um thick absorber, 3 ~100nm thick Al IR blocking filters, and LEX HT vacuum windows for both the TES and EBIT vacuums. We begin by importing efficiency_models and examining the EBIT efficiency model components. We can see that the model is made of many submodels (aka components) and that all the parameters have uncertainties. The EBIT system was particularly well characterized, so the uncertainties are fairly low. The presence of uncertainties requires some special handling in a few places, these docs will show some examples. # mkdocs: render import mass2 import mass2.materials # you have to explicitly import mass2.materials import numpy as np import pylab as plt from uncertainties import unumpy as unp # useful for working with arrays with uncertainties aka uarray from uncertainties import ufloat EBIT_model = mass2.materials.filterstack_models['EBIT 2018'] print(EBIT_model) <class 'mass2.materials.efficiency_models.FilterStack'>( Electroplated Au Absorber: <class 'mass2.materials.efficiency_models.Filter'>(Au 0.00186+/-0.00006 g/cm^2, fill_fraction=1.000+/-0, absorber=True) 50mK Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (3.04+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 3K Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.93+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 50K Filter: <class 'mass2.materials.efficiency_models.FilterStack'>( Al Film: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.77+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) Ni Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Ni 0.0134+/-0.0018 g/cm^2, fill_fraction=0.170+/-0.010, absorber=False) ) Luxel Window TES: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) Luxel Window EBIT: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) ) Next, we examine the function get_efficiency(xray_energies_eV) , which is an method of FilterStack . This can be called for the entire filter stack or for individual components in the filter stack. As an example, we look at the efficiency of the EBIT 2018 filter stack and the 50K filter component between 2ekV and 10 keV, at 1 keV steps. # mkdocs: render sparse_xray_energies_eV = np.arange(2000, 10000, 1000) stack_efficiency = EBIT_model.get_efficiency(sparse_xray_energies_eV) stack_efficiency_uncertain = EBIT_model.get_efficiency(sparse_xray_energies_eV, uncertain=True) # you have to opt into getting uncertainties out filter50K_efficiency = EBIT_model.components[3].get_efficiency(sparse_xray_energies_eV) print(\"stack efficiencies\") print([f\"{x}\" for x in stack_efficiency_uncertain]) # this is a hack to get uarrays to print with auto chosen number of sig figs print(stack_efficiency) # this is a hack to get uarrays to print with auto chosen number of sig figs print(unp.nominal_values(stack_efficiency)) # you can easily strip uncertainties, see uncertains package docs for more info print(\"filter50K efficiencies\") print(filter50K_efficiency) # if you want to remove the uncertainties, eg for plotting stack efficiencies ['0.335+/-0.008', '0.472+/-0.010', '0.456+/-0.010', '0.383+/-0.010', '0.307+/-0.009', '0.242+/-0.007', '0.191+/-0.006', '0.136+/-0.005'] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] filter50K efficiencies [0.77672107 0.81107679 0.8233861 0.84072724 0.86670307 0.89357999 0.9163624 0.83360284] Here, we use the function plot_efficiency(xray_energies_eV, ax) to plot the efficiencies. ax defaults to None, but it can be used to plot the efficiencies on a user provided axis. Just like get_efficiency , plot_efficiency works with FilterStack and its subclasses. Testing with energy range 100 to 20,000 eV, 1 eV steps. # mkdocs: render xray_energies_eV = np.arange(100,20000,10) EBIT_model.plot_efficiency(xray_energies_eV) # mkdocs: render EBIT_model.components[3].plot_efficiency(xray_energies_eV) Creating your own custom filter stack model using FilterStack objects Now we will explore creating custom FilterStack objects and building up your very own filter stack model. First, we will create a general FilterStack object, representing a stack of filters. We will then populate this object with filters, which take the form of the various FilterStack object subclasses, such as Film , or even other FilterStack objects to create more complicated filters with multiple components. The add argument can be used to add a premade FilterStack object as a component of a different FilterStack object. We will start by adding some simple Film objects to the filter stack. This class requires a the name and material arguments, and the optical depth can be specified by passing in either area_density_g_per_cm2 or thickness_nm (but not both). By default, most FilterStack objects use the bulk density of a material to calculate the optical depth when the thickness_nm is used, but a custom density can be specified with the density_g_per_cm3 argument. In addition, a meshed style filter can be modelled using the fill_fraction argument. Finally, most FilterStack subclasses can use the absorber argument (default False), which will cause the object to return absorption, instead of transmittance, as the efficiency. All numerical arguments can be passed with our without uncertainties. If you don't have at least one number with specified uncertainty in a particular Film, the code will add a \u00b1100% uncertainty on that component. This way, hopefully you will notice that your uncertainty is higher than you expect, and double check the inputs. Read up on the uncertainties package for more info about how it works. # mkdocs: render custom_model = mass2.materials.FilterStack(name='My Filter Stack') custom_model.add_filter(name='My Bi Absorber', material='Bi', thickness_nm=ufloat(4.0e3, .1e3), absorber=True) custom_model.add_filter(name='My Al 50mK Filter', material='Al', thickness_nm=ufloat(100.0, 10)) custom_model.add_filter(name='My Si 3K Filter', material='Si', thickness_nm=ufloat(500.0, 2)) custom_filter = mass2.materials.FilterStack(name='My meshed 50K Filter') custom_filter.add_filter(name='Al Film', material='Al', thickness_nm=ufloat(100.0, 10)) custom_filter.add_filter(name='Ni Mesh', material='Ni', thickness_nm=ufloat(10.0e3, .1e3), fill_fraction=ufloat(0.2, 0.01)) custom_model.add(custom_filter) custom_model.plot_efficiency(xray_energies_eV) There are also some premade filter classes for filters that commonly show up in our instrument filter stacks. At the moment, the FilterStack subclasses listed below are implemented: - AlFilmWithOxide - models a typical IR blocking filter with native oxide layers, which can be important for thin filters. - AlFilmWithPolymer - models a similar IR blocking filter, but with increased structural support from a polymer backing. - LEX_HT - models LEX_HT vacuum windows, which contain a polymer backed Al film and stainless steel mesh. Usage examples and efficiency curves of these classes are shown below. # mkdocs: render premade_filter_stack = mass2.materials.FilterStack(name='A Stack of Premade Filters') f1 = mass2.materials.AlFilmWithOxide(name='My Oxidized Al Filter', Al_thickness_nm=50.0) f2 = mass2.materials.AlFilmWithPolymer(name='My Polymer Backed Al Filter', Al_thickness_nm=100.0, polymer_thickness_nm=200.0) f3 = mass2.materials.LEX_HT(name=\"My LEX HT Filter\") premade_filter_stack.add(f1) premade_filter_stack.add(f2) premade_filter_stack.add(f3) low_xray_energies_eV = np.arange(100,3000,5) premade_filter_stack.plot_efficiency(low_xray_energies_eV)","title":"X-ray efficiency models"},{"location":"xray_efficiency_models/#detector-x-ray-efficiency-models","text":"This module requires the xraydb python package. It should be included in the mass2 installation. Otherwise, you should be able to install with pip install xraydb .","title":"Detector X-ray Efficiency Models"},{"location":"xray_efficiency_models/#motivation","text":"For many analyses, it is important to estimate a x-ray spectrum as it would be seen from the source rather than as it would be measured with a set of detectors. This can be important, for example, when trying to determine line intensity ratios of two lines separated in energy space. Here, we attempt to model the effects that would cause the measured spectrum to be different from the true spectrum, such as energy dependent losses in transmission due to IR blocking filters and vacuum windows. Energy-dependent absorber efficiency can also be modeled.","title":"Motivation"},{"location":"xray_efficiency_models/#filterstack-class-and-subclass-functions-with-premade-efficiency-models","text":"Here, we import the mass2.efficiency_models module and demonstrate the functionality with some of the premade efficiency models. Generally, these premade models are put in place for TES instruments with well known absorber and filter stack compositions. To demonstrate, we work with the 'EBIT 2018' model, which models the TES spectrometer setup at the NIST EBIT, as it was commissioned in 2018. This model includes a ~1um thick absorber, 3 ~100nm thick Al IR blocking filters, and LEX HT vacuum windows for both the TES and EBIT vacuums. We begin by importing efficiency_models and examining the EBIT efficiency model components. We can see that the model is made of many submodels (aka components) and that all the parameters have uncertainties. The EBIT system was particularly well characterized, so the uncertainties are fairly low. The presence of uncertainties requires some special handling in a few places, these docs will show some examples. # mkdocs: render import mass2 import mass2.materials # you have to explicitly import mass2.materials import numpy as np import pylab as plt from uncertainties import unumpy as unp # useful for working with arrays with uncertainties aka uarray from uncertainties import ufloat EBIT_model = mass2.materials.filterstack_models['EBIT 2018'] print(EBIT_model) <class 'mass2.materials.efficiency_models.FilterStack'>( Electroplated Au Absorber: <class 'mass2.materials.efficiency_models.Filter'>(Au 0.00186+/-0.00006 g/cm^2, fill_fraction=1.000+/-0, absorber=True) 50mK Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (3.04+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 3K Filter: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.93+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) 50K Filter: <class 'mass2.materials.efficiency_models.FilterStack'>( Al Film: <class 'mass2.materials.efficiency_models.Filter'>(Al (2.77+/-0.06)e-05 g/cm^2, Al 1.27e-06 g/cm^2, O 1.13e-06 g/cm^2, fill_fraction=1.000+/-0, absorber=False) Ni Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Ni 0.0134+/-0.0018 g/cm^2, fill_fraction=0.170+/-0.010, absorber=False) ) Luxel Window TES: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) Luxel Window EBIT: <class 'mass2.materials.efficiency_models.FilterStack'>( LEX_HT Film: <class 'mass2.materials.efficiency_models.Filter'>(C (6.70+/-0.20)e-05 g/cm^2, H (2.60+/-0.08)e-06 g/cm^2, N (7.20+/-0.22)e-06 g/cm^2, O (1.70+/-0.05)e-05 g/cm^2, Al (1.70+/-0.05)e-05 g/cm^2, fill_fraction=1.000+/-0, absorber=False) LEX_HT Mesh: <class 'mass2.materials.efficiency_models.Filter'>(Fe 0.0564+/-0.0011 g/cm^2, Cr 0.0152+/-0.0003 g/cm^2, Ni 0.00720+/-0.00014 g/cm^2, Mn 0.000800+/-0.000016 g/cm^2, Si 0.000400+/-0.000008 g/cm^2, fill_fraction=0.190+/-0.010, absorber=False) ) ) Next, we examine the function get_efficiency(xray_energies_eV) , which is an method of FilterStack . This can be called for the entire filter stack or for individual components in the filter stack. As an example, we look at the efficiency of the EBIT 2018 filter stack and the 50K filter component between 2ekV and 10 keV, at 1 keV steps. # mkdocs: render sparse_xray_energies_eV = np.arange(2000, 10000, 1000) stack_efficiency = EBIT_model.get_efficiency(sparse_xray_energies_eV) stack_efficiency_uncertain = EBIT_model.get_efficiency(sparse_xray_energies_eV, uncertain=True) # you have to opt into getting uncertainties out filter50K_efficiency = EBIT_model.components[3].get_efficiency(sparse_xray_energies_eV) print(\"stack efficiencies\") print([f\"{x}\" for x in stack_efficiency_uncertain]) # this is a hack to get uarrays to print with auto chosen number of sig figs print(stack_efficiency) # this is a hack to get uarrays to print with auto chosen number of sig figs print(unp.nominal_values(stack_efficiency)) # you can easily strip uncertainties, see uncertains package docs for more info print(\"filter50K efficiencies\") print(filter50K_efficiency) # if you want to remove the uncertainties, eg for plotting stack efficiencies ['0.335+/-0.008', '0.472+/-0.010', '0.456+/-0.010', '0.383+/-0.010', '0.307+/-0.009', '0.242+/-0.007', '0.191+/-0.006', '0.136+/-0.005'] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] [0.33535662 0.4719283 0.45559501 0.38309458 0.30687859 0.24201976 0.19141294 0.13581482] filter50K efficiencies [0.77672107 0.81107679 0.8233861 0.84072724 0.86670307 0.89357999 0.9163624 0.83360284] Here, we use the function plot_efficiency(xray_energies_eV, ax) to plot the efficiencies. ax defaults to None, but it can be used to plot the efficiencies on a user provided axis. Just like get_efficiency , plot_efficiency works with FilterStack and its subclasses. Testing with energy range 100 to 20,000 eV, 1 eV steps. # mkdocs: render xray_energies_eV = np.arange(100,20000,10) EBIT_model.plot_efficiency(xray_energies_eV) # mkdocs: render EBIT_model.components[3].plot_efficiency(xray_energies_eV)","title":"FilterStack class and subclass functions with premade efficiency models"},{"location":"xray_efficiency_models/#creating-your-own-custom-filter-stack-model-using-filterstack-objects","text":"Now we will explore creating custom FilterStack objects and building up your very own filter stack model. First, we will create a general FilterStack object, representing a stack of filters. We will then populate this object with filters, which take the form of the various FilterStack object subclasses, such as Film , or even other FilterStack objects to create more complicated filters with multiple components. The add argument can be used to add a premade FilterStack object as a component of a different FilterStack object. We will start by adding some simple Film objects to the filter stack. This class requires a the name and material arguments, and the optical depth can be specified by passing in either area_density_g_per_cm2 or thickness_nm (but not both). By default, most FilterStack objects use the bulk density of a material to calculate the optical depth when the thickness_nm is used, but a custom density can be specified with the density_g_per_cm3 argument. In addition, a meshed style filter can be modelled using the fill_fraction argument. Finally, most FilterStack subclasses can use the absorber argument (default False), which will cause the object to return absorption, instead of transmittance, as the efficiency. All numerical arguments can be passed with our without uncertainties. If you don't have at least one number with specified uncertainty in a particular Film, the code will add a \u00b1100% uncertainty on that component. This way, hopefully you will notice that your uncertainty is higher than you expect, and double check the inputs. Read up on the uncertainties package for more info about how it works. # mkdocs: render custom_model = mass2.materials.FilterStack(name='My Filter Stack') custom_model.add_filter(name='My Bi Absorber', material='Bi', thickness_nm=ufloat(4.0e3, .1e3), absorber=True) custom_model.add_filter(name='My Al 50mK Filter', material='Al', thickness_nm=ufloat(100.0, 10)) custom_model.add_filter(name='My Si 3K Filter', material='Si', thickness_nm=ufloat(500.0, 2)) custom_filter = mass2.materials.FilterStack(name='My meshed 50K Filter') custom_filter.add_filter(name='Al Film', material='Al', thickness_nm=ufloat(100.0, 10)) custom_filter.add_filter(name='Ni Mesh', material='Ni', thickness_nm=ufloat(10.0e3, .1e3), fill_fraction=ufloat(0.2, 0.01)) custom_model.add(custom_filter) custom_model.plot_efficiency(xray_energies_eV) There are also some premade filter classes for filters that commonly show up in our instrument filter stacks. At the moment, the FilterStack subclasses listed below are implemented: - AlFilmWithOxide - models a typical IR blocking filter with native oxide layers, which can be important for thin filters. - AlFilmWithPolymer - models a similar IR blocking filter, but with increased structural support from a polymer backing. - LEX_HT - models LEX_HT vacuum windows, which contain a polymer backed Al film and stainless steel mesh. Usage examples and efficiency curves of these classes are shown below. # mkdocs: render premade_filter_stack = mass2.materials.FilterStack(name='A Stack of Premade Filters') f1 = mass2.materials.AlFilmWithOxide(name='My Oxidized Al Filter', Al_thickness_nm=50.0) f2 = mass2.materials.AlFilmWithPolymer(name='My Polymer Backed Al Filter', Al_thickness_nm=100.0, polymer_thickness_nm=200.0) f3 = mass2.materials.LEX_HT(name=\"My LEX HT Filter\") premade_filter_stack.add(f1) premade_filter_stack.add(f2) premade_filter_stack.add(f3) low_xray_energies_eV = np.arange(100,3000,5) premade_filter_stack.plot_efficiency(low_xray_energies_eV)","title":"Creating your own custom filter stack model using FilterStack objects"}]}